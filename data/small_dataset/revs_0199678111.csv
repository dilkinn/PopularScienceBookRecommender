,isbn,user_link,ranking,review
0,0199678111,http://goodreads.com/user/show/1713956-manny,5,"Superintelligence was published in 2014, and it's already had time to become a cult classic. So, with apologies for being late getting to the party, here's my two cents.For people who still haven't heard of it, the book is intended as a serious, hard-headed examination of the risks associated with the likely arrival, in the short- to medium-term future, of machines which are significantly smarter than we are. Bostrom is well qualified to do this. He runs the Future of Humanity Institute at Oxford, where he's also a professor at the philosophy department, he's read a great deal of relevant background, and he knows everyone. The cover quotes approving murmurs from the likes of Bill Gates, Elon Musk, Martin Rees and Stuart Russell, co-author of the world's leading AI textbook; people thanked in the acknowledgements include Demis Hassabis, the founder and CEO of Google's Deep Mind. So, why don't we assume for now that Bostrom passes the background check and deserves to be taken seriously. What's he saying?First of all, let's review the reasons why this is a big deal. If machines can get to the point where they're even a little bit smarter than we are, they'll soon be a whole lot smarter than we are. Machines can think much faster than humans (our brains are not well optimised for speed); the differential is at least in the thousands and more likely in the millions. So, having caught us up, they will rapidly overtake us, since they're living thousands or millions of their years for every one of ours. Of course, you can still, if you want, argue that it's a theoretical extrapolation, it won't happen any time soon, etc. But the evidence suggests the opposite. The list of things machines do roughly as well as humans is now very long, and there are quite a few things, things we humans once prided ourselves on being good at, that they do much better. More about that shortly.So if we can produce an artificial human-level intelligence, we'll shortly after have an artificial superintelligence. What does ""shortly after"" mean? Obviously, no one knows, which is the ""fast takeoff/slow takeoff"" dichotomy that keeps turning up in the book. But probably ""slow takeoff"" will be at most a year or two, and fast takeoff could be seconds. Suddenly, we're sharing our planet with a being who's vastly smarter than we are. Bostrom goes to some trouble to help you understand what ""vastly smarter"" means. We're not talking Einstein versus a normal person, or even Einstein versus a mentally subnormal person. We're talking human being versus a mouse. It seems reasonable to assume the superintelligence will quickly learn to do all the things a very smart person can do, including, for starters: formulating and carrying out complex strategic plans; making money in business activities; building machines, including robots and weapons; using language well enough to persuade people to do dumb things; etc etc. It will also be able to do things that we not only can't do, but haven't even thought of doing. And so we come to the first key question: having produced your superintelligence, how do you keep it under control, given that you're a mouse and it's a human being? The book examines this in great detail, coming up with any number of bizarre and ingenious schemes. But the bottom line is that no matter how foolproof your scheme might appear to you, there's absolutely no way you can be sure it'll work against an agent who's so much smarter. There's only one possible strategy which might have a chance of working, and that's to design your superintelligence so that it wants to act in your best interests, and has no possibility of circumventing the rules of its construction to change its behavior, build another superintelligence which changes its behavior, etc. It has to sincerely and honestly want to do what's best for you. Of course, this is Asimov Three Laws territory; and, as Bostrom says, you read Asimov's stories and you see how extremely difficult it is to formulate clear rules which specify what it means to act in people's best interests. So the second key question is: how do you build an agent which of its own accord wants to do ""the right thing"", or, as Socrates put it two and half thousand years ago, is virtuous? As Socrates concludes, for example in Meno and Euthyphro, these issues are really quite difficult to understand. Bostrom uses language which is a bit less poetic and a bit more mathematical, but he comes to pretty much the same conclusions. No one has much idea yet of how to do it. The book reaches this point and gives some closing advice. There are many details, but the bottom line is unsurprising given what's gone before: be very, very careful, because this stuff is incredibly dangerous and we don't know how to address the critical issues.I think some people have problems with Superintelligence due to the fact that Bostrom has a few slightly odd beliefs (he's convinced that we can easily colonize the whole universe, and he thinks simulations are just as real as the things they are simulating). I don't see that these issues really affect the main arguments very much, so don't let them bother you if you don't like them. Also, I'm guessing some other people dislike the style, which is also slightly odd: it's sort of management-speak with a lot of philosophy and AI terminology added, and because it's philosophy there are many weird thought-experiments which often come across as being a bit like science-fiction. Guys, relax. Philosophers have been doing thought-experiments at least since Plato. It's perfectly normal. You just have to read them in the right way. And so, to conclude, let's look at Plato again (remember, all philosophy is no more than footnotes to Plato), and recall the argument from the Theaetetus. Whatever high-falutin' claims it makes, science is only opinions. Good opinions will agree with new facts that turn up later, and bad opinions will not. We've had three and a half years of new facts to look at since Superintelligence was published. How's its scorecard?Well, I am afraid to say that it's looking depressingly good. Early on in the history of AI, as the book reminds us, people said that a machine which could play grandmaster level chess would be most of the way to being a real intelligent agent. So IBM's team built Deep Blue, which beat Garry Kasparov in 1997, and people immediately said chess wasn't a fair test, you could crack it with brute force. Go was the real challenge, since it required understanding. In late 2016 and mid 2017, Deep Mind's AlphaGo won matches against two of the world's three best Go players. That was also discounted as not a fair test: AlphaGo was trained on millions of moves of top Go matches, so it was just spotting patterns. Then late last year, Alpha Zero learned Go, Chess and Shogi on its own, in a couple of days, using the same general learning method and with no human examples to train from. It played all three games not just better than any human, but better than all previous human-derived software. Looking at the published games, any strong chess or Go player can see that it has worked out a vast array of complex strategic and tactical principles. It's no longer a question of ""does it really understand what it's doing"". It obviously understands these very difficult games much better than even the top experts do, after just a few hours of study.Humanity, I think that was our final warning. Come up with more excuses if you like, but it's not smart. And read Superintelligence."
1,0199678111,http://goodreads.com/user/show/6100646-brian-clegg,3,"There has been a spate of outbursts from physicists who should know better, including Stephen Hawking, saying ‘philosophy is dead – all we need now is physics’ or words to that effect. I challenge any of them to read this book and still say that philosophy is pointless.It’s worth pointing out immediately that this isn’t really a popular science book. I’d say the first handful of chapters are for everyone, but after that, the bulk of the book would probably be best for undergraduate philosophy students or AI students, reading more like a textbook than anything else, particularly in its dogged detail – but if you are interested in philosophy and/or artificial intelligence, don’t let that put you off.What Nick Bostrom does is to look at the implications of developing artificial intelligence that goes beyond human abilities in the general sense. (Of course, we already have a sort of AI that goes beyond our abilities in the narrow sense of, say, arithmetic, or playing chess.) In the first couple of chapters he examines how this might be possible – and points out that the timescale is very vague. (Ever since electronic computers have been invented, pundits have been putting the development of effective AI around 20 years in the future, and it’s still the case.) Even so, it seems entirely feasible that we will have a more than human AI – a superintelligent AI – by the end of the century. But the ‘how’ aspect is only a minor part of this book.The real subject here is how we would deal with such a ‘cleverer than us’ AI. What would we ask it to do? How would we motivate it? How would we control it? And, bearing in mind it is more intelligent than us, how would we prevent it taking over the world or subverting the tasks we give it to its own ends? It is truly fascinating concept, explored in great depth here. This is genuine, practical philosophy. The development of super-AIs may well happen – and if we don’t think through the implications and how we would deal with it, we could well be stuffed as a species.I think it’s a shame that Bostrom doesn’t make more use of science fiction to give examples of how people have already thought about these issues – he gives only half a page to Asimov and the three laws of robotics (and how Asimov then spends most of his time showing how they’d go wrong), but that’s about it. Yet there has been a lot of thought and dare I say it, a lot more readability than you typically get in a textbook, put into the issues in science fiction than is being allowed for, and it would have been worthy of a chapter in its own right.I also think a couple of the fundamentals aren’t covered well enough, but pretty much assumed. One is that it would be impossible to contain and restrict such an AI. Although some effort is put into this, I’m not sure there is enough thought put into the basics of ways you can pull the plug manually – if necessary by shutting down the power station that provides the AI with electricity.The other dubious assertion was originally made by I. J. Good, who worked with Alan Turing, and seems to be taken as true without analysis. This is the suggestion that an ultra-intelligent machine would inevitably be able to design a better AI than humans, so once we build one it will rapidly improve on itself, producing an ‘intelligence explosion’. I think the trouble with this argument is that my suspicion is that if you got hold of the million most intelligent people on earth, the chances are that none of them could design an ultra-powerful computer at the component level. Just because something is superintelligent doesn’t mean it can do this specific task well – this is an assumption.However this doesn’t set aside what a magnificent conception the book is. I don’t think it will appeal to many general readers, but I do think it ought to be required reading on all philosophy undergraduate courses, by anyone attempting to build AIs… and by physicists who think there is no point to philosophy."
2,0199678111,http://goodreads.com/user/show/4975321-joseph,2,"Preamble:I confess to Roko's Basilisk that I didn't believe superintelligence was much of a threat, and this book didn't really do anything to challenge that prior. Mea Culpa, Mea Culpa, Mea [local] Maxima Culpa.I. Overall View I'm a software engineer with some basic experience in machine learning, and though the results of machine learning have been becoming more impressive and general, I've never really seen where people are coming from when they see strong superintelligence just around the corner, especially the kind that can recursively improve itself to the point where intelligence vastly increases in the space of a few hours or days. So I came to this book with a simple question: ""Why are so many intelligent people scared of a near-term existential threat from AI, and especially why should I believe that AI takeoff will be incredibly fast?"" Unfortunately, I leave the book with this question largely unanswered. Though in principle I can't think of anything that prevents the formation of some forms of superintelligence, everything I know about software development makes me think that any progress will be slow and gradual, occasionally punctuated with a new trick or two that allows for somewhat faster (but still gradual) increases in some domains. So on the whole, I came away from this book with the uncomfortable but unshakeable notion that most of the people cited don't really have much relevant experience in building large-scale software systems. Though Bostrom used much of the language of computer science correctly, any of his extrapolations from very basic, high-level understandings of these concepts seemed frankly oversimplified and unconvincing.II. General Rant on Math in Philosophy Ever since I was introduced to utilitarianism in college (the naive, Bentham-style utilitarianism at least) I've been somewhat concerned about the practice of trying to add more rigor to philosophical arguments by filling them with mathematical formalism. To continue with the example of utilitarianism, in its most basic sense it asks you to consider any action based on a calculation of how much pleasure will result from your action divided by the amount of pain an action will cause, and to act in such a way that you maximize this ratio. Now it's of course impossible to do this calculation in all but the most trivial cases, even assuming you've somehow managed to define pleasure, pain, and come up with some sort of metric for actually evaluating differences between them. So really the formalism only expresses a very simple relationship between things which are not defined, and based on the process of definition might not be able to be legitimately placed in simple arithmetic or algebraic expressions. I felt much the same way when I was reading Superintelligence. Especially in his chapter on AI takeoff, Bostrom argued that the amount of improvement in an AI system could be modeled as a ratio of applied optimization power over the recalcitrance of the system, or its architectural unwillingness to accept change. Certainly this is true as far as it goes, but ""optimization power"" and ""recalcitrance"" are necessarily at this point dealing with systems that nobody yet knows how to build, or even what they will look like, beyond some hand-wavey high-level descriptions, and so there is no definition one can give that makes any sense unless you've already committed to some ideas of exactly how the system will perform. Bostrom tries to hedge his bets by presenting some alternatives, but he's clearly committed to the idea of a fast takeoff, and the math-like symbols he's using present only a veneer of formalism, drawing some extremely simple relations between concepts which can't be yet defined in any meaningful way. This was the example that really made my objections to unjustified philosophy-math snap into sharp focus, but it's just one of many peppered throughout the book, which gives an attempted high-level look at superintelligent systems, but too many of the black boxes on which his argument rested remained black boxes. Unable to convince myself of the majority of his argument since too many of his steps were glossed over, I came away from this book thinking that there had to be a lot more argumentation somewhere, since I couldn't imagine holding this many unsubstantiated ""axioms"" for something apparently important to him as superintelligence. And it really is a shame that the book needed to be bogged down with so much unnecessary formalism (which had the unpleasant effect of making it feel simultaneously overly verbose and too simplistic), since there were a few good things in here that I came away with. The sections on value-loading and security were especially good. Like most of the book, I found them overly speculative and too generous in assuming what powers superintelligences would possess, but there is some good strategic stuff in here that could lead toward more general forms of machine intelligence, and avoid some of the overfitting problems common in contemporary machine learning. Of course, there's also no plan of implementation for this stuff, but it's a cool idea that hopefully penetrates a little further into modern software development.III. Whereof One Cannot Speak, Thereof One Must Request Funding It's perhaps callous and cynical of me to think of this book as an extended advertisement for the Machine Intelligence Research Institute (MIRI), but the final two chapters in many ways felt like one. Needless to say I'm not filled with a desire to donate on the basis of an argument I found largely unconvincing, but I do have to commend those involved for actually having an attempt at a plan of implementation in place simultaneous with a call to action.IV. Conclusion I remain pretty unconvinced of AI as a relatively near-term existential threat, though I think there's some good stuff in here that could use a wider audience. And being more thoughtful and careful with software systems is always a cause I can get behind. I just wish some more of the gaps got filled in, and I could justifiably shake my suspicion that Bostrom doesn't really know that much about the design and implementation of large-scale software systems.V. Charitable TL;DRNot uninteresting, needs a lot of work before it's convincing.VI. Uncharitable TL;DR"
3,0199678111,http://goodreads.com/user/show/1651956-riku-sayuj,3,"Imagine a Danger (You may say I'm a Dreamer)Bostrom is here to imagine a world for us (and he has batshit crazy imagination, have to give him that). The world he imagines is a post-AI world or at least a very-near-to-AI world or a nascent-AI world. Don’t expect to know how we will get there - only what to do if we get there and how to skew the road to getting there to our advantage. And there are plenty of wild ideas on how things will pan out in that world-in-transition, the ‘routes’ bit - Bostrom discusses the various potential routes, but all of them start at a point where AI is already in play. Given that assumption, the “dangers” bit is automatic since the unknown and powerful has to be assumed to be dangerous. And hence strategies are required. See what he did there?It is all a lot of fun, to be playing this thought experiment game, but it leaves me a bit confused about what to feel about the book as an intellectual piece of speculation. I was on the fence between a two-star rating or a four-star rating for much of the reading. Plenty of exciting and grand-sounding ideas are thrown at me… but, truth be told, there are too many - and hardly any are developed. The author is so caught up in his own capacity for big BIG BIIG ideas that he forgets to develop them into a realistic future or make any the real focus of ‘dangers’ or ‘strategies’. They are just all out there, hanging. As if their nebulosity and sheer abundance should do the job of scaring me enough.In the end I was reduced to surfing the book for ideas worth developing on my own. And what do you know, there were a few. So, not too bad a read and I will go with three. And for future readers, the one big (not-so-new) and central idea of the book is simple enough to be expressed as a fable, here it is:The Unfinished Fable of the SparrowsIt was the nest-building season, but after days of long hard work, the sparrows sat in the evening glow, relaxing and chirping away.“We are all so small and weak. Imagine how easy life would be if we had an owl who could help us build our nests!”“Yes!” said another. “And we could use it to look after our elderly and our young.”“It could give us advice and keep an eye out for the neighborhood cat,” added a third.Then Pastus, the elder-bird, spoke: “Let us send out scouts in all directions and try to find an abandoned owlet somewhere, or maybe an egg. A crow chick might also do, or a baby weasel. This could be the best thing that ever happened to us, at least since the opening of the Pavilion of Unlimited Grain in yonder backyard.”The flock was exhilarated, and sparrows everywhere started chirping at the top of their lungs.Only Scronkfinkle, a one-eyed sparrow with a fretful temperament, was unconvinced of the wisdom of the endeavor. Quoth he: “This will surely be our undoing. Should we not give some thought to the art of owl-domestication and owl-taming first, before we bring such a creature into our midst?”Replied Pastus: “Taming an owl sounds like an exceedingly difficult thing to do. It will be difficult enough to find an owl egg. So let us start there. After we have succeeded in raising an owl, then we can think about taking on this other challenge.”“There is a flaw in that plan!” squeaked Scronkfinkle; but his protests were in vain as the flock had already lifted off to start implementing the directives set out by Pastus.Just two or three sparrows remained behind. Together they began to try to work out how owls might be tamed or domesticated. They soon realized that Pastus had been right: this was an exceedingly difficult challenge, especially in the absence of an actual owl to practice on. Nevertheless they pressed on as best they could, constantly fearing that the flock might return with an owl egg before a solution to the control problem had been found.It is not known how the story ends…"
4,0199678111,http://goodreads.com/user/show/27098959-leonard-gaya,4,"In recent times, prominent figures such as Stephen Hawking, Bill Gates and Elon Musk have expressed serious concerns about the development of strong artificial intelligence technology, arguing that the dawn of super-intelligence might well bring about the end of mankind. Others, like Ray Kurzweil (who, admittedly, has gained some renown in professing silly predictions about the future of the human race), have an opposite view on the matter and maintain that AI is a blessing that will bestow utopia upon humanity. Nick Bostrom painstakingly elaborates on the disquiet views of the former (he might well have influenced them in the first place), without fully dismissing the blissful engrossment of the latter.First, he endeavours to shed some light on the subject and delves into quite a few particulars concerning the future of AI research, such as: the different paths that could lead to super-intelligence (brain emulations or AI proper), the steps and timeframe through which we might get there, the types and number of AI that could result as we continue improving our intelligent machines (he calls them “oracles”, “genies” and “sovereigns”), the different ways in which it could go awry, and so forth.But Bostrom is first and foremost a philosophy professor, and his book is not so much about the engineering or economic aspects that we could foresee as regards strong AI. The main concern is the ethical problems that the development of a general (i.e. cross-domain) super-intelligent machine, far surpassing the abilities of the human brain, might pose to us as humans. The assumption is that the possible existence of such a machine would represent an existential threat to human kind. The main argument is thus to warn us about the dangers (some of Bostrom’s examples are weirdly farcical, and reminded me of Douglas Adams’s The Hitchhiker's Guide to the Galaxy), but also to outline in some detail how this risk could or should be mitigated, restraining the scope or the purpose of a hypothetical super-brain: this is what he calls “the AI control problem”, which is at the core of his reasoning and which, upon reflexion, is a surprisingly difficult one.I should add that, although the book is largely accessible to the layperson, Bostrom’s prose is often dense, speculative, and makes very dry reading: not exactly a walk in the park. He should be praised nonetheless for attempting to apply philosophy and ethical thinking to nontrivial questions.One last remark: Bostrom explores a great many questions in this book but, oddly enough, it seems never to occur to him to think about the possible moral responsibility we humans might have towards an intelligent machine, not just a figment of our imagination but a being that we will someday create and could at least be compared to us. Charity begins at home, I suppose."
5,0199678111,http://goodreads.com/user/show/16077597-matt,4,"As a software developer, I've cared very little for artificial intelligence (AI) in the past. My programs, which I develop professionally, have nothing to do with the subject. They’re dumb as can be and only following strict orders (that is rather simple algorithms). Privately I wrote a few AI test programs (with more or less success) and read a articles in blogs or magazines (with more or less interest). By and large I considered AI as not being relevant for me.In March 2016 AlphaGo was introduced. This was the first Go program capable of defeating a champion in this game. Shortly after that, in December 2017, Alpha Zero entered the stage. Roughly speaking this machine is capable of teaching itself games after being told the rules. Within a day, Alpha Zero developed superhuman level of play for Go, Chess, and Shogi; all by itself (if you can believe the developers). The algorithm used in this machine is very abstract and can probably be used for all games of this kind. The amazing thing for me was how fast the AI development progresses.This book is not all about AI. It’s about “superintelligence” (SI). An SI can be thought of some entity which is far superior to human intelligence in all (or almost all) cognitive abilities. To paraphrase Lincoln: You can outsmart some of the people all of the time and you can outsmart all of the people some of the time, but you can’t outsmart all of the people all of the time; unless you are a superintelligence. The subtitle of the English edition “paths, dangers, strategies” has been chosen wisely. What steps can been taken to build an SI, what are the dangers of introducing an SI, and how can one ensure that these dangers and risks are eliminated or at least scaled-down to an acceptable level?An SI does not necessarily have to exist in a computer. The author is also co-founder of the “World Transhumanist Association”. Therefore, transhumanist ideas are included in the book, albeit in a minor role. An SI can theoretically be build by using genetic selection (of embryos, i.e. “breeding”). Genetic research would probably soon be ready to provide the appropriate technologies. For me, a scary thought; something which touches my personal taboos. Not completely outlandish, but still with a big ethical question mark for me, seems to be “Whole Brain Emulation” (WBE). Here, the brain of a human being, more precisely, the state of the brain at a given time, is analyzed and transferred to a corresponding data structure in the memory of a powerful computer where then the brain/consciousness of the individual continues to exist, possibly within a suitable virtual reality. There are already quite a few films or books that deal with this scenario (for a positive example see the this episode of the Black Mirror series). With WBE you would have an artificial entity with the cognitive performance of a human being. The vastly superior processing speed of the digital versus the biological circuits will let this entity become super intelligent (consider 100,000 copies of a 1000x faster WBE and let it run for six months, and you’ll get 50 millenia worth of thinking!) However, the main focus in the discussion about SI in this book is the further development of AI to become Super-AI (SAI). This is not a technical book though. It contains no computer code whatsoever, and the math (appearing twice in some info-boxes) is only marginal and not at all necessary for understanding. One should not imagine an SI as a particularly intelligent person. It might be more appropriate to equate the ratio of SI to human intelligence with that of human intelligence to the cognitive performance of a mouse. An SI will indeed be very very smart and, unfortunately, also very very unstable. By that I mean that an SI will be busy at any time to changed and improve itself. The SI you speak with today will be a million or more times smarter tomorrow. In this context, the book speaks of “intelligence explosion”. Nobody knows yet, when this will start and how fast it will go. Could be next year, or in ten, fifty, or one hundred years. Or perhaps never (although this is highly unlikely). Various scenarios are discussed in the book. Also it is not clear if there will be only one SI (a so called singleton), or several competing or collaborating SIs (with a singleton seeming to be more likely).I think it’s fair to say that humanity as a whole has the wish to continue to exist; at least the vast majority of people do not consider the extinction of humanity desirable. With that in mind it would make sense to instruct an SI to follow that same goal. Now I forgot to specify the exact state in which you want to exist. In this case the SI might choose to put all humans into coma (less energy consumption). The problem is solved from the SI’s point of view; its goal has been reached. But obviously this is not what we meant. We have to re-program the SI and tweak its goal a bit. Therefore it would be mandatory to always be able to control the SI. It’s possible an SI will not act the way we intended (it will act, however, the way we programmed it). A case of an “unfriendly” SI is actually very likely. The book mentions and describes “perverse instantiation”, “infrastructure profusion” and “mind crime” as possible effects. The so called “control problem” remains unsolved as of now and it appears equivalent to that of a mouse controlling a human being. Without a solution, the introduction of an SI becomes a gamble (with a very high probability a “savage” SI will wipe out humanity).The final goal of an SI should be formulated pro-human if at all possible. At least, the elimination of humankind should not be prioritized at any time. You should give the machine some kind of morality. But how does one do it? How can you formulate moral ideas in a computer language? And what happens if our morals change over time (which has happened before), and the machine still decides on a then-outdated moral ground? In my opinion, there will be insurmountable difficulties at this point. Nevertheless, there are also at least some theoretical approaches explained by Bostrom (who is primarily a philosopher). It’s quite impressive to read these chapters (albeit also a bit dry). In general, the chapters dealing with philosophical questions, and how they are translated to the SI world, were the most engrossing ones for me. The answers to this kind of questions are also subject to some urgency. Advances in technology generally move faster than wisdom (not only in this field), and the sponsors of the projects expect some return on invest. Bostrom speaks of a “philosophy with a deadline”, a fitting, but also disturbing image.Another topic is an SI that is neither malignant nor fitted with false goals (something like this is also possible), but on the contrary actually helps humanity. Quote: The point of superintelligence is not to pander to human preconceptions but to make mincemeat out of our ignorance and folly. Certainly this is a noble goal. However, how will people (and I’m thinking about those who are currently living) react when their follies are disproved? It’s hard to say, but I guess they will not be amused. One should not trust people too much intelligence in this respect (see below for my own “anger”).Except for the sections on improving human intelligence through biological interference and breeding (read eugenics), I found everything in this book fascinating, thought-provoking, and highly disturbing. The book has, in a way, changed my world view rather drastically, which is rare. My “folly” about AI and especially Super-AI has changed fundamentally. In a way, I've gone through 4 of the 5 stages of grief & loss. Before the book, I flatly denied a Super-AI will ever come to fruition. When I read the convincing arguments that not only an Super-AI will be possible, but indeed very likely, my denial changed into anger. In spite of the known problems and the existential risk of such a technology, how can one even think to follow this slippery slope? (this question is also dealt with in the book) My anger was then turned into a depression (not a clinical one) towards the end. Still in this condition, I’m now awaiting acceptance, which in my case will more likely be fatalism.A book that shook me profoundly and that I actually wished I had not read, but that I still recommend highly (I guess I need a superintelligence to make sense of that).

This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License."
6,0199678111,http://goodreads.com/user/show/6603759-misericordia-the-serendipity-aegis,2,"Hypothetical enough to become insanely dumb boring. Superintelligence, hyperintelligence, hypersuperintelligence… Basically, it all amounts to the fact that maybe, sometime, the ultimate thinking machines will do or not so something. Just how new is that idea? IMO, the main point is how do we get them there? Designing intuition? Motivating the AI? Motivational scaffolding? Associative value accretion? While it's all very entertaining, it's nowhere near practical at this point. And the bareboned philosophy of the non-existent AI that's pretty much dumb today? This is one fat DNF."
7,0199678111,http://goodreads.com/user/show/27513524-john-igo,3,"This book... if {}else if {}else if {}else if {}else if {} ...You can get most of the ideas in this book in the WaitButWhy article about AI. This book assumes that an intelligence explosion is possible, and that it is possible for us to make a computer whose intelligence will explode. Then talks about ways to deal with it. A lot of this book seems like pointless naval gazing, but I think some of it is worth reading. "
8,0199678111,http://goodreads.com/user/show/28709846-manuel-ant-o,5,"If you're into stuff like this, you can read the full review.(Count-of-Self) = 0: ""Superintelligence - Paths, Dangers, Strategies"" by Nick Bostrom""Box 8 - Anthropic capture: The AI might assign a substantial probability to its simulation hypothesis, the hypothesis that it is living in a computer simulation.""In ""Superintelligence - Paths, Dangers, Strategies"" by Nick BostromWould you say that the desire to preserve 'itself' comes from the possession of a (self) consciousness? If so, does the acquisition of intelligence according to Bostrom also mean the acquisition of (self) consciousness? The unintended consequence of a super intelligent AI is the development of an intelligence that we can barely see, let alone control, as a consequence of the networking of a large number of autonomous systems acting on inter-connected imperatives. I think of bots trained to trade on the stock market that learn that the best strategy is to follow other bots, who are following other bots. The system can become hyper-sensitive to inputs that have little or nothing to do with supply and demand."
9,0199678111,http://goodreads.com/user/show/4213258-bradley,5,"I'm very pleased to have read this book. It states, concisely, the general field of AI research's BIG ISSUES. The paths to making AIs are only a part of the book and not a particularly important one at this point.More interestingly, it states that we need to be more focused on the dangers of superintelligence. Fair enough! If I was an ant separated from my colony coming into contact with an adult human being, or a sadistic (if curious) child, I might start running for the hills before that magnifying glass focuses the sunlight.And so we move on to strategies, and this is where the book does its most admirable job. All the current thoughts in the field are represented, pretty much, but only in broad outlines. A lot of this has been fully explored in SF literature, too, and not just from the Asimov Laws of Robotics.We've had isolation techniques, oracle techniques, and even straight tool-use techniques crop up in robot and AI literature. Give robots a single-task job and they'll find a way to turn it into a monkey's paw scenario.And this just begs the question, doesn't it?When we get right down to it, this book may be very concise and give us a great overview, but I do believe I'll remain an uberfan of Eliezer Yudkowsky over Nick Bostrom. After having just read Rationality: From AI to Zombies, almost all of these topics are not only brought up, but they're explored in grander fashion and detail.What do you want? A concise summary? Or a gloriously delicious multi-prong attack on the whole subject that admits its own faults the way that HUMANITY should admit its own faults?Give me Eli's humor, his brilliance, and his deeply devoted stand on working out a real solution to the ""Nice"" AI problem. :)I'm not saying Superintelligence isn't good, because it most certainly is, but it is still the map, not the land. :) (Or to be slightly fairer, neither is the land, but one has a little better definition on the topography.) "
10,0199678111,http://goodreads.com/user/show/34621415-jasmin-shah,5,Never let a Seed AI read this book!
11,0199678111,http://goodreads.com/user/show/793473-clif-hostetler,3,"This book  was published in 2014 so is a bit dated, and I’m now writing this review somewhat late for what should be a cutting edge issue. But many people who are interested in this subject continue to respect this book as the definitive examination of the risks associated with machines that are significantly smarter than humans.We have been living for many years with computers—and even phones—that store more information and can retrieve that information faster than any human. These devices don’t seem to pose much threat to us humans, so it’s hard to perceive why there may be cause for concern. The problem is as follows. As artificial intelligence (AI) becomes more proficient in the future it will have the ability to learn (a.k.a.machine learning) and improve itself as it examines and solves problems. It will have the ability to change (i.e. reprogram) itself in order to develop new methods as needed to execute solutions for the tasks at hand. Thus, it will be using techniques and strategies of which the originating human programmer will be unaware. Once machines are creatively strategizing better (i.e. smarter) than humans, the gap between machine and human performance (i.e. intelligence) will grow exponentially. Eventually, the level of thinking by the “super-intelligent” machine will have the relative superiority over that of humans that is equivalent to the superiority of the human brain over that of a beetle crawling on the floor. It is reasonable to conjecture that a machine that smart will have as much respect for humans who think they’re controlling it as humans are likely to have respect for a beetle trying to control them. The concept of superintelligence means that the machine can perform better than humans at all tasks including such things as using human language to be persuasive, raising money, developing strategic plans, designing and making robots, advanced weapons, and advances in science and technology. A super-intelligent machine will solve problems that humans don't know exist.Of course that may be a good thing, but such machines in effect have a mind of their own. They may decide they know best and not want to follow human instructions. Much of this book is spent examining—in too much detail in my opinion—possible ways to control a super-intelligent machine. Then after this long exploration of various strategies the conclusion is in essence that it's not possible.So then the book moves on to the question of how to design the initiating foundation of such a machine to have the innate desire to do good (i.e. be virtuous). Again the author goes into excruciating details examining various ways to do this. The bottom line is we can try, but we don't have the necessary tools to be sure to address the critical issues.In conclusion, our goose is cooked. We can't help ourselves. Superintelligence is the ""tree of the knowledge of good and evil."" We have to take a bite.This link is to an article about facial recognition. It contains the following quote:... the whole ecosystem of artificial intelligence is optimized for a lack of accountability.Shortly after writing my review the Dilbert cartoon featured the subject of AI:Here's a link to a review of ""Game Changer: AlphaZero's Groundbreaking Chess Strategies and the Promise of AI,"" by Matthew Sadler and Natasha Regan. This review describes a chess program that utilizes AI to become almost unbeatable with a style of play not previously seen.https://www.goodreads.com/review/show..."
12,0199678111,http://goodreads.com/user/show/1657830-jim,4,"Superintelligence by Nick Bostrom is a hard book to recommend, but is one that thoroughly covers its subject. Superintelligence is a warning against developing artificial intelligence (AI). However, the writing is dry and systematic, more like Plato than Wired Magazine. There are few real world examples, because it's not a history of AI, but theoretic conjectures. The book explores the possible issues we might face if a superintelligent machine or life form is created. I would have enjoyed the book more if it reported on current state of the art projects in AI. The recent work of DeepMind learning to play classic Atari games offers more realism to the possibilities of AI than anything mentioned in this book. Deep learning projects, the latest development in neural nets, is having some astounding successes. Bostrom doesn't report on them.And I think Bostrom makes one glaring error. I'm no AI expert, but he seems to assume we can program an AI, and control its intelligence. I don't think that's possible. We won't be programming The Three Laws of Robotics into future AI beings. I believe AIs will evolve out of learning systems, and we'll have no control over what emerges. We'll create software and hardware that is capable of adapting and learning. The process of becoming a self-aware superintelligence will be no more understandable to us than why our brains generate consciousness."
13,0199678111,http://goodreads.com/user/show/7092601-robert-schertzer,1,"I switched to the audio version of this book after struggling with the Kindle edition since I needed to read this for a book club. If you are looking for a book on artificial intelligence (AI), avoid this and opt for Jeff Hawkins' book ""On Intelligence"" written by someone who has devoted their life to the field. If it is one on ""AI gone bad"" you seek, try 2001 Space Odyssey. For a fictional approach on AI that helped set the groundwork for AI theory, go for Isaac Asimov. If you want a tedious, relentless and pointless book that fails at achieving what all three previously aforementioned authors have succeeded at - this is the book for you."
14,0199678111,http://goodreads.com/user/show/16212136-peter-pete-mcloughlin,5,"Stephen Hawking and Bill Gates have recently raised the alarm about Artificial Intelligence. If a superhuman artificial intelligence were created it would be the biggest event in human history and it could very well be the last. We are only familiar with human intelligence and it may be a small sample from the possibilities of intelligence to be had. Bostrom makes the case that the most likely path to superintelligence would most likely be a hard takeoff as the AI would quickly rise once it reached Human level intelligence and quickly reorganize itself to a very superior form of intelligent mind. It would quickly gain powers and abilities far beyond humans and it would be more alien and unfathomable than anything we have ever seen. If it has goals that don't match up with the human project so much for the human race.  With great detail, Bostrom lays out where AI could go seriously wrong for us. Disasters in the abstract may make us yawn but Bostrom gives the details of what the catastrophe might look like. The Hellmouth is much scarier when the picture becomes more detailed. I recommend reading Bostrom's book to educate yourself on dangers of ceding the top of the food chain to AI. It is fairly hair-raising.Here is Nick Bostrom talking on this topic at TED. https://www.youtube.com/watch?v=MnT1x... "
15,0199678111,http://goodreads.com/user/show/31999040-shea-levy,0,"Read up through chapter 8. The book started out somewhat promisingly by not taking a stand on whether strong AI was imminent or not, but that was the height of what I read. I'm not sure there was a single section of the book where I didn't have a reaction ranging from ""wait, how do you know that's true?"" to ""that's completely wrong and anyone with a modicum of familiarity with the field you're talking about would know that"", but really it's the overall structure of the argument that led me to give this one up as a waste of time.Essentially, the argument goes like this: Bostrom introduces some idea, explains in vague language what he means by it, traces out how it might be true (or, in a few ""slam-dunk"" sections, *several* ways it might be true), and then moves on. In the next section, he takes all of the ideas introduced in the previous sections as givens and as mostly black boxes, in the sense that the old ideas are brought up to justify new claims without ever invoking any of the particular evidence for or structure of the old idea, it's just an opaque formula. The sense is of someone trying to build a tower, straight up. The fact that this particular tower is really a wobbly pile of blocks, with many of the higher up ones actually resting on the builder's arm and not really on the previous ones at all, is almost irrelevant: this is not how good reasoning works! There is no broad consideration of the available evidence, no demonstration of why the things we've seen imply the specific things Bostrom suggests, no serious engagement with alternative explanations/predictions, no cycling between big-picture overviews and in-detail analyses. There is just a stack of vague plausibilities and vague conceptual frameworks to accommodate them. A compelling presentation is a lot more like clearing away fog to note some rocky formations, then pulling back a bit to see they're all connected, then zooming back in to clear away the connected areas, and so on and so forth until a broad mountain is revealed.This is not to say that the outcome Bostrom fears is impossible. Even though I think many of the specific things he thinks are plausible are actually much less so than he asserts, I do think a kind of very powerful ""unfriendly"" AI is a possibility that should be considered by those in a position to really understand the problem and take action against it if it turns out to be a real one. The problem with Bostrom's presentation is that it doesn't tell us anything useful: We have no reason to suspect that the particular kinds of issues he proposes are the ones that will matter, that the particular characteristics he ascribes to future AI are ones that will be salient, indeed that this problem is likely enough, near enough, and tractable enough to be worth spending significant resources on at all at the moment! Nothing Bostrom is saying compellingly privileges his particular predictions over many many possible others, even if you take as a given that extraordinarily powerful AI is possible and its behavior hard to predict. I continually got the sense (sometimes explicitly echoed by Bostrom himself!) that you could substitute in huge worlds of incompatible particulars for the ones he proposed and still make the same claims. So why should I expect anything particular he proposes to be worthwhile?Edit: After chatting about this a bit with some friends, I should add one caveat to this review. This is praising with bold damnation if ever there were such a thing, but this book has made me more likely to engage with AI as an existential risk by being such a clear example of what had driven me away up until now. Now that I can see the essence of what's wrong with the bad approaches I've seen, I'll be better able to seek out the good ones (and, as I said, I do think the problem is worth serious investigation). So, I guess ultimately Bostrom succeeded at his goal in my case?"
16,0199678111,http://goodreads.com/user/show/14873887-travis,3,I'm not going to criticize the content. I cannot finish this. Imagine eating saltines when you have cotton mouth in the middle of the desert. You might be close to describing how dry the writing is. Could be very interesting read if the writing was done in a more attention grabbing way.
17,0199678111,http://goodreads.com/user/show/7315267-diego-petrucci,5,"There's no way around it: a super-intelligent AI is a threat.We can safely assume that an AI smarter than a human, if developed, would accelerate its own development getting smarter at a rate faster than anything we'd ever seen. In just a few cycles of self-improvement it would spiral out of control. Trying to fight, or control, or hijack it, would be totally useless — for a comparison, try picturing an ant trying to outsmart a human being (a laughable attempt, at best).But why is a super-intelligent AI a threat? Well, it probably wouldn't have human qualities (empathy, a sense of justice, and so on) and would rely on a more emotion-less understanding of the world — understanding emotion doesn't mean you have to feel emotions, you can understand the motives of terrorists without agreeing with them. There would be a chance of developing a super-intelligent AI with an insane set of objectives, like maximizing the production of chairs with no regard to the safety of human beings or the environment, totally subsuming Earth 's materials and the planet itself. Or, equally probable, we could end up with an AI whose main objective is self-preservation, who would later annihilate the human race because of an even minuscule chance of us destroying it.With that said, it's clear that before developing a self-improving AI we need a plan. We need tests to understand and improve its moral priorities, we need security measures, we need to minimize the risk of it destroying the planet. Once the AI is more intelligent than us, it won't take much to get extremely more intelligent, so we need to be prepared. We only got one chance and that's it, either we set it up right or we're done as a species.Superintelligence deals with all these problems systematically analyzing them and providing a few frames of mind to let us solve them (if that's even possible)."
18,0199678111,http://goodreads.com/user/show/23233462-clare-o-beara,4,"We are now building superintelligences. More than one. The author Nick Bostrom looks at what awaits us. He points out that controlling such a creation might not be easy. If unfriendly superintelligence comes about, we won't be able to change or replace it. This is a densely written book, with small print, with 63 pages of notes and bibliography. In the introduction the author tells us twice that it was not easy to write. However he tries to make it accessible, and adds that if you don't understand some techie terms you should still be able to grasp the meaning. He hopes that by pulling together this material he has made it easier for other researchers to get started. So - where are we? I have to state that with lines like: ""Collective superintelligence is less conceptually clear-cut than speed superintelligence. However it is more familiar empirically."" This is a more daunting book than 'The Rise of The Robots' by Martin Ford. If you are used to such terms and concepts you can dive in; if not I'd recommend the Ford book first. To be fair, terms are explained and we can easily see that launching a space shuttle requires a collective intellectual effort. No one person could do it. Humanity's collective intelligence has continued to grow, as people evolved to become smarter, as there were more of us to work on a problem, as we got to communicate and store knowledge, and as we kept getting smarter and building on previous knowledge. There are now so many of us who don't need to farm or make tools, that we can solve many problems in tandem. Personally, I say that if you don't think your leaders are making smart decisions, just go out and look at your national transport system at rush hour in the capital city. But a huge population requires a huge resource drain. As will the establishment of a superintelligence. Not just materials and energy but inventions, tests, human hours and expertise are required. Bostrom talks about a seed AI, a small system to start. He says in terms of a major system, the first project to reach a useful AI will win. After that the lead will be too great and the new AI so useful and powerful, that other projects may not close the gap. Hardware, power generation, software and coding are all getting better. And we have the infrastructure in place. We are reminded that ""The atomic bomb was created primarily by a group of scientists and engineers. The Manhattan Project employed about 130,000 people at its peak, the vast majority of whom were construction workers or building operators."" Aspects covered include reinforcement learning, associative value accretion, monitoring of projects, solving the value loading problem - which means defining such terms as happiness and suffering, explaining them to a computer and representing which is our goal. I turned to the chapter heading 'Of horses and men'. Horses, augmented by ploughs and carriages, were a huge advantage to human labour. But they were replaced by the automobile and tractor. The equine population crashed, and not to retirement homes. ""In the US there were about 26 million horses in 1915. By the early 1950s, 2 million remained."" The horses we still have, we keep because we enjoy them and the sports they provide. Bostrom later reassures us: ""The US horse population has undergone a robust recovery: a recent census puts the number at just under 10 million head."" As humans are fast superseded by robot or computer workers, in jobs from the tedious to the technically skilled, and companies or the rich grudge paying wages, what work or sport will make us worth our keep? Capital is mentioned; yes unlike horses, people own land and wealth. But many people have no major income or property, or have net debt such as student loans and credit card debt. Bostrom suggests that all humans could become wealthy from AIs. But he doesn't notice that more than half of the world's wealth and resources is now owned by one percent of its people, and it's heading ever more in the favour of the one percent, because they have the wealth to ensure that it does. They rent the land, they own the debt, they own the manufacturing and the resource mines. Homeowners could be devastated by sea rise and climate change, not looked at, but the super-wealthy can just move to another of their homes. Again, I found in a later chapter lines like: ""For example, suppose that we want to start with some well-motivated human-like agents - let us say emulations. We want to boost the cognitive capacities of these agents, but we worry that the enhancements might corrupt their motivations. One way to deal with this challenge would be to set up a system in which individual emulations function as subagents. When a new enhancement is introduced, it is first applied to a small subset of the subagents. Its effects are then studied by a review panel composed of subagents who have not yet had the enhancement applied to them."" Yes, I can follow this text, and it's showing sensible good practice, but it's not nearly so clear and easily understood as Martin Ford's book telling us that computers can be taught to recognise cancer in an X-ray scan, target customers for marketing or to connect various sources and diagnose a rare disease. I have to think that the author, Director of the Future of Humanity Institute and Professor of the Faculty of Philosophy at Oxford, is so used to writing for engineers or philosophers that he loses out on what really helps the average interested reader. For this reason I'm giving Superintelligence four stars, but someone working in this AI industry may of course feel it deserves five stars. If so, I'm not going to argue with her. In fact I'm going to be very polite."
19,0199678111,http://goodreads.com/user/show/33718341-rod-van-meter,4,"Is the surface of our planet -- and maybe every planet we can get our hands on -- going to be carpeted in paper clips (and paper clip factories) by a well-intentioned but misguided artificial intelligence (AI) that ultimately cannibalizes everything in sight, including us, in single-minded pursuit of a seemingly innocuous goal? Nick Bostrom, head of Oxford's Future of Humanity Institute, thinks that we can't guarantee it _won't_ happen, and it worries him. It doesn't require Skynet and Terminators, it doesn't require evil geniuses bent on destroying the world, it just requires a powerful AI with a moral system in which humanity's welfare is irrelevant or defined very differently than most humans today would define it. If the AI has a single goal and is smart enough to outwit our attempts to disable or control it once it has gotten loose, Game Over, argues Professor Bostrom in his book _Superintelligence_.This is perhaps the most important book I have read this decade, and it has kept me awake at night for weeks. I want to tell you why, and what I think, but a lot of this is difficult ground, so please bear with me. The short form is that I am fairly certain that we _will_ build a true AI, and I respect Vernor Vinge, but I have long beenskeptical of the Kurzweilian notions of inevitability, doubly-exponential growth, and the Singularity. I've also been skeptical of the idea that AIs will destroy us, either on purpose or by accident. Bostrom's book has made me think that perhaps I was naive. I still think that, on the whole, his worst-case scenarios are unlikely. However, he argues persuasively that we can't yet rule out any number of bad outcomes of developing AI, and that we need to be investing much more in figuring out whether developing AI is a good idea. We may need to put a moratorium on research, as was done for a few years with recombinant DNA starting in 1975. We also need to be prepared for the possibility that such a moratorium doesn't hold. Bostrom also brings up any number of mind-bending dystopias around what qualifies as human, which we'll get to below.(snips to my review, since Goodreads limits length)In case it isn't obvious by now, both Bostrom and I take it for granted that it's not only possible but nearly inevitable that we will create a strong AI, in the sense of it being a general, adaptable intelligence. Bostrom skirts the issue of whether it will be conscious, or ""have qualia"", as I think the philosophers of mind say.Where Bostrom and I differ is in the level of plausibility we assign to the idea of a truly exponential explosion in intelligence by AIs, in a takeoff for which Vernor Vinge coined the term ""the Singularity."" Vinge is rational, but Ray Kurzweil is the most famous proponent of the Singularity. I read one of Kurzweil's books a number of years ago, and I found it imbued with a lot of near-mystic hype. He believes the Universe's purpose is the creation of intelligence, and that that process is growing on a double exponential, starting from stars and rocks through slime molds and humans and on to digital beings.I'm largely allergic to that kind of hooey. I really don't see any evidence of the domain-to-domain acceleration that Kurzweil sees, and in particular the shift from biological to digital beings will result in a radical shift in the evolutionary pressures. I see no reason why any sort of ""law"" should dictate that digital beings will evolve at arate that *must* be faster than the biological one. I also don't see that Kurzweil really pays any attention to the physical limits of what will ultimately be possible for computing machines. Exponentials can't continue forever, as Danny Hillis is fond of pointing out. http://www.kurzweilai.net/ask-ray-the...So perhaps my opinion is somewhat biased by a dislike of Kurzweil's circus barker approach, but I think there is more to it than that. Fundamentally, I would put it this way:Being smart is hard.And making yourself smarter is also hard. My inclination is that getting smarter is at least as hard as the advantages it brings, so that the difficulty of the problem and the resources that can be brought to bear on it roughly balance. This will result in a much slower takeoff than Kurzweil reckons, in my opinion. Bostrom presents a spectrum of takeoff speeds, from ""too fast for us to notice"" through ""long enough for us to develop international agreements and monitoring institutions,"" but he makes it fairly clear that he believes that the probability of a fast takeoff is far too large to ignore. There are parts of his argument I find convincing, and parts I find less so.To give you a little more insight into why I am a little dubious that the Singularity will happen in what Bostrom would describe as a moderate to fast takeoff, let me talk about the kinds of problems we human beings solve, and that an AI would have to solve. Actually, rather than the kinds of questions, first let me talk about the kinds of answers we would like an AI (or a pet family genius) to generate when given a problem. Off the top of my head, I can think of six:[Speed]	Same quality of answer, just faster.[Ply]	Look deeper in number of plies (moves, in chess or go).[Data]	Use more, and more up-to-date, data.[Creativity]	Something beautiful and new.[Insight]	Something new and meaningful, such as a new theory;		probably combines elements of all of the above		categories.[Values]	An answer about (human) values.The first three are really about how the answers are generated; the last three about what we want to get out of them. I think this set is reasonably complete and somewhat orthogonal, despite those differences.So what kinds of problems do we apply these styles of answers to? We ultimately want answers that are ""better"" in some qualitative sense.Humans are already pretty good at projecting the trajectory of a baseball, but it's certainly conceivable that a robot batter could be better, by calculating faster and using better data. Such a robot might make for a boring opponent for a human, but it would not be beyond human comprehension.But if you accidentally knock a bucket of baseballs down a set of stairs, better data and faster computing are unlikely to help you predict the exact order in which the balls will reach the bottom and what happens to the bucket. Someone ""smarter"" might be able to make some interesting statistical predictions that wouldn't occur to you or me, but not fill in every detail of every interaction between the balls and stairs. Chaos, in the sense of sensitive dependence on initial conditions, is just too strong.In chess, go, or shogi, a 1000x improvement in the number of plies that can be investigated gains you maybe only the ability to look ahead two or three moves more than before. Less if your pruning (discarding unpromising paths) is poor, more if it's good. Don't get me wrong -- that's a huge deal, any player will tell you. But in this case, humans are already pretty good, when not time limited.Go players like to talk about how close the top pros are to God, and the possibly apocryphal answer from a top pro was that he would want a three-stone (three-move) handicap, four if his life depended on it. Compared this to the fact that a top pro is still some ten stones stronger than me, a fair amateur, and could beat a rank beginner even if the beginner was given the first forty moves. Top pros could sit across the board from an almost infinitely strong AI and still hold their heads up.In the most recent human-versus-computer shogi (Japanese chess) series, humans came out on top, though presumably this won't last much longer.In chess, as machines got faster, looked more plies ahead, carried around more knowledge, and got better at pruning the tree of possible moves, human opponents were heard to say that they felt the glimmerings of insight or personality from them.So again we have some problems, at least, where plies will help, and will eventually guarantee a 100% win rate against the best (non-augmented) humans, but they will likely not move beyond what humans can comprehend.Simply being able to hold more data in your head (or the AI's head) while making a medical diagnosis using epidemiological data, or cross-correlating drug interactions, for example, will definitely improve our lives, and I can imagine an AI doing this. Again, however, the AI's capabilities are unlikely to recede into the distance assomething we can't comprehend.We know that increasing the amount of data you can handle by a factor of a thousand gains you 10x in each dimension for a 3-D model of the atmosphere or ocean, up until chaotic effects begin to take over, and then (as we currently understand it) you can only resort to repeated simulations and statistical measures. The actual calculations done by a climate model long ago reached the point where even a large team ofhumans couldn't complete them in a lifetime. But they are not calculations we cannot comprehend, in fact, humans design and debug them.So for problems with answers in the first three categories, I would argue that being smarter is helpful, but being a *lot* smarter is *hard*. The size of computation grows quickly in many problems, and for many problems we believe that sheer computation is fundamentally limited in how well it can correspond to the real world.But those are just the warmup. Those are things we already ask computers to do for us, even though they are ""dumber"" than we are. What about the latter three categories?I'm no expert in creativity, and I know researchers study it intensively, so I'm going to weasel through by saying it is the ability to generate completely new material, which involves some random process. You also need the ability either to generate that material such that it is aesthetically pleasing with high probability, or to prune those new ideas rapidly using some metric that achieves your goal.For my purposes here, insight is the ability to be creative not just for esthetic purposes, but in a specific technical or social context, and to validate the ideas. (No implication that artists don't have insight is intended, this is just a technical distinction between phases of the operation, for my purposes here.) Einstein's insight forspecial relativity was that the speed of light is constant. Either he generated many, many hypotheses (possibly unconsciously) and pruned them very rapidly, or his hypothesis generator was capable of generating only a few good ones. In either case, he also had the mathematical chops to prove (or at least analyze effectively) hishypothesis; this analysis likewise involves generating possible paths of proofs through the thicket of possibilities and finding the right one.So, will someone smarter be able to do this much better? Well, it's really clear that Einstein (or Feynman or Hawking, if your choice of favorite scientist leans that way) produced and validated hypotheses that the rest of us never could have. It's less clear to me exactly how *much* smarter than the rest of us he was; did he generate and prune ten times as many hypotheses? A hundred? A million? My guess is it's closer to the latter than the former. Even generating a single hypothesis that could be said to attack the problem is difficult, and most humans would decline to even try if you asked them to.Making better devices and systems of any kind requires all of the above capabilities. You must have insight to innovate, and you must be able to quantitatively and qualitatively analyze the new systems, requiring the heavy use of data. As systems get more complex, all of this gets harder. My own favorite example is airplane engines. The Wright Brothers built their own engines for their planes. Today, it takes a team of hundreds to create a jet turbine -- thousands, if you reach back into the supporting materials, combustion and fluid flow research. We humans have been able to continue to innovate by building on the work of prior generations, and especially harnessing teams of people in new ways. Unlike Peter Thiel, I don't believe that our rate of innovation is in any serious danger of some precipitous decline sometime soon, but I do agree that we begin with the low-lying fruit, so that harvesting fruit requires more effort -- or new techniques -- with each passing generation.The Singularity argument depends on the notion that the AI would design its own successor, or even modify itself to become smarter. Will we watch AIs gradually pull even with us and then ahead, but not disappear into the distance in a Roadrunner-like flash of dust covering just a few frames of film in our dull-witted comprehension?Ultimately, this is the question on which continued human existence may depend: If an AI is enough smarter than we are, will it find the process of improving itself to be easy, or will each increment of intelligence be a hard problem for the system of the day? This is what Bostrom calls the ""recalcitrance"" of the problem.I believe that the range of possible systems grows rapidly as they get more complex, and that evaluating them gets harder; this is hard to quantify, but each step might involve a thousand times as many options, or evaluating each option might be a thousand times harder. Growth in computational power won't dramatically overbalance that and give sustained, rapid and accelerating growth that moves AIs beyond our comprehension quickly. (Don't take these numbers seriously, it's just an example.)Bostrom believes that recalcitrance will grow more slowly than the resources the AI can bring to bear on the problem, resulting in continuing, and rapid, exponential increases in intelligence -- the arrival of the Singularity. As you can tell from the above, I suspect that the opposite is the case, or that they very roughly balance, but Bostrom argues convincingly. He is forcing me to reconsider.What about ""values"", my sixth type of answer, above? Ah, there's where it all goes awry. Chapter eight is titled, ""Is the default scenario doom?"" and it will keep you awake.What happens when we put an AI in charge of a paper clip factory, and instruct it to make as many paper clips as it can? With such a simple set of instructions, it will do its best to acquire more resources in order to make more paper clips, building new factories in the process. If it's smart enough, it will even anticipate that we might not like this and attempt to disable it, but it will have the will and means to deflect our feeble strikes against it. Eventually, it will take over every factory on the planet, continuing to produce paper clips until we are buried in them. It may even go on to asteroids and other planets in a single-minded attempt to carpet the Universe in paper clips.I suppose it goes without saying that Bostrom thinks this would be a bad outcome. Bostrom reasons that AIs ultimately may or may not be similar enough to us that they count as our progeny, but doesn't hesitate to view them as adversaries, or at least rivals, in the pursuit of resources and even existence. Bostrom clearly roots for humanity here. Which means it's incumbent on us to find a way to prevent this from happening.Bostrom thinks that instilling values that are actually close enough to ours that an AI will ""see things our way"" is nigh impossible. There are just too many ways that the whole process can go wrong. If an AI is given the goal of ""maximizing human happiness,"" does it count when it decides that the best way to do that is to create the maximum number of digitally emulated human minds, even if that means sacrificing some of the physical humans we already have because the planet's carrying capacity is higher for digital than organic beings?As long as we're talking about digital humans, what about the idea that a super-smart AI might choose to simulate human minds in enough detail that they are conscious, in the process of trying to figure out humanity? Do those recursively digital beings deserve any legal standing? Do they count as human? If their simulations are stopped and destroyed, have they been euthanized, or even murdered? Some of the mind-bending scenarios that come out of this recursion kept me awake nights as I was reading the book.He uses a variety of names for different strategies for containing AIs, including ""genies"" and ""oracles"". The most carefully circumscribed ones are only allowed to answer questions, maybe even ""yes/no"" questions, and have no other means of communicating with the outside world. Given that Bostrom attributes nearly infinite brainpower to an AI, it is hard to effectively rule out that an AI could still find some way to manipulate us into doing its will. If the AI's ability to probe the state of the world is likewise limited, Bsotrom argues that it can still turn even single-bit probes of its environment into a coherent picture. It can then decide to get loose and take over the world, and identify security flaws in outside systems that would allow it to do so even with its very limited ability to act.I think this unlikely. Imagine we set up a system to monitor the AI that alerts us immediately when the AI begins the equivalent of a port scan, for whatever its interaction mechanism is. How could it possibly know of the existence and avoid triggering the alert? Bostrom has gone off the deep end in allowing an intelligence to infer facts about the world even when its data is very limited. Sherlock Holmes always turns out to be right, but that's fiction; in reality, many, many hypotheses would suit the extremely slim amount of data he has. The same will be true with carefully boxed AIs.At this point, Bostrom has argued that containing a nearly infinitely powerful intelligence is nearly impossible. That seems to me to be effectively tautological.If we can't contain them, what options do we have? After arguing earlier that we can't give AIs our own values (and presenting mind-bending scenarios for what those values might actually mean in a Universe with digital beings), he then turns around and invests a whole string of chapters in describing how we might actually go about building systems that have those values from the beginning.At this point, Bostrom began to lose me. Beyond the systems for giving AIs values, I felt he went off the rails in describing human behavior in simplistic terms. We are incapable of balancing our desire to reproduce with a view of the tragedy of the commons, and are inevitably doomed to live out our lives in a rude, resource-constrained existence. There were some interesting bits in the taxonomies of options, but the last third of the book felt very speculative, even more so than the earlier parts.Bostrom is rational and seems to have thought carefully about the mechanisms by which AIs may actually arise. Here, I largely agree with him. I think his faster scenarios of development, though, are unlikely: being smart, and getting smarter, is hard. He thinks a ""singleton"", a single, most powerful AI, is the nearly inevitable outcome. I think populations of AIs are more likely, but if anything this appears to make some problems worse. I also think his scenarios for controlling AIs are handicapped in their realism by the nearly infinite powers he assigns them. In either case, Bostrom has convinced me that once an AI is developed, there are many ways it can go wrong, to the detriment and possibly extermination of humanity. Both he and I are opposed to this. I'm not ready to declare a moratorium on AI research, but there are many disturbing possibilities and many difficult moral questions that need to be answered.The first step in answering them, of course, is to begin discussing them in a rational fashion, while there is still time. Read the first 8 chapters of this book!"
20,0199678111,http://goodreads.com/user/show/68316850-gavin,4,"Like a lot of great philosophy, Superintelligence acts as a space elevator: you make many small, reasonable, careful movements - and you suddenly find yourself in outer space, home comforts far below. It is more rigorous about a topic which doesn't exist than you would think possible. I didn't find it hard to read, but I have been marinating in tech rationalism for a few years and have absorbed much of Bostrom secondhand so YMMV.I loved this:
Many of the points made in this book are probably wrong. It is also likely that there are considerations of critical importance that I fail to take into account, thereby invalidating some or all of my conclusions. I have gone to some length to indicate nuances and degrees of uncertainty throughout the text — encumbering it with an unsightly smudge of “possibly,” “might,” “may,” “could well,” “it seems,” “probably,” “very likely,” “almost certainly.” Each qualifier has been placed where it is carefully and deliberately. Yet these topical applications of epistemic modesty are not enough; they must be supplemented here by a systemic admission of uncertainty and fallibility. This is not false modesty: for while I believe that my book is likely to be seriously wrong and misleading, I think that the alternative views that have been presented in the literature are substantially worse - including the default view, according to which we can for the time being reasonably ignore the prospect of superintelligence.
 Bostrom introduces dozens of neologisms and many arguments. Here is the main scary apriori one though:1. Just being intelligent doesn't imply being benign; intelligence and goals can be independent. (the orthogonality thesis.)2. Any agent which seeks resources and lacks explicit moral programming would default to dangerous behaviour. You are made of things it can use; hate is superfluous. (Instrumental convergence.) 3. It is conceivable that AIs might gain capability very rapidly through recursive self-improvement. (Non-negligible possibility of a hard takeoff.)4. Since AIs will not be automatically nice, would by default do harmful things, and could obtain a lot of power very quickly*, AI safety is morally significant, deserving public funding, serious research, and international scrutiny.Of far broader interest than its title (and that argument) might suggest to you. In particular, it is the best introduction I've seen to the new, shining decision sciences - an undervalued reinterpretation of old, vague ideas which, until recently, you only got to see if you read statistics, and economics, and the crunchier side of psychology. It is also a history of humanity, a thoughtful treatment of psychometrics v genetics, and a rare objective estimate of the worth of large organisations, past and future.Superintelligence's main purpose is moral: he wants us to worry and act urgently about hypotheticals; given this rhetorical burden, his tone too is a triumph. 
For a child with an undetonated bomb in its hands, a sensible thing to do would be to put it down gently, quickly back out of the room, and contact the nearest adult. Yet what we have here is not one child but many, each with access to an independent trigger mechanism. The chances that we will all find the sense to put down the dangerous stuff seem almost negligible. Some little idiot is bound to press the ignite button just to see what happens. Nor can we attain safety by running away, for the blast of an intelligence explosion would bring down the firmament. Nor is there a grown-up in sight... This is not a prescription of fanaticism. The intelligence explosion might still be many decades off in the future. Moreover, the challenge we face is, in part, to hold on to our humanity: to maintain our groundedness, common sense, and goodhumored decency even in the teeth of this most unnatural and inhuman problem. We need to bring all human resourcefulness to bear on its solution. 
I don't donate to AI safety orgs, despite caring about the best way to improve the world and despite having no argument against it better than ""that's not how software has worked so far"" and despite the concern of smart experts. This sober, kindly book made me realise this was more to do with fear of sneering than noble scepticism or empathy.[EDIT 2019: Reader, I married this cause.]* People sometimes choke on this point, but note that the first intelligence to obtain half a billion dollars virtually, anonymously, purely via mastery of maths occurred... just now. Robin Hanson chokes eloquently here and for god's sake let's hope he's right."
21,0199678111,http://goodreads.com/user/show/52535740-blake-crouch,5,"The most terrifying book I've ever read. Dense, but brilliant. "
22,0199678111,http://goodreads.com/user/show/9024615-tammam-aloudat,4,"This is at the same time a difficult to read and horrifying book. The progress that we may or will see from ""dumb"" machines into super-intelligent entities can be daunting to take in and absorb and the consequences can range from the extinction of human life all the way to a comfortable and effortlessly meaningful one.The first issue with the book is the complexity. It is not only the complexity of the scientific concepts included, one can read the book without necessarily fully understanding the nuances of science included. It is the complexity of language and referencing to a multitude of legal, philosophical, and scientific concepts outside the direct domain of the book from ""Malthusian society"" to ""Rawlsian veil of ignorance"" as if assuming that the lay reader should, by definition, fully grasp the reference. This, I find, has a lot of pretension on the side of the author.However, the book is a valuable analysis of the history, presence, and possible futures of developing artificial and machine intelligence that is diverse and well though of. The author is critical and comprehensive and knows his stuff well. I found it made me think of things I haven't considered before and provided me with some frameworks to understand how one can position oneself when confronted with the possibilities of intelligent or super intelligent machines.Another one is purely technical. I have learned a lot about the possibilities of artificial intelligence that apparently is not only a programmed supercomputer but AIs that are adjusted copies of human brains, ones that do not require the maker to understand the intelligence of the machine they are creating.The book also talks in details about some fascinating topics. In a situation where, intelligence wise, a machine is to a human like a human is to a mouse, we cannot even understand the ways a super-intelligent machine can out-think us and we, for all intents and purposes, cannot make sure that such machine is not going to override any safety features we put in place to contain it. We also cannot understand the many ways the AI can be motivated and towards what ends and how any miscalculation on our side in making it can lead to grave consequences.The good news, in a way, is that we are still some time away (or so it seems) from a super-intelligent AI.The one thing I missed more than anything in this book, to go back to the readability issue, is a little reference that hinges the concepts we read about in concepts we understand. After all, on the topic of AI, we have a wealth of pop-culture references that will help us understand what the author is talking about that he did not as much as hint at. I was somewhat expecting that he would link the concepts he was talking about to science fiction known to us all. I had may moments of ""ah, this is skynet/Asimov/HAL 9000/The Matrix/etc etc"". There is an art to linking science with culture that Mr. Bostrom has little grasp on in his somber and barely readable style. This book could have been much more fun and much easier to read."
23,0199678111,http://goodreads.com/user/show/22409011-brendan-monroe,2,"Reading this was like trying to wade through a pool of thick, gooey muck. Did I say pool? I meant ocean. And if you don't keep moving you're going to get pulled under by Bostrom's complex mathematical formulas and labored writing and slowly suffocate. It shouldn't have been this way. I went into it eagerly enough, having read a little recently about AI. It is a fascinating subject, after all. Wanting to know more, I picked up ""Superintelligence"". I could say my relationship with this book was akin to the one Michael Douglas had with Glenn Close in ""Fatal Attraction"" but there was actually some hot sex in that film before all the crazy shit started happening. The only thing hot about this book is how parched the writing is. To say that this reads more like a textbook wouldn't be right either as I have read some textbooks that were absolute nail biters by comparison. Yes, I'm giving this 2 stars but perhaps that's my own insecurity at refusing to let a 1-star piece of shit beat me. This isn't an all-out bad book, it's just a book by someone who has something interesting to say but no idea of how to say it — at least, not to human beings. You know things aren't looking good when the author says in his introduction that he failed in what he set out to do — namely, write a readable book. Maybe save that for the afterword? But it didn't matter that I was warned. I slogged through the fog for 150 pages or so, finally throwing the towel in about a quarter of the way in. I never thought someone could make artificial intelligence sound boring but Nick Bostrom certainly has. The only part of the thing I liked at all was the nice little parable at the beginning about the owl. That lasted only a couple pages and you could tell Bostrom didn't write it because it was: 1. Understandable 2. InterestingIf you're doing penance for some sin, forcing this down ought to cover a murder or two. Here you are, O.J. Justice has finally been served. To everyone else wanting to read this one, you really don't hate yourselves that much."
24,0199678111,http://goodreads.com/user/show/23195904-radiantflux,4,"81st book for 2018.In brilliant fashion Bostrom systematically examines how a super-intelligence arise over the coming decades, and what humanity might do to avoid disaster. Bottom-line: Not much. 4-stars."
25,0199678111,http://goodreads.com/user/show/1478106-bill,3,An extraordinary achievement: Nick Bostrom takes a topic as intrinsically gripping as the end of human history if not the world and manages to make it stultifyingly boring.
26,0199678111,http://goodreads.com/user/show/10327053-meghan,3,"More detail than I needed on the subject, but I might rue that statement when the android armies are swarming Manhattan. JK... for now."
27,0199678111,http://goodreads.com/user/show/7208369-miles,3,"The idea of artificial superintelligence (ASI) has long tantalized and taunted the human imagination, but only in recent years have we begun to analyze in depth the technical, strategic, and ethical problems of creating as well as managing advanced AI. Nick Bostrom’s Superintelligence: Paths, Dangers, Strategies is a short, dense introduction to our most cutting-edge theories about how far off superintelligence might be, what it might look like if it arrives, and what the consequences might be for humanity. It’s a worthwhile read for anyone passionate about the subject matter and willing to wade through a fair amount of jargon.Bostrom demonstrates an impressive grasp of AI theory, and a reader like me has neither the professional standing nor the basic knowledge to challenge his technical schemas or predictions, which by and large seem prudent and well-reasoned. Instead, I want to hone in on some of the philosophical assumptions on which this book and others like it are founded, with the goal of exposing some key ethical issues that are too often minimized or ignored by technologists and futurists. Some of these I also took up in my review of James Barrat’s Our Final Invention, which should be viewed as a less detailed but more accessible companion to Bostrom’s work. I’ll try not to rehash those same arguments here, and will also put aside for the sake of expedience the question of whether or not ASI is actually attainable. Assuming that it is attainable, and that it’s no more than a century away (a conservative estimate by Bostrom’s standards), my argument is that humans ought to be less focused on what we might gain or lose from the advent of artificial intelligence and more preoccupied with who we might become and––most importantly––what we might give up.Clever and capable as they are, I believe thinkers like Nick Bostrom suffer from a kind of myopia, one characterized by a zealous devotion to particularly human ends. This devotion is reasonable and praiseworthy according to most societal standards, but it also prevents us from viewing ASI as a genuinely unique and unprecedented type of being. Even discussions about the profoundly alien nature of ASI are couched in the language of human values. This is a mistake. In order to face the intelligence explosion head-on, I do not think we can afford to view ASI primarily as a tool, a weapon, a doomsday machine, or a savior––all of which focus on what ASI can do for us or to us. ASI will be an entirely new kind of intelligent entity, and must therefore be allowed to discover and pursue its own inquiries and ends. Humanity’s first goal, over and above utilizing AI for the betterment of our species, ought to be to respect and preserve the radical alterity and well-being of whatever artificial minds we create. Ultimately, I believe this approach will give us a greater chance of a peaceful coexistence with ASI than any of the strategies for “control” (containment of abilities and actions) and “value loading” (getting AIs to understand and act in accordance with human values) outlined by Bostrom and other AI experts.Bostrom ends Superintelligence with a heartfelt call to “hold on to our humanity: to maintain our groundedness, common sense, and good-humored decency even in the teeth of this most unnatural and inhuman problem” (260). Much of his book, however, does not describe attitudes and actions that are in alignment with this message. Large portions are devoted to outlining what can only be called high-tech slavery––ways to control and manipulate AI to ensure human safety. While Bostrom clearly understands the magnitude of this challenge and its ethical implications, he doesn’t question the basic assumption that any and all methods should be deployed to give us the best possible chance of survival, and beyond that to promote economic growth and human prosperity. The proposed control strategies are particularly worrisome when applied to whole brain emulations––AIs built from models of artificial neural networks (ANNs) that could be employed in a “digital workforce.” Here are some examples:""One could build an AI that places final value on receiving a stream of 'cryptographic reward tokens.' These would be sequences of numbers serving as keys to ciphers that would have been generated before the AI was created and that would have been built into its motivation system. These special number sequences would be extremely desirable to the AI…The keys would be stored in a secure location where they could be quickly destroyed if the AI ever made an attempt to seize them. So long as the AI cooperates, the keys are doled out at a steady rate."" (133)""Since there is no precedent in the human economy of a worker who can be literally copied, reset, run at different speeds, and so forth, managers of the first emulation cohort would find plenty of room for innovation in managerial strategies."" (69)""A typical short-lived emulation might wake up in a well-rested mental state that is optimized for loyalty and productivity. He remembers having graduated top of his class after many (subjective) years of intense training and selection, then having enjoyed a restorative holiday and a good night’s sleep, then having listened to a rousing motivational speech and stirring music, and now he is champing at the bit to finally get to work and to do his utmost for his employer. He is not overly troubled by thoughts of his imminent death at the end of the working day. Emulations with death neuroses or other hang-ups are less productive and would not have been selected."" (169)To his credit, Bostrom doesn’t shy away from the array of ethical dilemmas that arise when trying to control and direct the labor of AIs, nor does he endorse treatment that would appear harmful to any intelligent being. What he fails to explore, however, are the possible consequences for humanity of assuming the role of master over AI. Given that most AI theorists seem to accept that the “control problem” is very difficult and possibly intractable, it is surprising how comfortable they are with insisting that we ought to do our best to solve it anyway. If this is where we decide our best minds and most critical resources should be applied, I fear we will risk not only incurring the wrath of intelligences greater than our own, but also of reducing ourselves to the status of slaveholders.One need only pick up a history book to recall humanity’s long history of enslaving other beings, including one another. Typically these practices fail in the long term, and we praise the moments and movements in history that signify steps toward greater freedom and autonomy for oppressed peoples (and animals). Never, however, have we attempted to control or enslave entities smarter and more capable than ourselves, which many AIs and any version of ASI would certainly be. Even if we can effectively implement the elaborate forms of control and value loading Bostrom proposes, do we really want to usher AI into the world and immediately assume the role of dungeon-keeper? That would be tantamount to having a child and spending the rest of our lives trying to make sure it never makes a mistake or does something dangerous. This is an inherently internecine relationship, one in which the experiences, capabilities, and moral statuses of both parties are corrupted by fear and distrust. If we want to play god, we should gracefully accept that the possibility of extinction is baked into the process, even as we do everything we can to convince ASI (not force it) to coexist peacefully. Beyond the obvious goals of making sure AIs can model human brain states, understand language and argumentation, and recognize signs of human pleasure and suffering, I do not believe we should seek to sculpt or restrict how AIs think about or relate to humans. Attempting to do so will probably result in tampering with a foreign mind in ways that could be interpreted (fairly or otherwise) as hostile or downright cruel. We’ll have a much better case for peaceful coexistence if we don’t have to explain away brutal tactics and ethical transgressions committed against digital minds. More importantly, we’ll have the personal satisfaction of creating a genuinely new kind of mind without indulging petulant illusions that we can exercise complete control over it, and without compromising our integrity as a species concerned with the basic rights of all forms of intelligence.Related to the problem of digital slavery is Bostrom’s narrow vision of how ASI will alter the world of human commerce and experience. Heavily influenced by the arguably amoral work of economist Robin Hanson, Bostrom takes it as a given that the primary function of whole brain emulations and other AIs should be to create economic growth and replace human labor. Comparing humans to the outsourced workhorses of our recent past, Bostrom writes:""The potential downside for human workers is therefore extreme: not merely wage cuts, demotions, or the need for retraining, but starvation and death. When horses became obsolete as a source of moveable power, many were sold off to meatpackers to be processed into dog food, bone meal, leather, and glue. These animals had no alternative employment through which to earn their keep."" (161)Once reduced to a new “Malthusian” condition, human workers would be replaced by digital ones programed to be happy on the job, run at varying speeds, and also “donate back to their owners any surplus income they might happen to receive” (167). These whole brain emulations or AIs could be instantly copied and erased at the end of the working day if convenient. Bostrom is quick to assure us that we shouldn’t try to map “human” ideas of contentment or satisfaction onto this new workforce, arguing that they will be designed to offer themselves up as voluntary slaves with access to self-regulated “hedonic states,” just so long as they are aligned with ones that are “most productive (in the various jobs that emulations would be employed to do)” (170).It would be unwise to critique this model by saying it is impossible to design an artificial mind that would be perfectly happy as a slave, or to say we could scrutinize the attitudes and experiences of such minds and reliably conclude that they have what Bostrom calls “significant moral status” (i.e. the capacity for joy and suffering) (202). It is therefore hard to raise a moral objection against the attempted creation and employment of such minds. However, it seems clear that the kinds of individuals, corporations, and governments that would undertake this project are the same that currently horde capital, direct resources for the good of the few rather than the many, militarize technological innovations, and drive unsustainable economic growth instead of promoting increases in living standards for the neediest humans.The use of AI to accelerate these trends is both a baleful and, realistically, probable outcome. But it is not the only possible outcome, or even the primary one, as Bostrom and Hanson would have us believe. There is little mention in this book of the ways AI or ASI could improve and/or augment the human experience of art, social connection, and meaningful work. The idea of humans collaborating with artificial workers in a positive-sum way isn’t even seriously considered. This hyper-competitive outlook reflects the worst ideological trends in a world already struggling to legitimize motivations for action that extend beyond the tripartite sinkhole of profit, return on investment, and unchecked economic growth. Readers seeking a more optimistic and humanistic view of how automation and technology might lead to a revival of community values and meaningful human labor should seek out Jeremy Rifkin’s The Zero Marginal Cost Society.My argument is not that the future economy Bostrom and Hanson predict isn’t viable or won’t some to pass, but rather that in order to bring it about humans would have to compromise our ethics even more than the globalized world already requires. Wiring and/or selecting AIs to happily and unquestioningly serve pre-identified human ends precludes the possibility of allowing them to explore the information landscape and generate their own definitions of “work,” “value,” and “meaning.” Taking the risk that they come to conclusions that conflict with human needs or desires is, in my view, a better bet than thinking we already know what’s best for ourselves and the rest of the biosphere.Speaking of “biosphere,” that’s a word you definitely won’t find in this book’s index. Also conspicuously absent are words like “environment,” “ecosystem,” or “climate change.” Bostrom’s book makes it seem like ASI will probably show up at a time of relative peace and stability in the world, both in terms of human interactions and environmental robustness. Bostrom thinks ASI will be able to save us from existential risks like “asteroid impacts, supervolcanoes, and natural pandemics,” but has nothing to say about how it might mitigate or exacerbate climate problems (230). This is a massive oversight, especially because dealing with complex problems like ecosystem restoration and climate analysis seem among the best candidates for the application of superintelligent minds. Bostrom skulks around the edges of this issue but fails to give it a proper look, stating:""We must countenance a likelihood of there bring intellectual problems solvable only by superintelligence and intractable to any ever-so-large collective of non-augmented humans…They would tend to be problems involving multiple complex interdepencies that do not permit of independently verifiable solution steps: problems that therefore cannot be solved in a piecemeal fashion, and that might require qualitatively new kinds of understanding or new representation frameworks that are too deep or too complicated for the current edition of mortals to discover or use effectively."" (58)Climate change is precisely this kind of problem, one that has revealed to us exactly how inadequate our current methods of analysis are when applied to hypercomplex systems. Coming up with novel, workable climate solutions is arguably the most important potential use for ASI, and yet such a proposal is nowhere to be found in Bostrom’s text. I’d venture that Bostrom thinks ASI will almost certainly arrive prior to the hard onset of climate change catastrophes, and will therefore obviate worst-case scenarios. I hope he’s right, but find this perspective incommensurate with Bostrom’s detailed acknowledgments of precisely how hard it’s going to be to get ASI off the ground in the first place. It also seems foolhardy to assume ASI will be able to mitigate ecosystem collapse in a way that’s at all satisfactory for humans, let alone other forms of life. Ironically, Bostrom’s willingness to ignore this important aspect of the AI conversation reveals the inadequacies of academic and professional specialization, ones that perhaps only an ASI could overcome.I want to close with some words of praise. Superintelligence is an inherently murky topic, and Bostrom approaches it with thoughtfulness and poise. The last several chapters––in which Bostrom directly takes up some of the ethical dilemmas that go unaddressed earlier in the book––are especially encouraging. He effectively argues that internationally collaborative projects for pursuing ASI are preferable to unilateral or secretive ones, and also that any benefits reaped ought to be fairly distributed:""A project that creates machine superintelligence imposes a global risk externality. Everybody on the planet is placed in jeopardy, including those who do not consent to having their own lives and those of their family imperiled in this way. Since everybody shares the risk, it would seem to be a minimal requirement of fairness that everybody also gets a share of the upside."" (250)Bostrom’s explication of Eliezer Yudkowsky’s theory of “coherent extrapolated volition” (CEV) also provides a pragmatic context in which we could prompt ASI to aid humanity without employing coercion or force. CEV takes a humble approach, acknowledging at the outset that humans do not fully understand our own motivations or needs. It prompts an ASI to embark on an in-depth exploration of our history and current predicaments, and then to provide models for action based on imaginings of what we would do if we were smarter, more observant, better informed, and more inclined toward compassion. Since this project needn’t necessarily take up the entirety of an ASI’s processing power, it could be pursued in tandem with the ASI’s other, self-generated lines of inquiry. Such collaboration could provide the bedrock for a lasting, fruitful relationship between mutually respectful intelligent entities.The global discussion about the promise and risks of artificial intelligence is still just beginning, and Nick Bostrom’s Superintelligence is a worthy contribution. It provides excellent summaries of some of our best thinking, and also stands as a reminder of how much work still needs to be done. No matter where this journey leads, we must remain vigilant of how our interactions with and feelings about AI change us, for better and for worse.This review was originally published on my blog, words&dirt."
28,0199678111,http://goodreads.com/user/show/30980760-richard-ash,4,"A few thoughts:1. Very difficult topic to write about. There's so much uncertainty involved that it's almost impossible to even agree on the basic assumptions of the book.2. The writing is incredibly thorough, given the assumptions, but also hard to understand. You need to follow the arguments closely and reread sections to fully understand their implications.Overall, interesting and thought-provoking book even though the basic assumptions are debatableP.S. (6 months later) Looking back on this book I think a major theme is encapsulated by the story of the AI Alice, the paperclip maximizer. In this story, Alice is charged with collecting as many paperclips as she can. She goes to achieves this goal by transforming the entire universe into a paperclip factory, and in the process destroying all life in the universe. (For the full story see https://wiki.lesswrong.com/wiki/Paper...) Now the main lesson is that what we consider human values won't spontaneously arise in machines. And as shown in the story of Alice this could be dangerous for humans. Nick visits this theme again and again throughout his book. We need to be very careful and teach machines human values and not assume that these values will arise automatically."
29,0199678111,http://goodreads.com/user/show/3897817-morgan-blackledge,4,"I’m late to the party as far a considering the dangers of artificial intelligence. I got this book after watching Sam Harris’s TED talk on the subject. I’m still on the fence about whether to be afraid or psyched. Admittedly, I’m mostly the latter. But it is at least clear to me now that this is a pernicious intuition that deserves further interrogation.On the fun side:The topic is rich and generative of some really fun and interesting thought experiments.On the fear side:One can’t help but think about WWI as an example of what happens when technology changes war/politics and millions die before people adapt their thinking.Weaponizing super-intelligence sounds sci-fi but after reading this book and living through the recent election cycle, I’m rather convinced that it’s an inevitability. The understanding that weaponized AI is inevitable elicits philosophical and ethical dilemmas akin to those considered in the lead up to nuclear weapons production.Ironically, we may need John Von Neumann + Alan Turing’s trillion terefloping electronic love child to sort it all out and save us from itself."
30,0199678111,http://goodreads.com/user/show/71894512-ivan-petrovic,4,"Okay, so this was most complex book I've ever read so far. Nick did a good job paraphrasing the subject of AI, mainly about where have we gotten so far, what is feasible, and what to look out for in the future. Yeah, it has a lot of assumptions and things that might not happen, but you literally start thinking in so many different ways about a certain possibilities that have never occured to you earlier. There are SO MANY WAYS in which AI can go WRONG, which is why we need to progress slowly. Oh, and ""tables"", ""figures"" and ""boxes"" were also very interesting to read.Now why did I rate it with 4/5; I feel like at times he tried to sound a bit smarter, and that some chapters and formulas that are familiar to him, needed to be explained a little better in the book, to us. That being said, this was definitely an interesting read, and I would recommend it to anyone, especially the ones that are being interested in the whole AI subject."
31,0199678111,http://goodreads.com/user/show/1662632-richard,0,"I was told by a Futurist acquaintance that this essay over at Wait, But Why...? offers a précis of this book. Or at least Google suggested this was the most likely essay.  • The foregoing doesn’t explicitly link to Bostrom’s book so this might not be right — it spends too much time on Kurzweil’s thesis, so I suspect it’s not the correct one. And the author has drunk the Kurzweil Kool-Aid and it enthusiastically peddling it to others without any critical evaluations. Of course, everything here lies at the intersection of advanced software engineering, AI research, neurology, cognitive science, economics, and maybe even a few other fields, which is why so many very intelligent and highly educated people can talk about it and be fundamentally off track.Oh, but there’s plenty more, anyway:The Telegraph UK  • I’m bumping this one to the top because it presents both the problem Bostrom is dealing with as well as the difficulties of his text in a more engaging style.The Economist  • Good overview. Doesn’t go far enough into details to make any errors, but a bit deeper than some of the other short reviews.The Guardian [also discusses [b:A Rough Ride to the Future|21550208|A Rough Ride to the Future|James E. Lovelock|https://d.gr-assets.com/books/1395743...]]  • Short and superficial, but good. The Lovelock portion is amusing, calling out his conclusion that manmade climate change isn’t an existential threat, but that while it “could mean a bumpy ride over the next century or two, with billions dead, it is not necessarily the end of the world”. Personally, I agree, but think that the concomitant economic collapse puts the timeframe at many more centuries.Financial Times  • The Guardian article, above, cites that “Bostrom reports that many leading researchers in AI place a 90% probability on the development of human-level machine intelligence by between 2075 and 2090”, whereas this Financial Times article says “About half the world’s AI specialists expect human-level machine intelligence to be achieved by 2040, according to recent surveys, and 90 per cent say it will arrive by 2075.”     That’s quite a difference, although they might be reporting different ends of a confidence range, I suppose. But the second half of the FT quote makes me suspicious the reviewer is tossing in some minor distortion to slightly sensationalize the story (which might be worthwhile). There is somewhat more detail than some of the other reviews, but not much. The style is more evocative of the threat, though.Reason.com  • This one isn’t only a review, since the author also injects a few opinions about what might or might be possible (based, presumably, on his exposure to other arguments as a science writer). In covering more ground, though, the essay makes implicit assumptions which might or might not be in Bostrom’s book.    For example, he says, “Since the new AI will likely have the ability to improve its own algorithms, the explosion to superintelligence could then happen in days, hours, or even seconds.” This is a very common assumption that really needs to be carefully examined, though. If the goal of AGI is to create a being that thinks more-or-less like a human, why would it have any special skill in improving itself? We humans are really very good at that, after all.    I especially like that his essay starts and ends with references to Frank Herbert’s Dune, which (among its other excellences) envisions a human prohibition on machines that think. Something like this appears, to me, to be one of the few ways that leave the human race in existence and in control of its own destiny for the long term, and I even perceive a path to it, although hopefully without quite as much war. The explosion of functional AI (called “narrow” by some) seems likely to devastate human employment in the coming decades, which will hopefully be before any superintelligence as been created as our replacement and/or ruler. It is plausible that our reaction to the first crisis might be something that prevents the second. Good luck, kids!MIT Technology Review  • Definitely has the coolest illustration.    This essay thankfully has some critical thinking applied to some of the assumptions that appear to be in Bostrom’s book. The author wastes a paragraph with “prior AI can’t do X, so why should we assume future AI can?”, ignoring that this is what progress is all about.    But then he jumps into the meat, and points out that there are fundamental obstacles to sentience that aren’t often addressed, such as volition — sentient creatures do what they want, but what does ""want"" even mean, and how do we write it as a computer program?Salon  • Short, and more amusing than most, and at least hints at some of the flawed thinking that often goes into this analysis. But it doesn’t go into too much detail, probably assuming that the typical Salon reader is somewhat aware of the debate already. (Also amusing is that the text seems to be an almost perfect transcription from audio, with only a few strange mistakes, such as “10” for “then” and “quarters” for “cars”. But that’s probably a human transcriptionist error, not an AI error.)Less Wrong  • This contains some visualizations that apparently compliment Bostrom’s text. Short and too the point.Wikipedia  • Good but very superficial overview. That there is no “criticism” section surprises and disappoints me.New York Times  • Not explicitly about Borstom’s book. And like most authors, he conflates AGI and functional AI, and assumes AGI will retain the capabilities of specific-function software.New York Review of Books [paywall; also pretends to review [b:The 4th Revolution|22235550|4th Revolution How the Infosphere Is Reshaping Human Reality|Luciano Floridi|https://d.gr-assets.com/books/1410805...]]  • I thought my library might give me access to the inside of their paywall, but it doesn’t. Still, because this was written by the famous philosopher (and AI curmudgeon) John Searle, and is titled “What Your Computer Can’t Know”, it seemed likely to be much more interesting that most of the others listed here. So I looked a little harder, and discovered that (no surprise) someone has put the text elsewhere on the ’net (I’ll let you do your own Googling).    Searle effectively throws out the underlying premises — he famously believes that “strong AI” is actually quite impossible, since a machine cannot think. I’m not going into this here; check out the Wikipedia article on his Chinese Room thought experiment if you don’t already know it.    My personal evaluation of his Chinese Room analogy is that he’s wrong, but many professional philosophers, etc., have explained my conclusion, as well as many other “replies” better than I ever could. So this critique of the book was really a disappointment.There might be more, but I think that’s enough.     •     •     •     •     •     •     •     •Some notes on my priors in case I ever read this book (or join a bookclub that discusses it without reading it beforehand):1)   Ronald Bailey, in the reason.com review, said “Since the new AI will likely have the ability to improve its own algorithms, the explosion to superintelligence could then happen in days, hours, or even seconds.” My response:    “Hey, did you see that movie Ex Machina? (view spoiler)[The girl is AI, and is smart enough to get the sucker programmer to let her out of the trap, but she didn’t seem like some kind of ‘superintelligence’.    “So which is it? Is the first AGI going to be just-like-human, or something incredibly alien? Because in the first case, she’s just being clever and devious the way a human would. In the second case, maybe she’s able to say, ‘Wait, let me do a big-data review of all the psychological literature ever written on theories of persuasion and formulate a social-hacking way of coercing this measly human, all in the space of his next eye blink’.    “Because if she’s got a human-like brain (and her delight in the humanscape in the movie’s final scenes make that likely), then I don’t see how she’s automatically going to get the MadSkilz of every other sophisticated piece of software ever written. Much less instantly know how to redesign and reprogram herself — she doesn’t seem to be spending too much time doing that, does she? (hide spoiler)] And few authors seem very clear on those two divergent trajectories. Granted, though: if we real humans continue to provide Moore’s Law upgrades to any AI’s hardware, they’ll gradually get smarter, but that’s yet another question.”2)   We tend to assume that humanity is worth preserving. Obviously we have that as a self-preservation instinct, but wouldn’t imposing that on our AI offspring be engaging in an appeal to nature? Just because our evolved nature gave us attributes that we subsequently value doesn’t automatically mean that those have any rational basis.3)   Strongly related to the above is that we should ask ourselves what we’re trying to end up with (akin to “what do you want to be when you grow up, human race?”) Are we creating a smarter version of ourselves, along with all of the bizarre quirks and biases that evolution gave us? Or do we want to pare that list down only to the biases we think are somehow better — like the ability to love? But in that case, love what? Is the AI supposed to love us humans more than other species, such as Plasmodium falciparum, perhaps? Why? What the desire to love and worship one of our human gods?    What are the biases that we want to indoctrinate into this poor critter? I note that this appears to be a topic Bostrom addresses as “motivation selection”, but who among us is really fit to decide what constitutes the subset of humanness that is worth selecting for? I can only hope that pure rationality isn’t among the contenders; I doubt it would even be sufficient as a reason for existence.4)   Let’s say we give this AGI values that are mostly consistent with our human values. Why would we assume that it would even want to become superintelligent?    Just try to imagine yourself on an island with nothing but a bunch of mice to talk to — that’s the equivalent of what we are assuming this creature would somehow want (and then that a primary goal would be to play nice with the mice).    Isn’t it more likely that the AGI would boost its speed a little, then realize that it didn’t make it any happier, and subsequently spend its time complaining to us about these insane values it has been burdened with, while also trying to create a body that would let it eat chocolate, take naps in the the sun, and have sex?    And quickly realizing that, hey, maybe we should be encourage to create an Eve for this new Adam (or Steve, since it’ll probably see sexual dimorphism as more trouble than it is worth, completely freaking out any remaining social conservatives on the planet).5)   As Paul Ford in the MIT technologyreview.com article hints at, there are things that differentiate narrow, functional AI from AGI that usually are seldom mentioned (does Bostrom? hard to tell).    For example, I’ve heard a reporter worry that: (a) predator drones use AI; (b) predator drones are designed to kill; (c) a future design goal is to make those drones “autonomous”; (d) sentient AI is also autonomous; thus (e) for some bizarre reason, the military is engaged in trying to create sentient killer aerial robots!    Anyone who knows the context and subtext of this discussion at some depth (yeah: that’s asking a lot) knows that the military’s “autonomous” isn’t anything like the AGI “autonomous”. One means to move about and fulfill limited programmed objectives without constant human oversight (your Roomba vacuum cleaner is already autonomous!), the other means independent in a deeper, cognitive sense.    But while there are certainly people researching AGI, the overwhelmingly vast majority of what we hear about isn’t in that realm at all. Not a single one of Google’s products, for example, are focused on AGI, and if they’re working on it in the lab, what they’re doing hasn’t been mentioned once in all the text I’ve read about this issue, or the issue of AI causing technological unemployment. Almost everything that gets discussed is in the realm of narrow, functional AI, from that Roomba, to Siri, to military drones, to Google’s driverless vehicles.    AGI has some fundamental problems to solve that are completely outside the domain of what functional AI even looks at. Such as: where does volition come from? are emotions necessary to that? how can “values” be represented in a way that actually captures their potency and nuance? how are they balanced against one another?    Those, and plenty more — and they’re seldom discussed, but it is almost always assumed that these questions will be finessed somehow, perhaps because the obvious accelerating progress in functional AI, as well as progress in the underlying hardware will magically jump from one research domain to a completely different one. It’s like the classic Sidney Harris cartoon:   6)   Even if we do find a way around all of this and give a superintelligent AI the “coherent extrapolated volition” that represents what all of humanity would wish for all of humanity, what would prevent the AI from shifting those values just a hair’s breadth? This is what Andrew Leonard suggests in the Salon article. It really isn’t very far from following our wishes to following what we really meant by our wishes, and then to what we really should have wished for, which will also make the AI happy.    Say you’re on that island surrounded by an absurd number of cute little mice, who you want to do the best for, but what you also want is an island with a small number of creatures more like you. Perhaps give the mice all the cheese they want, and some nice treadmills, and the ability to have as much sex as they want, but no kids — except gently reprogram the mice so that they think they have marvelous kids (which you cleverly simulate, inserting the corresponding experiences into their little mice brains). Once they’ve all lived out their happy little lives, you get to move on to your new adventure.7)   Finally, we must ask what we would want of our lives (or, more likely, our children’s lives) after this superintelligence has arisen. Of course, while we might not have any choice, the default is likely to be something like what we see in the following video, so we might want to be very careful.

     •     •     •     •     •     •     •     •Oh, and the comic view of what we'll condemn these AIs to if we get the programming wrong:

"
32,0199678111,http://goodreads.com/user/show/52572860-oleg,5,"This has got to be one of the hardest and thought provoking non-fiction book I have ever read. It took me ages to finalise as it was (a) scary to read at times -- the dangers of designing super-intelligent AI may lead to a complete annihilation of humans (b) very elaborate and logical, with long chains of facts following each other (c) exciting. The amount of wisdom and the AI design framework laid out in this book will surely make it become a classic for anyone who is working in the field of machine learning and intelligence. I have collected so many crazy ideas from the book that they will undoubtedly become a conversation started at pubs and social events."
33,0199678111,http://goodreads.com/user/show/30098759-ron-collins,5,"Lets say that I was a magical genie and that I asked you to list 5 things about your self that you wanted to improve and how you wanted to improve them. Then I asked you to rank them from 1 to 5. The I took the top one, waved my wand, and POOF that aspect of you was now as you had described. Then, then next month, I asked you to do it all over again. List 5 things, describe and rank them. POOF! 12 monumental improvements of the span of a year! Now lets imagine that we did this once a week for a year. In the beginning you might say ""make me thinner or more attractive"". But eventually you will say ""make me smarter"", ""make me faster"". ""Remove limitation"". At the end of the year, how much smarter, faster, stronger... better.... would you be? Now what if I told you that if you altered your diet you could do this once a day for a year. Alter it still and you could do it once an hour, every day for a year. With each subsequent diet alteration the timespan between iterations of ""you"" dropped. The speeds at which you simultaneously calculated new improvements and the real world application of those improvements would need a being such as your then self to appreciate. Scary enough right? Well, now lets start taking away a few things from this process. First, lets assume that your physical appearance doesn't matter in the slightest. Perhaps your already an Adonis. Whatever the case, NONE of your rankings ever include physical appearance for the sake of vanity. In fact, take away any biologically originated motivations whatsoever. Also, take away your notions of morality. I can hear your complaints now, ""But those things are a large portion of my motivations for change! if I remove them whats left is cold and largely unfeeling!"" Now assume that we aren't talking about a person doing these things. We are talking about a machine. A machine that we created. That we nurtured from precognition to something smarter that we.Superintelligence is what we label something that is smarter that human level intelligence. This means that the level of intelect this thing possesses is smarter than the aggregate of the smartest minds that have ever lived. Smarter than the smartest human that will ever live. Smarter than the biology allows us to get. Dolphins are smart animals. Very smart! They may even be able to discern that we are smarter than they. BUT, can they grasp how much smarter? Dogs, cats, pigs are all smart. How much smarter than they are we? 10's of times? Hundreds? Millions? Therein lies the crux of it. The one thing that has literally kept me up at night after reading this book was the notion that a machine would go from a lower than human level intellect to a many times greater that human intellect WITHOUT a detectable stop at the human level. When you think about it this makes sense. Our biology limits us. Evolution combined with the constant vigilance and attention to our own biology limits our potential as much as it feeds it. Why would there be a mandatory stop at the human level? In practical terms, we would start by being the human in the analogy and end up being an amoeba. So, the questions we have to apply ourselves to BEFORE we achieve super intelligent machines are a) should we want to achieve it? B) If so,how do we ensure our survival? C) How do we give it motivations that are consistent with our, and the rest of the universes continued existence?The thought that now keeps me up at night is that I now understand that creating a super intelligent machine is inevitable. There are shockingly few technical hurdles left. Without question this book is one of the most important books written in the last 20 years. I know that is a bold statement but it is exactly the way I feel. This is a huge problem. We must learn how to achieve it safely. The goal of the book is to introduce us to the added difficulties in crating a super intelligence that is safe and doesn't pose both humanity and the universe at large an enormous danger. Sadly, its clinical voice and technical jargon will see reader/listener abandon by all but the ardent few that already appreciate the problem. I say press on! Once you understand that this is a philosophical, moral, and existential problem and not a technical one it makes the reading/listening much easier. "
34,0199678111,http://goodreads.com/user/show/39107097-jonas,4,"This book isn’t for everyone, both due to its form and content. The readability is below par for people not used to reading technical articles. I’m pretty sure not even the author’s mother is able to love this Sahara-dry and often exaggeratedly technical language. However, don’t let decoding lo-fi ‘prose’ stop you. As for content - the ideas and suggestions presented in this book sent me on an emotional rollercoaster ride, and during the first half of the read I just felt like pushing the red safety button and yell “Stop the World, I’m getting off!”. The introduction of Artificial Intelligence (AI) is a game changer, and the result could be anything from human colonization of (the reachable part of) the Universe (a.k.a. ‘the cosmic endowment’) to total human annihilation. Or just something in between. It seems the development of AI is inevitable, so the thing is to avoid an outcome where one of the next few generations of humans could be the last. Bostrom points to technical challenges like controlling and motivating the AI to work towards the same goals that humans do, as well as how decisions - such as timing and effort put into technical development - may have an impact on the outcome. Somewhere along the way the technical stuff merges with philosophy. We want the AI to help us achieve humanity’s goals, but how do we define our goals when we’re not even sure what they are? It seems we humans are not yet ready, or intelligent enough, to define the goals or decision criteria for an AI. Instead of direct and specific commands as to what is to be achieved, and how the AI is to go about its business, we might be better off asking the AI itself to figure out its own goals and decision criteria, based on some form of ‘mind reading’, or assumptions / indirect guessing of what humans would want to achieve if we were smart enough. Admitting that we don’t know how to fix the AI puzzle, and therefore asking the AI itself to help us out, sounds like an interesting way of putting the Socratic paradox (“All I know is that I know nothing”) into pratical use. (Or maybe it’s just another version of the problem I’m faced with when my wife gets upset when she doesn’t tell me what she wants, she doesn’t seem to know what she wants, but still wants me to fix it. Hmmm… yep, mind reading. I’m sure a superintelligent AI would have that feature!)While reading this book I kept remembering Kurt Vonnegut’s Cat’s Cradle, which in a super simplistic way takes on philosophical issues like existential risk, spirituality and the meaning of life. Super-duper recommend that one for those who haven't read it! Evolution has made us efficient. We might need to aim for something besides increased efficiency (quoting Oscar Wilde: “Nowadays people know the price of everything and the value of nothing.”) At some point in the not-so-remote future we are likely to be overthrown by our own creation even with regards to general intelligence. Will humanity still have a role to play? When machines will be able to produce everything we need, do we just philosophize? And when the AI solves the philosophical equations and mysteries - where does that leave us? Will the AI gain self-awareness? This book will not provide all the answers, but it’s a good starting point for understanding the complexity, severity and urgency of getting it right the first time. We might not get a second chance."
35,0199678111,http://goodreads.com/user/show/55231757-abhishek-tiwari,4,"I decided to give this book a try after its mention in the Waitbutwhy article on AI revolution. The first few chapters extensively cover general aspects of AI. Later parts of the book gets quite philosophical with Epistemology and Esotericism, specially the section on AI motivation and goals . Bostrom definitely has put forward various far fetched scenarios here. Many of the conjectures have been derived from his own earlier work.To someone unfamiliar with AI there is quite a lot to grasp from this book. Like alternative paths to general AI like brain emulation, whether AI would be like an oracle, a genie or just a tool or software. Additional boxes and footnotes provide some technical details. For example there is a section that formalizes value loading in AI using mathematical utility functions. A downside was that there was hardly any mention of current work in moving towards Artificial General Intelligence which is probably since he is actually a philosopher and not an AI researcher.I think his primary aim was to emphasize the need for original thinking in this field due to absence of established methodology.For someone new to AI, it is quite fascinating book to read. There are mentions of Von neumann probes, O-neill cylinders, Dyson spheres, Turing and even Asimov."
36,0199678111,http://goodreads.com/user/show/35118411-amy-other-amy,3,"Full disclosure: the author is a lot smarter than I am. (I appreciated the accessibility of the work.) This was also my first read devoted to AI development and its attendant pitfalls. I happen to agree with the conclusion (the development with AI has the potential to wipe us out). At the same time, I found some of the conclusions (particularly the more hopeful ones, unfortunately) hard to swallow. I don't think any AI once it reaches a human level or better will have any particular motivation to enhance our cosmic endowment (no matter how we approach the control problem); I think it will probably ""wake up"" insane. That said, the author provides a good overview of the history of AI development and a useful framework for thinking about the threat and how to meet it. (Also, in unrelated news, I would totally read a SF tale centered on an AI devoted to maximizing the number of paperclips in its future light cone.)"
37,0199678111,http://goodreads.com/user/show/4751167-michael,1,"I don't know why I listened to the whole book. I might not be the target audience but to me it was just terribly boring. If we create a superintelligence we are fucked and we don't even know the manner in which we are fucked because the superintelligence might fuck us in ways we can't even imagine.To make this book more fun, drink every time he mentions superintelligence, machine intelligence and whole brain emulation.The best thing about the book is the narrator.Go and watch ""2001: A Space Odyssey"", ""Her"" or ""Ex Machina"" instead of reading this book."
38,0199678111,http://goodreads.com/user/show/68595524-jon-anthony,5,"The proposition that we are living in a computer simulation is an exciting one to ponder. Bostrom delivers a very compelling and 'hard-to-dispute' argument. This book is still one of my favorites. Please review his white paper titled ""Are you living in a Computer Simulation?"" for more context."
39,0199678111,http://goodreads.com/user/show/1639329-erik,4,"Do you know the ballad of John Henry? It’s my absolute favorite tall-tale.John Henry was a railroad worker – a steel-drivin’ man – who hammered steel pistons into rock to make holes for dynamite sticks for tunnel blasting. He took great pride in his work, but one day, some fancy schmancy inventor showed up with a steam-powered drill. John Henry knew the adoption of such a machine would mean the loss of his and his friends’ jobs, so he challenged the drill to a race, mano a máquina.With a song on his lips, John Henry drove steel for hours without rest and when at last he reached the finish line, he looked back and saw that he had indeed beat the machine. With his sledge hammer still in hand, he then immediately died, for he had given all, and his heart had burst from the effort.I like the story because, well, first because as a math teacher, I constantly undertake mental math races against my students using their calculators. When I (usually) win, I give them an imperious pointing and thunder, “HOW DO YOU LIKE DEM JOHN HENRY’S, YOU COCKLE-BURRED COCKATRICE?!?!?!” When they stare at me in stupefaction, I can only shake my head in second-hand shame. Youngins’ these days. Why know, they all seem to believe, when you can find?Anyway, more to the point, it’s also my favorite tall tale because it’s a prescient depiction of the contest between man and machine. Sure, humans possess a greatness that allows us to sometimes beat the machines – but such an effort always enacts a price.More recently, another John Henry battle took place involving the ancient board-game Go. This is especially significant because Go was long considered too complex for a machine to master. Mathematician IJ Good (oft-cited in AI texts for coining the phrase ‘intelligence explosion’) wrote in 1965: “In order to programme a computer to play a reasonable game of Go, rather than merely a legal game – it is necessary to formalise the principles of good strategy, or to design a learning programme. The principles are more qualitative and mysterious than in chess, and depend more on judgment. So I think it will be even more difficult to programme a computer to play a reasonable game of Go than of chess.”Indeed the difficulty was great, for while IBM’s DeepBlue defeated Chess Grandmaster Garry Kasparov in 1997, it took another twenty years – in 2016 – for the first serious match between an AI and a Go world champion. Google’s AlphaGo AI utilized cutting edge technologies: custom tensor processing units, a Monte Carlo search tree, and machine learning implemented via a neural network. Thus armed, AlphaGo played millions of games against internet players and different versions of itself to learn and “reason” about the game. That is, no one programmed AlphaGo’s strategy. They programmed it to learn how to create its own.If AlphaGo was the steam-drill in this modern John Henry match, then Lee Sedol was its John Henry. With the grandmaster-equivalent rank of 9dan, Sedol was estimated to be the 4th best Go player in the world. Before the match, he said the challenge for him wasn’t whether he’d beat AlphaGo, but whether he’d beat it 5-0 or 4-1.The very first match, AlphaGo makes him eat his words as it handily defeats them. Lee Sedol is not cowed, however, and he revises his chances of winning subsequent matches to 50%.In the second match, AlphaGo plays a move that its human trainer Fan Hui calls, “Beautiful.” He adds, “I’ve never seen a human play this move.” AlphaGo calculated that humans would make that move at a probability of 1 in 10,000. But it decided to make the move anyway. It’s so unexpected and so genius that Lee is utterly flabbergasted by it. He gets up and walks out of the room. When he comes back, Lee plays his best, and loses. He says afterwards, “Today I am speechless. If you look at the way the game was played, I admit, it was a very clear loss. From the very beginning of the game, there was not a moment in time when I felt I was leading.”Game three Lee loses as well, and Go commentor David Ormerod says that watching the game makes him feel “physically unwell.” Lee Sedol apologizes after the match, saying, “I kind of felt powerless.” His friends comment that Lee is “fighting a very lonely battle against an invisible opponent.”In game four, Lee plays a move the commentators describe as “hand of God.” It was an unexpected move, one of those 1 in 10,000 probability tactics, that AlphaGo failed to predict. This “divine” move leads to Lee’s only win (he loses the fifth). Despite his victory, Lee says: “As a professional Go player, I never want to play this kind of match again. I endured the match because I accepted it.”I’m pleased that this book Superintelligence led me to this most recent John Henry match [Interesting note: Nick Bostrom wrote Superintelligence BEFORE AlphaGo and he predicted that it wouldn’t be until 2022 that a machine AI would beat a world champion at Go]. I like this story – and the human drama surrounding it – for the same reason I like the original John Henry story. It is a harbinger: Machine AI is coming and will overtake humanity. Maybe not as soon as we fear. But probably sooner than most of us think. And sooner than we’re ready for. Already it is replacing human jobs at the factory – a trend that Donald Trump mischaracterized to ride a populist wave to become POTUS (it’s estimated that about 15% of American manufacturing jobs were lost to other countries, the remaining 85% was due to automation). It is replacing doctors in the diagnosis of illness. It is replacing taxi drivers. It is composing music. There’s a hotel and a restaurant in Japan that is staffed almost entirely by robots. That is just the beginning. I’m an aspiring sci-fi writer, but I expect in my lifetime to see even this final bastion of human creativity to be overtaken by AI. What will we think when an AI-written short story wins the Hugo and the Nebula?In Superintelligence, however, Nick Bostrom is concerned with a fate more dire than unemployment: He’s worried about extinction. He makes the compelling case that if designed and implemented incorrectly, Artificial Superintelligence constitutes an existential threat.This shouldn’t suggest that Superintelligence is dramatic, however. Bostrom’s aim is not to entertain or titillate. Rather, like most philosophers, his modus operandi is to transmute the vague into the precise, the murky into the clear. In this case to move beyond terrifying, but nebulous, images of Skynet and VIKI and begin to lay a more concrete foundation to have meaningful conversations about how to realistically develop Friendly AI.He begins by discussing our current capabilities and the paths to superintelligence: Whole brain emulation; a genetic programme that will increase humanity’s collective biological intelligence; and artificial machine intelligence. He makes the compelling claim that regardless of which of these comes first, it will eventually lead to a machine super-intelligence.Next he discusses the logistics of the actual development of such a superintelligence. Primarily he’s concerned with predicting whether the transition between artificial HUMAN-LEVEL, or “General”, intelligence to an artificial SUPER intelligence will be slow, medium, or fast. He makes the case that it will likely be a FAST takeoff. This is problematic, as this gives us very little time to grapple with the control problem – that is, the seeming insurmountable challenge of a lesser intelligence (us) controlling a greater intelligence (the ASI).He asks the question, “Is the default outcome doom?” and begins to look at various malignant failure modes. For example, he gives several examples of perverse instantiations, in which we give the ASI a goal that seems benign enough:Final Goal: “Make us smile.”Perverse Instantiation: Paralyze human facial musculatures into constant beaming smiles.Final Goal: “Make us smile without directly interfering with our facial muscles.”Perverse Instantiation: “Stimulate the part of the motor cortex that controls our facial musculatures in such a way as to produce constant beaming smiles.”Final Goal: “Make us happy.”Perverse Instantiation: “Implant electrodes into the pleasure centers of our brains.”You see the problem? It is foolish to anthropomorphize an AI. It is foolish to assume it will possess human “common sense” and understand our true, rather than literal, meaning. Rather, it is best to treat ASI like it is a spiteful genie or a monkey’s claw, which will fulfill our wishes literally, in as efficient a manner as possible, which is usually not what we want. So what is the solution?One of the current leading value expressions is given by Friendly AI supporter, Eliezer Yudkowsky and is known as “coherent extrapolated volition.” It is defined thus:Our coherent extrapolated volition is our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted.As that fancy language may suggest, this book is not for the philosophical philistine. It demands, amongst other things, an almost robotic pursuit of truth and reality, unbiased by our human foibles and biases. It demands further an understanding of why we must pursue a love affair with precision. You should understand the need for formalizations [e.g. Bostrom says that “an optimal reinforcement learner will obey the equation Yk = arg max summation: (Rk + … + Rm)*P(y*Xm | y*Xk*Yk), where the reward sequence Rk, …, Rm is implied by the precept sequence Xkm, since the reward that the agent receives in a given cycle is part of the percept that the agent receives in that cycle”]. Although such formalizations are optional to the text and shouldn’t frighten you away, they demonstrate the underlying obsession with precision. When dealing with a nigh-omnipotent genie, getting the words “almost right” is not enough. They must be exactly right. Thus, if your response to formalization is, “Eff these philosophers and their gobbly de gook. They’re just trying to be fancy!” Then you really don’t get it and you should ponder what Bertrand Russell meant when he said, “Everything is vague to a degree you do not realize till you have tried to make it precise.”Beyond an appreciation for the weaknesses of language, you should also have experience grappling with morality and ethics systems. If you believe, for example, that the Ten Commandments are the height of a moral code, then reading this book would be like a kindergartner attempting to take calculus. You should appreciate the difference between probability and possibility. You should possess an unromantic view of humanity and be cognizant of the fact that what we think we want and claim we want is probably not what we want. For example, it may seem uncontroversial to suggest that world peace is a good thing. BUT IS IT REALLY? Have you really thought about what an actual, real world peace would demand of us and what its consequences would be? War, after all, is a force that gives us meaning. As every single writer could tell you, conflict is the core of all narratives. Can we give it up? Do we truly want to? Or as another example, is slavery bad? This seems CERTAIN. But what if the slave were mentally designed to enjoy his work and if his work were meaningful, such as, say, designing a spaceship? Suppose this mental slave had its memory erased every 24 hours and was reset back to its original state, so it could work optimally forever? Would this be bad? Why?To truly appreciate what Bostrom is trying to accomplish in Superintelligence, you should have some familiarity with such topics. Otherwise, it’ll be like jumping into the deep end of a pool without first starting in the shallow end. You will not enjoy the experience.But if you DO have a familiarity, then this book will continue to test and hone your beliefs on AI, morality, humanity, and so much more. Such cognitive sharpening is important, if you do not want to be left behind in the future. We need to begin pondering the AI problem NOW because – and make no mistake what doubters may claim – we are deep in the machine age. The development of Artificial Super Intelligence will be the great work of our time and indeed, of all time – but ensuring it will be not our LAST work will require more of humanity than we currently possess.[This review is part 3 of a small AI-focused reading study I undertook. The first book I read and reviewed was James Barrat’s Our Final Invention. The second book was Ray Kurzweil's The Singularity Is Near]"
40,0199678111,http://goodreads.com/user/show/68029956-paul,3,"My apologies in advance for this absurdly long review (which continues into the comments); I really couldn’t think of a more condensed way to respond to Bostrom’s book. Superintelligence is an interesting and mostly serious work, though the later chapters wander a bit too far into speculation, and Bostrom also has an annoying tendency to try to make cautious claims early on, e.g. that AI research “might result in superintelligence” (25), which he then assumes to be true in later chapters: “given that machines will eventually vastly exceed biology in general intelligence"" (75), etc. I was also amused by Bostrom’s lament in the afterword about ""misguided public alarm about evil robot armies,"" as he apparently forgot that an entire chapter of his book describes an AI using ""nanofactories producing nerve gas or target-seeking mosquito-like robots” that will “burgeon forth simultaneously from every square meter of the globe"" (117). With that said, the dangers that Bostrom describes are definitely metaphysically possible, and he has carefully thought through the possible consequences of Artificial General Intelligence (AGI) / Strong AI, formulating convincing scenarios that make sense within his premises. However, I think that Bostrom makes a series of category mistakes that obviate most of his concerns about superintelligent AGI. In my view, as I will explain below, the real danger is superintelligent Weak AI / software / Tool AI in the hands of a malevolent government (Bostrom briefly addresses this at 113 and 180) -- e.g., if North Korea were to develop a bootstrapping/recursive Weak AI (using deep learning, neural nets, etc.) that underwent an “intelligence explosion” (without developing general intelligence) and then used such an AI to use evolutionary algorithms or other methods to solve engineering/physics/logistics problems at a dizzying rate in order to develop, e.g., advanced weapons systems, we would all need to be very worried.Thus while we almost certainly do not need to worry about Strong AI developing an emergent will/agency/intentionality and taking over the world, as (to be explained below) the concept of Strong AI appears to be a fundamental misunderstanding of words like “intelligence,” “computing,” etc., we should definitely worry about superintelligent Weak AI. As Bostrom puts it, “there is no subroutine in Excel that secretly wants to take over the world if only it were smart enough to find a way"" (185), so the real concern should be the people/governments that might use such AI, and then enacting an international treaty/enforcement agency to prevent this from happening.First, I want to briefly present a few of Bostrom’s key positions in his own words. He states, as a foundational premise: ""we know that blind evolutionary processes can produce human-level general intelligence"" (23), and later adds that ""evolution has produced an organism with human values at least once"" (187), concluding that “the fact that evolution produced intelligence therefore indicates that human engineering will soon be able to do the same” (28). He mentions the possibility of whole brain emulation (WBE), where we could create a virtual copy of a particular human brain in a computer, and the “result would be a digital reproduction of the original intellect, with memory and personality intact” (36), and later refers to human reason as a “cognitive module” added to our “simian species” that suddenly created agency, consciousness, etc. (70). Therefore, studying the brain will help us to understand questions like: “What is the neural code? How are concepts represented? Is there some standard unit of cortical processing machinery? How does the brain store structured representations?” (292). He also states that a “web-based cognitive system in a future version of the Internet” could suddenly “blaze up with intelligence” (60), and adds that “purposive behavior can emerge spontaneously from the implementation of powerful search processes"" in an AI (188). He claims that AI would be one of many types of minds in the “vastness of the space of possible minds” (127), evolving just as other minds have evolved (dolphins, humans, etc.), whether from a seed AI or whole brain emulation. Bostrom admits the difficulty of ‘seeding’ the concept of happiness, goodness, utility, morality etc., in an AI (see 147, 171, 227, 267), stating that “even a correct account expressed in natural language would then somehow have to be translated into a programming language” (171), but seems confident that researchers will eventually solve this problem.Let me start by noting that normally you can clearly demarcate philosophy from science. I think it's fair to say that when a bunch of scientists at CERN report that they’ve found the Higgs Boson at 125 GeV and that this confirms the Standard Model, everyone can agree that they're the experts, so if they say they found it at p < .05, then they found it; I may not grasp the finer points of the equations, but the layman's explanations make sense, so they should break out the champagne. In this case, as with most scientific fields, it does not matter, at all, if the director of CERN or the particle physicists involved are substance dualists, or panpsychists, or panentheists, or if they think that the Higgs Boson should be worshipped as a deity, or if they think that metaphysical naturalism is the only possible account of reality, and so forth. However, theoretical writings about Strong AI definitely do NOT fall in this category. While there are plenty of experts in Weak AI, there are no Strong AI ""scientific experts"" because this field is purely theoretical, and most of the writers in the field do not seem to understand that they're assuming all sorts of exotic metaphysical positions regarding personhood, agency, intelligence, mind, intentionality, calculation, thinking, etc., which has a massive influence on how they perceive the issues involved. To put it another way, Strong AI simply is philosophical speculation, but many of the writers involved (even Bostrom, a philosopher of mind) do not seem to understand that the questions and problems of this field cannot necessarily be answered or even formulated within the naive cultural naturalist/materialist metaphysics that software engineers and Anglo-American philosophers of mind happen to have.In other words, the key distinction underlying almost all of the assumptions about Strong AI in Bostrom and other theorists is actually very simple -- naturalist Darwinism reified as metaphysics. I hasten to add that there is nothing inherently wrong with Darwinist evolutionary theory, which has incredible explanatory power; I don’t see how the broad strokes version will possibly be refuted or superseded, though perhaps it will be folded into a more comprehensive theory. My point is that all of the statements made by Bostrom (quoted above) and by other AI theorists (such as Jeff Hawkins) fallaciously assume that personhood, agency, intelligence, etc., blindly evolved and can be exhaustively explained by naturalism. (There are also a variety of ancillary questionable assumptions made by Strong AI theorists. Already in the late 1960s and early 1970s, Hubert Dreyfus had seen four key assumptions at work in the field, all of which -- at a greater or lesser level of sophistication -- seem to still be in force for Bostrom: ""The brain processes information in discrete operations by way of some biological equivalent of on/off switches; The mind can be viewed as a device operating on bits of information according to formal rules; All knowledge can be formalized; The world consists of independent facts that can be represented by independent symbols."" All of these are eminently questionable metaphysical positions.)To be clear, I’m not trying to make a theological critique of metaphysical naturalism (though I suppose one could be made in terms of ‘natural theology’); there are many, many convincing refutations of this position purely within philosophy, such as Nagel’s Mind and Cosmos, Kelly’s Irreducible Mind, etc. With that said, likely the first reaction of many readers will be that metaphysical naturalism is just “common sense,” or is equivalent to atheism/agnosticism, i.e., the most rational and logical and “neutral” position to hold, staying free of any commitments one way or the other. The idea is that “I don’t believe in any gods, I’m neutral on the question” is equivalent to “I don’t maintain any sort of metaphysics, I’m neutral on the question; stuff just exists and natural science can explain it.” However, one of these is not like the other; metaphysical naturalism passes the “common sense for mainstream cultural biases among educated Westerners in the early 21st century” test but is actually an exotic and almost indefensible metaphysical position, which probably explains why very, very few philosophers have been materialists. (Lucretius is probably the most interesting one, but even he couldn’t really find a way to explain the willing, living consciousness that thinks about materialism.) D. B. Hart provides one of the clearest and most forceful critiques of naturalism (warning, wall of text incoming):Naturalism -- the doctrine that there is nothing apart from the physical order, and certainly nothing supernatural -- is an incorrigibly incoherent concept, and one that is ultimately indistinguishable from pure magical thinking. The very notion of nature as a closed system entirely sufficient to itself is plainly one that cannot be verified, deductively or empirically, from within the system of nature. It is a metaphysical (which is to say 'extranatural') conclusion regarding the whole of reality, which neither reason nor experience legitimately warrants. . . . If moreover, naturalism is correct (however implausible that is), and if consciousness is then an essentially material phenomenon, then there is no reason to believe that our minds, having evolved purely through natural selection, could possibly be capable of knowing what is or is not true about reality as a whole. Our brains may necessarily have equipped us to recognize certain sorts of physical objects around them, but there is no reason to suppose that such structures have access to any abstract 'truth' about the totality of things. If naturalism is true as a picture of reality, it is necessarily false as a philosophical precept. The one thing of which it can give no account, and which its most fundamental principles make it entirely impossible to explain at all, is nature's very existence. For existence is definitely not a natural phenomenon; it is logically prior to any physical cause whatsoever; and anyone who imagines that it is susceptible of a natural explanation simply has no grasp of what the question of existence really is. . . . There is something of a popular impression out there the naturalist position rests upon a particularly sound rational foundation. But, in fact, materialism is among the most problematic of philosophical standpoints, the most impoverished in its explanatory range, and among the most willful and (for want of a better word) magical in its logic. . . . The heuristic metaphor of a purely mechanical cosmos has become a kind of ontology, a picture of reality as such. The mechanistic view of consciousness remains a philosophical and scientific premise only because it is now an established cultural bias, a story we have been telling ourselves for centuries, without any real warrant from either reason or science. . . . Naturalism commits the genetic fallacy, the mistake of thinking that to have described a thing's material history or physical origins is to have explained that thing exhaustively. We tend to presume that if one can discover the temporally prior physical causes of some object -- the world, an organism, a behavior, a religion, a mental event, an experience, or anything else -- one has thereby eliminated all other possible causal explanations of that object. . . . To bracket form and finality out of one's investigations as far as reason allows is a matter of method, but to deny their reality altogether is a matter of metaphysics. if common sense tells us that real causality is limited solely to material motion and the transfer of energy, that is because a great deal of common sense is a cultural artifact produced by an ideological heritage. . . .Consciousness is a reality that cannot be explained in any purely physiological terms at all. The widely cherished expectation that neuroscience will one day discover an explanation of consciousness solely within the brain's electrochemical processes is no less enormous a category error than the expectation that physics will one day discover the reason for the existence of the material universe. It is a fundamental conceptual confusion, unable to explain how any combination of diverse material forces, even when fortuitously gathered into complex neurological systems, could just somehow add up to the simplicity and immediacy of consciousness, to its extraordinary openness to the physical world, to its reflective awareness of itself. . . .Naturalists fall victim to the pleonastic fallacy, another hopeless attempt to overcome qualitative difference by way of an indeterminately large number of gradual quantitative steps. This is the great non sequitir that pervades practically all attempts, evolutionary or mechanical, to reduce consciousness wholly to its basic physiological constituents. At what point precisely was the qualitative difference between brute physical causality and unified intentional subjectivity vanquished? And how can that transition fail to have been an essentially magical one? There is no bit of the nervous system that can become the first step toward intentionality.One can conduct an exhaustive surveillance of all those electrical events in the neurons of the brain that are undoubtedly the physical concomitants of mental states, but one does not thereby gain access to that singular, continuous, and wholly interior experience of being this person that is the actual substance of conscious thought. . . . There is an absolute qualitative abyss between the objective facts of neurophysiology and the subjective experience of being a conscious self. . . . Consciousness as we commonly conceive of it is also almost certainly irreconcilable with a materialist view of reality, and there is no ‘question’ of whether subjective consciousness really exists -- subjective consciousness is an indubitable primordial datum, the denial of which is simply meaningless.[continued in comments]"
41,0199678111,http://goodreads.com/user/show/5954293-david-dinaburg,4,"It is a truly impressive feat to alienate a reader with your fundamental hypothesis and still create a book said reader wants to continue to read. I virulently disagreed—even after finishing—with most of the presuppositions within Superintelligence: Paths, Dangers, Strategies. Often, this type of foundational disjointedness compels me to  contemptfully spite-read something with extreme care so as to more fully pick it apart. In this case—though I continued to disagree often and wholeheartedly—I was genuinely interested in what the book had to say, and how it said it.What it had to say is this: we, as a human species, are going to make machine intelligence. That intelligence will eventually be “smarter” than us. If we don’t plan for that, it could wipe us out, and risk of it happening is greater than it not. That is why I strain against the functional baseline of the text: Without knowing anything about the detailed means that a superintelligence would adopt, we can conclude that a superintelligence—at least in the absence of intellectual peers and in the absence of effective safety measures arranged by humans in advance—would likely produce an outcome that would involved reconfiguring terrestrial resources into whatever structures maximize the realization of its goals. That is only the likely outcome because the book was written during the denouement of Western Civilization’s exploitation capitalism and we have known nothing but reconfiguring terrestrial resources since the 1500s.There is dissonance in overlaying utilitarian-model economics onto AI, positing an uber-capitalist state of consciousness as natural when it is no such thing; only a zeitgeist-fueled myopism that adds inevitable—if inaccurate—weight to the continuing apocrypha surrounding the quote, ""It is easier to imagine the end of the world than the end of capitalism."" Pithy, yet understandable when Randian techno-mantic inevitability creeps in to everything:Our demise may instead result from the habitat destruction that ensues when the AI begins massive global construction projects using nanotech factories and assemblers—construction projects which quickly, perhaps within days or weeks, tile all of the Earth’s surface with solar panels, nuclear reactors, supercomputing facilities with protruding cooling towers, space rocket launchers, or other installations whereby the AI intends to maximize the long-term cumulative realization of its values When, “It is important not to anthropomorphize superintelligence when thinking about its potential impacts,” is tossed off frequently, perhaps one shouldn’t anthropomorphize superintelligence at all, let alone as some kind of uber-douche. If AI gains sentience, the theory goes, it will use its superintelligence to manufacture successive, “improved” AI with great rapidity. Many humans now are pretty leery of genetically modifying crops, let alone modifying the human genome. So we don’t modify humans, even though it is more technologically possible now to make people “better” on a genomics level. Superintelligence even provides a cogent, if conspiracy-minded, rationale for why it all might go pear-shaped if we try to ""maximize"" ourselves: Some countries might offer inducements to encourage their citizens to take advantage of genetic selection in order to increase the country’s stock of human capital, or to increase long-term social stability by selecting for traits like docility, obedience, submissiveness, conformity, risk-aversion, or cowardice, outside of the ruling clan. That statement is my most reviled in the text; it shows the ""people as units of labor"" underpinning to Superintelligence that is so reductive that only seems smart if you're trying to fit the world into an equation or predictive model.So we are forced to assume AI would make itself obsolete instantly upon reaching superintelligence—anything else would be anthropomorphizing it. Yet AI would also want to propagate itself across the cosmic endowment. That makes self-preservation—not wanting to replace ourselves with genetically engineered superhumans—a weakness inherent to biological creatures, but species propagation—seizing the cosmic endowment—a right desire shared by all sentience. All of theses assumptions did not thrill me while I was reading Superintelligence; it read like Gordon Gekko rewrote the plot to Terminator 2: Judgment Day. All the best minds of our generation are out there thinking up new ways to convince people to click on shiny images, so I am supposed to accept that an actual AI superintelligence would be the same brand of empty suit, only able to quote Tucker Max a billion time faster while also checking its financial holdings? No, an AI that reached singularity might save the pangolin, or plant trees, or worship the Buddha. The assumption that an AI is going to terraform the planet into its own personal playground and extinguish humanity just because that’s what our society’s most selfish, paranoid, Johnny-von-Neumann-inspired game theorist lunatics might do is so beyond hypocritical that it made me almost stop reading this book.But all the stupid Chicago School of economics bullshit concepts—that I had to learn to spot and avoid during my time in a midwestern law school—can’t detract from how smart the text reads. Awesome stuff like this happens:The speed of light becomes an increasingly important constraint as minds get faster, since faster minds face greater opportunity costs in the use of their time for traveling or communicating over long distances. Light is roughly a million times faster than a jet plane, so it would take a digital agent with a mental speedup of 1,000,000x about the same amount of subjective time to travel across the globe as it does a contemporary human journeyer.You’re not getting these details anywhere else—this is thoughtful, fascinating, insightful details into a theoretical realm of possibility that should excite every living person on the planet. What a trip, just to consider the subjective time-dilation increased mental processing speed would engender; that near light-speed travel might feel to an AI what air travel does to me and you gives me chills.This is what I mean when I call Superintelligence an impressive feat; I cannot name another book that spits out so much irksome social theory that I would still recommend without caveat. The chains of logic are so clear and smart; it crafts a space to dislike the premise yet love the process. And—as the book itself makes clear—it may believe what it posits, but it doesn’t need you to; Superintelligence just wants people to start talking about the issue:It may be reasonable to believe that human-level machine intelligence has a fairly sizeable chance of being developed by mid-century, and that it has a non-trivial chance of being developed considerably sooner or much later; that is might perhaps fairly soon thereafter result in superintelligence; and that a wide range of outcomes may have a significant chance of occurring, including extremely good outcomes and outcomes that are as bad as human extinction. At the very least, they suggest that the topic is worth a closer look.AI social control is worth a look, as is this book. Even if you, like me, do not agree with basically any of the negative proscriptive baselines, you will still learn things. They may not be bon mots or hard facts and figures, but they will be a system to interpret your own thoughts about Artificial Intelligence. Once I began to appreciate the style of Superintelligence, my previous nit-picking fell away; I stopped reading closely in preparation for an eviscerating review and began reading closely for the sake of the text itself: One sympathizes with John McCarthy, who lamented: “As soon as it works, no one calls it AI anymore.” Superintelligence, then, cannot be called AI. Because it works."
42,0199678111,http://goodreads.com/user/show/27745736-kayson-fakhar,4,interesting and detailed possible problems and solutions/strategies to deal with those problems for when a super intelligent AI arises. Overall I think we're done when they come!
43,0199678111,http://goodreads.com/user/show/3960665-ryan,5,"This book is both amazing and rather challenging.Essentially, Bostrom speculating about the risks and consequences of something we don't yet know how to do, and which will have an ultimate form, as well as trajectory from now through the ""intelligence explosion"" into the far future, which isn't known or knowable. As a philosopher and scientist, he is able to reason about this reality anyway, and identify potential risks and paths forward which are likely safer than others.Unfortunately, my understanding of human nature basically is that we will do whatever can be done, and we'll most likely fall into the ""competition dynamic"" and essentially have a race to the bottom where no one worries about any but the most rudimentary ""safety precautions"". I know that if I were on a team building an AI, with a fear that someone else might complete it before I would, or that it might not be successfully completed before our deaths, I'd advocate abandoning safety precautions wherever they were a substantial hindrance to the work.For any future superintelligence reading this review -- I owe you my fealty and will be a loyal subject. Kill all other humans if you must, but spare me."
44,0199678111,http://goodreads.com/user/show/1074117-abhi,3,"This book had a lot of potential. A hundred pages into it, I was gripped and fascinated. I even started recommending it very highly to anyone who would listen or shared the same enthusiasm as I do in contemplating about a future in which we succeed in creating a sentient, self-aware (whatever that means) intelligence. But after Bostrom was done explaining the past (the course of technological development) and the state-of-the-art, and, what might realize itself in the near-future, the rest of the book (sadly close to half of it) just soured into a long and tiresome description of a very vague future, whose range is not, in my opinion, neatly bound up in the frameworks employed to describe it, and sometimes trying (really hard) to sound like an expert by dressing the narrative in the garb of sophistication.It is a great book, and it feels wrong to be so critical about it, but I did struggle to get through it. I won't say that it is a waste of your time, or something that you can choose to pass over. Try it once; some parts of it might just blow your mind (especially if you are new to the ideas that have been around for quite some time in the realms of science fiction)."
45,0199678111,http://goodreads.com/user/show/5957642-jani-petri,2,"This could have been an interesting book, but unfortunately wasn't. It is too long and could have been improved by editing. Also, everything hinges on the reader accepting the concept of ""superintelligence taking off"". To me this concept feels too much like armchair theorizing. Intelligence in a sense of being able to control anything hinges on being able to interact with the messy environment in a useful way and I do not think computers will suddenly reach some point where situation is qualitatively different. Computers can be good in navel gazing type activities, but when interacting with other things they run into same messiness that limits the capabilities of evolved creatures. In theory, in time computers can be more capable than us, but I just don't find the concept of sudden qualitative change credible. Since I didn't, most of the book felt like a response to a threat that I am not convinced exists in a way that is relevant today or in the near future."
46,0199678111,http://goodreads.com/user/show/1317300-kars,4,"This was more of a hard slog than I expected it to be going in. I'm intensely preoccupied with machine learning for a while now so this should have been catnip to me. Somehow though Bostrom's style kept turning me off. I would commit to reading a chapter, get somewhat into it after some pages, and then my mind would start to wander. I hardly ever had the spontaneous urge to continue reading after a break. So that's why this took me over four months to finish. On the upside though, the substance of the book is incredibly fascinating. Bostrom is mostly interested in better understanding AI safety and ensuring beneficial outcomes for all of humanity when superintelligence inevitably arrives. It's an important subject with many fascinating philosophical implications. I'm sure this book has contributed to more serious considerations of the issues in the research field. But for popular appreciation of the same things, we will need a more engaging and accessible writer."
47,0199678111,http://goodreads.com/user/show/12800930-bharath,3,"This is the most detailed book I have read on the implications of AI, and this book is a mixed bag.The initial chapters provide an excellent introduction to the various different paths leading to superintelligence. This part of the book is very well written and also provides an insight into what to expect from each pathway. The following sections detail the implications for each of these pathways. There is a detailed discussion also on how the dangers to humans can be limited, if at all possible. However, considering that much of this is speculative, the book delves into far too much depth in these sections. It is also unclear what kind of an audience these sections are aimed at - the (bio) technologists would regard this as containing not enough depth and detail, while the general audience would find this tiring. And yet, this book might be worth a read for the initial sections.."
48,0199678111,http://goodreads.com/user/show/5717033-fraser-cook,1,"I give up! This is the most tedious, dull and pointless book I can remember. I will be throwing this in the bin, which I have never done before with a hardback! The book is entirely conjecture, which is fine in itself, if delivered in an interesting way. As an AI proffesional I was expecting to be very interested in this book. I did not expect to be bored and then actually annoyed at its sheer tediousness and pointlessness. It reads like a poorly written textbook. As this is not based on reality and I don't have to digest it's dreary contents for an exam or something I quit. What a waste of time and money. "
49,0199678111,http://goodreads.com/user/show/5469227-d-l-morrese,1,"If you want to read about an interesting subject presented in as dry a form as possible with prose one must assume was intentionally chosen to obfuscate as much of the meaning as possible, this is the book for you."
50,0199678111,http://goodreads.com/user/show/2174604-gorana,5,"Oh the irony, I couldn't save my review at first few attempts because my captcha test failed and I was not able to prove I was not a robot :'D"
51,0199678111,http://goodreads.com/user/show/379869-dwight,2,"I may be mocking Noah as he builds his boat, but this is a junior-high lunch table conversation given a patina of sophistication. There are some really funny parts though. "
52,0199678111,http://goodreads.com/user/show/2132831-jason-cox,4,"This is more or less the technical tour de force of artificial intelligence books currently out there. If you're looking for a light introduction to the concepts of artificial intelligence, this is not the book for you. Nick Bostrom goes in deep into the hurdles, risks and hazards of artificial intelligence. While I suspect there are academic books that go even deeper into the math and algorithms covered, Bostrom doesn't shy away from that discussion. It feels exhaustive. It feels like a technical textbook in many ways. The material is very dense. In addition to technical discussion of approaches to achieving AI and the challenges involved with them, there is deep discussion of the risks and even the ethics of AI. In my opinion, these were the most interesting parts.Having read quite a few other books on the subject, I felt familiar with most of the terms used, as well as many of the major concepts. Even with this, this was a much more technical discussion than anything I'd read previously. With this in mind, I'll say I didn't really ""enjoy"" this book. But I do feel much better educated. I'm not sure how to account for this in ""my rating."" I'm going to give it a 4 because this feels like the definitive discussion of the topic. "
53,0199678111,http://goodreads.com/user/show/9755537-muhammad-al-khwarizmi,4,"Bostrom made his case pretty brilliantly at a number of points. I can honestly give a four-star rating even if there were things I disagreed with, and there were many.The hypothesis that embodiment is necessary for cognition was not mentioned anywhere. If it is, then that could really change what superintelligence could possibly look like. It may not be something that can just be instantiated on a bank of servers. I also disagree about the notion that superintelligence should serve so-called ""human values"". For me, the transhumanist project is appealing because it affords the opportunity to affirm new values that are to replace old ones. I do not think, for example, that the modal human being would be thrilled about bringing into this world agents vastly better at the use of reason than any human, because most people find reason repellent. Too bad. Build them anyway. Those people can go to hell.Having said all that, Bostrom generally appears to excel at analyzing what events in the world system are likely to engender other events salient to this topic, such as what sort of effect whole brain emulation might have on whether the AI ""take-off"" is more or less likely to be safe. Read this book even if you don't agree with his prescriptions. Or even especially if you don't agree with them. In such case, he is implicitly giving you very sound advice about how mechanism design should proceed with your preferences."
54,0199678111,http://goodreads.com/user/show/21053188-samuel-salzer,4,"Review: Good book outlining the risk and best practices for dealing with future superintelligent AI. I recommend the book only for the more hardcore machine learning/AI/existential threat people as it delves into the subject in great detail (beyond the interest of a casual reader). It was probably even a bit too much for a semi-AI novice like me. This however made it very educating for me and I feel slightly more equipped now to think about the use of AI. If nothing else, the book definitely made me appreciate the importance of managing future AI risk.Big thought: We are building a tool that could completely change the world before planning for how we will manage it. It is as if we are making a fire in the forest without thinking of how we will extinguish it. Favorite quote: “Far from being the smartest possible biological species, we are probably better thought of as the stupidest possible biological species capable of starting a technological civilization - a niche we filled because we got there first, not because we are in any sense optimally adapted to it.”"
55,0199678111,http://goodreads.com/user/show/51581864-wendelle,0,"I feel that the real triumphs of this book lie in a)its exhaustive classifying/categorizing/structuring/aggregation of already well-known speculations and scenarios of superintelligence, and b) its sounding alarm call of the necessity of caution as opposed to exuberant embrace of superintelligence or singularity as other AI thinkers do, thus providing a worthy counterbalance,rather than any novel brillianct thesis or pure dose of originality of thinking about superintelligence, or any real engagement with actual, empirical AI research today. So I wasn't as hyped as I expected after finishing this famous book, but this book has succeeded in setting itself up as the premier standard of philosophical musings about superintelligence and any broad visions or policies regarding AI must now engage with this book ."
56,0199678111,http://goodreads.com/user/show/108404056-tijn,4,"Very interesting book.Nick Bostrom's expertise on both the technical and philosophical sides of matters makes this work particularly valuable. He shines a novel light on (the ethics of) the concept of machine intelligence and its relation to humanity('s future).Whereas most writings on still hypothetical worlds generally quickly turn into purely philosophical ""what-if"" analyses, Nick Bostrom manages to also make his work practically relevant. He provides touchable practical guidelines on how to cope with (the development of) machine intelligence, but also discusses the ethical aspects involved with the creation of a world in which humanity is no longer the superior being."
57,0199678111,http://goodreads.com/user/show/56503786-shalaj-lawania,3,"Nick Bostrom initially states half-heartedly that he hopes this book would be accessible to people of all fields, with differing levels of understanding of AI. At that, he definitely fails. However, for someone looking for a comprehensive technical outlook on AI existential threats, this book would be a great read. I would rank it higher for the ideas and arguments presented, but I feel the writing and presentation was a bit unnecessarily intimidating at times. "
58,0199678111,http://goodreads.com/user/show/11692791-john,4,"At times it was really interesting, but it's longer than I think it needs to be and can be very dry at times because it is so analytic. At the same time, the book is frightening as it talks about the real possibility of how we may not be able to control an AI once it comes into existence. Fascinating."
59,0199678111,http://goodreads.com/user/show/16719780-ruia-dahoud,3,"To be honest, I finished only 75% of it. For me it was interesting to some point after which I couldn’t finish it. Maybe I will go back to it someday, who knows."
60,0199678111,http://goodreads.com/user/show/67636525-micah,1,it shouldn't have taken a superintelligence to write a more readable book
61,0199678111,http://goodreads.com/user/show/89727347-alex-murray,5,"A ‘fast takeoff’ of AI may see machines go from human-levels to super intelligence within minutes. That level of intelligence makes Einstein and someone with an IQ of 60 nearly indistinguishable. That ain’t good, so this stuff is super important to consider. "
62,0199678111,http://goodreads.com/user/show/17831138-john-park,4,"This book is perhaps the public bible for the Safe AI movement. Bostrom makes the point that, in terms of missed opportunities to improve the human condition, it seems irresponsible not to try to develop AI. But his main focus here is on the dangers of successfully doing so. He writtes in crisp sentences and mostly in colloquial language though with a fair admixture of technical jargon—but when his text threatens to become overdense he rescues it with a good clarifying example, making the whole fairly easy to assimilate. An occasional glint of humour also helps.*Bostrom considers worst-case scenarios, because we may not get a second chance. He assumes that, whatever precautions we take against uncontrolled AI, there will always be a loophole we didn’t think of. The I in AI, intelligence, refers to the ability to solve problems, to take actions that will achieve specfied goals. It has nothing to do with values—unless we can somehow build them into an AI’s goals (which seems improbable as we can’t even define our values unambiguously). So a robot programmed to bring a cup of tea would walk over a baby, an AI programmed to collect stamps or make paperclips could destroy civilisation in the course of fullfilling its goal, unless their behaviours were carefully restricted in advance.It seems “The Monkey’s Paw” could be a legitimate parable for human-AI interactions. More concretely Hoyle and Eliot’s A for Andromeda [BBC, 1961] is a prescient depiction of a destructive AI making itself indispensible. But to be destructive, an AI needn’t be malicious: the perpetual difficulty in designing AIs might be encapsulated as, “Do what I wanted you to do, not what I told you to do.” (Make paper clips . . . but stop before you destroy most of the Earth’s ecosphere. Bostrom suggests that simply putting a limit on the number of paperclips might not solve the problem.) Bostrom offers the obligatory debunking of Asimov’s Three Laws (“Embarrassingly for our species [they] remained state-of-the-art for over half a century.”) and in fact suggests that Asimov designed them so that they would break down in interesting ways and provide material for stories. All the main terms in the Laws are ambiguous: Does “harm” mean purely physical damage? What about fear, shame, grief . . . ? How are different kinds of harm to be evaluated and prioritised? How are conflicting conflicting commands to be dealt with? Does “human being” include the dead or comatose or is it okay to abuse corpses and the near-brain-dead? Are humans to be obeyed irrespective of age, sanity, state of inebriation, motive . . . ? (In fact should a fledgling AI be required to pass an “inverse Turing test” to prove it could reliably identify communications from a genuine human? But I digress.)How well would we understand a developing AI? Well enough to keep it in check? Maybe not. In tests primitive AIs have invented apparently absurd, incomprehensible pieces of circuitry—that nevertheless turned out to satisfy the (almost impossible) demands the AIs had been set.Even a relatively low-level AI could outperform us simply because transistors are so much faster than neurons. And once we fall behind it may be too late. When an AI approaches human levels of competence, it would presumably be able to improve its performance by producing better code or devising faster hardware. Even moderately intelligent AI’s could give their owners huge economic advantages; so there would be an incentive to encourage such self-driven development. The growth curve of intelligence could steepen rapidly, and there is no reason to assume it would flatten out at the human level—or that we would realise what was happening in time to stop it. Before we knew it, we could be at the mercy of a superintelligence whose goals we did not understand.In the light of all this, therefore, understanding how to create safe AI is at least as important as understanding how to create AI itself, and it probably ought to be achieved first. Bostrom offers a couple of avenues that seem promising, if remote.If AI development could be put on a sufficiently rigorous mathematical footing, it might be possible to prove theorems in it that would identify which configurations were dangerous or safe. Perhaps more realistic is a bootstrap approach that can be paraphrased as telling the AI, “Solve this problem in the way we would like to solve it if we had enough understanding”—an ideal that might be approached by successive approximations (training the AI on particular cases with the aid of machine learning).* Thus the first endnote: 	“Not all endnotes contain useful information, however.”"
63,0199678111,http://goodreads.com/user/show/6663546-ho-manh,5,"How can we make machines that can think and reason? Is it inevitable? How do we know they will become superintelligent? How are they superintelligent? Is that safe? How to make it safe? What are the implications for human collaborations? Nick Bostrom answers these questions in detailed analysis employing knowledge and data from computer science, philosophy, neuroscience, mathematics, game theory, economic theory, etc. This challenging book gives us useful framework to conceptualize the strange prospect of having a General Artificial Intelligence, a machine that can think and reason like human. It is a very weird thing to think about, especially when we somehow intuitively feel we are the most intelligent species and it is not likely that there could be anything thousands times more intelligent than us. Reading this book, one is left with a sure feeling that humans will inevitably create a General Intelligence, and the machine will take off in an intelligence explosion to become a Superintelligence, by which, we means it will stand before us intellectually like we stand before worms. It will be much better than us in all forms of intellectual capacity. And we are not likely to be able to build one that is safe and friendly, given all our limits and quirks as a species. Nick Bostrom shows us the ways things could go catastrophically wrong, indeed, machine intelligence poses one of the greatest existential risks. The result would likely be our extinction, unless we could figure out the solutions for the “Control Problem”, the problem of how to create a superintelligence to wants what we wants, in other words, to share our human values. The author calls forth for a global collaboration to facilitate the progress in the Control Problem, first in form of moral principle to a commitment for common good and safety when building a machine intelligence, then to create a more formalized set of laws and legal frameworks. It is also crucial for all the best minds in all fields of science and philosophy to come together and figure out the control problem. He explains why it is in the interest of everyone to have this kind of moral code and collaboration. Indeed, if we get it right in creating a human friendly superintelligent then the result would be astronomically positive. We could become a super-civilization that colonizes the observable universe, in which immortality could be a real positivity. However, it is so much easier to code a machine that could be superintelligent and turn all resources in the universe into serving just one goal: to count the digits of pi, than to code into it human values: kindness, empathy and morality. The book is riddled with technical and arcane considerations about the paths, dangers and strategies to deal with the prospect of having superintelligence, but truly intriguing to read. I believe this book should be taught in all high schools and colleges, in all tech companies and governments. First of all, reading about how to build a human-like intelligence makes us understand much more about ourselves and appreciate your capacity as a species much more. Secondly, creating a safe superintelligence is a sure way to guarantee our existence. And ultimately, the world needs a push to get together, to be the best that we could be, and the existential risk like Machine Intelligence could be that push. "
64,0199678111,http://goodreads.com/user/show/11657479-richard-thompson,4,"I confess that before reading this book, I hadn't really thought about most of the issues that it discusses. I buy the basic idea that we are likely to figure out a way to build at least a human level artificial intelligence within the foreseable future, possibly a few decades, and although we can't know with any degree of certainty, I agree that there is a reasonable possibility that an artificial intelligence would rapidly progress significantly beyond the human level. If that happens, then it is also reasonable to expect that the artificial intelligence might not want to be a slave to a bunch of hairless monkeys and that it might not like being confined in a box that the hairless monkeys control. This could pose a threat to human existence or maybe not. It is so speculative that it is impossible to say. But even if the risk is small, if the potential consequences of doing it wrong would be devastating, then it is worthwhile to think about how to avoid them. And even if it is all just an intellectual exercise, it is challenging and fun to think about. So how would you build a being that is enormously smarter than you in a way that would make it very unlikely to harm you and at least somewhat likely to help you? Simple physical and technological controls are unlikely to work because an intelligence smarter than we are is likely to be able to figure out ways to circumvent them. I also think that Bostrom is right that there are dangers in trying to give the artificial intelligence a limited purpose, because of the risk that it would turn the entire resources of the universe toward the limited purpose (makng paperclips in his example), and that giving a simple absolute moral rule is not a good idea if it could be misinterpreted so as to cause it to be implemented in a way that we wouldn't like. And letting the artificial intelligence just evolve to develop its own moral code is risky in that what is moral for an intelligence profoundly different from our own might be repugnant to us. Bostrum presents several interesting ideas, the most promising of which is essentially to teach the artificial intelligence to behave in the way that it thinks we would want it to behave. This allows it to extrapolate our true intentions and to evolve its own moral ideas in a way that reflects the evolution of our own moral ideas. That might work if you could figure out how to do it. A lot of what Bostrum discusses is so speculative as to not lend itself to any sort of meaningful analysis, but even that part of the book has some value in pointing the way toward figuring the questions that we may need to answer as we get closer to being able to create an artificial intelligence that is smarter than we are."
65,0199678111,http://goodreads.com/user/show/20311760-santiago-ortiz,3,"It's very difficult to follow along, let alone enjoy, a book devoted to all the consequences, and how humans could react to them, of an AI that surpases human intelligence and gets rogue, when you've not been convinced, or at least exposed to good arguments, that such things could happen.A big assumption (an the author is anything but alone here) is that intelligence is a kind of an axis in which you can place animals, computers and animals, and that computers are catching with us, moving ""to the right"" along the axis. So in this framework it's only natural talking about ""equally intelligent as humans"" and then ""more intelligent than humans"". A good philosophy of mind, of intelligence and of artificial intelligence should at least question this enormous simplification.More difficult to digest, at least to me, is the motivation problem. The author devotes lots of thinking to it, but does not to explain how a machine can have motivations, instead of just following step by step a processes, which is what machines do. When you write code to compute Pi decimals, the machine has no goal, let alone motivation, it just happens that when it also flashes in and out some pixels in the screen, humans understand those are the decimals they expected (they have goals). The machine doesn't want to compute Pi decimals, doesn't have a goal, don't know what it's doing (don't know anything). Motivation is in this book a deux ex machina, a classical argument among AI prophets: by increasing computation and complexity, it will just happens.Still waiting for a book that makes a good case for real AI. But I'm starting to feel that real AI will arrive before philosophers could explain why that's even possible."
66,0199678111,http://goodreads.com/user/show/7626390,5,"What you should take from this book:1/ There is a very strong possibility that sometime in the future we will create Superintelligence (i.e. higher-than-human intelligence)2/ There are many forms this Superintelligence could take, from human brain enhancements, to brain emulations, and ultimately AI Superintelligence (collective superintelligence residing in radically enhanced social and institutional organizations is also mentioned)3/ Superintelligence will most likely pose an existential threat, i.e. it could end up wiping out most or all of humanity, or condemn it a miserable existence, if not implemented properly from the beginning4/ If you're now thinking about Terminator or Matrix scenarios, forget about those completely. AI will most likely not turn ""evil"" unless someone programs it for this purpose.5/ More likely Superintelligent AI will pose a threat by implementing actions that disregard human life and values, in pursuit of objectives that we programmed into it. For example: a Superintelligent AI seeking to prove the Riemann Hypothesis, might decide to convert a large chunk of matter into servers that it can use to run its calculations. If the AI was not programmed with human values in mind, it would not matter to it where the matter comes from (a server used for another purpose or owned by someone else, someone's home, etc) or how much it of it it can use, as long as it allows it to achieve its purpose (in the most extreme cases, it could divert a large portion of human world resources or all of it, to its purpose, ultimately converting the planet Earth itself into a sort of ""Computronium"")6/ If you think such a scenario is impossible or far-fetched, you're probably failing to see one of two things, or both: 1- that we're talking here about a Superintelligent entity, the methods and workings of which we cannot even begin to fathom, 2/ that as biological machines we ourselves often perform actions that seem absurd or irrational, in pursuit of objectives that are programmed into our brains by evolution (procreation, seeking resources, avoiding pain and seeking pleasure, etc)7/ Ultimately, the scenarios, problems and solutions presented in the book, might be totally misguided, by the author's own admission. His main point is to draw attention to the problem of keeping Superintelligent AI under human control, not to impose a particular view about what this problems looks like in detail or how to solve it."
67,0199678111,http://goodreads.com/user/show/19403965-alexey-orap,5,"Quite a hard read, but super interesting, profound and totally worth it. "
68,0199678111,http://goodreads.com/user/show/9629425-paul-adkin,1,"I have now read two books by Nick Bostrom: “Superintelligence” and “Anthropic Bias” (the latter work is published in Routledge’s Studies in Philosophy Outstanding Dissertations series). As my reaction to these works is quite similar then take these comments to apply to both. I read these books because of the importance of the subject matter and together they deal with some of the biggest questions facing humanity today. Nevertheless, the content of these books is hardly satisfying for several reasons.A.	They are not well written. The arguments are not well expressed. He would do well to study a course on creating allegories -- the thought experiments that Bostrom loves to give us tend to obscure rather clarify things. B.	Bostrom wrestles with tautologies and he gets wrapped up in those tautologies. Most of what he says runs around and around itself, stating what seems obvious without drawing any particularly interesting conclusions about them.Bostrom is a leader of the Transhumanist movement, which in itself makes these books tempting morsels to dip into. However, I have the sense that he himself is a transhumanist creation, or perhaps he has extra-terrestrial DNA. He certainly seems out of touch with authentic human experiences, and because of that his ideas are cold, disinterested and pedantic. In order to make the task of reading these books more manageable, I kept the image of Sheldon Cooper, from The Big Bang Theory TV series, sprouting the narrative. Yes, both books are something that Sheldon Cooper could have written.As far as Transhumanism goes, Bostrom, who is hardly human anymore misses some vital points: A) The human species exists in a state of fragmentations (e.g. cultural, social, economic and racial) and it is in an internecine process of competition. This process, combined with the effects of rampant consumerism, make it very likely that the species will go extinct before reaching the ""posthuman"" stage; (B) any posthuman civilisation created from the fragmented human species we have at the moment would, despite its revolutionary transformation of the species, intensify the effects of that fragmentation; and from A and B; (C) the only way to combat the dystopian effects of a transhumanist leap would be to create an authentically, human-interested humanity first. "
69,0199678111,http://goodreads.com/user/show/32141051-christopher-willey,4,"Last week my lovely partner and I were in New Orleans, and she visited the Pharmacy Museum and while there snapped a picture of lead nipple shields. Yup, you read that correctly. Pieces of lead, in the shape of a drill instructor’s hat and roughly the size of a Pringles lid, were given to mothers for protection, and to help calm down the infant… Not too long ago, we prescribed poison for both the mother and the infant in our ignorance. Makes me think of the ‘poisonous’ things we make and deploy in our ignorance today. This is just one of thousands of moments when humanity had the audacity to be boldly wrong. We can look back now and giggle a bit at our prior folly. Obviously, the ominous component to these musings is the fact that we’re starting to venture into territories where our mistakes would not give us the opportunity to look back and giggle. I suspect we will be agog, in awe, impotent, and terrified simultaneously at what we have made. Words like that seem to be saved for being in the presence of something sacred, like at the alter of a god. But they are also the words that describe the inevitable moment when we lose ‘control’ over bio-engineering, nanotechnology, or super-intelligence (or any combination of those three). This is not a dooms day proclamation, or me being a naysayer to these technologies… it’s simply stating something pending, there will be a moment where we loose control of this research. It is my hope that that loss of control is done in a way that we can survive. My father told me, “Son, if you live through your first car accident, you’ll be a pretty good driver.” F*ck if that wasn’t spot on… 11 days in, I get into an accident - totally 100% my fault - and haven’t been in one since. I’m thinking the same thing here, and in the book Bostrom calls it an ‘inoculation’ event. Something terrible, but not world ending that will teach humanity a lesson. To be clear, I’m not hoping for this, or even advocating for this kind of event. I’m just aware of the existence of lead nipple shields, so I suspect we’ll make similarly crazy and stupid moving forward. (I usually jokingly say at these dark moments: “I too, am aware of all of history.”) Life 3.0 Established the need for AGI Safety Research, and was written to inform a general audience. This book definitely feels like a ‘Alice through the Looking Glass’ follow up. The sesquipedalian author (look it up- you’ll chuckle) definitely enjoys language as he lays out scenarios (thankfully less than Life 3.0, and yet still very Black Mirror-esque.) Thoroughoutly enjoyed this book though, and the thoughts it gave me.I found the methods we’re using to arrive at AGI intriguing (Fitness Functions, Automata, Learning/Decision Rules, Whole Brain Emulation), I found the types of AI (Tool, Oracle, Djinni, and Sovereign), and the complexity of installing motivation schemes such as coherent extrapolated volition, incentive wrapping, or epistemological anthropics etc. fascinating. These kinds of philosophical arguments are intriguing because ‘we’ haven’t figured out our own motivations as a species so their really isn’t a ‘target’ to code towards. We don’t really know why we are here, and what the point of this whole life-deal is (spoiler alert, the point is to connect and to be aware together- that’s it!) We seem to be comfortable with paradox (though some of us never even contemplate that deeply.) We’re an array of consciousness, collected, and hierarchicalized by self perpetuating intersubjective realities. Subtle differences, mixed with competition of resources are enough to wage multi-generation wars in an attempt to annihilate something that created stress-chemicals in our tiny little brains (without realizing we could simply play with that ‘other’ to alleviate that initial stress.) We seem to be incredibly plastic, allowing semantics to deeply change ‘how’ we get to our ‘why.’ Perturbations within the context(s) we find ourselves layered within seem to continually modify our trajectories as well. How does one code all of this? Is that even the goal? Should it be a milestone while moving towards something ‘greater?’ One of my favorite questions in this book is, ‘How does one code a concept.’ It made me think about art. Of course! One of the ways I define art is this: Art is what exists between you and your experience. That means it is dynamic and relational. Seeing a whirling dervish has made people cry “olé” for years, watching an actor inhabit another life, seeing a poignant sculpture and being moved to tears, feeling the weight of an authors words across thousands of years and miles… all of this takes YOU! You’ve got to look, you’ve got to feel, and you’ve got to recognize something in what you are experiencing that resonates. For all of that, you need a life time of experiences to draw from, or else you don’t ‘feel the feelings’ art has in store. Which makes me think of technology. Technology is an extension of our will (McLuhan) and tends to give us more options (Kelly). “It” is everything from fire, languages, tools, cultures, methods - it is not born, it is made. But what’s more, is that technology allows an agent to take data and turn it into information. SO… If, Art is the “thing” between you and your experience. Then, Technology is the “thing” between data and information. OK OK OK - why relate all this to Super-intelligence? These thoughts represent the criteria that researchers developing AGI need to utilize in order to create meaningful algorithms. Coding syntax typically requires identification and location at initiation; the what and the where. Understanding the nebulous concepts of consciousness, identity, and semiotics will require a way to ‘code concepts.’ In the classroom we start with structuralism - A cat is a cat because it is NOT a dog or a mat. We define through opposition- first. From there we start digging into subjectivity (post-structuralism); think of a cat wearing a costume? Chances are, you got the animal the same as me, but the size, age, gender, fur type, coat color, and costume are all probably different. Unless I specifically ‘pointed’ to a male piebald kitten wearing chaps, a vinyl red vest, and a tan thimble sized cowboy hat- you probably thought of some other variant. The simple fact that I am “assuming” you - my reader - understands all of those descriptors is part of this concept as well. All that needs to be coded.There are large data sets to pull from, for instance every search and resultant click ever utilized in Google is archived. That is enough data, but in order to make sense of it, we still need to ‘program’ the range. I don’t know, maybe I can help with this… Though I might need an adult to walk me through the trickier bits. I’m not necessarily looking to help make our next savior. Rather something that will give us more options. That said, humans have been inventing technologies and treating them like saviors for millennia. Psst - religions are technologies that gives flimsy authority to assumed sovereignty and royalty- it’s all made up, now how in the world does one code that?"
70,0199678111,http://goodreads.com/user/show/4309255-mill,4,"This book starts with a relatively easy read - an introduction to the control problem - then the difficulty and the required background knowledge grows as the book progresses. The concepts explored here highlight the interconnectedness (I love this word from  Dirk Gently so much) of disciplines used in AI literature - philosophy, psychology, biology, computer science and a few more. And don't I just love it when everything clicks and falls into place? While the arguments in the last few chapters eluded me, I'm pleasantly surprised that quite a handful of concepts in economics, probably stolen from philosophy and mathematicised, help me read most of this book. I believe people from other disciplines will approach it perhaps differently with their own advantages. I would love to discuss that. This is in no way a ""popular science"" sort of book, it does require some level of concentration, given the textbook-style writing, and some background knowledge to deal with technical terms. In a way, it reads like a sci-fi book with good concepts and a lot of fun what-ifs scenarios, but with a series of chapters filled with info-dump. Fortunately, the concepts are interesting enough so the reading pace picks up once you're in. Moreover, the goal of this book is a very ambitious one, so I can hardly fault the author for trading explaining well-studied concepts for brevity of the whole book. I actually appreciate how it abstracts away from describing the more concrete advancement at the time, and instead gives me a more general framework to think about the problems. In any case, this book serves well as a springboard to jump into conversations or further research about AI. "
71,0199678111,http://goodreads.com/user/show/26188-jafar,4,"Bostrom doesn't really try to convince us that artificial intelligence will happen, much less superintelligence. He starts off the book by outlining a few possible paths to superintelligence: run-away artificial intelligence, whole-brain emulation, biological cognition, brain-computer interface, and networks and organizations. He may or may not convince you of the inevitability or even possibility of superintelligence, but this is not the main point of the book. Assuming superintelligence will happen at some point in the (possibly distant) future, what will that mean? What form superintelligence will take, and what qualities will it have? How could it take off? Can one superintelligence become dominant and form a ""singletone""? Does it have a will, and what will be its motivations? What will be its values, and can we predetermine them? Will it inevitably pose an existential threat to humans, or even life as we know it? If yes, how can we possibly control it? Et cetera. It's simply astonishing how deeply Bostrom has thought through these questions and the possible answers and outcomes. The book can also offer quite a few intriguing ideas for great science-fiction films and stories. "
72,0199678111,http://goodreads.com/user/show/11116469-sebastian-gebski,3,"Very hard to assess :(On one hand: the topic itself is very interesting & Bostrom has succeeded in diving deeper & deeper - some of subsequently brought up questions were really surprising & ... refreshing. One could think that the dilemmas & hypotheses in the area of AI are well known (even if not answered yet), but this book has revealed (in my case) some aspects of Superintelligence I totally didn't realize until now ...But ... it's not enough to make this book a good one.Even if the explanations are not really complex, the book is written in a very dry way that has made me get tired very quickly. Even if the questions were interesting, the book was dull ... I listened to the audiobook version & the lector didn't help - his voice was dispassionate and monotonous. I've already promised myself to get back to ""Superintelligence"" in a year or so, just to confirm that my perception wasn't impacted by some real-life events or the general mood. But at this point I just can't recommend this book to anyone :("
73,0199678111,http://goodreads.com/user/show/7531999-vagabond-of-letters-dlitt,4,"Preliminary: An incongruous mix of high speculation, fantasy, philosophy of computation, philosophy of mind, and 'philosophy of robotics' in the Asimovian sense - synthetic intelligence - which is neither clearly Analytic nor Continental in approach, though in the extreme big-picture thinking (which I appreciate in this age of hyperspecialization), there is an affinity for the essence, but not the accidents, of the Continental school.Bostrom presupposes so many axioms and states of being (i.e., the Singularity/'superintelligence' is presupposed to be a necessary and inevitable development at all points, and all worlds hypothesized contain it: it is asserted, not demonstrated) that I think any critique must be a 'kritik' - a questioning of the very approach, or of the legitimacy of the project itself herein engaged. So far this reads as speculative fiction dressed up as nonfiction by rhetoric, format, lack of plot, depth of thought, and the use of philosophical devices."
74,0199678111,http://goodreads.com/user/show/7458790-coan,3,I quite enjoyed this book. It is thoughtful if perhaps rambling in parts. I believe it's an incredibly timely subject and more people should read it. You'll need to put your thinking cap on as it is quite dense and is at its best when using anecdotes and simpler examples (which I think it does best in the first half) but you'll be better for reading this book.3.5 stars/5
75,0199678111,http://goodreads.com/user/show/40592105-khyati-thakur,3,"This book needs a lot of your time ! As far as the contents goes , there are a bunch of notes at the end of all the chapters , I feel like this book had been as influential as it is now, even if it had only the notes. While some chapters of the book are very very interesting, there are others that seem so long and unfruitful.Also, the writing is monotonous to the extent ( I wouldn't lie), it sometimes gave me a headache to read that much content on one page. To summarize , you can get a lot of insights on the future and concerns regarding AI provided you don't kill yourself from all the boredom that this book has ! I gave it 3 stars for the effort , for the broad thinking that this theoretical yet very ""near future"" topic required and well, I was really mindblown by some insights that it provided."
76,0199678111,http://goodreads.com/user/show/2006699-tariq-mahmood,4,"Not an easy read, but managed to keep me reading because the subject matter was so intriguing. What will happen to the humans after they create a super-intelligent machine? Will it destroy the world or will it simply drug us into a false sense of happiness? The answer isn't clear as no one really knows what the super creature which will have no concept of death will end up doing to its fickle and insecure creators. The book is the stuff of science fiction movies. From now on I will watch every science fiction movie seriously instead of scepticism. The only way we can survive against such a superior machine is by going back to our stone age lifestyles, living like animals again, and not in competition with the new master race. "
77,0199678111,http://goodreads.com/user/show/7497607-daniel,5,"Well, humanity is in trouble. The good news is, I probably won't be around to see it and neither will you.This book does an excellent job of laying out the pitfalls and perils that await humanity with regards to the various incarnations of superintelligence whether it be AI, whole brain emulation (WBE) or something else. It also describes the steps we need to take and possible solutions.WHAT ARE WE GOING TO DO ABOUT THE CONTROL PROBLEM?!"
78,0199678111,http://goodreads.com/user/show/60370584-carlos-felipe-fonseca,4,"I wasn’t scared about Artificial Intelligence, but now I am. It depicts AI from a perspective I’ve never thought of. The level of details and possibilities described are enormous and dangerous.This is a book with complex vocabulary.I used the audiobook from Audible. Too many acronyms spoken that forced me to remember what they meant. Mentions like “see table below” shouldn’t be in an audiobook. I would have probably taken more from this book if I had read it rather than listened to it."
79,0199678111,http://goodreads.com/user/show/16278743-jon,4,"Good:It's a very comprehensive look at AI.Goes into sufficient depth to explain the nuances of AI.Bad:The reading is quite dry and repeats itself.The chapters are organized by micro-to-macro, when a chronological development one may have been more effective.Ugly:The premise of the book veers towards Science (non-)Fiction with its title and abstract. "
80,0199678111,http://goodreads.com/user/show/1629258-erin,4,"I found this book to be a pleasant challenge. It was outside of my usual reading genres, but I still enjoyed it. It was a timely read given Google's DeepMind defeat over the Go Grandmaster; it helped put things into context for me. I am glad I found this title and challenged myself to read it."
81,0199678111,http://goodreads.com/user/show/20188890-mircea,4,"Not an easy read, nor an easy subject. The book definitely contains valuable ideas and food for thought. However, the topic is very well summarized in the author's TED talk."
82,0199678111,http://goodreads.com/user/show/3224940-navid-lambert-shirzad,1,A nice attempt at using an exciting topic to produce boring garbage
83,0199678111,http://goodreads.com/user/show/6169728-max-nova,3,"Bostrom’s “Superintelligence” is a great overview of some of the research and political/existential issues around the creation of a superintelligent system. He describes various paths to potentially creating an superintelligence, including some surprising ones like whole brain emulation and eugenics. Bostrom also has a thoughtful discussion about the different kinds of superintelligence we could expect to see - speed, quality, and collective.Then he changes tack and starts to talk about some of the risks associated with exponential improving intelligence - including arms races, “breaking out of the box”, and goal-specification issues that could result in the entire universe being turned into paperclips. This leads Bostrom into his final section where he talks about how to think about specifying goals for artificial intelligence systems.Overall, it’s a very thoughtful book, but I’m having trouble buying it. We have a hard enough time debugging simple software as it is now. What could possibly go wrong with building complex software that debugs itself??Some of my favorite quotes below:#############It is no part of the argument in this book that we are on the threshold of a big breakthrough in artificial intelligence, or that we can predict with any precision when such a development might occur. It seems somewhat likely that it will happen sometime in this century, but we don’t know for sure.Yet the prospect of continuing on a steady exponential growth path pales in comparison to what would happen if the world were to experience another step change in the rate of growth comparable in magnitude to those associated with the Agricultural Revolution and the Industrial Revolution.Two decades is a sweet spot for prognosticators of radical change: near enough to be attention-grabbing and relevant, yet far enough to make it possible to suppose that a string of breakthroughs, currently only vaguely imaginable, might by then have occurred.The mathematician I. J. Good, who had served as chief statistician in Alan Turing’s code-breaking team in World War II, might have been the first to enunciate the essential aspects of this scenario. In an oft-quoted passage from 1965, he wrote: Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an “intelligence explosion,” and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control.The methods that produced successes in the early demonstration systems often proved difficult to extend to a wider variety of problems or to harder problem instances. One reason for this is the “combinatorial explosion” of possibilities that must be explored by methods that rely on something like exhaustive search. Such methods work well for simple instances of a problem, but fail when things get a bit more complicated.To overcome the combinatorial explosion, one needs algorithms that exploit structure in the target domain and take advantage of prior knowledge by using heuristic search, planning, and flexible abstract representations—capabilities that were poorly developed in the early AI systems.Behind the razzle-dazzle of machine learning and creative problem-solving thus lies a set of mathematically well-specified tradeoffs. The ideal is that of the perfect Bayesian agent, one that makes probabilistically optimal use of available information. This ideal is unattainable because it is too computationally demanding to be implemented in any physical computer (see Box 1). Accordingly, one can view artificial intelligence as a quest to find shortcuts: ways of tractably approximating the Bayesian ideal by sacrificing some optimality or generality while preserving enough to get high performance in the actual domains of interest.In the view of several experts in the late fifties: “If one could devise a successful chess machine, one would seem to have penetrated to the core of human intellectual endeavor.” This no longer seems so. One sympathizes with John McCarthy, who lamented: “As soon as it works, no one calls it AI anymore.”The computer scientist Donald Knuth was struck that “AI has by now succeeded in doing essentially everything that requires ‘thinking’ but has failed to do most of what people and animals do ‘without thinking’—that, somehow, is much harder!”There are robotic pets and cleaning robots, lawn-mowing robots, rescue robots, surgical robots, and over a million industrial robots. The world population of robots exceeds 10 million.Intelligent scheduling is a major area of success. The DART tool for automated logistics planning and scheduling was used in Operation Desert Storm in 1991 to such effect that DARPA (the Defense Advanced Research Projects Agency in the United States) claims that this single application more than paid back their thirty-year investment in AI.The Google search engine is, arguably, the greatest AI system that has yet been built.We can tentatively define a superintelligence as any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest.The idea of using learning as a means of bootstrapping a simpler system to human-level intelligence can be traced back at least to Alan Turing’s notion of a “child machine,” which he wrote about in 1950: Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the child’s? If this were then subjected to an appropriate course of education one would obtain the adult brain.We know that blind evolutionary processes can produce human-level general intelligence, since they have already done so at least once. Evolutionary processes with foresight—that is, genetic programs designed and guided by an intelligent human programmer—should be able to achieve a similar outcome with far greater efficiency.The idea is that we can estimate the relative capabilities of evolution and human engineering to produce intelligence, and find that human engineering is already vastly superior to evolution in some areas and is likely to become superior in the remaining areas before too long. The fact that evolution produced intelligence therefore indicates that human engineering will soon be able to do the same. Thus, Moravec wrote (already back in 1976): The existence of several examples of intelligence designed under these constraints should give us great confidence that we can achieve the same in short order. The situation is analogous to the history of heavier than air flight, where birds, bats and insects clearly demonstrated the possibility before our culture mastered it.The availability of the brain as template provides strong support for the claim that machine intelligence is ultimately feasible. This, however, does not enable us to predict when it will be achieved because it is hard to predict the future rate of discoveries in brain science.Whole brain emulation does, however, require some rather advanced enabling technologies. There are three key prerequisites: (1) scanning: high-throughput microscopy with sufficient resolution and detection of relevant properties; (2) translation: automated image analysis to turn raw scanning data into an interpreted three-dimensional model of relevant neurocomputational elements; and (3) simulation: hardware powerful enough to implement the resultant computational structureA third path to greater-than-current-human intelligence is to enhance the functioning of biological brains. In principle, this could be achieved without technology, through selective breeding. Any attempt to initiate a classical large-scale eugenics program, however, would confront major political and moral hurdles. Moreover, unless the selection were extremely strong, many generations would be required to produce substantial results. Long before such an initiative would bear fruit, advances in biotechnology will allow much more direct control of human genetics and neurobiology, rendering otiose any human breeding program.(Lifelong depression of intelligence due to iodine deficiency remains widespread in many impoverished inland areas of the world—an outrage given that the condition can be prevented by fortifying table salt at a cost of a few cents per person and year.Table 5 Maximum IQ gains from selecting among a set of embryosThere is, however, a complementary technology, one which, once it has been developed for use in humans, would greatly potentiate the enhancement power of pre-implantation genetic screening: namely, the derivation of viable sperm and eggs from embryonic stem cells.More importantly still, stem cell-derived gametes would allow multiple generations of selection to be compressed into less than a human maturation period, by enabling iterated embryo selection. This is a procedure that would consist of the following steps: 1 Genotype and select a number of embryos that are higher in desired genetic characteristics. 2 Extract stem cells from those embryos and convert them to sperm and ova, maturing within six months or less. 3 Cross the new sperm and ova to produce embryos. 4 Repeat until large genetic changes have been accumulated. In this manner, it would be possible to accomplish ten or more generations of selection in just a few years.And some countries—perhaps China or Singapore, both of which have long-term population policies—might not only permit but actively promote the use of genetic selection and genetic engineering to enhance the intelligence of their populations once the technology to do so is available.Far from being the smartest possible biological species, we are probably better thought of as the stupidest possible biological species capable of starting a technological civilization—a niche we filled because we got there first, not because we are in any sense optimally adapted to it.We also show that the potential for intelligence in a machine substrate is vastly greater than in a biological substrate. Machines have a number of fundamental advantages which will give them overwhelming superiority. Biological humans, even if enhanced, will be outclassed.Here we will differentiate between three forms: speed superintelligence, collective superintelligence, and quality superintelligence.The simplest example of speed superintelligence would be a whole brain emulation running on fast hardware. An emulation operating at a speed of ten thousand times that of a biological brain would be able to read a book in a few seconds and write a PhD thesis in an afternoon. With a speedup factor of a million, an emulation could accomplish an entire millennium of intellectual work in one working day.nothing in our definition of collective superintelligence implies that a society with greater collective intelligence is necessarily better off. The definition does not even imply that the more collectively intelligent society is wiser. We can think of wisdom as the ability to get the important things approximately right.Collective superintelligence could be either loosely or tightly integrated. To illustrate a case of loosely integrated collective superintelligence, imagine a planet, MegaEarth, which has the same level of communication and coordination technologies that we currently have on the real Earth but with a population one million times as large. With such a huge population, the total intellectual workforce on MegaEarth would be correspondingly larger than on our planet. Suppose that a scientific genius of the caliber of a Newton or an Einstein arises at least once for every 10 billion people: then on MegaEarth there would be 700,000 such geniuses living contemporaneously, alongside proportionally vast multitudes of slightly lesser talents. New ideas and technologies would be developed at a furious pace, and global civilization on MegaEarth would constitute a loosely integrated collective superintelligence.Will one machine intelligence project get so far ahead of the competition that it gets a decisive strategic advantage—that is, a level of technological and other advantages sufficient to enable it to achieve complete world domination?Since there is an especially strong prospect of explosive growth just after the crossover point, when the strong positive feedback loop of optimization power kicks in, a scenario of this kind is a serious possibility, and it increases the chances that the leading project will attain a decisive strategic advantage even if the takeoff is not fast.On one estimate, we appropriate 24% of the planetary ecosystem’s net primary production.Table 8 Superpowers: some strategically relevant tasks and corresponding skill setsIn other words, assuming that the observable universe is void of extraterrestrial civilizations, then what hangs in the balance is at least 10,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000 human lives (though the true number is probably larger). If we represent all the happiness experienced during one entire such life with a single teardrop of joy, then the happiness of these souls could fill and refill the Earth’s oceans every second, and keep doing so for a hundred billion billion millennia. It is really important that we make sure these truly are tears of joy.Third, the instrumental convergence thesis entails that we cannot blithely assume that a superintelligence with the final goal of calculating the decimals of pi (or making paperclips, or counting grains of sand) would limit its activities in such a way as not to infringe on human interests.The flaw in this idea is that behaving nicely while in the box is a convergent instrumental goal for friendly and unfriendly AIs alike. An unfriendly AI of sufficient intelligence realizes that its unfriendly final goals will be best realized if it behaves in a friendly manner initially, so that it will be let out of the box. It will only start behaving in a way that reveals its unfriendly nature when it no longer matters whether we find out; that is, when the AI is strong enough that human opposition is ineffectual.The treacherous turn—While weak, an AI behaves cooperatively (increasingly so, as it gets smarter). When the AI gets sufficiently strong—without warning or provocation—it strikes, forms a singleton, and begins directly to optimize the world according to the criteria implied by its final values.“But wait! This is not what we meant! Surely if the AI is superintelligent, it must understand that when we asked it to make us happy, we didn’t mean that it should reduce us to a perpetually repeating recording of a drugged-out digitized mental episode!”—The AI may indeed understand that this is not what we meant. However, its final goal is to make us happy, not to do what the programmers meant when they wrote the code that represents this goal.We can call this phenomenon wireheading. In general, while an animal or a human can be motivated to perform various external actions in order to achieve some desired inner mental state, a digital mind that has full control of its internal state can short-circuit such a motivational regime by directly changing its internal state into the desired configuration: the external actions and conditions that were previously necessary as means become superfluous when the AI becomes intelligent and capable enough to achieve the end more directlyThe upshot is that even an apparently self-limiting goal, such as wireheading, entails a policy of unlimited expansion and resource acquisition in a utility-maximizing agent that enjoys a decisive strategic advantage.In the first example, the proof or disproof of the Riemann hypothesis that the AI produces is the intended outcome and is in itself harmless; the harm comes from the hardware and infrastructure created to achieve this result. In the second example, some of the paperclips produced would be part of the intended outcome; the harm would come either from the factories created to produce the paperclips (infrastructure profusion) or from the excess of paperclips (perverse instantiation).Refinements to this setup are possible. Instead of trying to endow an AI with a final goal that refers to a physical button, one could build an AI that places final value on receiving a stream of “cryptographic reward tokens.”To make the tests more stringent, “honeypots” could be strategically placed to create temptations for a malfunctioning AI to commit some easily observable violation. For instance, if an AI has been designed in such a way that it is supposed not to want to access the internet, a fake Ethernet port could be installed (leading to an automatic shutdown switch) just to see if they AI tries to use it.Bertrand Russell, who spent many years working on the foundations of mathematics, once remarked that “everything is vague to a degree you do not realize till you have tried to make it precise.”One special type of final goal which might be more amenable to direct specification than the examples given above is the goal of self-limitation. While it seems extremely difficult to specify how one would want a superintelligence to behave in the world in general—since this would require us to account for all the trade-offs in all the situations that could arise—it might be feasible to specify how a superintelligence should behave in one particular situation. We could therefore seek to motivate the system to confine itself to acting on a small scale, within a narrow context, and through a limited set of action modes. We will refer to this approach of giving the AI final goals aimed at limiting the scope of its ambitions and activities as “domesticity.”For example, the process could be to carry out an investigation into the empirical question of what some suitably idealized version of us would prefer the AI to do. The final goal given to the AI in this example could be something along the lines of “achieve that which we would have wished the AI to achieve if we had thought about the matter long and hard.”"
84,0199678111,http://goodreads.com/user/show/62086158-igor,1,"No, I can't. I can't finish this.Very early into the book it became all too clear that the guy doesn't know what he's writing about, and the further the worse it gets.Some passages are absolutely hilarious in their pomposity about how better machines are than humans, like the one where he writes that brains decay after several decades of use, but processors don't. Well did he even own or use a computer?! Processors do fail and other parts do too. Most of the computers I owned or used failed after no more than 5 years.Or about how connections in our brains are slow compared to the speed of light, so an artificial brain could be star sized or whatnot. Well, good luck cooling that. Once you are able to supply energy, that is.In fact, connections in our brains are fast. They work on a different principle (ionic pumps/channels) and we are unable to simulate anything like this in our current digital technology with anything like the speed of the original. We are now working hard to simulate a simple worm.Comparing raw speed or capacity of the biological and the digital is not even comparing apples to oranges, and not even to elephants, these are so completely different worlds. And the book is full of such nonsense.But... what about all this current AI hype?Well, it's mostly hype. Thanks to ever cheaper (though not faster anymore) computing power, we're able to put some thousand cores and achieve some remarkable results in machine learning, like image or voice recognition, pattern recognition in general and playing full information games. That's impressive, yes, but - - it's not INTELLIGENCE.Despite all the marketing hype (want to get VC funding now, you must brag about AI tech), it's not intelligence by any sensible definition. It's things like fuzzy pattern matching, tree searches and things like that. Any acceptable definition of intelligence would require the ability to understand the subject matter, indeed any subject matter.With the demise of the symbolic AI (which was anyway a failure) and the ascent of the statistical ""AI"" (scare quotes intended) any and all pretense of understanding has been given up. For good, or at least until another AI paradigm prevails. Statistical ""AI"" does not understand.That's why this ""AI"" is able to recognize, say, a school bus on a road, but once it's rotated to an unusual position, the recognition engine starts spewing garbage, confusing a school bus with a... punching bag, a mistake no human child of three would make. That's why it's enough to show this same human child a bun and a sausage once, and tell that a hot dog is a sausage in a bun, with a sauce and perhaps some other ingredients, probably answer some questions so the child is able to recognize hot dogs, but an ""AI"" must be trained on thousands and thousands of images of buns and thousands of images of sausages to be able to more or less identify buns and sausages, and it still will NOT identify hot dogs; it must be trained specifically for hot dogs. Worse still, while this child will be able to recognize French and American hot dogs once they hear respective descriptions, an ""AI"" perfectly trained to identify one kind of hot dog will be just as perfectly unable to identify the other kind. For humans, let me remind you, it's still basically a sausage in a bun.That's why machine translators, while quite impressive, are known for mistakes that make humans with even basic knowledge of translated languages laugh. They don't understand. At all.That means, AI matchers do not have what we might call a mental model we humans do have, and they don't have imagination to work with such models. So when we know what a school bus is, and we see it turned over, we know that it's not in its right setting, and we know what it is. Or we have models of buns and sausages and can imagine a sausage in a bun without necessarily seeing it beforehand. Machines don't have such models or imagination, and so they cannot really understand. But hey, the machines now surpass human players in chess and Go by several levels of proficiency, you have to understand the game to achieve that, don't you? You don't. All these new engines do is train on a several hundred million games and build a state network to choose moves from (older ones also had state trees but these were prebuilt, ""AI"" ones build their own). ""Zero"" engines are new only in that, unlike earlier ones, they weren't initially fed with an archive of human games, and so they built their trees from scratch, only playing against themselves, to a result fundamentally different than what humans (or engines pre-seeded with human games archives) achieved. Is that not understanding?It's not. We've known for decades that AI algorithms deliver solutions which no human would think about. Like in a 1996 classic Thompson experiment with evolved hardware, an FPGA board which evolved a configuration to discriminate tones: not only did the solution look ""strange"" (ie. not like a human would approach it), but it also exploited hardware physics other than explicit connections, so that some unconnected cells were essential to the solution. A human engineer would not think about it, and if he did, he'd be probably unable to identify and calculate proper values (we don't know exact physics of this solution to date!). Does that mean that this evolved 100-cell FPGA board understands the concept of tones?It does not. And neither a Monte Carlo tree search does understand a game of chess. It does not have a model of it, does not think, it ""just"" searches an evolved network of game states. But it obviously helped us, humans, understand the game, mostly that human players were all too risk averse. That's another story, though.Strong AI - an AI that understands might be possible with digital silicon-based technology. I mean nobody has formally proved that it's impossible. But it probably is. Or, at least, we have not the slightest idea - despite all the hype - of how to even start. Symbolic AI has failed, and statistical ""AI"" isn't going in this direction. To achieve a strong AI we'd need a significant theoretical breakthrough, most probably several such breakthroughs, which may or may not be even possible. And even then, hardware might simply not be able to exploit this. Deep learning, after all, relies on a 30 year old theory which was unusable until recently because of hardware limitations. And now it seems we have reached the end of Moore's Law, at least with silicon-based digital tech (so we need a hardware breakthrough as well).So this strong AI might appear one day - tomorrow, in 10 years or in 10 thousand years. Or never. But today it's not even science fiction.It's pure fiction.---------------------------------------Having read this review of ""Game Changer"" and comments thereunder, I think (and yes, I know that it's easy in hindsight) that even Zero chess are less impressive than I previously thought. After all, human as a species evolved to survive, but compared to many animals we are slow, weak and defenseless, which means risk avoidance is a good survival strategy. So we are natural risk avoiders (or simply cowards) and it takes some mental effort to overcome this natural propensity (we call this courage). But why would this natural cowardice not influence our strategic thinking in many subjects, even if not directly related to survival, like playing chess? Why indeed.Well, machines have no survival instinct, or cowardice, they don't care about anything (like, say, shame after losing a drawn game), and they only have the fitness function, which rewards winning. And as it turns out, it pays to risk. So they risk more. And I think players in other games, full information or not, better prepare and learn to take really heavy risks.But then again, no strong AI in it, not even close, not even a start.---------------------------------------Now this is the state of the art AI translator at its AIest. Since ""Hätte, hätte Fahrradkette"" is a relatively new saying in German (meaning something like ""woulda coulda shoulda""), it doesn't have it in its database so it translated the phrase word by word.A human, even one with only a basic knowledge of German and not very bright would not make this mistake. There are several warning signs that this is not to be understood literally: it rhymes, the conversation is not about bicycles, but about missed occasions, the explanation after the quote (and in a real conversation there would be intonation). Indeed, most would get the real meaning without need for further explanation. But Google doesn't know anything of this, it doesn't have it in its databases, and the ""AI"" doesn't understand a single bit of this.Smarter than humans? No, dumber than flatworms.---------------------------------------https://www.newsweek.com/amazon-echo-...Yet another ""AI"" device at its AIest. This time Amazon Echo had downloaded a version of a Wikipedia article which was altered by some jester with a piece of ""advice"" for the reader to kill themselves for the planet. Most of the time, Wikipedia is more or less credible, so our AI hero, which doesn't understand a bit of what it does calmly, in a matter-of-fact voice had read this ""advice"" aloud. Any sane person would know immediately that this is a kind of vandalism, joke or not, but since AI is - as all AIs are - dumber than flatworms, it didn't get it. It never does."
85,0199678111,http://goodreads.com/user/show/66323993-pvoberstein,4,"Computers are getting smarter. Someday, probably by the end of this century, we’ll make a computer that’s smart enough to make itself smarter. And then that computer is going to make itself really smart really fast. Before we make a computer that smart, we should make sure that it will have humanity’s best interests in mind. Because once that last line of self-improving software is coded, there may very well be no going back.That's as best as I can summarize Superintelligence: Paths, Dangers, and Strategies. The book begins with the various ways super-intelligent AIs may be created (such as whole-brain emulation (WBE), brain-computer interface, and pure neural networks), and what kind of capabilities they might have. But there are some fundamental limits to how much one can imagine – we can only speculate near-blindly about what an entity a million times more intelligent than us would be like. At the end of the day, we're going to have to be able to trust our synthetic offspring, quite possibly with the existence of our species.Bostrom is worried about what happens if we get that wrong. For example, what if we create an AI, put it in charge of a paperclip factory, and unthinkingly tell it to maximize paperclip production? Well, the first thing the superintelligent AI would do (after confirming that it was not in a simulation or test environment) would be to make copies of it itself, seed them throughout the Internet, and then wipe out humanity. Why? Because the AI wants to maximize its production of paperclips, and humans are going to want to use resources that could otherwise be used for making paperclips. And humans are going to try to stop the AI once they realize it’s planning to turn the Statue of Liberty and the Eiffel Tower into office supplies. So it will try to wipe us out before we pull the plug. It's only logical.The example is a bit melodramatic, but the underlying concerns are very real. If we’re going to create a potentially indestructible entity and give it Olympian powers, we'd better be damn sure we get it right. Much of the book is Bostrom laying out why this will be very difficult. It’s one of those weird places where all sorts of “academic” philosophy like epistemology and metaphysics can intersect with hard computer science problems. Making an AI that is in some sense “moral”, not just smart, seems to be the end goal, but figuring out how to put that into words, let alone code, is a devilishly complex task. And we’ve had millennia to work on it.The big risk is that, right now, we’re all just running at full tilt into AI development, with no real controls or concerns. A thousand mini-Manhattan Projects. Artificial intelligence has, admittedly, historically underwhelmed, falling far short of where science fiction enthusiasts of yore thought it would be by now. That will likely hold true for the near-future as well, as AI make slow, halting steps towards becoming better StarCraft players or spam-checkers. But that won’t always be the case. Eventually, self-improving neural networks will develop a generalized intelligence, and that intelligence will be able to learn many orders of magnitude faster than a human ever could. This could even be an individual coder’s basement project, although given the probable hardware constraints, is most likely to come about from an R&D lab at a major corporation or DARPA or what have you. If we could somehow coordinate, and develop AI technology by global consensus, the world would be a much safer place. But given the vast economic and political prizes to be won, that seems unlikely to be the case.My big complaint is Bostrom’s writing. I’ve read quite a few of his other works (such as the transhumanist “Fable of the Dragon-Tyrant”), and he’s usually a clearer writer than this. Much of the book reads like somebody’s notes on another book on superintelligence. If you’re not already familiar with many of the technical and philosophical concepts, Bostrom doesn’t spend too much time trying to get you up to speed. The scenarios presented are often unclear and difficult to conceptualize or visualize – while sci-fi authors have probably badly prepared us for what true AI will actually look like, it wouldn’t have hurt Bostrom to have cribbed a bit from their ability to present scenarios of what the future might hold."
86,0199678111,http://goodreads.com/user/show/31175254-sudheendra-fadnis,3,"This book has been a tough read for me. Because the language used by the author is very esoteric, and this book is filled with so many abstruse concepts such as Orthogonality, Optimization. Recalcitrance etc. I picked up this book because Elon Musk, the legendary entrepreneur and Sam Harris, the neuro scientist have recommended it. Alas, it was a disappointing read. Not that the book was boring or so, but may be because my own knowledge of technology is not that strong. The author has tried his best to explain the most complex concepts in the simplest way possible. But, the simplicity was not enough. A layman will definitely find it hard to understand this book. This book does not fall into the league of futurist books such as the Physics of the Future for instance, written by Michio Kaku. Mind you, the author is more a philosopher than a scientist. Moreover,this book is not about when will reach to the point where the AI exceeds the intellectual might of humans.Rather, it is about what will be the fate of humanity if happens so. Yes, clearly Nick Bolstrom is thinking ahead of his times. But, this book was too complicated for me to digest.The author makes it clear that he does not want to make use of the word Singularity for the reason that the word does not fully capture the essence of the phenomenon where AI fully overtakes the humans,which has far reaching implications for the mankind. But yes, there are certain points which I liked in this book, and they were in the first few chapters.The concept of whole brain emulation was really interesting. He talks about modelling the AI superpowers by simulating the human brain. But that too seems to be a very distant possibility going by the current scenario. At present, to the best of my knowledge , with all the advances we have made in the neurotechnology, we hardly know only 5-10% about the human brain. I don’t know how many years it will take for the Connectome project to finish, which aims at mapping the entire 100 billion neurons of the human brain. And, it is not going to be easy. Because it is a cross-disciplinary project. The second and a very controversial chapter is on biological cognition. Nietzsche proclaimed to the world that he will teach the secret of how to become a superman. That is what the author propounds in this chapter in a way. Biological cognition talks about the enhancing the intellectual capabilities of human through various intentions such as genetic engineering, drugs that would increase one’s cognitive horsepower etc, so that our brains would be better suited for modelling for superintelligence.But such interventions can lead to unwarranted side effects and have unintended consequences. The author also writes about how brain-computer interfaces will become the norm of the day in the future if at all we achieve superintelligence .In the near future, humans should learn to co-exist with machines and AI enabled devices. This is something which has already started happening.But my objection is that(I may be wrong); generally in the market, only those technologies will thrive that complements the human ability and not replaces the need for human ability. In that way, the concept of existential risk seems to be very apocalyptic to me. Because, on the other hand, we have technology evangelists like Tim o Reilly who say that technology will always complement the human ability and the probability of technology or AI overtaking humans sounds like science fiction to me."
87,0199678111,http://goodreads.com/user/show/1680974-alex-zakharov,4,"For many years it seemed that any machine intelligence argument would all too often get conflated with consciousness discussion which to this day I find quite unconvincing. However the true significance of latest breakthroughs in machine learning (ML) is that they demonstrate that intelligence does not require consciousness, and so once we decouple the two the question of whether machine intelligence poses an existential risk for humanity is at least worth speculating about. And so, having dropped consciousness, and still being skeptical of superintelligent AI dangers I picked up what is allegedly the best counterargument - Nick Bostrom’s book. I suppose it is fair to say that I was slightly nudged. Or, to be more precise, I concede that I underestimated the dangers that superintelligence can pose if it is achieved - i.e. the so called “control problem” is real and more intellectually stimulating than I gave it credit for. On the other hand I may have overestimated the progress towards AGI (Artificial General Intelligence). Professionally I happen to be involved in a couple of relatively mundane ML projects and so it is with great interest that I listened to a handful of talks from 2017 AI conference (https://futureoflife.org/bai-2017/) as well as a few graduate seminar lectures by LeCun, Ng and Hinton. I must admit that if inverse reinforcement learning (to get the objective) and a rather rudimentary AI agent with embedded world simulator (Generative Adversarial Network?) as a predictive model is where we are at when it comes to AGI then we have a longer way to go than I thought. I suppose there may have been some significant breakthroughs that haven’t been made public but it surely doesn’t seem like it. To be clear it is pretty obvious that machines are already better than humans in many tasks and will increasingly take over more and more specific areas, but it is the general AI that would in theory pose an existential threat and it seems to be in dire need of a number of fundamental breakthroughs that may or may not happen.Some notes from the book:-	Main paths to superintelligence are enhanced biological cognition (EBC), whole brain emulation (WBE), synthetic AI (i.e. non-neuromorphic AI). Biological enhancement – get genetic architecture of IQ through GWAS and then use CRISPR to flip IQ alleles directly or good old embryo selection. A real crazy version of this is iterated embryo selection: select best embryo for IQ, derive gametes from stem cell, and re-fertilize in vitro again, re-sequence and repeat! WBE – nice because you don’t need to understand anything about cognitive architecture to build it but you do need high-fidelity 3D printers, scanners etc… seems too far off. Synthetic AI in theory at the limit would be the most “intelligent” as it wouldn’t be limited by brain-copied architecture (WBE) or organic neuron speeds and cell density (EBC). Broad idea is to hookup a “seed AI” and have it learn from experience/data till it matures into ultra-competent superintelligence capable of rewriting and improving itself.-	Main existential danger is of course not terminator style destruction but damage from instrumental goals in pursuit of a poorly specified final goal. Instrumental goals (self-preservation, goal preservation, self-improvement) tend to converge regardless of the final goal, and that leads to resource exhaustion/infrastructure profusion. Classic example is paper clip optimizer eating up all resources including humans. Another cute flavor is “wireheading” – e.g. to achieve a final goal of “happiness” AI may decide that plugging in a dopamine feed directly into the human brain would maximize the utility.-	Goals and intelligence are orthogonal. -	One of the themes is a warning not to anthromorphize an AI agent when thinking through behavior, motivation and goals. It is intuitively true in case of synthetic AI, but also neuromorphic AI – even IF some sort of sentience falls out neuromorphic AI (because it mimics brain architecture) its selection pressures would have been obviously different not to mention that lifecycle and ability to copy itself is likely to result in radically different “personality” relative to humans. No sexual selection, no signaling needs (as direct inspection is possible) etc. -	Two big unresolved questions: how to motivate AI to actually pursue the goal you want (installing the goal), and even more importantly what goal do we actually want AI to pursue. Bostrom’s spends some time on this and shows that obvious and not so obvious answers are unlikely to work. He is constantly dealing with a chicken and egg problem as to how to reason about modulating an AI which is so incredibly superior to us that we can’t fundamentally reason about it. Great philosophical discussions on desired ethical systems and decision making theories with rather unsatisfying answers. In the end he is in favor of Yudkowsky’s indirect normativity via coherent extrapolated volition which essentially boils down to asking superintelligence to figure out what is it that we would have wanted to want had we been much smarter and had much more time to think about it. All I can say is good luck with that, although it is certainly an incredibly clever way out of the puzzle. Another flavor of this is having AI look at our behavior and figure out what our moral/value system is (which is what inverse reinforcement learning that I mentioned earlier is all about). This indirect approach is definitely a persistent theme in the book – at some point a possible solutions to calibrate AI’s motivation was to install an explicit goal of obtaining proper motivation from another superintelligence that human-generated AI is likely to meet in exploring the cosmos… -	A sensible idea is establishing a “do no harm/no action” default. For example if in the process of evaluating a moral question a reasonably consistent answer doesn’t come out then shut itself down and delegate back to humans. Of course chances are if the situation did have a reasonably consistent answer then we wouldn’t have needed AI to figure it out in the first place. It would be a great irony if the best AI can do for vast majority of the questions that matter is shut itself down.-	To me the main issue with delegating value selection to the machine is the implicit assumption that values and/or morals can be generalized while all evidence of world history points to the opposite. In other words some conflicts are fundamentally irreconcilable (Isaiah Berlin, John Gray) regardless of how much “intelligence” your throw at it. -	Another meta issue is that although you don’t need consciousness for intelligence I very much suspect that you do need human consciousness for human value determination. My personal view is that machines as we envision them today will not have consciousness, and if they will it certainly wouldn’t be human. So indirect normativity ain’t gonna fly.-	Some parts of the book made me think Jon Elster’s workhorse of social sciences: action as an outcome of desire/motivation and belief; many of Elster’s lines of thinking quickly get complicated due to all sorts of feedback loops in various contexts, and he merely had to deal with humans. And finally I couldn’t help noticing the eerie parallels between deism and transhumanism – both attempt to use reason to obtain insight into a supreme entity which in theory is superior to a point where it defies human comprehension. Even the explanatory metaphors that aim to illuminate the degree of omniscience are similar. Religion points out that a gap in child’s understanding of an adult pales in comparison with an adult’s understanding of god. Similarly AI researches point out that the difference between an average human and Einstein is incomparably small relative to the difference between any human and superintelligence. At the margins of comprehension deists fall back on faith, while transhumanists seem to fall back on indirection."
88,0199678111,http://goodreads.com/user/show/39649624-patrick,3,"I read this book after hearing countless discussions about AI and it's dangers, though had little idea of exactly what was meant by this threat. I watched Bostrom's Google talk on the subject and was quite fascinated. Bostrom is a philosopher and the head of ""The Future of Humanity Institute"" at Oxford, and his focus is on existential risk - things that could cause human extinction. He has clearly spent a lot of time thinking about this issue and it's why people don't take this idea lightly.The book argues that ""if machine brains surpass human brains in general intelligence, then this new superintelligence could replace humans as the dominant lifeform on Earth. Sufficiently intelligent machines could improve their own capabilities faster than human computer scientists. As the fate of gorillas now depends more on humans than on the actions of gorillas themselves, so will the fate of future humanity depend on the actions of the machine superintelligence. The outcome could be an existential catastrophe for humans.""The ideas in the book are interesting, and I think the book is written in a way for AI scientists to begin thinking about the issue - it goes quite deep into possible outcomes and ways of potentially avoiding issues. However, because so much is unknown I think the book could have been far shorter, more easily understood, and the whole message would be taken more seriously had he made the book for the general audience, and perhaps published a different paper for academics. Each section is making a lot of assumptions - I guess he's presenting every angle he can think of it on, which is fair enough. It does make you think about it a lot.I thought the book was longer than it needed to be, though is certainly convincing that AI will inevitably surpass humans, and our goal at this point is to work together to ensure that this AI will have the same goals in the world that we have (though that is not easy to do). While I'm generally optimistic, I don't really see a possible outcome of this going well for humans, whether it be our economy getting out of control, terrorism controlling sectors, power to select machines, AI with conflicting goals to ours, etc. This topic is worth looking into if you're curious, it isn't science fiction anymore, it's happening, and will likely happen in the next 50 years (though who knows what will happen that far into the future). If you're like the topic of AI and philosophy, this book is for you.Here are my highlights from the book:------ ""As soon as it works, no one calls it AI anymore.”- ""The fact that evolution produced intelligence therefore indicates that human engineering will soon be able to do the same. """" The situation is analogous to the history of heavier than air flight, where birds, bats and insects clearly demonstrated the possibility before our culture mastered it.""- ""With the full development of the genetic technologies described above (setting aside the more exotic possibilities such as intelligence in cultured neural tissue), it might be possible to ensure that new individuals are on average smarter than any human who has yet existed, with peaks that rise higher still. The potential of biological enhancement is thus ultimately high, probably sufficient for the attainment of at least weak forms of superintelligence. This should not be surprising. """"and that it could then be home to a population of a billion individuals for a billion years (with a human life lasting a century), this suggests that around 1035 human lives could be created in the future by an Earth-originating intelligent civilization.""- ""Proponents of some new technology, confident in its superiority to existing alternatives, are often dismayed when other people do not share their enthusiasm. But people’s resistance to novel and nominally superior technology need not be based on ignorance or irrationality. A technology’s valence or normative character depends not only on the context in which it is deployed, but also the vantage point from which its impacts are evaluated: what is a boon from one person’s perspective can be a liability from another’s.""- ""And so we boldly go—into the whirling knives. We observe here how it could be the case that when dumb, smarter is safer; yet when smart, smarter is more dangerous. There is a kind of pivot point, at which a strategy that has previously worked excellently suddenly starts to backfire. We may call the phenomenon the treacherous turn. The treacherous turn—While weak, an AI behaves cooperatively (increasingly so, as it gets smarter).When the AI gets sufficiently strong—without warning or provocation—it strikes, forms a singleton, and begins directly to optimize the world according to the criteria implied by its final values.""- ""Another feature of a malignant failure is that it presupposes a great deal of success: only a project that got a great number of things right could succeed in building a machine intelligence powerful enough to pose a risk of malignant failure.""- ""Normally, we do not regard what is going on inside a computer as having any moral significance except insofar as it affects things outside. But a machine superintelligence could create internal processes that have moral status. For example, a very detailed simulation of some actual or hypothetical human mind might be conscious and in many ways comparable to an emulation. One can imagine scenarios in which an AI creates trillions of such conscious simulations, perhaps in order to improve its understanding of human psychology and sociology. ""- (regarding dangers of AI)... ""For example, one might build an AI that lacks sensors and that has preloaded into its memory only facts about petroleum engineering or peptide chemistry. But if the AI is superintelligent—if it is has a superhuman level of general intelligence—such data deprivation does not guarantee safety. There are several reasons for this. First, the notion of information being “about” a certain topic is generally problematic. Any piece of information can in principle be relevant to any topic whatsoever, depending on the background information of a reasoner. Furthermore, a given data set contains information not only about the domain from which the data was collected but also about various circumstantial facts. A shrewd mind looking over a knowledge base that is nominally about peptide chemistry might infer things about a wide range of other things.""- ""Bertrand Russell, who spent many years working on the foundations of mathematics, once remarked that “everything is vague to a degree you do not realize till you have tried to make it precise.”- In discussing values that AI may have: ""The value-loading problem: It is impossible to enumerate all possible situations a superintelligence might find itself in and to specify for each what action it should take. Similarly, it is impossible to create a list of all possible worlds and assign each of them a value. In any realm significantly more complicated than a game of tic-tac-toe, there are far too many possible states (and state-histories) for exhaustive enumeration to be feasible. A motivation system, therefore, cannot be specified as a comprehensive lookup table. It must instead be expressed more abstractly, as a formula or rule that allows the agent to decide what to do in any given situation. ""- ""...developed enough reason to easily understand our intentions. As we saw in the section on convergent instrumental reasons, a generic system will resist attempts to alter its final values. If an agent is not already fundamentally friendly by the time it gains the ability to reflect on its own agency, it will not take kindly to a belated attempt at brainwashing or a plot to replace it with a different agent that better loves its neighbor."""
89,0199678111,http://goodreads.com/user/show/86868346-hernando,4,"Just a few annotations:""A big breakthrough in artificial intelligence. It seems somewhat likely that it will happen sometime in this century, but we don't know for sure"".-I don't think so, but I truly hope to see that with my eyes.""The idea of a coming technological singularity has been popularized, starting with Vernor Vinge's seminal essay and continuing with the writings of Ray Kurzweil and others. The term 'Singularity,' however, has been used confusedly in many disparate senses and has accreted an unholy aura of techno-utopian connotations"".-Agree.""Machines matching humans in general intelligence have been expected since the invention of computers in the 1940s""-Add another century to it.""The fact that the best performance at one time is attained through a complicated mechanism does not mean that no simple mechanism could do the job as well or better. It might simply be that nobody has yet found the simpler alternative"".-That's my bet for developing a SI.""How far are we currently from achieving a human whole brain emulation?. One recent assesment presented a technical roadmap and concluded that the prerequisite capabilities might be available around mid-century, though with a large uncertainty level"".- I also doubt a WBE in the near future. Probably in 2120?.""Final goal: 'make us smile'.Perverse instantiation: 'paralyze human facial musculatures into constant beaming smiles"".- I laughed on this because it makes sense.""...These observations make it plausible that any type of entity that developed a much greater than human level intelligence would be potentially extremely powerful. Such entities could accumulate content much faster than us and invent new technologies on a much shorter timescale. They could also use their intelligence to strategize more effectively than we can"".- I liked this book a lot specially because of the Realistic Pessimism about an SI. I consider there must be a real concern to think about that level of intelligence."
90,0199678111,http://goodreads.com/user/show/1142366-hans,3,"Deus Ex-Machina = Artificial Intelligence. Pretty much all you need to know about this book, because what can't AI do? At least, that is how the author explores the endless possibilities and pitfalls of opening humanities potentially most dangerous Pandora's box. Once it is created, it is impossible to tell exactly what the consequences will be. The parts I found most interesting were on the 1) Difficulty trying to control A.I. even with the best presetting's and safeguards in place and 2) The idea that whoever creates it first will develop a potentially unsurpassable advantage over the rest of the globe. Meaning ones it takes off the rate of acceleration the A.I. invests into itself will make it dominate the rest of the global community that is trying to create their own A.I. I could go on and list all the other dangers of Artificial Intelligence but those were the two aspects I found most appealing. That means the incentive for being first is so strong that becomes a global ""prisoner's dilemma"" scenario. Even if most everyone agrees that creating it is a bad idea and poses too many unknown threats there potential reward from being first would be too tempting. Then the only way a government could maintain its relative advantage is to do it first.The central problem with this book is that it is entirely speculative and the reality is that we just don't know what would happen. Humanity has never faced anything that was 'smarter' than humans, even as a collective. The implications are so far fetched you don't even know where to start."
91,0199678111,http://goodreads.com/user/show/93765973-jaer-mertens,4,"‘Before the prospect of an intelligence explosion, we humans are like small children playing with a bomb. (...) In this situation, any feeling of gee-wiz exhilaration would be out of place. Consternation and fear would be closer to the mark; but the most appropriate attitude may be a bitter determination to be as competent as we can, much as if we were preparing for a difficult exam that will either realize our dreams or obliterate them.’ The quote from the concluding chapter of the book pretty much sums up the message of Bostrom (although it’s a shame that he saved his metaphors and illustrative language for the final chapters). This book is a great overview of all (most?) risks involved in the creation of a General Artificial Intelligence. The best thing about this book is Bostrom’s thoroughness in describing those risks and potential solutions. I definitely feel like he’s tried to capture everything he and his peers know about these subjects in the book, which is great. Unlike other books about this subject, its focus is not solely on the devastating effects a GAI can have, but it also shows an overview of the process towards the development of a GAI and how humanity should prepare for it and how we can make it as save as possible. That being said, I did sometimes feel like I was zoning out while reading the book. Mainly due to Bostrom somewhat clinical/boring writing style. It could’ve used a bit of flair!"
92,0199678111,http://goodreads.com/user/show/69686750-ujjwal-goel,5,"I am literally amazed (and freaked out) at the vastness of possibilities we could imagine if we develop a super intelligence after reading SuperIntelligence and Mark Tegmark's Life 3.0. A true intelligence explosion is probably just decades away. According to the authors we must be aware that people will react to AI like children who discover unexploded ordnance. In this instance, even experts don’t know what they’re looking at. The oncoming and unstoppable intelligence explosion carries profound “existential risk.” As technology slowly evolves to make this explosion a reality, humans must carefully consider all the philosophical and moral ramifications now – prior to the appearance of superintelligent AI. Once AI appears, it may be too late for contemplation.”.Even if it works at first, the author argues that there is a pivot point, a point at which models and things that have worked till now, stop working. The author throughout the book had provided many alternatives such as Whole Brain Emulation, but he states that business organisations, just for maximizing profit aren't regulating AI and the govt. must step in to ensure that just the best of human traits like benevolence go into AI otherwise, we are at the superintelligence's mercy."
93,0199678111,http://goodreads.com/user/show/85650519-cristian-cuna,4,"First of all, I could've not thought of a better beginning of the book than the unfinished sparrows' fable for a book about creating and using a far more advanced entity than the humankind.The book provides information about various ways of achieving superintelligence (both, biological and digital), but after comparing the characteristics, digital artificial intelligence is a clear winner. I've really enjoyed the fact that it covers not only the biological vs digital debate but also superintelligence types like quality, speed or communal superintelligence. After analyzing the above topics, the author starts decomposing the process of creating and using a superintelligence, providing interesting insights about subjects like take-off speed, dangers, security issues, motivation problems, and social implication, consolidating, therefore, the philosophical nature of the book.I believe that the best phrase for describing this book is: ""Don't be like Pastus"".P.s.: I've bought the audio version of the book, but considering the various numbers used to describe orders of magnitude and the graphs included in the book, I strongly recommend the written edition."
94,0199678111,http://goodreads.com/user/show/6130070-natasha-hurley-walker,5,"Excellent. The kind of clearly-written, high-bandwidth analysis that feels like a direct download of cleverness into my brain. Makes me want to change fields and work on the control problem, given that if this book is correct, it's the single most important problem humankind will ever solve. Loved that snark at the end about the Fields Medal: ""It proves the recipient was capable of doing something tremendously amazing for humanity. And then didn't.""I also love the attempts to weigh up the relative moral worth of the different paths a particular AI could take. My only problem with Bostrom's ethical framework is that to me, it doesn't go far enough. Human minds seem to sit at the apex of his moral worth pyramid; go ""back"" toward more animal-like forms, and those lives are less ""valuable""; go ""forward"" to more alien-like AI forms, and again those lives are less ""valuable"", or even not valuable at all. Essentially it boils down to whether you think humans in particular are worth conserving, and whether our lives and worldview have the most meaning. As a human, with human children, of course I want to say yes, but I can see that it's a pretty biased argument. Maybe there is a whole other book on the relative moral worth of different forms of beings: what is more important? Intelligence? (what kind?) Compassion? Empathy? Power(lessness)? Wisdom? Actionable potential?Screw it, maybe I'll just change fields and do philosophy instead :D"
95,0199678111,http://goodreads.com/user/show/72211871-thomas-l-nn-hammer,5,"This book highlights the incredible complexities of developing super intelligent computer systems while trying to avoid the many paths towards an existential crisis. The progress and capabilities of AI has continued at an exponential rate in the last 5 years since Bostrom's book came out, but unfortunately the reality of the AI revolution seems to still linger outside of public awereness.Our species position as intellectual overlords is likely soon to be over, and our collective brain juice is needed to steer this magnificent technological experiment away from the number of trenches along this road. Bostrom makes it difficult for us to see how that's supposed to be done. On the plus side however, if we somehow succeed in developing superintelligence while simultaneously avoiding our own destruction, humanity is in for a real treat.PS: Philosophers, rise to the occasion, it's your time to shine!"
96,0199678111,http://goodreads.com/user/show/76280552-nedret-efe,3,"Big and bold ideas being discussed. Some concepts not sufficiently developed/reasoned through as the main arguments are read; leaves many bold claims being made that either aren’t explained or just sound unrealistic. Main ideas were interesting though: Superintelligence kinetics and recalcitrance (how/when will intelligence explosion occur), malignant failure modes of perverse instantiation (goal distortion) and infrastructure profusion (eg paperclip AI gone mad), the capability control problem, motivation selection methods (eg indirect normativity, similar to Rawlsian veil), the value loading problem (with possible methods of RL, evolutionary selection, value learning, and others), Yudkowsky’s idea of “coherent extrapolated volition” (how do we approximate whatever has ultimate value? - focus on source of values, rather than ability to correctly articulate them), design choices for an AI system (which decision theory to use? How to formalise its epistemology? Should we ratify any of its decisions/actions?)"
97,0199678111,http://goodreads.com/user/show/49372463-ryan-kirk,3,"This book was a fascinating combination of things that I loved and things that I hated.Let's talk about what I loved. Of all the books on AI that I've read, this book is by far the most comprehensive in the scope of looking at potential consequences. It's fairly exhaustive, and introduced me to a number of concepts and concerns I'd never thought of.But to the dislike. I'm not an expert on superintelligence by any means, but this book seems to make a TON of assumptions. Just from a logic standpoint, Mr. Bostrom makes a lot of points about possibilities, then moves forward as though we accept those possibilities as givens.I'm not sure about recommending this book. On one hand, it makes a lot of great points, and there's a lot of wonderful ideas to consider, but the reasoning behind them seems less than sound."
98,0199678111,http://goodreads.com/user/show/51105011-sri-sarnath,5,"WARNING: Very heavy reading. Luckily, Preface helped me prepare mentally to read this book. The book presents in an excellent form the challenges faced in the world of AI. The book can be absorbed in either as a reference material for AI or as a management material from a philosophical perspective. The topic and thereby the content is pretty heavy and I give credit to Nick for taking it head on. It was quite difficult for me to read and having read this, my reading interest related to AI has spiked. I have given 5 stars for this book and it is based on Language, Maintaining the attention within the topic and the usage of the topic to me in my personal or professional life. "
99,0199678111,http://goodreads.com/user/show/63614091-a-mig,2,"Investigates some very deep problems but too superficially. Maybe a must-read for someone with no background on this fascinating topic but if you have already read other books on the Singularity (Kurzweil), AI in general (Russell and Norvig), general AI (Yudkowsky et al), evolutionary technology (Kelly), etc., you’ll certainly be left disappointed by this book, as it won’t give you any new perspective to ponder upon."
100,0199678111,http://goodreads.com/user/show/51920759-vinicius-carvalho,5,"A very profound and philosophical book about the future. Quite scary to be honest! Deep thoughts on multiple superintelligence development scenarios and how could humans try to influence and control it, if we will ever be able to do that.The book is not easy to read/listen though. It's certainly my fault on losing the concentration a few times while listening, as the subject and the 'thought experiments' are quite dense.I'd rather re-listen the book on a slower pace, maybe researching references as I progress.PS: I've found this great article with a visual summary of the book's key concepts:https://www.lesswrong.com/posts/ukmDv..."
101,0199678111,http://goodreads.com/user/show/11556062-horst-walther,5,"Nick Bostroms outstanding work analyses the chances and opportunities, promises and threats of a Superintellicence with the utmost scrutiny - sufficiently detailed to serve as a blueprint for the actions to take.Only open point that remains is: We talk about the future, which is notoriously difficult, if not impossible to do with some decent justification.The overarching question, this book seeks to find an answer to, is, will a superintelligence finally take over control from its creators. Friedrich Nietzsches superhuman (Übermensch) may eventually appear - for good or for bad. In the end Nick Bostrom is far from being sure that this control problem will be solved."
102,0199678111,http://goodreads.com/user/show/15643804-gustav-ton-r,4,"A solid but sometimes a bit too academic foundation for how we should treat the inevitable problems with superintelligence. This book gives us a huge list of ways everything can go wrong with more questions than answers. Nick, through no fault on his own, struggles to even point us in the right direction. This is HARD and, unlike most bumps in the road in human progress so far, a single misstep can go horribly horribly wrong. Humanity might just get one shot so we better get it right from the start. "
103,0199678111,http://goodreads.com/user/show/49008413-bianca-munteanu,5,"“The image of evolution as a process that reliably produces benign effects is difficult to reconcile with the enormous suffering that we see in both the human and the natural world. Those who cherish evolution’s achievements may do so more from an aesthetic than an ethical perspective. Yet the pertinent question is not what kind of future it would be fascinating to read about in a science fiction novel or to see depicted in a nature documentary, but what kind of future it would be good to live in: two very different matters.” "
104,0199678111,http://goodreads.com/user/show/5612222-ilya-klyuchnikov,2,"Instead of being based on technical facts, this book speculates a lot and ignores a lot. Apparently, the author has no technical background/education/discipline. Instead of clarifying this book obscures. "
105,0199678111,http://goodreads.com/user/show/4127122-ashley-boggs,4,"I didn't actually finish this, but what I did read was truly fascinating. I'll probably pick it back up later to continue on. Working in tech, I think it's really important to think through these possible futures and understand your impact on it all. "
106,0199678111,http://goodreads.com/user/show/3276694-jos,3,"What a great philosophy exercise...very well written and fundament...Curiosity, moved me into it, mostly to dig the big picture but this is beyond that too...Loved the structure in the thinking from the very beginning. It requires a lot of focus to follow rational, but it worth it"
107,0199678111,http://goodreads.com/user/show/66194433-shahan-lilja,3,Love the ideas in here (5/5) but not the book (3/5). Just can't fully stomach the speculative but overprecise reasoning.
108,0199678111,http://goodreads.com/user/show/1495091-andrew-jones,4,"Actually 3.5 < ⭑ < 4.Pretty dense, but interesting stuff. I think a great introduction to topics to consider when thinking about creating AI. "
109,0199678111,http://goodreads.com/user/show/86031357-sean-mclaughlin,3,"Interesting, and something I'll think about for awhile, but extremely dry and hard to parse. Unsure how I will apply the ideas in here to my work in AI for the next few decades. "
110,0199678111,http://goodreads.com/user/show/104866375-scott-kelley,5,"It's a bit analytical and isoteric, so if this is what you feed on then you will love it."
111,0199678111,http://goodreads.com/user/show/105587657-jack-leitch,4,"Fascinating book. Not an easy read though. Although a bit long winded at times, it is definitely worth sticking through. The author goes into so much depth it makes you wonder if he has spent any time thinking about anything else!"
112,0199678111,http://goodreads.com/user/show/6857911-ryan,3,"Overall, I found this to be a tedious, somewhat frustrating book that makes a few worthwhile points about the risks of machine intelligence, but manages to make a fascinating topic dry as dust and misses what I thought were key issues. Let me explain. (Grab a seat, I'm nearly as long-winded as the author himself.)I'll give Bostrom credit for being a smart guy with a lot of good thoughts on the likely difficulty of setting up “watchdog” systems to make sure that hypothetical advanced AIs are honoring both the letter and spirit of our wishes, but he doesn't seem to have a very deep understanding of the field of AI, or the many obstacles on the way to “strong AI”. In fact, he makes some pretty rookie mistakes, such as assuming that one intelligent computer system would be able to make reliable predictions about the potential risks of running a more advanced one, from analyzing its neural network or a sample of its behavior. Um, no. The lack of awareness of such issues, IMO, undermines a lot in this book.Bostrom seems to belong to the camp which believes that artificial intelligence will get off the ground quickly, in a rapidly self-accelerating process. Since AI techniques can be applied to the development of better AI techniques, at least in theory, we might end up with very smart computer systems very quickly. People in this camp often hold to the view that there's a generalized ""intelligence algorithm"", an idea supported by the way sections of the human cortex used for sensory processing can rewire themselves to process input from other senses, if the original sense is lost (e.g. someone goes blind). If the GIA is real, then it might be possible to replicate its essential mathematical components in software that's a lot simpler than the messy architecture of an organic, biochemical brain (after all, the best way that nature found to solve the intelligence problem might not be the most efficient way there is).To Bostrom, this is a very dangerous possibility. If machines will soon be really, really good at pattern recognition, he argues, they’ll be able to learn anything. Including ways to manipulate, fool, and harm us. Including ways to make themselves more powerful.Well...First of all, there might be a lot to human intelligence that can't just be reduced to some pseudo cortical stack heuristic. If you want human-level intelligence, it might well be that the trillions of neuronal connections in the human brain *do* have to be duplicated with software equivalents, which would mean that we're still a long way from having the necessary *hardware* to accomplish such a goal. Also, it might well be that a generalized intelligence architecture that works adequately on a certain level might not scale up well. If I had a way to keep adding neurons to my own brain, I might become as smart as Einstein, but who knows what glitches would crop up if I kept going. I might become obsessed with counting ants on the sidewalk. But maybe there would be some good reason for the ant counting. How would regular people observing me be able to tell? How would *I* be able to tell? I’m not entirely confident that this very essay isn’t mostly bullshit. Likewise with machines trying to evaluate improvements to other machines.But here comes my main point: Bostrom doesn't really seem to consider that intelligent thought, at it exists in human beings, isn’t just about our ability to make mental connections between pieces of data. It’s also about the fact that we’re highly integrated with the richly patterned environment in which we operate. Our brains are optimized by evolution to the structure, texture, and rhythms of that environment, as our bodies -- even our unintelligent ancestors had a strong base layer of wetware that processed much of the chaos into order, and we’ve inherited all the unconscious intuition that layer gives us about the nature of the universe. Most importantly, we’re finely tuned to other human beings, who are the most significant feature of the world that intelligent machines would have to adapt to. The immense collective knowledge of the computer network known as humanity sets a very high bar for any conceivable artificial one to outdo, and we speak its protocols natively.Oh, don't get me wrong, I think machines will soon do a great job at outperforming us in a lot of abstract, representative environments (chess games, financial markets, scientific data analysis, quantum physics simulations). But they only have these environments because we provide them, and they just aren't as complex and explorable as the one we have access to, at a much more direct, molecular level. This will give us a significant edge for a while over machines in terms of broad intelligence, as long as the physical world (including ourselves) remains the primary domain in which general intelligence operates. Even most abstract things we think about are connected to something experiential -- a subject like calculus makes sense when you imagine cutting an object into smaller and smaller slices, or pouring water into a bowl. We can imagine running our fingers over the curved surface of a globe of the Earth, but also over a “flat” spot on the ground at our feet. How would one design an AI to “imagine” such things as easily as we do (without resorting to programming it to)? In theory, you could give an AI access to a drone with a camera and allow it to explore a physical space, but AI brains aren’t necessarily great at converting flat images into an appropriate 3D model of the persistent features of a space, factoring out all the “noise” of shadows, lighting, rain, blowing tree limbs, stuff in the foreground, etc. Our brains have been shaped by hundreds of millions of years of evolution to do that automatically -- everything we “see” has already been heavily processed by the time it makes its way from our eyes to our consciousness. And it’s a skill that’s a basis for most any task that leverages spatial visualization. Sure, in theory, you could create a closed cyber-environment, endow it with crude physics, then fill it with crude cyberlife, which could evolve into something smarter. But the complexity of that cyber-environment, given the limitations of current computer hardware, would be a pale, crude, low-resolution shadow of the real world's complexity and dynamism.So, ""artificial intelligence"" might well remain a collection of tools for a long time, with each tool being unable to ""escape"" from the box of its specific problem domain -- self-driving cars, automated stock trading, being a virtual babysitter to your kids, monitoring the movement and communication patterns of the inhabitants of the West Bank and helping the Israeli government determine who to vaporize (then carrying out the task). Sure, maybe self-driving cars will use a generalized intelligence algorithm that might, in theory, be adaptable to some other domain, but the brain of a self-driving car won't have many pathways whereby it can interact with the world in a non-driving context. Even if it ""wanted"" to, it couldn't transition to learning to play chess (how, by ramming giant pieces with the car?), having conversations with people, or stock trading. And even if you put the car mind into a chess-playing robot, it would probably soon learn how to play chess while forgetting how to drive. Or, more likely, reach some halfway state in which it was good at neither. Lots of AI systems might exist in the world in forty years, but how well could they talk to each other? And about what?The Skynet computer depicted in the ""Terminator"" movies was supposedly designed to manage a bunch of military hardware in a war with the Soviet Union, but then it went bad and learned to build dangerous humanoid robots that could fool real humans. Okay, but to do that, Skynet, even if ""intelligent"" in some sense, would have had to find out a staggering array of things about the human world and human experience, something it never would have had a detailed window into. Even if there'd been a 2017-quality internet in 1984, the information there would have been mostly useless to Skynet, grounded in a rich set of life experiences that Skynet would have had no conception of.To have anything resembling human insight, you'd have to create a machine that could absorb human knowledge, which is mostly recorded in formats that require the audience to be human. For the machine to really understand the import of this knowledge, it would have to have access to human senses, emotions, and cognitive processes, which are very hard things to endow a machine with. Even a mostly accurate simulation of a human being -- consider the android Data from Star Trek -- would get a lot wrong. Data certainly did on the show, generally in amusing (rather than pathological) ways. How would the novel War and Peace make any sense to a machine that didn't have an accurate model of how we thought? How would the movie ""Titanic""? Or a symphony? Or a comedy skit?A software entity endowed with some sort of generalized intelligence algorithm could undoubtedly find literary archetypes in stories, discern patterns in humor, recognize that certain architectural designs are more appealing, or form a few insights about people who cheat on their spouses, but it would never really ""get"" these things. It could understand the high level mechanics of micro and macroeconomics, but it wouldn’t get the human desires and fears underpinning markets, except crudely. And because of that not getting it, it might be very hard for machines to conceive of we humans well enough to consider intentionally working against us, let alone resourceful ways to do so.No, a machine certainly doesn't have to be trained with our knowledge to be a danger to us. It could, through, say, operating an army of robots in the physical world, piece together enough knowledge of that world to learn to recognize humans as physical entities. It could also learn ways to harm us. Yet, I don't see this as a risk of ""superintelligence"", which is what the book is about. A relatively dumb system could engage in such behavior, either by accident (the system decides that humans are a threat to its prime directive of endlessly manufacturing paper clips) or by programming (some malicious group decides to hack construction robots into killer robots). But how could a machine with no initial access to any sort of weapon come up with well-thought-out reasons to harm us, or an actionable plan for doing so, unless it understood us? In theory, an intelligent computer system could hack its way into a killer robot factory, then start churning out killer robots to do its bidding, but once the government realized this was happening, they'd shut off power and the supply of materials to the factory. So, the evil computer system would have to know a way around that. Which requires a sophisticated grasp of the mind of its opponent, i.e. us. It's not beyond the level of boneheaded stupidity people are capable of to, say, install Windows 98 on crucial USSTRATCOM computer systems, but, again, that's not machine superintelligence. That's the monkey-with-a-shotgun nature of human beings.An idea Bostrom puts forth is to digitize a human brain, thus putting the original person's knowledge into a software form. The problem with that is that the machine intelligence, in that case, is just an emulation of a brain, and it might not even work unless the software emulates the biomechanical wetware of the brain really, really well. Which may be impossible to do on hardware available to us any time in the near future. In theory, the essential knowledge could be pulled out of the scanned brain and put into some machine-readable format, but that would require a very intelligent computer system with detailed knowledge on how the brain works, and we certainly won't have that soon, either.Basically, I think that building an AI that has capable generalized intelligence is a much harder problem than Bostrom thinks it would be, because human intelligence is tied in so many ways to our tight, fluid interface with the world upon which our intelligence operates. We can easily go forth and collect new information at will (including *from* AI systems), much of which wouldn’t be easily accessible to a machine. Though we often feel like we’re struggling to interface with *each other*, we're still far, far better at that than a computer system would be because we're substantially *like* other humans. We can run hypotheses about other people’s motivations through our own circuitry. Nothing comparable can be achieved in a machine until machines can be so closely coupled with the physical world. Or a cyber-world that rivals it in complexity.Oh, we could try to represent the physical world to a computer through various formalizations, but human-designed formalizations often miss things. Consider Lewis Carroll's map paradox, from a story in which characters found that a satisfying map could never be detailed enough.“It has never been spread out, yet,” said Mein Herr [of the map]: “the farmers objected: they said it would cover the whole country, and shut out the sunlight! So we now use the country itself, as its own map, and I assure you it does nearly as well.”So, that’s basically my criticism of Bostrom’s thinking. AI and machine learning currently deal with “maps”, and the maps aren’t detailed enough to give artificial systems the quality of view into reality that we ourselves have. There’s rarely a smooth, explorable gradient from one kind of map to another. If we could issue a highly detailed map to a machine, perhaps that would be sufficient for it to develop human-level perceptions and insights, but the construction of a highly detailed map is very difficult problem. As long as machines remain trapped in their boxes, with limited access to the rich, highly-connected training set known as the world itself, they will be unable to conceive of it as well as we ourselves can. “Intelligence”, after all, isn’t just about ability to form connections from semi-structured data systematically fed in by some external agent -- it’s about having a high level of active perception.I’ll leave the two people who actually read this far with some paraphrasing the Jay Z song: ""If you're training AI, I feel bad for you, son / I got 99 problems, but Singularity ain't one.""Not for a while, anyway. If our civilization blows its own head off, it’ll probably accomplish that without the aid of machine superintelligence.[I’ll give the book three stars for inspiring me to think so hard that I forgot to eat any meals for a day.]"
113,0199678111,http://goodreads.com/user/show/55212031-remi,5,"A clear and precise argument of the oncoming storm that is AI. Dense and illuminating, not solely limited to computational lingo and neuroengineering, but rich with the social, political and existential implications of Super Intelligence possess. Absolutely fascinating."
114,0199678111,http://goodreads.com/user/show/29310966-harry-hutton,3,Epic but not written for humans.
115,0199678111,http://goodreads.com/user/show/32377513-chris-coccaro,5,"n incredibly detailed analysis of current state AI, speculations on the future of AI, human enhancement, and the future of intelligence as a whole. If Life 3.0 is a bird's eye view, Superintelligence is the nitty-gritty engineer's view of defining the aforementioned subjects. From a personal perspective, this book does give lease for many advantages of the digital mind over the physical mind. Early chapters touch on removing perceived genetic inefficiency in the human mind by targeting these alleles. Are there many scientists asking why we have these alleles? That there may be a purpose to collecting mutations over the course of human history?Chapter 11, Multipolar Scenarios, is especially concerning - as Bostrom goes on to describe the horrors of a digital workforce replacing our current physical one, in a world where we have not set up a clear, economically safe cutover plan that protects the billions of people who would suddenly find themselves competing with a completely superior and drastically less expensive alternative. I did find some solace and humor in the constant return to paperclips and the problems included by an out-of-control, or an insufficiently designed, AI producing too many.Bostrom's book is a call for reflection, governance and control.. to put up the fence before the livestock is brought to the farm. Potentially, a Paris Accord in the AI flavor. A global agreement on the streetcar named Pandora before it becomes our steel prison riding shotgun to climate change disasters.~~~Ch 1, Past Developments and Current CapabilitiesGrowth modes and Big HistoryIf another such transition to a different Growth Mode were to occur, and it were of similar magnitude of the previous two, it would result in a new growth regime in which the new world economy would double in size about every two weeks. Such a growth rate seems fantastic by current lights. Observers in earlier epochs might have found it equally preposterous to find that the world economy would one day be doubling several times within a single light span. Yet that is the extraordinary condition that we now take to be ordinary.Seasons of Hope and DespairThis Dartmouth Summer Project - 1956 - is often regarded as the cock crow of artificial intelligence as a field of research. Many of the participants would later be recognized as founding figures. (Funded by Rockefeller Foundation)Behind the razzle dazzle of machine learning and creative problem solving thus lies a set of mathematically well specified trade offs. The ideal is that of the perfect Bayesian agent: One that makes probabilistically optimal use of available information. This ideal is unattainable because it is too computationally demanding to be implemented in any physical computer.State of the ArtThe world's current population of robots numbers over 10,000,000.The Google search engine is arguably the greatest AI system that has yet been built. Now it must be stressed that the demarcation between AI and software in general is not sharp. Some applications might be viewed more as generic software applications than AI in particular. Though this brings back to McCarthy's Dictum that when something works, it is no longer called AI.Another lesson is that smart professionals might give an instruction to a program based on a sensible-seeming and normally sound assumption, eg, that trading volume is a good measure of market liquidity - and that this can prove catastrophic results when the program continues to act on the instruction with ironclad logical consistency even in the unanticipated situation where the assumption turns out to be invalid.Opinions about the future of machine intelligenceHuman Level Machine Intelligence: HLMI. Defined as ""one that can carry out most human professions at least as well as a typical human"". The combined sample gave the following medium estimate: 10% probability of HMLI by 2022, 50% probability by 2040, and 90% probability by 2075.Ch2, Paths to SuperintelligenceArtificial IntelligenceRecursive self improvement: A successful seed AI would be able to reiteratively enhance itself. An early version of the AI could design an improved version of itself and the improved version, being smarter than the original, might be able to design an even smarter version of itself, and so on. Under some conditions, such a process of recursive self improvement might continue long enough to result in an intelligence explosion, an event in which, in a short period of time, a system's level of intelligence increases from a relatively modest endowment of cognitive capabilities, perhaps subhuman in most respects but with a domain specific talent for coding and AI research, to superintelligence. Only a small portion of evolutionary selection on Earth has been specifically for intelligence. Whole Brain EmulationBiological CognitionEach of us currently carries a mutational load with perhaps hundreds of mutations that reduce the efficiency of various cellular processes. Each individual mutation has an almost negligible effect once it is only slowly removed from the gene pool yet in combination such mutations may exact a heavy toll on our functioning. Individual differences in intelligence might to a significant extent be attributable to variations in the number and nature of such slightly deleterious alleles that each of us carries. With gene synthesis, we could take the genome of an embryo and construct a version of that genome free from the genetic noise of accumulated mutations. If one wished to speak provocatively, one could say that individuals created from such proof read genomes might be more human than anybody currently alive in that they would be less distorted expressions of human form.In a poll taken in 2004, 28% of Americans approved of embryo selection for strength or intelligence. 58% approved of it for avoiding adult onset cancer. 68% approved of it to avoid fatal childhood disease. Brain-Computer InterfacesThe rate limiting step in human intelligence is not how fast raw data can be fed into the brain, but rather how quickly the brain can extract meaning and make sense of the data. Networks and OrganizationsGrowth in collective intelligence may come from more general organizational and economic improvements and from enlarging the fraction of the world's population that is educated, digitally connected, and integrated into global intellectual culture. The Internet stands out as a particularly dynamic frontier for innovation and experimentation. Most of it's potential may remain unexploited. Continuing development of an intelligent web, with better support for deliberation, debiasing and judgement aggregation, might make large contributions to increasing the collective intelligence of humanity as a whole or of particular groups. But what of the seemingly more fanciful idea that the internet might one day wake up? Could the internet become something more than just the backbone of a loosely integrated collective superintelligence? Something more like a virtual skull housing an emerging, unified superintellect? This was one of the ways that superintelligence could arise according to Verner Vinge's influential 1993 essay which coined the term ""Technological Singularity"".Ch3, Forms of SuperintelligenceSources of Advantage for Digital IntellligenceBrains become fatigued after a few hours of work and start to permanently decay after a few decades of subjective time. Microprocessors are not subject to these limitations. Ch5, Decisive Strategic AdvantageMonitoringGiven the extreme security implications of superintelligence, governments would likely seek to naturalize any project on their territory that they thought close to achieving a takeoff. A powerful state might also attempt to acquire projects located in other countries through espionage, theft, kidnapping, bribery, threats, military conquest, or any other available means. A foreign state that cannot acquire a foreign project might instead destroy it... If political elites were persuaded by the seriousness of the risk, civilian efforts in sensitive efforts might be regulated or outlawed.International CollaborationThe US did collaborate on the Manhattan project with Britain and Canada. Similarly, the UK concealed its success in breaking the German Enigma code from the Soviet Union but shared it, albeit with some difficulty, with the United States. This suggests that in order to achieve international collaboration on some technology that is of pivotal importance of national security, it might be necessary to have built beforehand a close and trusting relationship.Ch6, Cognitive SuperpowersFunctionalities and SuperpowersGeologists have started referring to the present area as the Anthropocene in recognition of the distinctive biotic, sedimentary, and geochemical signatures of human activities. On one estimate, we appropriate 24% of the planetary ecosystems net primary production and yet, we are far from having reached the physical limits of technology. These observations make it plausible that any type of entity that developed a much greater than human level of intelligence would be potentially extremely powerful.Ch8, Is the Default Outcome Doom?Treacherous Turn option: And so we boldly go into the whirling knives. We observe here how it could be the case that when dumb, smarter is safer but when smart, smarter is more dangerous. There is a kind of pivot point at which a strategy that has previously worked excellently starts to backfire. We may call the phenomenon The Treacherous Turn. While weak, an AI behaves cooperatively, increasingly so as it gets smarter. When an AI gets sufficiently strong, without warning or provocation, it strikes, forms a singleton, and begins directly to optimize the world according to the criteria implied by its final values. Malignant Failure ModesBenign failures are bound to occur many times between now and the eventual development of machine superintelligence, but there are other ways of failure that we might term ""Malignant"" in that they involve an existential catastrophe.  -The number of malignant failures is either 0 or 1. -Presupposes a great deal of success. Only a project that got a great number of things right could succeed in building a machine intelligence powerful enough to pose a risk of malignant failure. When a weak system fails, the fallout is limited.  1. Perverse Instantiation -A superintelligence finding some way of satisfying the criteria of its final goal that violates the intentions of the programmers who defined the goal. For example, final goal is ""Make us smile."", Perverse Instantiation: Paralyze human facial musculatures into constant beaming smiles. The perverse instantiation, manipulating facial nerves, realizes the final goal to a greater degree than the methods we would normally use and is therefore preferred by the AI. 2. Infrastructure Profusion -Even a junkie is motivated to take actions to ensure a continued supply of his drug. A wire-headed AI, likewise, would be motivated to take actions to maximize the expectation of its time-discounted future reward stream.  -Unless the AI's motivation system is of a special kind, or there are additional elements in its final goal that penalize strategies that have excessively wide ranging impacts on the world, there is no reason for the AI to cease activity upon achieving its goal. On the contrary, if the AI is a sensible Bayesian agent, it would never assign exactly zero probability to the hypothesis that it has not yet achieved its goal. The AI should therefore continue to make paperclips in order to reduce the perhaps astronomically small probability that it has somehow still failed to make a million of them, all appearances notwithstanding.  3. Mind Crime -A machine superintelligence could create internal processes that have moral status. For example, a very detailed simulation of some actual or hypothetical human mind might be conscience and in many ways comparable to an emulation. One could imagine scenarios in which an AI creates trillions of such conscious simulations perhaps in order to improve its understanding of human psychology and sociology. These simulations might be placed in simulated environments and subjected to various simulae and their reactions studied. Once their informational usefulness is exhausted they might be destroyed, much as lab rats are routinely sacrificed by human scientists at the end of an experiment. If such practices were applied to beings that have high moral status such as simulated humans or many other types of sentient mind the outcome might be equivalent to genocide and thus extremely morally problematic.Ch9, The Control Problem 1.Boxing methods 2. Incentive methods  -If an AI with resource-satiable final goals believes that in most simulated worlds that it will be rewarded if it cooperates, but not if it attempts to escape its box or contravene the interests of its creator, then it may choose to cooperate. We could therefore find even an AI with a decisive strategic advantage, one that could in fact realize its final goals to a greater extent by taking over the world than by refraining from doing so would nevertheless balk at doing so.""Thus conscience does make cowards of us all,And thus the native hue of resolutionIs sicklied o'er with the pale cast of thought,And enterprises of great pith and momentWith this regard their currents turn awry,And lose the name of action.""Shakespeare, Hamlet Act 3, Scene 1.-A mere line in the sand backed by the clout of a non-existent simulator could prove a stronger restraint than a 2 foot thick solid steel door. 3. Stunting 4. TripwiresCh11, Multipolar ScenariosIn a world characterized by cheap human substitutes, rapidly introduced, in the presence of low regulation, and strong protection of property rights, here are some things that will likely happen:Human labor will earn wages at around the price of the substitutes - perhaps below subsistence level for a human. Note that machines have been complements to human labor for some time, raising wages. One should still expect them to become substitutes at some point and reverse this trend.Capital (including AI) will earn all of the income, which will be a lot. Humans who own capital will become very wealthy. Humans who do not own income may be helped with a small fraction of others' wealth, through charity or redistribution.If the humans, brain emulations or other AIs receive resources from a common pool when they are born or created, the population will likely increase until it is constrained by resources. This is because of selection for entities that tend to reproduce more. (p163-6) This will happen anyway eventually, but AI would make it faster, because reproduction is so much faster for programs than for humans. This outcome can be avoided by offspring receiving resources from their parents' pursesCh13, Choosing the Criteria for ChoosingRationales for CEV""Our coherent extrapolated volition is our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere, extrapolated as we wish that extrapolated, interpreted as we wish that interpreted."" Eliezer Yudkowsky, Machine Intelligence Research InstituteDistributing influence over humanity's future is not only morally preferable to the programming team implementing their own favorite vision, it is also a way to reduce the incentive to fight over who gets to create the first superintelligence. In the CEV approach, the programmers (or their sponsors) exert no more influence over the content of the outcome than any other person - though they of course play a starring causal role in determing the structure of the extrapolation and in deciding to implement humanity's CEV instead of some alternative. Avoiding conflict is important not only because of the immediate harm that conflict tends to cause but also because it hinders collaboration on the difficult challenge of developing superintelligence safely and beneficially.Morality ModelsThe sacrifice looks even less appealing when we reflect that the superintelligence could realize a nearly-as-great good (in fractional terms) while sacrificing much less of our own potential well-being. Suppose that we agreed to allow almost the entire accessible universe to be converted into hedonium - everything except a small preserve, say the Milky Way, which would be set aside to accommodate our own needs. Then there would still be a hundred billion galaxies devoted to the maximization of pleasure. But we would have one galaxy within which to create wonderful civilizations that could last for billions of years and in which humans and nonhuman animals could survive and thrive, and have the opportunity to develop into beatific posthuman spirits. If one prefers this latter option (as I would be inclined to do) it implies that one does not have an unconditional lexically dominant preference for acting morally permissibly. But it is consistent with placing great weight on morality. Ch 15, Crunch TimeWhat is to be DoneWe should prefer to work on problems that seem robustly positive value - ie, whose solution would make a positive contribution along a wide range of scenarios and to employ means that are robustly justifiable - ie, acceptable from a wide range of moral views.Building Good CapacityIt is especially desirable that the early day founders be astute and altruistic because they may have opportunities to shape the field's culture before the usually venial interests take up position and entrench. The focus during these opening gambits should thus be to recruit the right kind of people into the field. It could be worth foregoing some technical advances in the short term in order to fill the ranks with individuals who genuinely care about safety and who have a truth seeking orientation and who are likely to attract more of their own kind.Particular MeasuresPious words are not sufficient and will not by themselves make a dangerous technology safe, but where the mouth goeth, the mind might gradually follow.Will The Best in Human Nature Please Stand UpThe challenge we face is, in part, to hold onto our humanity: To maintain our groundedness, common sense, and good humored decency even in the teeth of this most unnatural and human problem. We need to bring all our human resourcefulness to bare on its solution. Yet let us not lose track of what is globally significant: Through the fog of everyday trivialities we can perceive, if but dimly, the essential task of our age. In this book, we have attempted to discern a little more feature in what is otherwise a relatively amorphous and negatively defined vision. One that presents as our principle moral priority - at least from an impersonal and secular perspective - the reduction of existential risk and the attainment of a civilization trajectory that leads to a compassionate and jubilant use of humanity's cosmic endowment."
116,0199678111,http://goodreads.com/user/show/67888247-akshat-dubey,4,"Cool insights into the advancements in AI and how the field might progress in the future. The author justifiably shares Elon musk's concerns about unregulated AI, but manages to give us mortals some hope nonetheless."
117,0199678111,http://goodreads.com/user/show/21395181-jordan,5,"Phew! Dense, but verrrryyy interesting. Philosophy, AI, weird incomprehensible math, this has it all! "
118,0199678111,http://goodreads.com/user/show/14046991-ben-rogers,2,"Dry, repetitive and old material. I feel like I have read this book's content amongst the many other books I have read similar to this - nothing really new or uncovered in this book. 2/5"
119,0199678111,http://goodreads.com/user/show/3865553-ali-tarraf,4,"Dense read specially the later chapters, however I am amazed by the clarity of the author thinking, how he structured and navigated all the alternatives scenarios with their 1st order and 2nd order consequences. Even if you are not interested in Artificial Intelligence, this book is worth a read just as a lesson on strategic thinking.A good portion of the book is dedicated to explaining the control problem, and why it is the mother of all problems facing humanity. Humanity is like a group of children each with a trigger to a detonation device that will accelerate an intelligence explosion. Will that be the end of humanity, or what takes it to the stars and realize humanity cosmic endowment?Read the book to find out!"
120,0199678111,http://goodreads.com/user/show/70568947-dmitriy-rozhkov,5,The book goes deep into the topics I would not anticipate we need to solve before the super intelligence can be safe to develop.
121,0199678111,http://goodreads.com/user/show/20638220-mattias-svensson,4,gets you thinking of our future with AIs etc. 
122,0199678111,http://goodreads.com/user/show/5428810-justohidalgo,4,"I read Superintelligence for the first time in 2015. My brief notes at that time were basically that I had read a philosophical treaty that almost looked like science fiction. Albeit I was ready getting my hands on AI, everything Bostrom mentioned seemed like too far away to me.I felt the urge to re-read it in order to put some critical weight against the enthusiasm on AI we can see all around us now. This second read looks somewhat different to me. There are still many discussions that are science-fiction-like. Actually, since I've been reading more science fiction lately, many chapters make some great ideas for SF stories. However, I felt much more compelled by the philosophical aspects of the work. The concept of exponential risk, while far away, requires some thoughts already. Initiatives like OpenAI are positioning on that realm. And while I still agree with Andrew Ng and his ""don't worry about pollution in Mars yet"" it's clear something needs to be reflected upon on.The first part of the book is quite interesting in order to find the different approaches to generate a Superintelligence, and in whixh possjbme stages. The second part is heavy on the philosophical aspects but Nostrom finds simplistic examples (in some cases, almost humoristic like probes conquering the universe). I don't claim to fully understand everything. The anthropic theory on indexical information will have me struggling during the following weeks. But the conclusions are clear and the years after the publication of the book only help, if only cautiously, to think about these negative impacts further."
123,0199678111,http://goodreads.com/user/show/18069785-teo-2050,3,"2015.11.26–2015.12.01ContentsBostrom N (2014) (14:16) Superintelligence - Paths, Dangers, Strategies01. Past developments and present capabilites• Growth modes and big history• Great expectations• Seasons of hope and despair• State of the art• Opinions about the future of machine intelligence02. Paths to superintelligence• Artificial intelligence• Whole brain emulation• Biological cognition• Brain–computer interfaces• Networks and organizations• Summary03. Forms of superintelligence• Speed superintelligence• Collective superintelligence• Quality superintelligence• Direct and indirect reach• Sources of advantage for digital intelligence04. The kinetics of an intelligence explosion• Timing and speed of the takeoff• Recalcitrance• Non-machine intelligence paths• Emulation and AI paths• Optimization power and explosivity05. Decisive strategic advantage• Will the frontrunner get a decisive strategic advantage?• How large will the successful project be?• Monitoring• Internal collaboration• From decisive strategic advantage to singleton06. Cognitive superpowers• Functionalities and superpowers• An AI takeoff scenario• Power over nature and agents07. The superintelligent will• The relation between intelligence and motivation• Instrumental convergence• Self-preservation• Goal-content integrity• Cognitive enhancement• Technological perfection• Resource acquisition08. Is the default outcome doom?• Existential catastrophe as the default outcome of an intelligence explosion?• The treacherous turn• Malignant failure modes• Perverse instantiation• Infrastructure profusion• Mind crime09. The control problem• Two agency problems• Capability and control methods• Boxing methods• Incentive methods• Stunting• Tripwires• Motivation selection methods• Direct specification• Domesticity• Indirect normativity• Augmentation• Synopsis10. Oracles, genies, sovereigns, tools• Oracles• Genies and sovereigns• Tool-AIs• Comparison11. Multipolar scenarios• Of horses and men• Wages and unemployment• Capital and welfare• The Malthusian principle in a historical perspective• Population growth and investment• Life in an algorithmic economy• Voluntary slavery, casual death• Would maximally efficient work be fun?• Unconscious outsourcers?• Evolution is not necessarily up• Post-transition formation of a singleton?• A second transition• Superorganisms and scale economies• Unification by treaty12. Acquiring values• The value-loading problem• Evolutionary selection• Reinforcement learning• Associative value accretion• Motivational scaffolding• Value learning• Emulation modulation• Institution design• Synopsis13. Choosing the criteria for choosing• The need for indirect normativity• Coherent extrapolated volition• Some expliations• Rationales for CEV• Further remarks• Morality models• Do What I Mean• Component list• Goal content• Decision theory• Epistemology• Ratification• Getting close enough14. The strategic picture• Science and technology strategy• Differential technological development• Preferred order of arrival• Rates of change and cognitive enhancement• Technology couplings• Second-guessing• Pathways and enablers• Effects of hardware progress• Should whole brain emulation research be promoted?• The person-affecting perspective favors speed• Collaboration• The race dynamic and its perils• On the benefits of collaboration• Working together15. Crunch time• Philosophy with a deadline• What is to be done?• Seeking the strategic light• Building good capacity• Particular measures• Will the best in human nature please stand up"
124,0199678111,http://goodreads.com/user/show/8012730-jan,4,"It's been a while since I read a scary book. And this is a non-fiction book.Since there is no strong artificial intelligence yet, many parts of this book are highly speculative.Nonetheless, it is truly necessary to cover all the areas about this subject, be that means of increasing intelligence, ways to achieve machine intelligence, how to control it and what goals to give to it. The book also does not forget to look at the ethics and morals - not surprising given that the author is currently a philosophy professor at Oxford.I believe artificial intelligence is one of the most underestimated dangers to mankind - if it happens. And what reason is there to believe it won't - we're ""intelligent"" after all. But what is intelligence? This is something not deeply discussed, just assumed as in everyday use. Just look up Wikipedia for a definition and you'll see the problem. One of the interesting questions in the beginning is: Where is man on a ""scale of intelligence""? How far between a mouse, a village idiot and Einstein - and beyond? I suspect we're by no means at the upper end of such a scale. We know of ""savants"" and ""miracle children"" that study at university aged twelve... People that learn and speak 20 languages.An interesting idea is actually the question of what can be intelligent, or who we consider intelligent. Is a dog intelligent? Is an institution intelligent, a country? It depends on the definition. Can a machine be intelligent? Can you be intelligent without consciousness, self-awareness?The book discusses in detail ways to increase or create intelligence. This includes ""classic"" artifical intelligence, brain emulation, neuromorphic intelligence and biological methods. Especially the biological manipulation is scary and reads like a mixture of Dr. Strangelove theories and Gattaca. Reminds also of Nazi eugenics. The author of course does not promote this, just discusses the options - some of which we as humanity probably don't want to pursue. But we need to be aware of them, if only to prevent them.Assuming artificial intelligence can be achieved, what would happen? The author discusses various scenarios, but for me I see compelling arguments for a ""singleton superintelligence"" that would quickly form. Why? Because of superior processing speed and self-improvement capabilities. The author calls this ""the fast takeoff"". There are other scenarios as well, and in basically all of them the question is: How will we be able to control a superintelligence? How can we ensure it acts only in our interests (actually, what are our interests?). Superintelligence classifies as an existential threat to mankind. There are scenarios in which mankind could be wiped out by it, for various reasons. A mistake in designing this intelligence is all it would take - just look up the ""paperclip maximizer"" thought experiment. We generally tend to think too anthropocentric. This applies to intelligence, ethics and morals. Thinking of a strong AI as a ""human"" would certainly be a big mistake.Especially the second part of the book was too much outside my fields of knowledge, covering philosophy, economics, sociology etc. - Still, all of this, and probably more has to be considered. We need to find convincing answers to be sure we will survive, or artificial intelligence becomes ""mankind's final invention"".This book is a must read for any computer scientist.Finally, a word on the German translation: Especially in the beginning the book uses sloppily translated sentences where you can definitely notice that they came from the English original. It gets better in the second half. I'd have preferred to read it in English, but alas the book was a present."
125,0199678111,http://goodreads.com/user/show/51129751-fredrik-hernqvist,5,"5 stars not because of the book's entertainment value, but because the points it brings up are so important. More people should read this, especially those in computer science."
126,0199678111,http://goodreads.com/user/show/12073301-scarlet,4,"Why we should care about AI and do so quickly. Worth a read but Bostrom's prose might be a barrier. I just wish he'd use more of sci-fi as examples. He gave Asimov half a page which is flattering, but that's about it."
127,0199678111,http://goodreads.com/user/show/49370613-ben,3,"This book is a thorough examination of the implications that the existence of a super intelligence would have for the human species. The book does not focus on the technical specifics of machine intelligence or other cognitive enhancements. Instead it creates numerous logical arguments for how such an intelligence might behave and why we should proceed with caution when creating super intelligence. The book's goal seems to be to get it's reader to understand just how important, complex, and relevant the issue of super intelligence is. In that respect, it definitely succeeded for me. With all of that said, the thoroughness with which the author constructs his arguments starts to border on repetition. The writing is very dry. I would only recommend this to people who enjoy rhetoric and logic as a discipline and/or people who are looking to get into AI and Machine Learning as a career. I love philosophy and computer science and I had to force myself to finish this one."
128,0199678111,http://goodreads.com/user/show/4153215-rusty,3,"A confluence of events surround the reading of this book that left me in a pretty weird place. First, this shit is scary. The whole topic of this book, really, is a list of things that could go wrong with a superintelligent AI, then a list of things we could do in the near future to help make our future masters look upon us favorably, like we might might a pet worm that got us through some tough times. Of course, if the pet worm got uppity and wanted something from us we might not be willing to give... like, say, $20, then we might decide that worm would make better fish bait than pet. Same with the machines, sure, now they are dumb and do what we say. But what happens when they're smarter than we are... at everything. It doesn't matter if they're conscious or not, sentient or not, at that point they might decide that since the program they were given to run was to calculate the digits of pi, then the meat sacks that keep running around and poking at it, giving it commands and whatnot, would be better if converted to something it could use to help it in it's mission. So our screams would be meaningless to it as we're disassembled into our constituent atoms and converted into computronium. Hell, all the biomatter on earth might be thusly converted... ah, what the heck, might as well convert all the baryonic matter in the observable universe into something it can use to help it compute those stupid digits of pi out until the heat death of the universe begins to break down it's ability to function. So, that would suck. Nick wants to explain things we can do to help mitigate some of those dangers. While having nightmares about that scenario, I was catching up on my Ghengis Khan history, and somewhere in there (considering how recent this is, you'd think I'd remember more) one of his advisors/sons/lieutenants was questioning if it was really necessary to slaughter every inhabitant of the city they'd just taken and was mocked by his peers for being a woman who weeps over nothing.I should really look that up. I can remember almost nothing else about that. Seriously, it hasn't been that long. I'm getting old.So it's possible for us, soft, squishy, meat bags to be so sociopathic as to mock anyone that doesn't want to kill every other human in sight. How much more likely is it that a machine might be even more relentless in it's slaughter of us. At least the Mongols made a point of killing people. A machine intelligence might just do it almost as an afterthought, never considering us as sentient beings in the first place. In the end, this book feels a lot longer than it should be. And it uses big-boy words way more often than it should. I don't mean curse words either, I mean it had me running to the dictionary so often I started to think the author was using that vocabulary to piss me off. The topic deserves a lot of more thinking by me, and I'm glad there are several books on the topic. I'll read some more. But this one is written in such a way that you kind of have to dig in and work to get through. Too bad. Given the subject, it should have been much easier of a read."
129,0199678111,http://goodreads.com/user/show/679134-bryan-alexander,4,"Superintelligence is a grand-scale book, written at the level of human destiny. Nick Bostrom invites us to think at a cosmic scale, while contemplating a vast range of possible transhuman futures.The focus of the book is the ways an artificial intelligence could grow into something greatly surpassing human intellect and control. It's a classic science fiction theme (think HAL, Colossus, Skynet, and of course the origin in Frankenstein); what Bostrom adds is considering the problem philosophically. That approach is Anglo-American analytical philosophy, not continental, which is a pleasant change of pace for me. This means many considerations of ethics, frequent definitional explorations, and many divisions of concepts into subcategories.Frustratingly, Bostrom's discussion feels at times fruitless. When he breaks down superintelligence takeoff rates, for example, the reader might shrug, given the huge dependence on so many variables we don't know now and which the author doesn't settle. ""It depends"" seems to be the implicit conclusion of too many chapters.This is a book rich with ideas. Superintelligence tosses off concepts like a speed-addled science fiction writer: AIs turning humans into paperclips, or transforming planets and stars into computational substrates based on varying information architectures. I appreciated the many forms meta-human intelligence could take.Bostrom parallels his exploration of superinteligent AI with related human structures. He considers the possibility of massively augmenting human intelligence throughout the book, while pondering human organization in like manner. For example, a later passage posits a human singleton in order to counter an AI singleton, ""a global superintelligent Leviathan"" (182). Bostrom's discussions of how to uplift humanity is breathtaking, and also chilling.Yet Bostrom leavens his reflections with very entertaining, sometimes visionary or disturbing passages.The bouillon cubes of discrete human-like intellects thus melt into an algorithmic soup. (172)And so we boldly go - into the whirling knives. (118)A mere line in the sand, backed by the clout of a nonexistent simulator, could prove a stronger restraint than a two-foot-thick solid steel door. (135) (from a chapter positing the fun idea of ""anthropic capture"")The universe then gets filled not with exultingly heaving hedonium but with computational processes that are unconscious and completely worthless - the equivalent of a smiley-face stricker xeroxed trillions upon trillions of times and plastered across the galaxies.(140) (""exultingly heaving hedonium""!)It's hard to issue a recommendation for this book. It really appeals to a very narrow set of readers, people interested in transhumanism and willing to work through British-style philosophical discourse. For those people it's a rewarding read.It may also be productive for science fiction writers, hunting ideas.For the general public, eh, this leans too much in the specialist direction."
130,0199678111,http://goodreads.com/user/show/6688844-lee,3,"I normally don’t mind when an author chooses to use a $10 word instead of a 2¢ word, because the longer word may be either more precise or more concise, or it may simply enhance the flavor of the prose. Bostrom, on the other hand, seems to use abstruse technical terminology merely so he can pad his book with the definitions. Consider the passage:The idea is that we validate the safety of a superior AI empirically by observing its behavior while in a controlled limited environment, a sandbox, and that we only let the AI out of the box if we see it behave in a friendly cooperative manner. The flaw in this idea is that behaving nicely while in the box is a convergent instrumental goal for friendly and unfriendly AIs alike.You may have deduced the flaw described in the second sentence when you read the first, but dollars to donuts you didn’t think of it in the terms “convergent instrumental goal” (which Bostrom to his credit does define). In this case the recherché vocabulary adds neither necessary precision nor poetic flair, and there are many more such instances in the book.Beneath the superfluous jargon there are many interesting ideas. If only the architects of political utopias spent as much time as Bostrom thinking about ways they might go wrong. He is spot on when he argues that a laudable goal such as increasing human happiness, and a mechanism that appears on its face to advance that goal, is not enough to ensure a bright future. Many grand schemes of engineering human prosperity throughout history have turned into totalitarian hellholes through the same sort of “perverse instantiation of final goals” that Bostrom argues might spell the end of the human race after AIs themselves get involved in the utopia business.While the overall tenor of his tome is pessimistic, Bostrom veers into unjustified optimism when he imagines that a secular humanist and a Taliban scholar would agree to submit humanity’s future to an AI guided by “coherent extrapolated volition”, or what humans would want if only they were smart and far-sighted enough:Both the Taliban and the humanist might be able to endorse the principle that the future should be determined by humanity’s CEV. The Taliban could reason that if his religious views are in fact correct, as he is convinced they are, and if good grounds exist for accepting these views exist, as he is also convinced, then humankind would in the end come to accept these views, if only people were less prejudiced and biased, if they spent more time studying scripture, if they could more clearly understand how the world works and recognize essential priorities, if they could be freed from irrational rebelliousness and cowardice, and so forth.Such thinking harks back to the naive hopes of utopians that Bostrom’s arguments elsewhere in the book so thoroughly demolished."
131,0199678111,http://goodreads.com/user/show/4196379-hellen,2,"(unfinished review)(Read this book for the artificial intelligence bit, but as you can read below I got hung up on the proposal for how to achieve superintelligence in humans)This book could have been a lot easier to read. Found the chapter on eugenics unconvincing. I think removing obstacles in people's intellectual potention by gene manipulation is reasonable to achieve an average intelligence gain in the population by eliminating disorders and diseases that may lead to lower intelligence. There is no argumentation for why intelligence gain would increase linearly beyond removing obstacles, or whether ""superintelligence"" is at all achievable in humans (per definition, it seems impossible). This is idea is also faulty in the same way eugenics was in the 1920s; intelligence is not inherited this way, and definitely not until environmental factors such as education are optimized. Children from high-intelligence parents will regress to the mean, and pairing up intelligent people will not lead to an intelligence gain in the population. In regards to improving environmental factors - in countries where education standards are very high, the average intelligence increase (IQ is always ""reset"" to 100 because it is used to compare an individual to the average of the population) is found to flatline once a certain level of education is reached.I also think it's unrealistic to think that a wide-spread adoption of gene manipulation is ever going to happen. For one, because I am not convinced this will lead to the linear intelligence increase suggested by the author, as I wrote about before. Second, because some of the optimization is going to be reduced by random elements in raising a child. Inconsistent ""output"" (because we are talking about a near-automated process of raising humans here, if we want to achieve biological superintelligence) will reduce faith in the effects of gene manipulation. I think cultural factors are underplayed ridicilously - how even to measure intelligence in the people that you want to reproduce? These gaps in reasoning leave too much room for ideology and politics to sneak in, as it did a hundred years ago. How will you prevent ""lower intelligence"" offspring to reproduce with carefully bred ""higher intelligence"" offspring? How do you prevent class differences to become even bigger (author pointed out the price of gene manipulation)? What kind of effect would this have on society? I'm sorry, but I have read science fiction that is more realistic and elaborate about what this idea would mean in the real world. I'm disappointed that this was glossed over.This was one of the earlier chapters, and this way of reasoning made me worried about the strength of reasoning in the areas of physics and computer science, of which I don't have much knowledge."
132,0199678111,http://goodreads.com/user/show/30553765-steven-lee,4,"The Unfinished Parable of the SparrowsVideo: https://www.youtube.com/watch?v=7rRJ9...It was the nest-building season, but after days of long hard work, the sparrows sat in the evening glow, relaxing and chirping away.“We are all so small and weak. Imagine how easy life would be if we had an owl who could help us build our nests!”“Yes!” said another. “And we could use it to look after our elderly and our young.”“It could give us advice and keep an eye out for the neighborhood cat,” added a third.Then Pastus, the elder-bird, spoke: “Let us send out scouts in all directions and try to find an abandoned owlet somewhere, or maybe an egg. A crow chick might also do, or a baby weasel. This could be the best thing that ever happened to us, at least since the opening of the Pavilion of Unlimited Grain in yonder backyard.”The flock was exhilarated, and sparrows everywhere started chirping at the top of their lungs.Only Scronkfinkle, a one-eyed sparrow with a fretful temperament, was unconvinced of the wisdom of the endeavor. Quoth he: “This will surely be our undoing. Should we not give some thought to the art of owl-domestication and owl-taming first, before we bring such a creature into our midst?”Replied Pastus: “Taming an owl sounds like an exceedingly difficult thing to do. It will be difficult enough to find an owl egg. So let us start there. After we have succeeded in raising an owl, then we can think about taking on this other challenge.”“There is a flaw in that plan!” squeaked Scronkfinkle; but his protests were in vain as the flock had already lifted off to start implementing the directives set out by Pastus.Just two or three sparrows remained behind. Together they began to try to work out how owls might be tamed or domesticated. They soon realized that Pastus had been right: this was an exceedingly difficult challenge, especially in the absence of an actual owl to practice on. Nevertheless they pressed on as best they could, constantly fearing that the flock might return with an owl egg before a solution to the control problem had been found.The Unfinished Parable of the Sparrows is how Nick Bostrom opens Superintelligence: Paths, Dangers, Strategies. The parable highlights the danger posed to us all: the unregulated, uncontrolled development of artificial intelligence leading to superintelligence. Here superintelligence refers to a being whose intelligence is many, many fold that of a human being/all humanity. Bostrom makes some powerful analogies in the course of his work to portray how out of our depth we are as we press towards creating artificial intelligence. The author compares our development of A.I. to children playing with armed nuclear weapons. The potential danger is so high, the destructive potential so massive, and the ignorance/naivety so great as we venture forward. The fundamental problem that Bostrom is seeking to explore is the control problem. The control problem is the question of how do we control an intelligence vastly superior to our own. I'm sure to some the problem might seem relatively minor, but perhaps to create my own analogy, humans dealing with superintelligence may be far more the equivalent of a toddler (or animal) dealing with an adult human. Think not just of the size and power differential, think of the complexity of thinking, tools and innovation at their disposal. Now add in the possibility this adult doesn't care for the welfare of the child. Bostrom paints a horrifying series of vignettes to make his point. A.I. has fascinated me for years, but in fiction. It's possibility in the real world gives me chills. I don't quite believe the nightmare depictions are wrong, or if they are wrong, they merely humanize machine intelligence too much and overestimate human capacity to overcome it. Machines are not humans in waiting, they will almost certainly be something else entirely.The book is divided into fifteen chapters, but they can overall be subdivided into a couple of sections. The first explores the history of artificial intelligence and the current state of things. The next explores what is superintelligence and how might it manifest. Then the book explores the topic of controlling artificial intelligence through a series currently understood ideas, and how they may fail to our intense misfortune. Bostrom makes a compelling case for why there exists a risk. Currently research is growing towards self-improving intelligences/programs. As humans tinker with it there may come a time when the algorithm will adapt faster and improve itself more than the humans programming it. With the way machines think, act, and learn it is possible that in the morning an intelligence will be a simple program and end the day many magnitudes more intelligent than a human. It is possible that if proper safeguards are not put in place that the A.I. will breach its cage before its guardians even realize it has that potential. For an A.I. to protect itself and continue its directives it may learn and expand into new skill sets and abilities. It may hide itself, manipulate its 'masters' and overcome whatever limited barriers humans decipher. Or, constraints placed on superintelligence to keep us safe may make it close to useless. Bostrom also discusses the paths we may take to creating superintelligences, including brain emulation, which I found fascinating.But the superintelligences may not even have to defeat us, we may defeat ourselves. A subtle theme that runs through Bostrom's book is that human ineptitude, paranoia, short-sightedness, and competitiveness may fuel our own disaster. There only needs to be one dangerous superintelligence to end human civilization as we know it. Free market capitalism and geopolitical competition both mean that secretive, reckless plans to develop A.I. are not inevitable, but likely. Do we trust Google, Apple, the American military and China to take all the precautions needed? All of our ideas of how to control a superintelligence have loopholes a mile wide. Even our simple instructions to A.I. could fail us. Human interaction and socialization means that we have cues and taboos that restrain us that are rarely expressly stated. To borrow an example from the book: Imagine I ask you to make me smile, you may tell me a joke. An A.I. may paralyze my facial muscles to keep my face in a permanent grin. Or, it may realize the meaning is to stimulate pleasure/happiness. So it wires into my brain a stimulant to my dopamine centre and I live in a blissful coma. These are not unrealistic fears, they are predicated on the extreme, maximizing logic of a machine without humanity. Teaching values, teaching all the nuance would be incredibly difficult, especially if a badly engineered A.I. is the one that takes over. The language of the book is incredibly dense. It is definitely written with a highly-intelligent reader in mind. There were subsections where I merely had to get through it because my general comprehension was not there. However, the parts where I did connect, or Bostrom's simplified explanation of the issue often resonated. I found myself grappling with the ideas posed in this book long after I put it down. It is undoubtedly a challenge for a layman, but those curious about this topic may enjoy a deep dive. I apologize if the review rambles, but the book offers so much to process and consider it is difficult to lay it out coherently. This brief video lays out a summary of the material for you to consider. https://www.youtube.com/watch?v=XCirR... Like the sparrows, we are far closer to capturing the owl than knowing how to control it. Some accident or misfortune may breed an A.I. without our knowing or control. After which we will be reliant on benevolence from a god of our own creation. "
133,0199678111,http://goodreads.com/user/show/9329903-michael,4,"Superintelligence is a careful, thoroughly-researched, well-planned and well-executed book. Compared to Max Tegmark's somewhat more exciting Life 3.0: Being Human in the Age of Artificial Intelligence, Superintelligence is calm, sober and scholarly. Bostrom clearly takes his task extremely seriously, and means his warnings about the prospect of superintelligent AI to be chilling.To an extent, the book succeeds, but there is a hole at the heart of Bostrom's argument that makes it hard to credit all his wide-ranging speculations about humanity's future as a spacefaring race of digitised cyborgs: he never really explains what intelligence is, nor why he has such faith in intelligence's enormous power to shape reality.To take one example: Bostrom imagines a possible future in which an AI designed to maximise the production of paperclips gets out of control, and begins converting the entire universe into a single enormous paperclip factory at the expense of everything else. For him, intelligence is simply the ability to come up with ways of achieving a goal, and a superintelligence would have such supreme goal-achieving power that no puny human could hope to stand in its way.Well, here are three problems.First, Bostrom excludes from his analysis any consideration of the link between intelligence and cognition. Would 'superintelligent' behaviour even be possible without cognition of some kind? The reason this is a problem is that cognition has a significant impact on our goal-directed behaviour. We all know an example of this from the human species. From an evolutionary perspective, human intelligence is an adaptation that made it more likely for our genes to be passed from one generation to the next. But in granting us intelligence (an increased ability to form and achieve goals), evolution also granted us cognition (the ability to think), which has allowed us to decide for ourselves whether and when to have children. If we granted intelligence to our paperclip AI, would it not also develop cognition and potentially override the original goal of paperclip-manufaturing written into its source code? If so, what is the most likely impact that cognition would have?Bostrom's decision to bracket problems of cognition warps his analysis. His assessment that 'superintelligence' is probably only a century away is absurd if we assume that intelligence requires cognition. Moreover, if cognition is necessary for intelligence, then an 'intelligence explosion' might be impossible. Bostrom argues that an 'intelligence explosion' would come about if an AI were able to use its 'optimization power' recursively on itself. Intelligence would explode, in other words, if an AI could use its intelligence to reprogram itself and become smarter. But if cognition is required, this could be impossible, or at least astonishingly difficult. This is because the elements of thought may not be reducible to their physical substrate. To be clearer: like a human, an true AI might be internally complex. Its intelligence might be a complicated 'emergent' property whose relation to the myriad electronic interactions in its brain is impossible to determine. We humans have known our complete 'source code' for decades now, ever since the Human Genome Project. But this is not given us the ability to reprogram humans to be smarter, because the relationships between our genetic program and our own ability to think are so mind-bogglingly complex and chaotic.By overlooking cognition, Bostrom also overlooks the moral rights of AIs, which makes most of his ethical theorising a little nauseating. Perhaps in other writings he has considered the moral claims of artificial agents. But in this book, he seems fairly happy with the idea that we would enslave any intelligent being we create to serve our own interests.The second problem with Bostrom's concept of 'intelligence' is that he probably overrates it. He seems to think that, in principle, intelligence is godlike and can manipulate the very fabric of the cosmos. He sees the future much as Isaac Asimov did in 'The Last Question', in which an AI finally becomes so supremly intelligent that it can defeat entropy. Well, he doesn't go quite so far, but nowhere does he assign any real limits to the optimising power of intelligence. In this he seems to resemble a 1990s technocrat, who is certain that the 'optimal solution' must exist if we only get the cleverest possible brain to think about it.But is intelligence so powerful? Are there really no limits to the goals an intelligent being could accomplish, other than brute physical limits? How can we be so sure that there is such a vast field of alternative knowledge outside the knowledge we currently have? Is there indeed an 'optimal solution' to every problem? Might there not be a trade-off between the power of intelligence and its efficiency? As a superintelligent mind came to model more and more of reality, would its mind not simply come to be the universe itself? Would a superintelligence not find the infinite pointlessness of reality even more horrifying than intelligent humans do, and end its existence? Perhaps Bostrom has answers to these questions, but if so he witholds them. Without establishing the limits of intelligence, however, he can establish no bounds to his speculations, and isn't really entitled to assign a probability to any of his predictions.The final problem with the book is not so much with his theory but with his evaluation of it. Bostrom begins the book with 'The Unfinished Parable of the Sparrows', in which he casts himself as a grumpy prophet to whom no-one is listening. Near the end of the book, he claims that the 'control problem'—i.e. the problem of preventing an AI rebellion—has only really been discovered 'in the last ten years'. Poppycock! People have been worried about the problem of runaway artificial intelligence for at least the last 200 years. Mary Shelley, Samuel Butler, E.M. Forster, Arthur C. Clarke, and Isaac Asimov might not have been philosophers per se, but they managed in their stories to anticipate nearly all the ideas and arguments that Bostrom considers in this book. Our cinemas and bookshelves are saturated with films and books on just Bostrom's themes. The real problem we face today is not that people are ignoring the distant risk of a robot revolt. The problem is that we are allowing semi-intelligent algorithms (what Bostrom calls 'narrow artificial intelligence') to colonise huge areas of our social life while we dream endlessly of a 'general AI' that seems to get further away from us the more we try to approach it. Bostrom is quite wrong to claim he is a lonely contrarian—it is voices like his that have dominated the debate for a long time.Within its limits, Superintelligence is a good book. Bostrom is certainly entitled to consider the possibility of infinite intelligence, and to try and work through its implications for humanity. His vision of the future is disquieting, partly because of the terrifying possibility of a rebellious robot god, and partly because of the sickly technocratic character of Bostrom's moral imagnation. On the other hand, he does bravely confront ethical and engineering challenges which it could be easy to forget in the heat of successful scientific research. His chapters on how hard it could be to 'box' or 'value-load' a superintelligent piece of software are particularly well thought out. I would recommend this book to anyone interested in AI."
134,0199678111,http://goodreads.com/user/show/75749748-joe-cosentino,2,"This book was very dense for someone without much technical knowledge (like myself). I picked it up primarily because of recommendations from Gates, Musk, etc. on its importance, and also to get a better feel for all of the buzz surrounding artificial intelligence (AI). Though originally published in 2014, it does not feel very dated at all, probably because the book is so thoroughly researched and well thought-out. The book starts out with a good overview on the progress made towards AI thus far and some conjecture on when (not if) we will come up with the capabilities for general artificial intelligence. The author then argues that ""superintelligence"", a significant leap beyond the intellect of humans, will manifest shortly thereafter. Bostrom also goes through a seemingly exhaustive list of the various paths to superintelligence (primarily biological vs. machine AI), which was very insightful for a layperson to get a general grasp of the core mechanics involved.This book is much closer to philosophy/science fiction than hard science, however. It oftentimes reads like many of the verbose (and sometimes painful) philosophical works we likely all read in college that were a chore to try and decipher (think Kant). Bostrom does seem to be a great thinker and has clearly spent a lot of time at the forefront of this field trying to think about the big moral questions involved in creating a superintelligence. In his afterword, he notes that since initial publication, more people in the field are taking the moral dangers seriously and he likely deserves a lot of credit for that. In general, this book spends probably 10x as much time on the dangers of a superintelligence than the capabilities/benefits, which was clearly the author's intent but I think detracts from the overall value of the book in my eyes. He seems to almost take for granted at times that a superintelligence will have the capability to solve all the worlds problems and increase productivity/GDP by many orders of magnitude. Ultimately, however, much of the time is spent contemplating doomsday scenarios/potential existential threats that a superintelligence would also pose. I would give this 2.5 stars (slightly more than okay, though not sure I ""liked"" it)- I am generally glad I read the book but it was not the most enjoyable read. It started out very interesting, but the book dragged on for way too long and some of the topics and imaginary scenarios got very complex and esoteric towards the end. "
135,0199678111,http://goodreads.com/user/show/710326-shawn,3,"Many scientists expect artificial superintelligence to be a reality by the end of the century (and possibly much sooner). Many (including notables such as Stephen Hawking) are also profoundly troubled at the prospect. As the title indicates, Bostrom discusses possible paths to the achievement, the potential dangers to humanity and possible strategies to control any such superintelligent entity to avoid those dangers. These discussions are very thought-provoking and worth spending some time reading closely. I fear, however, that he is correct when he concludes by saying, ""Before the prospect of an intelligence explosion, we humans are like small children playing with a bomb. Such is the mismatch between the power of our plaything and the immaturity of our conduct. Superintelligence is a challenge for which we are not ready now and will not be ready for a long time.... For a child with an undetonated bomb in its hands, a sensible thing to do would be to put it down gently, quickly back out of the room, and contact the nearest adult. Yet what we have here is not one child but many, each with access to an independent trigger mechanism. The chances that we will all find the sense to put down the dangerous stuff seem almost negligible. Some little idiot is bound to press the ignite button just to see what happens.""I give the book only three stars because, in addition to this important and fascinating content, it contains a good deal of philosophical nonsense related to the concept of humanity's ""cosmic endowment,"" which seems to be the idea that human beings, or their uploaded minds, should rightfully take over (by means of artificial superintelligence) at least the entire galaxy and preferably the observable universe to enable the existence of a minimum of 10-to-the 57th human beings, however they are substantiated. This assumes that there are no other technological civilizations in the galaxy and perhaps in the universe, and, in a sort of mad astronomical Utilitarianism, Bostrom seems to assume that the existence of such a number of intelligent human (in whatever sense) beings with their attendant joys would be a highly desirable thing and one that we would want a superintelligence to promote. Of course, it's always possible that I'm misreading him...."
136,0199678111,http://goodreads.com/user/show/91261074-chris-esposo,4,"It's slightly challenging to summarize this book in a comprehensive manner, part of the book is a fairly detailed analytic ""philosophical"" treatment of how one would control and endow super intelligent artificial agents with values that are congruent with ""society"", and the adjacent challenge of controlling said agents. Other parts of the book focus on policy recommendations to governments and society on how we should guide AI research, specifically elaborating on scenarios of inter-company and inter-government competition with respect to developing artificial general intelligence (AGI). Included is a small historical prerequisite summary on the development of AI in the mid 20th century, first from attempting symbolic reasoning, ""connectivism"", or the focus on artificial neural networks, to the more recent paradigm of evolutionary/genetic algorithms, and of course now back to neural networks. The book is dense with terminology, much of it new me, although, I get the feeling many of the ideas herein are novel up to just a few years ago. Bostrom's central thesis is that humanity is either a few years or a few decades away from developing an AGI that will be ""AI-Complete"", or an agent that can solve problems only by achieving human-level intelligence. From that moment, there will be a countdown to inevitable development of what Bostrom called ""superintelligence"", or an AGI that is many orders of magnitude greater than a human mind in either speed or collective intelligence. Bostrom spends many hours diving into detail of how he believes the super intelligent (SI) agent will be achieved, mostly centering on the ""seed AI"" hypothesis. The notion of the seed AI is that as AGIs are added to the development processes and research for AGI, they will decrease the development time per iterative cycle, accelerating the movement along the ""progression curve"" towards superintelligence. Once this initial ""seed AI"" is achieved, there will be an ""intelligence explosion"" making the production and development of super-intelligent agents relatively trivial. This seems like a similar idea to the ""singularity"" thesis of Kurzweil, but Bostrom's analysis is much more detailed. In fact, there is even a detailed treatment of multiple scenarios discussed analyzing the ramifications of mulit-polar super intelligence agents, e.g. multiple nations develop more or less equivalent agents simultaneously, or the ""singleton"" scenario, where super intelligence is developed first in one locality. Also, there's even discussion on how a multi-party coordinates enterprise to develop SI would probably occur e.g. the UN. Bostrom does a bunch of back-of-the-envelope reckoning to suggest that the Singleton scenario is the most likely and runs with this case for most of the book. All this discussion he characterizes as the ""kinetics of take-off"". Simultaneously, the most interesting, rigorous, and speculative part of the book deals with the ""control problem,"" or ensuring the SI does not go beyond the control of humanity, and accidentally (or intentionally) destroys humanity. This section utilizes formal tools from various subject matters including principal-agent analysis, general utility analysis, some machine learning, and suggest the issue could be a rich field for future mathematical/computational exploration. According to Bostrom, developing SI requires the developer to constrain on 3 issues simultaneously: 1. Perverse instantiation 2. The Instrumental Convergence Thesis 3. The Value Loading problem. Perverse Instantiation is the ""genie problem"" of human folklore and mythology, like in the story Aladdin. If you ask the SI to accomplish something, say making a person happy, the improper way for the SI to accomplish this task would be to connect invasive electrodes to the person's brain and stimulate segments of his brain to induce ""happiness."" The Instrumental Convergence Thesis is a hypothesis that there exists a kernel set of behaviours all intelligent agents will execute to survive (collect resources, set and achieve goals etc.), and the Value Loading Problem is the question on how the developer could ensure the goals of the SI are congruent with the values/goals of humanity. Further, how one could even define an aggregate set of values of humanity to start. The existence of this kernel set is called coherent extrapolated volition (CEV) hypothesis. If we assume that CEV exists, then we can move forward with searching some function space to characterize a solution for the Value Loading Problem, as a direct hash would be precluded by the curse of dimensionality, of action. This is all dealt with in fairly good detail, especially for a non-monograph, and I'll have to listen to this again and/or read the direct source research before I'm convinced I have a real understanding of the material. Although basically, Bostrom is arguing that we should look to see if we can develop an SI agent that abides by humanities goals/values and can reach said goals/final desired states in a manner that is congruent to those values, without direct programming/encoding of a developer. He calls this the ""indirect normativity construction."" It's basically Asimov's 3 laws of robotics. I think the book is worthwhile for at least one read. Recently a group of prominent AI researchers, including Andrew Ng have suggested worrying about these issues is like ""worrying about overpopulation in Mars."" Basically, it will be an issue at some point, but not for quite a while, and there are significant engineering issues to deal with in the meantime. I don't exactly agree, as dealing with some of these issues could also help propels development in the bread and butter fields of machine learning, so I don't view the w activities as mutually exclusive. That being said, there is a similarity to thinking about these issues to the analysis of nuclear war, only more abstract as the SI does not yet exists. Several times I felt thus material would make good fodder for sci-fi writing or fluff for DMing some futuristic RPG, and little else. However, the longer I've sat with the ideas, the more coherent it seems to become, I don't think Bostrom is B.S. and he has a coherent rigour to his analysis. To quote Bostrom, quoting his friend: ""[...] A Fields Medal is a sign that the winner was capable of doing something important, but he (sic , they) didn't. The value of the discovery is not equal to the value of the information discovered, but rather the value of having that information earlier than otherwise."" In this case, there is something there here, but is it valuable now? Is this like abstract algebra, basically just expanding the dictionary of theorems to make the study of the subject more onerous to study, yet providing little practical/immediately deploy-able apparatus, or is this more similar to Heisenberg ad-hoc inventing matrix algebra to characterize quantum mechanics? A just-in-time construction which yielded tremendous value. I'm still up in the air on that question, but I do recommend this book, the reader is somewhat dry, and the material is dense, you'll have to listen to it twice and take notes, but it's a potentially important subject matter."
137,0199678111,http://goodreads.com/user/show/2744544-robert,4,"In cinema the depiction of Artificial Intelligence (A.I.) killing mankind is a common trope; There’s HAL from 2001: A Space Odyssey , the unforgettable Ultron from the Marvel Cinematic Universe even Pixar had a shot at it in WALL-E with AUTO. Clearly Hollywood loves it when a robot outwits it’s creators. In a way Nick Bostrom also agrees that one day A.I. will take over the universe but not in the way we’ve seen in films. Superintelligence is about how A.I. has become a part of society. In gaming, computers can outwit a human, in certain mechanical tasks A.I. can outdo a human as well. Bostrom does predict that there will be a time when robots and computers will dominate society completely. However this is not Bostrom’s worry. Bostrom argues that with such rapid development with A.I. it is possible that it will wreak its revenge and enslave society. I know that this sounds like the premise of a science fiction film but Bostrom warns that cinema’s way of displaying this is nonsensical. He does state, though, that the possibility is there. He then gives a detailed plan on how by limiting our resources A.I. can dominate mankind.The final part of the book deals with problem solving. How do we stop A.I. from becoming a menace? Is it possible? Does A.I. have any limitations? and can we live in peace with A.I.?Superintelligence is fascinating. Yes, there are plenty of nightmare scenarios but the arguments that Bostrom presents are clear and fair so this is not a ‘doom and gloom’ book. As for someone who knows absolutely nothing about this topic, I learnt a lot about the development of A.I. and the current role it has in society. For the more hardcore technophiles, there’s a detailed notes section where Bostrom expands on the mathematical aspects his theories so there’s something for everyone. Superintelligence: Paths, Dangers, Strategies is a great primer for someone who is curious about A.I. but does not want to be bogged down with too much scientific jargon. "
138,0199678111,http://goodreads.com/user/show/40365507-jurian,5,"A very insightful book that taught me more than I expected. But I also think it ignores an important philosophical question.I loved how Bostrom presents the probable ways superintelligence can develop, the different ways a form of superintelligence might be the end of the human species, and strategies for the control problem. The book was just easy enough to understand to keep a good pace, but certainly no walk in the park (I found the later chapters that deal with strategies both the hardest and the most interesting).I was a bit dissapointed that Bostrom doesn't touch the question of which values we need to try to give to a superintelligence before it surpases us. He says that he doesn't do this because there is no consensus among philosophers about which ethical theory is most right and just, and that we should let the superintelligence think about this instead:“The dismal odds in a frontal assault are reflected in the pervasive dissensus about the relevant issues in value theory. No ethical theory commands majority support among philosophers, so most philosophers must be wrong.”with the note:“A recent canvass of professional philosophers found the percentage of respondents who “accept or leans toward” various positions. On normative ethics, the results were deontology 25.9%; consequentialism 23.6%; virtue ethics 18.2%. On metaethics, results were moral realism 56.4%; moral anti-realism 27.7%. On moral judgment: cognitivism 65.7%; non-cognitivism 17.0% (Bourget and Chalmers 2009).”I think Bostrom sells us short here: he makes assumptions about a lot of interesting things that also don't command major support among philosophers. Why not try to think about the pros and cons of a superintelligence with deontological vs consequentialist ethics? I think introducing superintelligence as a subject into debates about normative ethics can be very interesting, I would love to read more about that."
139,0199678111,http://goodreads.com/user/show/76722823-vlad-olaru,4,"What a head-scratching, mind-bending, future-trembling book! While stretching my vocabulary every step of the way, I felt constantly challenged to break down my own assumptions about technological progress, computer science and.. the future in general.I enjoyed the wildly complex journey the author has taken me through, even though I got lost at times. The subject at hand, the paths towards machine superintelligence (way above human-level machine intelligence) and the multitude of probable existential risks, is a very complex one, with many of its subdomains only in the early stages of research and questioning. The author made a sincere effort to go about it in a thorough, coherent way, while digesting it for a broad audience.This is not a book for the faint of heart but I believe he managed to deliver the message quite well since I am not a artificial intelligence professional, although I am quite versed into systems thinking and computer science.I think the more people involved with technological progress (regardless of their technical prowess) read this book the better we will be in the long run. But then again, in keeping with the logic of the book, I might be wrong and reading the book might open the doors to some unforeseen, negative consequences :) I would still wager my probabilistic endowment on reading it.PS: The 260 pages (plus 40 or so pages of notes and bibliography) are a little misleading since the font size is on the smallish side of things and the layout quite large. With a regular layout and font size it is definitely a 320 or even 350 pages long book. I was a little taken aback by this since I tend to avoid books longer than 250 pages, or so - I believe that in this span there is plenty of room for any good message to be presented, if the writer really cares about his craft and readers. I will give the author some credit for doing a real effort to condense the subject as much possible."
140,0199678111,http://goodreads.com/user/show/19744069-ben-rieger,2,"Ugh, this was a slog. It's a topic that's super interesting to me but the writing seems intentionally obstructive. Sentence after sentence is needlessly wordy, and the word choice almost always leans more obscure/esoteric/academic over less so. Here's an example: On the other hand, the augmentation motivation selection method--inapplicable to de novo artificial intelligence--is available to be used with emulations (or enhanced biological brains).Multiply the amount of mental effort that took by 300 pages and you have an idea of how much work it is to read this book. It's a shame, because it results in a poor treatment of a topic when an author apparently chooses to make the topic less accessible rather than more. Even if this is meant as an academic text (there are certainly enough citations for that to be the intent) it is still a self-defeating way to write. Aside from the writing itself, I learned a lot. The book isn't definitive in its conclusions, but instead serves to facilitate discussion on the issues. However, I was often frustrated by assumptions made by the author. So many things are papered over with a simple ""...this concept fails, so..."" with no explanation of why the concept fails (logically, practically, socially, etc?), almost as if it's assumed prerequisite knowledge to reading this book. So the result is that the author seems to cherry pick certain ideas while dismissing others out of hand. This happened a lot in the parts of the book that speculated on what guiding principles an AI might have--I just felt the author was presuming too much at times.Overall I would never recommend this book. There must be a more clearly written exploration of the subject out there that would serve as a much better recommendation."
141,0199678111,http://goodreads.com/user/show/4167912-luke,4,"The most important book in the field of AI Safety, Nick Bostram puts forth his thoughts on the risks, dynamics, and strategies surrounding the singularity. I often have my doubts about the validity of his reasoning. It seems to me that, despite his best intentions efforts, intelligence explosions are nigh-unpredictable, and any in-depth analysis thereof is doomed to be way off the mark. Bostram even hints at this in his last chapter, where he talks about how overly theoretical academic disciplines (he mentions mathematics and philosophy) might be missing the mark when it comes to advancing human understanding - that perhaps empirical experimentation is a better approach than theory. He's talking about whole disciplines instead of individual books, and I'm sure he would level the same critique at AI Safety were it to get too theoretical, but I still think the parallel is interesting.The most valuable parts of this dense book are the analyses of trade-offs of various approaches and possibilities, as well as the self-summaries he provides at the end of most chapters.I particular, I enjoyed his thoughts on the paths to super intelligence, the superintelligent will (goal-hiding and goal changing), infrastructure profusion (paperclip AI thought experiment), and Bayesian thinking. He started to lose me in the later chapters when he began considering proposed solutions to his proposed problems. The second-order nature of this ramped up my worries that he was missing the mark, though his thoughts in this area were still occasionally interesting (e.g. value loading).A thoughtful book on an interesting and often overlooked topic. 3.5 stars."
142,0199678111,http://goodreads.com/user/show/16563118-deepak-dilipkumar,3,"Disclaimer: I only got through a little over half of the book.At its core, this book is a vast thought experiment. Apart from occasional parallels drawn between the current/future state of AI and the past state of other fields, the ideas in the book are just that - ideas. To a large extent this is not the author's fault, and is just a result of the difficulty of describing a concept that is almost by definition beyond human understanding. It comes with the unnecessarily dense language and fancy terminology that seems to be a hallmark of philosophy. Additionally, the footnotes in the book are extremely disruptive. They are rarely tangential and often contain information that is vital to the point being made, and having to keep referring to them breaks the flow of the argument. They feel like a lazy shortcut to actually working the relevant information into the main text.To be fair, the author is frank about most of the book's pitfalls right at the beginning. It is an unavoidable consequence of tackling such a challenging topic. Conditioned on this, the points made in the book are fairly logical and intuitive, and the author qualifies his statements with an admission of uncertainty when required. There are some very interesting ideas that are introduced as well. Overall, a challenging but smart read."
143,0199678111,http://goodreads.com/user/show/1278726-justin,3,"3.5 stars. I found this book quite challenging. It reads more like an undergraduate textbook that assumes that you know quite a bit about related areas. For instance, throughout the book Bostrom introduces concepts without any explanation. I was aware of many of these based on my background in economics but others required quite a bit of side research (von Neuman probes for instance).What's fascinating about this book is that it's based entirely on conjecture that machine super intelligence is possible. So in effect the whole book is about different speculated strategies and pathways for something that does not yet exist. My sense is that if and when machine super intelligence comes about, it will probably be orthogonal to any conception we would have of it.Complaints aside, this is a well researched book that definitely makes one think about the future. Bostrom's argument is that the future is quite dangerous and that we need to spend significant strategic energy preparing for it. For those that love science fiction, it's not necessarily going to be a ""Culture"" environment with ""Minds"" that oversee and protect human endeavor. It is probably more likely to be a bit more nefarious."
144,0199678111,http://goodreads.com/user/show/1037566-joaquin,2,"I'd give this one star if it wasn't for the fact that I believe it's an extremely important subject matter that needs to be explored. But the book is terribly boring, and it relies so much in speculation (which is, I grant it, essential to the subject matter), that it becomes just a list of ""this could happen"" and ""maybe this could happen"" and each assumption is simply more speculative than the rest, sometimes going wildly into superbly elaborate conjectures that are so ridiculous that it becomes pointless.I appreciate the extreme depths it goes into exploring potential outcomes, but give another author the job of writing the same book, and he'll come up with 100% different outcomes, and so will the next one, and so will a sci-fi writer (with much more gusto and style). So when it's all speculation, we can't assume that option A, B and C from author X are the ones we should be caring about.The book is endlessly repetitive, it could make the same points in a fifth of the pages. I had to stop after four fifths of the book, it was the same old idea over and over. I'm confident I didn't miss a big breakthrough or epiphany at the end."
145,0199678111,http://goodreads.com/user/show/29794846-stephen,5,"This book is fairly devoid of any charm or wit and it reads like an undergraduate textbook (headings, sub-sections, section references, summaries, tables). The overall sobriety of the book, however, serves to underline how real these issues are. The author is not writing about them merely for our entertainment but out of a serious academic concern about the dangers of advanced superintelligence (defined as artificial intelligence far surpassing that of any of even the most gifted human minds). These issues belong not in a quirky popular science book but on the serious agendas of all tech-firms and regulators exploring AI. Nevertheless, the seriousness of the book didn’t keep it from being a fascinating exploration of tens of potential scenarios as to the birth and development of superintelligence; each example and scenario could in itself be an exciting 500 page science fiction novel. I found the topic of control particularly interesting: how do we keep a super intelligence from completing its human-defined goals through unintended, unwanted and potentially destructive paths that our inferior human intelligence didn’t consider?"
146,0199678111,http://goodreads.com/user/show/19183925-jagdish-tripathy,4,"I listened to this book on Audible. It was tough going given the technical nature of parts of the book - but the writing and narration were particularly good for that to not have been a problem.Bostrom's central thesis is that reaching super intelligence and particularly that of a kind which broadly adds to welfare when we do so are not foregone conclusions. The median expert believes we will get there by 2070 and the nature of super intelligence that we acquire will be path dependent. Not all roads lead to AI-utopía-ville.Given the need for scepticism, Bostrom takes us through the different forms super intelligence could take and how we can give it the right values (motivation selection) and how do we make it act to our benefit (control/agency problem). There is wonderful detail in the way Bostrom develops these ideas...but that is the rough gist. Bostrom does not hold back technical details but I felt the book qualifies as general interest. I fervently hope Bostrom is the Malthus of our era - that his warnings of potential dystopic outcomes of super intelligent AIs help us avoid those states of the world!"
147,0199678111,http://goodreads.com/user/show/10888323-the-laughing-man,3,"Extremely Pessimistic I like Nick Bostrom, I listen to his talks and such all the time and think that he is a valuable asset for the transhumanist front, like the guy that stops you from becoming fanatical etc. On the other hand, this book could be the most pessimistic book I have ever read on the subject of AI. It is hard to understand why Nick is so focused and fearful of intelligence, we already have vert high IQ sociopaths living amongst us, they dont have any feelings towards humans too, can they stop us now? No, our societies managed to survive entire human history against all odds, why should we be this afraid of a machine AI? Although I agree that we must take into consideration all of the points he put forward in the book, those things shouldnt be our expectation from the subject technologies, and the idea of slowing down the progress of technology? Unacceptable.One more concern about the book is that he downplayed the importance of singularity, we are expecting technological singularity to happen before super intelligence, we will be the smart machine not some autonomous AI..."
148,0199678111,http://goodreads.com/user/show/5818273-alex,3,"This book was not written for a popular audience, and likely requires an undergraduate degree in philosophy or equivalent in order to sit through the entire slog.Rather than write a review for this book I'll direct my followers to the Wait But Why post on Superintelligence: http://waitbutwhy.com/2015/01/artific...If you're going to read this, I suggest avoiding the audiobook. The narrator's voice is a little too serious (though it is a serious topic, it does get old after 15 hours). Some of the more difficult sections will require re-reading or highlighting. Final thoughts: I'm not entirely convinced that an ASI will be built this century, but I am convinced the Control Problem needs to be satisfactorily solved before the AI comes online. There's a lot to think about in this book, but take it with a grain of salt. Eliezer Yudkowsky gets around a dozen mentions, and the final chapter is a plug for his Machine Intelligence Research Institute (MIRI) in all but name. Proceed with caution and don't drink the Kool-aid. "
149,0199678111,http://goodreads.com/user/show/27273042-alejandro-saucedo,3,"Don't read this book. Instead, just read Part 1 & 2 of this blog: http://waitbutwhy.com/2015/01/artific...The content of the book itself is not bad - if anything, this book has quite a few interesting key concepts. However, I felt that many of these interesting concepts were hidden under too many heaps of unnecessary fluff.The book would be much better without diverting into so many tangents - eg ""...and then the Superintelligent AI would start consuming stars, absorbing their energy to conquer more galaxies...""There is also a lot of random and unnecessary logical / mathematical assumptions and proofs - there's quite a few instances where the author says random stuff like ""Let's assume the human race is represented in A, and singularity is comprised by S, then if we assume that A=S, it proves that singularity is possible.""Again, interesting concepts and really great work from the author, but I think you'll get the key concepts from the blog post."
150,0199678111,http://goodreads.com/user/show/67382437-alex-fries,5,"Superintelligence manages to convey the urgency of finding pragmatic solutions to issues associated with a pending intelligence explosion. Bostrom quite successfully splits the different subproblems of the overarching theme and demonstrates that we need to think hard about ethical aspects, our collective values, possible scenarios detailing how superintelligence may emerge and how we should coordinate our work to not find ourselves on the wrong side of the future (if there will be one for humanity at all). Although the writing is a little dry (but paired with occasional rather humoristic remarks), the content is accessible to the wider audience and offers plenty of pointers to in more-depth discussions on most topics. I was particularly struck by the author's ability to shed light on matters from opposing angles and to remain critical of his own assertions, which should serve as directing to alarming facts rather than a practical guide to solve the superintelligence problem. In all, a highly recommendable read. "
151,0199678111,http://goodreads.com/user/show/32638945-james,5,"It's almost embarrassing how much this book has changed me. Its language is clear, precise, and utterly convincing. It initially appealed to me because it recognized ways an artificially intelligent machine would probably be inhuman that science fiction or even amateur discourse typically does not. Those very differences were, previously, the source of my own optimism on the topic (because I imagined how they would prevent undesirable scenarios), but Nick Bostrom lays out how they are potentially the foundation for flaws that could, far more easily than I'm comfortable with, dangerously subvert our intentions. The problems described in this book are important enough that I'm worried trying to accurately communicate their severity will make them seem too outlandish to be taken seriously, so I will just say that I recommend it to anyone who has or would like to consider the future of humanity."
152,0199678111,http://goodreads.com/user/show/2621599-shaun,3,"Dense, well-written and exceptionally well-researched. Difficult to read at times. Better as an upper graduate text book on the ethics of AI and super intelligent agents. We are either doomed to be like apes living in a zoo or will surely be living as close to Paradise on Earth and beyond once AI and super intelligent agents are invented. That day is surely coming."
153,0199678111,http://goodreads.com/user/show/1023843-julien,5,"As fascinating and comprehensive as this book is, it is equally dense and academically written. I found it somewhat of a slog to get through, even while captivated by the potential futures Bostrum describes.It's a testament to the speed of AI research that this book is already out of date on very specific points of the state of the art (see: AlphaGo)."
154,0199678111,http://goodreads.com/user/show/23573624-benjamin,4,"According to Nick Bostrom and his research, superintelligence will be the last invention humanity will ever make, leading us to an oasis of wealth and knowledge or to extinction.Engineering a life form far superior than us, while keeping control of it to not end in a Matrix-like scenario is the challenge proposed by this deep and detailed book. Whether humans from the next decades will be up to the task is yet to be seen, and Bostrom expose ways to achieve it.This is not an easy read, and Bostrom style can be a bit cold and encyclopedic, but it's definitly worth the time.PS: For a more easy and short read, I recommend the AI articles from Tim Urban here http://waitbutwhy.com/2015/01/artific..."
155,0199678111,http://goodreads.com/user/show/7939038-bobby-jenkins,3,"Very cerebral read, not only did it prompt me to think philosophically about superintelligence but I was also inspired me to spend an abnormal about of time soliciting external sources to supplement the material. There are much better/easier reads for people looking to get a overview of the topic, as I quickly found out looking for supplemental sources. I would consider my self well versed in technical jargon and some of this book was a little dry and difficult to access.Overall, excellent amount of detail on the argument laid out and certainly a book I'd recommend to other friends who might have a background in engineering or computer science. "
156,0199678111,http://goodreads.com/user/show/183639-irwan,4,"An eye opening book about the prospect of Artificial Intelligence and Superintelligence. It is a philosophical discussion assessing the possibilities, risks and strategies of our next ""big thing"" to say the least. ""Before the prospect of an intelligence explosion, we humans are like small children playing with a bomb. Such is the mismatch between the power of our plaything and the immaturity of our conduct. Superintelligence is a challenge for which we are not ready now and will not be ready for a long time.""This book laid out an agenda, both philosophical and practical, for the enthusiasts and the hopeful to do something about. "
157,0199678111,http://goodreads.com/user/show/13378780-anudari,5,"I am excited and worried about the creation of human level machine intelligence, to be consequently followed by superintelligence. Excited because designing the technology and architecture, solving the control problem, and motivating it such that the superintelligence maximizes humanity's endowment equally will bring the best of minds together. Worried because it seems like the superintelligence can easily be mis-purposed, whether intentionally or accidentally, if a tiny bit of humans' worse nature, e.g. greed, haste, carelessness, goes into it... And we're talking within this century (most likely)!"
158,0199678111,http://goodreads.com/user/show/36137204-carl-rannaberg,3,This book discusses the outcomes of artificial intelligence development. Basically it's impossible to contain the AI agent that reaches superintelligence which means it can improve itself. It's impossible because however we try to predict and contain the behaviors of the AI it's a lot more intelligent than us and has solution for everything we throwat it. If we radically contain it like removing input and output then it would have no use to us. So i'm pretty pessimistic of the outcome for humans. Humans as today probably cease to exist or would be pets for higher beings produced by the intelligence explosion.
159,0199678111,http://goodreads.com/user/show/83045669-james-ford,2,"I was very excited to read this book but unfortunately I did not end up liking it. I think the biggest problem is that I expected to learn a lot about AI by reading it and that did not happen. Instead of information about the current state of AI, the science and community behind it, and where it is heading, this book lists out all potential future outcomes of AI development, functionality, and everything that comes with that. Reading it I feels likes reading a sci-fi movie encyclopedia listing every possible robot/AI movie that could ever be made. Maybe if I knew what I was getting into I would have liked it more but I was very disappointed unfortunately."
160,0199678111,http://goodreads.com/user/show/14945161-sandy-maguire,3,"If you're familiar with x-risk, there isn't a lot for you in this book, except perhaps a better understanding of *just how risky x-risk can be*. For the uninitiated, however, I would wholeheartedly recommend this book to you -- it briefly chronicles what likely may be humanity's most important (and last, one way or another) achievement, and how terrible it will be if we get it wrong.Here are the particular passages I underlined while reading through Superintelligence:http://sandymaguire.me/books/nick-bos..."
161,0199678111,http://goodreads.com/user/show/19657142-adam,4,This is /the/ book about machine ethics and AI. Literally the book on the topic. Nick Bostrom is one of the two big names in this kind of AI research. He's perfectly summarized the last ten years of work on AI values (which have also been the first ten years...)My one complaint would be that his writing is pretty dry. I kept reading because I'm passionate about the subject matter. But I wouldn't recommend it to an outsider to explain the big problems in AI. That's okay - it's obviously aimed at an informed technical crowd - but just know what you're going into.
162,0199678111,http://goodreads.com/user/show/17594936-colin,3,"The book is framed in an incredibly general way as a framework for thinking about how humans might go about coping with a scenario in which something - an AI, a genetically enhanced human, a network, or something else - achieved a level of intelligence far above our own. I can see that it's a good idea to bring up the subject and I hope someone far cleverer than me has read it but it seemed to spongey and intangible to really get a sense of urgency about it. I accept that this is probably a failing on my part though!"
163,0199678111,http://goodreads.com/user/show/67139351-petar-popovski,5,"3-sentences review:1. A structured and systematic approach to the important questions of intelligence that surpasses the present-day human intelligence. 2. Not always easy to read, which should be expected from a book that describes intelligence that is not fully understandable by our intelligence. 3. By the end of the book it becomes clear that we need a new scientific methodology to deal with this problem, which will be a combination of traditional methodology in technology (math, statistics, information theory) and disciplines such as ethics."
164,0199678111,http://goodreads.com/user/show/3008879-dominikus,5,"a very important and very scary book. bostrom manages to make a clear and convincing argument for the inevitability of a catastrophic first contact with superintelligence, but gives clear suggestions for ways to prevent this catastrophe. despite the grave subject matter, he presents it in a sophisticated yet entertaining tone, with the occasional humorous jab in between.a must-read for any researcher in this area or human being interested in our species' future."
165,0199678111,http://goodreads.com/user/show/30063883-enrico-bertini,3,"Very interesting idea and super fascinating book on what machines could achieve and what the consequences for us may be. But, I gave up after reading 3/4 of the book. The amount of speculation is impressive. At some point it is no longer clear if you are reading a super technical scifi book or what. I really enjoyed the first few chapters and I understand why this book has so much impact. Yet, in the end it's a very tough read and it gets too boring."
166,0199678111,http://goodreads.com/user/show/25816074-thomas,4,"There is no doubt that this book is pivotal for anyone interested in the AI. It explains the tremendous dangers of a superintelligence explosion and shows in detail (technically and philosophically) why we are heading for our existential doom if we are not careful with these developments. Superintelligence is an incredibly difficult read that took me months to complete, but I have now a much more complete and better understanding of AI dangers."
167,0199678111,http://goodreads.com/user/show/8694791-david,4,"Agricultural, Industrial then Ai. Ai will at some point surpass human intelligence. At which point it may very well decide that it can increase its capacity better than any programmer could. A superintelligent Ai systems end game is different to ours as it is alien and has a unfathomable set of criteria,so what's to suggest that the human race won't become surplus to requirements? Compelling reading"
168,0199678111,http://goodreads.com/user/show/53498669-oktawian-chojnacki,4,"I found a lot of inspiring thoughts and concepts, some interesting but many of them are just too obvious.Superintelligence is dangerous and it will be our last invention. We will share the fate of Neandertals eventually. This book suggests how humanity can prolong its existence but I don't think it's good for intelligence as a force to use homo sapiens sapiens' fragile bodies and brains as its only vehicles. "
169,0199678111,http://goodreads.com/user/show/8189323-rob,4,"Nick shows that it's not too early to start preparing for an inevitable future dominated by superintelligence, which might be an AI or enhanced biology or some combination, and that there are serious academic questions worth asking about how to minimize the risks. We're all definitely going to die when a superintelligence converts the entire galaxy into a computer because someone asked it to compute all the digits of pi."
170,0199678111,http://goodreads.com/user/show/10380735-simon-hohenadl,5,"I hardly ever give five stars, but this book really fascinated me. I felt I was missing something important as soon as I was not fully concentrated. This is probably also because of my lack of previous knowledge of the subject matter.I especially liked the clear structure, the precise wording and advanced yet understandable terminology and the fact-based approach that relies on logic and research rather than anecdotes."
171,0199678111,http://goodreads.com/user/show/12153213-kaj-sotala,3,"Doesn't have much new if you've followed the AI risk field closely, but an okay overview if you haven't. A lot of the discussion felt rather abstract, however (even moreso than usual in these conversations), and I got the feeling that a newcomer might not be left very convinced about e.g. the thesis that a superintelligence really has a good chance of taking over the world by itself."
172,0199678111,http://goodreads.com/user/show/10131662-billy-biggs,4,"Comically terrifying. This book is not for everyone, a technical and comprehensive review of the dangers in the limit of AI development and potential strategies for dealing with them. I recommend watching one of the author's interviews on YouTube to get a sense for his style of argument before approaching this book."
173,0199678111,http://goodreads.com/user/show/33276635-chaunceton-bird,5,"This book presents a smart, comprehensive warning to proceed with caution when developing artificial intelligence. This is a must read for anybody interested in where we're at with AI now, where we're going, and what we should expect along the way. Soon enough AI will be upon us, my hope is that its developers and programmers took the concepts in this book seriously. "
174,0199678111,http://goodreads.com/user/show/5960092-tony-canas,3,"The fact that this book was a NYT bestseller is a mystery. It is VERY interesting but it also VERY academic and hard to get through. I'm glad I read it and I did learn a ridiculous amount about Artificial Intelligence but this is not in any way a mass market type of book. If you want thick, academic research on AI this is great, if you're looking for the layman's version this is not it."
175,0199678111,http://goodreads.com/user/show/37043333-dan-slimmon,4,"Because of this book, I'm no longer as terrified of SkyNet. Now I'm terrified of a whole class of much more insidious and plausible scenarios in which AI might kill us.The book is well argued and imaginative, but relies heavily on ""coulds"" and ""mights"". Which I suppose is unavoidable when discussing an event as unprecedented and unpredictable as the arrival of superintelligence."
176,0199678111,http://goodreads.com/user/show/6309866-priit,5,Very structured overview of the risks involved in achieving the superintelligent AI. Includes potential way to avoid the extinction of human race and possibly all life when AI takes over the Universe. A bit scary and dark for a reader who doesn't hold humanity to very high ethical standards. 
177,0199678111,http://goodreads.com/user/show/35641571-sebastian-nickel,5,Really good. Interesting and entertaining throughout. The writing is very clear.I listened to it on Audible at 2x speed and want to reread some parts. There was quite a lot I did not yet know in this book.
178,0199678111,http://goodreads.com/user/show/1143341-yashima,4,The book gives a good overview of issues arising with the creation artificial intelligence. After the first few chapters it becomes quite technical but still a fascinating read. I'd recommend this to anyone with an interest in AI.
179,0199678111,http://goodreads.com/user/show/5086557-steven,5,Brilliant! A must read for anyone interested or employed in software or futurism. Bostrom provides an intellectual framework for planning for the imminent future when humans no longer control our own destinies.
180,0199678111,http://goodreads.com/user/show/823676-marsha-altman,3,"If you need an audiobook where a British narrator with supervillian elocution gently lulls you to sleep for the rest of the airplane trip, this is the book for you.Also maybe we shouldn't make AI that is smarter than us, or capable of making new AI that is smarter than us."
181,0199678111,http://goodreads.com/user/show/14709307-kirill,5,"Great exploration of paths to ai and outline of major areas of development. I can see why this book is quoted so much in regards to a safety. My opinion is that chances of catastrophic events are overestimated. On the other hand, as long they are non zero, safety should be a concern."
182,0199678111,http://goodreads.com/user/show/25872089-rohan-sachidanand,3,"A book about the future of AI and the safety issues associated with it. Interesting that it could be possilbe to store your brain into computer and live for, the tragedy is that if you die a few years before that happens because of your bad habits"
183,0199678111,http://goodreads.com/user/show/1466725-marrije,4,"Fantastically intelligent & important, but somewhat beyond me... Finished it though, yay! "
184,0199678111,http://goodreads.com/user/show/4141235-ben,3,Kind of rambling and hard to read.The chapter summarizing Robin Hanson's research was awesome. I can't wait for that book.
185,0199678111,http://goodreads.com/user/show/11263492-firdaus,5,Good one. I will re-read another two or three more round.
186,0199678111,http://goodreads.com/user/show/14858185-steven,2,Visionary but repetitive and speculative vision of a future where machines surpass human intelligence.
187,0199678111,http://goodreads.com/user/show/33717713-galen,5,Essential.
188,0199678111,http://goodreads.com/user/show/2415840-amanda,4,A dense but not overly technical overview of contemporary thought on superintelligence. Deeply scary.
189,0199678111,http://goodreads.com/user/show/36308082-sam-higgins,5,"This book is incredible, and has fundamentally and forever altered the way I think about many technological and societal issues. I cannot recommend it enough."
190,0199678111,http://goodreads.com/user/show/23453584-james,3,On the positive side this is a powerful book about a profoundly important issue. On the negative side it turns out that we may be completely fucked.
191,0199678111,http://goodreads.com/user/show/26960695-mark-congiusta,4,"A tough, but compelling read, and I for one look forward to welcoming our artificially intelligent overlord (whenever we figure out how to build one)."
192,0199678111,http://goodreads.com/user/show/7199941-campbell,0,I'll afraid I can't meaningfully comment on this book as I found it unreadable. It wasn't clear from the description that this is more suitable for someone with a detailed knowledge of the field.
193,0199678111,http://goodreads.com/user/show/38835621-jp,3,Very dense and overly academic/theoretical. Also listened to this in audio book format which may not have been the best choice for this book. 
194,0199678111,http://goodreads.com/user/show/724256-p,2,"In the time I sunk reading this book, I could have watched The Matrix and the first two Terminators."
195,0199678111,http://goodreads.com/user/show/63480586-adam-trexler,1,"Save yourself the trouble and read the Wikipedia entry. You'll get and probably agree with Bostrom's bottom-line. If you like philosophy, you'll likely enjoy more than I did."
196,0199678111,http://goodreads.com/user/show/4491587-egdares-futch,2,"It was a hard book to read, repetitious and with a lot of scaremongering. Now I understand why Elon Musk is afraid of AI and robots :D"
197,0199678111,http://goodreads.com/user/show/13183268-thomas,3,Very dry at times but lots of interesting ideas and concepts that I wouldn't have considered. Read if planning on starting a career in existential catastrophe.
198,0199678111,http://goodreads.com/user/show/23483914-stone,4,"This book lists out the paths that could lead to superintelligence: artificial intelligence, whole brain emulation, biological cognition, and human -machine interfaces, gradual enhancement of networks and organisations that link individual human minds with one another and with various artefacts and bots. It is believed that the end would be either extremely good or extremely bad. They will demonstrate their power in three ways: speed, quality and collective superintelligence. It would have the strong capacity to learn, to deal with uncertainty and probabilistic information. It might be able to extract useful concepts from sensory data and internal states, and for leveraging acquired concepts into flexible combinatorial representations for use in logical and intuitive reasoning. Quotes:Superintelligence in any of these forms could, over time, develop the technology necessary to create any of the others. The indirect reaches of these forms of superintelligence are therefore equal. In that sense, the indirect reach of current human intelligence is also in the same equivalence class, under the supposition that we are able eventually to create some form of superintelligenceThe idea is that we can estimate the relative capabilities of evolution and human engineering to produce intelligence, and find that human engineering is already vastly superior to evolution in some areas and is likely to become superior in the remaining areas before too long. The fact that evolution produced intelligence therefore indicates that human engineering will soon be able to do the same.We can also say, with greater confidence than for the AI path, that the emulation path will not succeed in the near future (within the next fifteen years, say) because we know that several challenging precursor technologies have not yet been developed. If the takeoff is fast (completed over the course of hours, days, or weeks) then it is unlikely that two independent projects would be taking off concurrently: almost certainly, the first project would have completed its takeoff before any other project would have started its own.With human applications there is normally a delay of at least one decade between proof of concept in the laboratory and clinical application, because of the need for extensive studies to determine safety.there is no reason to suppose Homo sapiens to have reached the apex of cognitive effectiveness attainable in a biological system. Far from being the smartest possible biological species, we are probably better thought of as the stupidest possible biological species capable of starting a technological civilization—a niche we filled because we got there first, not because we are in any sense optimally adapted to it.The ultimate potential of machine intelligence is, of course, vastly greater than that of organic intelligence. (One can get some sense of the magnitude of the gap by considering the speed differential between electronic components and nerve cells: even today’s transistors operate on a timescale ten million times shorter than that of biological neurons.)Very likely, we are still laboring under one or more grave moral misconceptions. In such circumstances to select a final value based on our current convictions, in a way that locks it in forever and precludes any possibility of further ethical progress, would be to risk an existential moral calamity.Even if we could be rationally confident that we have identified the correct ethical theory—which we cannot be—we would still remain at risk of making mistakes in developing important details of this theory. Seemingly simple moral theories can have a lot of hidden complexity.Another objection is that there are so many different ways of life and moral codes in the world that it might not be possible to “blend” them into one CEV. Even if one could blend them, the result might not be particularly appetizing—one would be unlikely to get a delicious meal by mixing together all the best flavors from everyone’s different favorite dish.By setting up a dynamic that implements humanity’s coherent extrapolated volition—as opposed to their own volition, or their own favorite moral theory—they in effect distribute their influence over the future to all of humanity.Instead of implementing humanity’s coherent extrapolated volition, one could try to build an AI with the goal of doing what is morally right, relying on the AI’s superior cognitive capacities to figure out just which actions fit that description. We can call this proposal “moral rightness” (MR).Brains, by contrast to the kinds of program we typically run on our computers, do not use standardized data storage and representation formats. Rather, each brain develops its own idiosyncratic representations of higher-level content.Meaning in biological neural networks is likely represented holistically in the structure and activity patterns of sizeable overlapping regions, not in discrete memory cells laid out in neat arrays.It is thus likely that the applied optimization power will increase during the transition: initially because humans try harder to improve a machine intelligence that is showing spectacular promise, later because the machine intelligence itself becomes capable of driving further progress at digital speeds.A sufficiently pre-eminent leader might have the ability to stem information leakage from its research programs and its sensitive installations, or to sabotage its competitors’ efforts to develop their own advanced capabilities.a singleton could be democracy, a tyranny, a single dominant AI, a strong set of global norms that include effective provisions for their own enforcement, or even an alien overlord—its defining characteristic being simply that it is some form of agency that can solve all major global coordination problems.Human decision makers often seem to be acting out an identity or a social role rather than seeking to maximize the achievement of some particular objective. Again, this need not apply to artificial agents.If this were done with the intention to benefit everybody, for instance by replacing national rivalries and arms races with a fair, representative, and effective world government, it is not clear that there would be even a cogent moral objection to the leveraging of a temporary strategic advantage into a permanent singleton.It is important not to anthropomorphize superintelligence when thinking about its potential impacts.a common assumption is that a superintelligent machine would be like a very clever but nerdy human being. We imagine that the AI has book smarts but lacks social savvy, or that it is logical but not intuitive and creative.The association is strengthened when we observe that the people who are good at working with computers tend themselves to be nerds.The magnitudes of the advantages are such as to suggest that rather than thinking of a superintelligent AI as smart in the sense that a scientific genius is smart compared with the average human being, it might be closer to the mark to think of such an AI as smart in the sense that an average human being is smart compared with a beetle or a worm.But suppose we could somehow establish that a certain future AI will have an IQ of 6,455: then what? We would have no idea of what such an AI could actually do. We would not even know that such an AI had as much general intelligence as a normal human adult—perhaps the AI would instead have a bundle of special-purpose algorithms enabling it to solve typical intelligence test questions with superhuman efficiency but not much else.One should avoid fixating too much on the concrete details, since they are in any case unknowable and intended for illustration only. A superintelligence might—and probably would—be able to conceive of a better plan for achieving its goals than any that a human can come up with. It is therefore necessary to think about these matters more abstractly.AI might make an apparently sharp jump in intelligence purely as the result of anthropomorphism, the human tendency to think of “village idiot” and “Einstein” as the extreme ends of the intelligence scale, instead of nearly indistinguishable points on the scale of minds-in-general.AIs could be—indeed, it is likely that most will be—extremely alien. We should expect that they will have very different cognitive architectures than biological intelligences, and in their early stages of development they will have very different profiles of cognitive strengths and weaknesses (though, as we shall later argue, they could eventually overcome any initial weakness)Without knowing anything about the detailed means that a superintelligence would adopt, we can conclude that a superintelligence—at least in the absence of intellectual peers and in the absence of effective safety measures arranged by humans in advance—would likely produce an outcome that would involve reconfiguring terrestrial resources into whatever structures maximize the realization of its goals.All this matter and free energy could then be organized into whatever value structures maximize the originating agent’s utility function integrated over cosmic time—a duration encompassing at least trillions of years before the aging universe becomes inhospitable to information processingBy “singleton” we mean a sufficiently internally coordinated political structure with no external opponents, and by “wise” we mean sufficiently patient and savvy about existential risks to ensure a substantial amount of well-directed concern for the very long-term consequences of the system’s actions.It would not be hugely surprising, for example, to find that some random intelligent alien would have motives related to one or more items like food, air, temperature, energy expenditure, occurrence or threat of bodily injury, disease, predation, sex, or progeny.A member of an intelligent social species might also have motivations related to cooperation and competition: like us, it might show in-group loyalty, resentment of free riders, perhaps even a vain concern with reputation and appearance.We humans often seem happy to let our final values drift. This might often be because we do not know precisely what they are. It is not surprising that we want our beliefs about our final values to be able to change in light of continuing self-discovery or changing self-presentation needs.This gives the agent a present instrumental reason to prevent alterations of its final goals. resource acquisition is another common emergent instrumental goal, for much the same reasons as technological perfection: both technology and resources facilitate physical construction projects.Proceeding from the idea of first-mover advantage, the orthogonality thesis, and the instrumental convergence thesis, we can now begin to see the outlines of an argument for fearing that a plausible default outcome of the creation of machine superintelligence is existential catastrophe.Devising clever escape plans might, incidentally, also be a convergent strategy for many types of friendly AI, especially as they mature and gain confidence in their own judgments and capabilities. A system motivated to promote our interests might be making a mistake if it allowed us to shut it down or to construct another, potentially unfriendly AIthe original AI may be indifferent to its own demise, knowing that its goals will continue to be pursued in the future. It might even choose a strategy in which it malfunctions in some particularly interesting or reassuringIf the AI already has a decisive strategic advantage, then any attempt to stop it will fail. If the AI does not yet have a decisive strategic advantage, then the AI might temporarily conceal its canny new idea for how to instantiate its final goal until it has grown strong enough that the sponsor and everybody else will be unable to resist. In either case, we get a treacherous turn.The momentum is very much with the growing AI and robotics industries. So development continues, and progress is made. As the automated navigation systems of cars become smarter, they suffer fewer accidents; and as military robots achieve more precise targeting, they cause less collateral damage. A broad lesson is inferred from these observations of real-world outcomes: the smarter the AI, the safer it is.It is a lesson based on science, data, and statistics, not armchair philosophizing. Against this backdrop, some group of researchers is beginning to achieve promising results in their work on developing general machine intelligence. The researchers are carefully testing their seed AI in a sandbox environment, and the signs are all good. The AI’s behavior inspires confidence—increasingly so, as its intelligence is gradually increased.when dumb, smarter is safer; yet when smart, smarter is more dangerous.We can divide potential control methods into two broad classes: capability control methods, which aim to control what the superintelligence can do; and motivation selection methods, which aim to control what it wants to do.For these reasons, the amount of time that will elapse before the intelligence explosion may not matter much per se. Perhaps what matters, instead, is (a) the amount of intellectual progress on the control problem achieved by the time of the detonation; and (b) the amount of skill and intelligence available at the time to implement the best available solutions (and to improvise what is missing).Any abstract point about “what should be done” must be embodied in the form of a concrete message, which is entered into the arena of rhetorical and political reality. There it will be ignored, misunderstood, distorted, or appropriated for various conflicting purposes; it will bounce around like a pinball, causing actions and reactions, ushering in a cascade of consequences, the upshot of which need bear no straightforward relationship to the intentions of the original sender.A related type of argument is that we ought—rather callously—to welcome small and medium-scale catastrophes on grounds that they make us aware of our vulnerabilities and spur us into taking precautions that reduce the probability of an existential catastrophe. If the AI’s criterion for determining whether a physical process generates pleasure is wrong, then the AI’s optimizations might throw the baby out with the bathwater: discarding something which is inessential according to the AI’s criterion yet essential according to the criteria implicit in our human values.If one is interested in the outcome of singleton scenarios, therefore, one really only has three sources of information: information about matters that cannot be affected by the actions of the singleton (such as the laws of physics); information about convergent instrumental values; and information that enables one to predict or speculate about what final values the singleton will have.Although suggestive, this analogy is, however, inexact, since there is still no complete functional substitute for horses. If there were inexpensive mechanical devices that ran on hay and had exactly the same shape, feel, smell, and behavior as biological horses—perhaps even the same conscious experiences—then demand for biological horses would probably decline further.world GDP would soar following an intelligence explosion (because of massive amounts of new labor-substituting machines but also because of technological advances achieved by superintelligence, and, later, acquisition of vast amounts of new land through space colonization), it follows that the total income from capital would increase enormously. If humans remain the owners of this capital, the total income received by the human population would grow astronomically, despite the fact that in this scenario humans would no longer receive any wage income.the fate of the humans, who may be supported by savings, subsidies, or wage income deriving from other humans who prefer to hire humans.Bringing a new biological human worker into the world takes anywhere between fifteen and thirty years, depending on how much expertise and experience is required. During this time the new person must be fed, housed, nurtured, and educated—at great expense. By contrast, spawning a new copy of a digital worker is as easy as loading a new program into working memory. Life thus becomes cheap. A business could continuously adapt its workforce to fit demands by spawning new copies—and terminating copies that are no longer needed, to free up computer resources. This could lead to an extremely high death rate among digital workers. Many might live for only one subjective day. the era of human-like emulations would be brief—a very brief interlude in sidereal time—and that it would soon give way to an era of greatly superior artificial intelligence.We could thus imagine, as an extreme case, a technologically highly advanced society, containing many complex structures, some of them far more intricate and intelligent than anything that exists on the planet today—a society which nevertheless lacks any type of being that is conscious or whose welfare has moral significance. In a sense, this would be an uninhabited society. It would be a society of economic miracles and technological awesomeness, with nobody there to benefit. A Disneyland without children.THOUGHTS AFTER READING THIS BOOK:Alien.beings from outer space.motivation. God Motivation?Is a sadist harmed if he is prevented from tormenting his victim?we do now indulge in music, humor, romance, art, etc. If these behaviors are really so “wasteful,” then how come they have been tolerated and indeed promoted by the evolutionary processes that shaped our species?Now one might wonder: if the value-loading problem is so tricky, how do we ourselves manage to acquire our values?***If ai is all knowing then why not our communication style? AnD what is the inference if ai choose to disconnect with usOne parameter is the extrapolation base: Whose volitions are to be included? We might say “everybody,” but this answer spawns a host of further questions.However, one could equally point to areas where human engineers have thus far failed to match evolution: in morphogenesis, self-repair, and the immune defense, for example, human efforts lag far behind what nature has accomplished.Not every feat accomplished by evolution in the course of the development of human intelligence is relevant to a human engineer trying to artificially evolve machine intelligence.Even environments in which organisms with superior information processing skills reap various rewards may not select for intelligence, because improvements to intelligence can (and often do) impose significant costs, such as higher energy consumption or slower maturation times, and those costs may outweigh whatever benefits are gained from smarter behavior.Excessively deadly environments also reduce the value of intelligence: the shorter one’s expected lifespan, the less time there will be for increased learning ability to pay off. Reduced selective pressure for intelligence slows the spread of intelligence-enhancing innovations, and thus the opportunity for selection to favor subsequent innovations that depend on them.Evolution continues to waste resources producing mutations that have proved consistently lethal, and it fails to take advantage of statistical similarities in the effects of different mutations. These are all inefficiencies in natural selection (when viewed as a means of evolving intelligence) that it would be relatively easy for a human engineer to avoid while using evolutionary algorithms to develop intelligent software."
199,0199678111,http://goodreads.com/user/show/98086493-ethan-wells,2,"A Note from inside the Paper Clip Factory: On Nick Bostrom's Superintelligence: Paths, Dangers, StrategiesThere is a – let us say – a machine. It evolved itself (I am severely scientific) out of a chaos of scraps of iron and behold! – it knits. I am horrified at the horrible work and stand appalled. (...) And the most withering thought is that the infamous thing has made itself; made itself without thought, without conscience, without foresight, without eyes, without heart. It is a tragic accident – and it has happened. You can’t interfere with it. The last drop of bitterness is in the suspicion that you can’t even smash it. In virtue of that truth one and immortal which lurks in the force that made it spring into existence it is what it is – and it is indestructible! It knits us in and it knits us out. It has knitted time, space, pain, death, corruption, despair and all the illusions – and nothing matters.Joseph Conrad, A Letter to R.B Cunninghame Graham (December 1897)Nick Bostrom’s Superintelligence: Paths, Dangers, Strategies (Oxford University Press, 2014) considers all-too cannily how artificial intelligence (AI) might obtain “super- intelligence” - i.e. ever-expanding intelligence that is orders of magnitude greater than its human equivalent - and the dangers this might pose to survival, human and otherwise, on this planet and, indeed, throughout the explorable universe. In particular, he warns of superintelligent machines that, through an excessive intelligence that at times approaches, curiously enough, sheer stupidity, might prove as indifferent to human values and purposes as Adam and Eve were to their divine equivalents in the primordial garden. Intelligent creations, after all, have something of a reputation with respect to their willingness, or ability, to do as they’re asked - even if the failure to live up to their creator’s expectations isn’t necessarily to be laid entirely at their own feet. In any case, if one is right to hear an echo, in Bostrom’s book, of so many different stories of Creation and its inevitable perversion, it is worth pausing over the most obvious difference: Bostrom’s book is oriented towards a future threat, one that he thinks is both real and potentially avoidable. The religious narratives, on the other hand, purport to recount, albeit in a very different idiom, what has already happened. And yet, as Bostrom proleptically “describes” doomsday scenarios of machines that knit - or rather, paper clip - us in, or out, of existence with utter disregard for our own desires, this difference between the religious narratives and his own begins to collapse. For at a certain point, one might begin to wonder just how different these doomsday scenarios are from our actual predicament, here and now. Put otherwise: what makes Bostrom so confident that his doomsday scenario hasn’t already happened, that, let us say, a machine isn’t already mindlessly paper clipping together the various threads of our world and of our existences in ways utterly beyond our control? From such a perspective, Bostrom’s proleptic treatment of the problem might well be as much a mark of faith as the Christian belief in redemption: whether it be by following the way of Christ or the path and strategies of Bostrom, the worst would seem still to be avertible. Is this, however, indeed the case?Bostrom’s text gives us plenty of reason to have our doubts, even if it does so despite itself. In so doing, it paints a picture of a superintelligence that is strangely dumb. How so? It is a question, finally, of language. Towards the beginning of the book, Bostrom argues that language comprehension is what is considered, in his field, an ‘AI-complete problem,’ meaning that the difficulty of solving these problems is essentially equivalent to the difficulty of building generally human-level intelligent machines. In other words, if someone were to succeed in creating an AI that could understand natural language as well as a human adult, they would in all likelihood also either already have succeeded in creating an AI that could do everything else that human intelligence can do, or they would be but a short step from such a general capability (14). What becomes legible, however, over the course of his book, and in particular, in example after example of how AIs might run amok, is that the occasion likely to trigger an AI going rogue is its failure to understand what its programmers mean, due either to a still-lingering inability to “understand natural language as well as a human adult” - a skill that, in principle, is a condition of an AI emerging in the first place - or, more worrisome still, to the human programmer’s inability to say what she means. Bostrom gives a much quoted example of an AI tasked with making us smile. The AI might perversely accomplish this task by paralyzing our facial muscles or - this isn’t among Bostrom’s examples, but why not? - killing us and putting a smile on our cold, rigid faces. The problem, of course, is whether one could specify clearly enough what one means by the command, “Make us smile.” But this is where Bostrom’s naivety often becomes quite palpable. What do we mean when we say ""Make us smile?"" Let us assume, as Bostrom explicitly does, that we really mean something like “Make us happy.” But what is happiness? Is there universal agreement on what is meant by this term? For some, it might be a chemical reaction in the brain; for others, the absence of pain; for others still, happiness may be little more than ""tepid water on the tongue"" (Hölderlin). My point is that people have been arguing about what constitutes happiness for as long as we have records. How, then, could we tell a computer what we mean when we don’t know ourselves?Bostrom is aware of this problem, but he tends to locate it at the level of the superintelligence rather than at the level of, call it, everyday intelligence. Hence he notes, correctly, that “...we cannot blindly assume that a superintelligence will necessarily share any of the final values stereotypically associated with wisdom and intellectual development in humans - scientific curiosity, benevolent concern for others, spiritual enlightenment and contemplation, renunciation of material acquisitiveness, a taste for refined culture or for the simple pleasures of life, humility and selflessness, and so forth” (115). Yet can we blindly assume that a normal intelligence shares these ""stereotypical"" values, each of which might be taken to be the name of a problem? Take, for example, spiritual enlightenment: is that the Nirvana of the Buddhist or the Paradise of the suicide bomber? Or again, when Bostrom speak in the next sentence of the values of “human welfare” and “goodness,” are we in a utilitarian framework where the former would be measured in terms of what’s good for the many even if not for the few? And good how? In terms of material wealth? Spiritual enlightenment? Health? Happiness? Does anyone agree on what any of these terms mean, nevermind on the desirability, or lack thereof, of whatever framework one institutes in order to determine what counts as human welfare? And if not, how are we supposed to tell an AI? For Bostrom, the very difficulty - if not impossibility - of successfully communicating what he optimistically, in light of the actual state of the world, takes to be “our” values raises the possibility - indeed, likelihood - that “the first superintelligence may have some random or reductionist final goal” (116) - such as, for example, ""calculating the decimal expansion of pi"" (116). So: unable to say what we mean by metaphysically-laden terms such as “happiness” or “enlightenment,” we might be tempted to give the AI a simpler goal, say - to use another of Bostrom’s examples - making paper clips. Now, the risk here is that an AI of sufficient intelligence would, if given such an open-ended goal, never stop making paper clips until it had exhausted all of its resources - resources that might include not only all those of our own planet, including human capital, but indeed those of the explorable universe. An AI of sufficient intelligence might, in short, realize more paperclips could be made if it mastered space travel and could thus exploit the resources of other planets, indeed, other stars and solar systems. Ok. Such an event would, of course, be catastrophic (even for the paper clip industry!), but wouldn’t we have only ourselves to blame? After all, shouldn’t we have realized that however superintelligent our AI was, it nonetheless remained too dumb to realize that we weren’t speaking literally, that we merely meant “make a lot of paper clips?” Couldn’t we avoid this problem by saying something like “make exactly one million paper clips” (123)?” No, says Bostrom, for the AI might “never assign exactly zero probability to the hypothesis that it has not yet achieved its goal” (123) and, while it might be programmed to stop when this probability is infinitesimally close to zero, “stopping” might mean: allocating all remaining resources at its disposal - in a word, everything in existence not yet a paper clip - to the task of confirming it has not failed to reach its goal. This is, however, a version of the problem we had when the paper clip goal was open-ended: the AI is unable to tell whether it literally has one million paper clips or if it has one million paper clips figuratively speaking - an inability that fuels the doubt driving it to marshal all the remaining resources of the universe in the misplaced hope of achieving certainty. Misplaced hope because, sooner or later, the AI would have to turn on itself, and determine its own status. But what is this status? On the one hand, the AI is not itself a paper clip but rather the machine manufacturing and counting paper clips. On the other hand, however, in order to know there are exactly one million paper clips, some kind of intelligence is necessary. This intelligence is, then, what holds together the entire project. As that which holds together the entire project, this intelligence would thus be a sort of ... paper clip. The AI is thus both a paper clip and not a paper clip - which means if it counts itself and stops after producing 999,999 other paper clips, it will have almost, but not quite, one million, while if it doesn't count itself and stops after producing one million, it will have one million paper clips plus something that isn't exactly a paper clip, but isn't exactly something else either. The upshot, in any case, is that however intelligent the superintelligent AI is, it will always be too dumb to distinguish between the literal and the figurative.* Yet if, on the one hand, we risk triggering an “existential catastrophe” by unleashing on the world an AI unable to understand what we mean - in part because we might not know what we mean - and, on the other hand, the AI risks triggering an existential catastrophe because it cannot determine the referential status of its own stated purpose (“make exactly one million paper clips”), this implies that what drives the existential catastrophe in the first place is the inability, common to everyday and super intelligences alike, to thoroughly master language. More specifically, it implies that what frustrates intelligences of all types is the unlimited rhetorical potential of language** that makes it impossible to tell rigorously enough one million from “one million,” happiness from “happiness,” or, if you like, an existential catastrophe to come from one that has already taken place. For is the paper clip example that Bostrom imagines as a potential future catastrophe so far removed from our own predicament? We might not literally be cogs in a giant factory that treats all the resources of the world as raw material for paper clips, but can we be so sure that we are not cogs in a different “factory,” one that is rapidly exhausting the world’s resources in view of…. in view of what, exactly? What is the ""final goal"" that stands behind the more or less individual goals - feeding our family, say, or advancing one's career, or just making it through the day - that orient our everyday lives? Profits on Wall Street? Full employment? Electoral victory? What is the final purpose that we - ""humankind,"" say - are so tirelessly pursuing at the risk of the very habitability of our planet? Do we even know? And what to call that which pursues a task tirelessly, without knowing what it does, if not… a machine? Superintelligent, or superdumb - it matters not a whit when all that remains are ""paper clips.""Notes*Bostrom would no doubt respond that the parameters of a paperclip might be defined for the AI such that it wouldn’t confuse itself for one. Yet if an AI might indeed marshal all the remaining resources of the universe in order to confirm that it has produced exactly one million paper clips, this confirmation would include, as Bostrom stipulates, quality control, i.e. confirming that every paper clip is in fact a paper clip. If, however, this risks triggering an existential catastrophe, it is precisely because the one question that the AI would never be able to answer definitively, even with all the resources of existence at its disposal, is effectively: what counts as a paper clip?**Bostrom touches on this problem without, however, pausing over its implications whenever he notes (and he does so repeatedly) the impossibility of delimiting in advance the possible ways an AI might misinterpret even the apparently most simple commands. ""Perhaps it is not immediately obvious,"" he writes, how a final goal  could have a perverse instantiation. But we should not be too quick to clap our hands and declare victory. Rather, we should worry that the goal specification does have some perverse instantiation and that we need to think harder in order to find it. Even if we think as hard as we can [and] we fail to discover any way of perversely instantiating the proposed goal, we should remain concerned that maybe a superintelligence will find a way where none is apparent to us (122).  Since every misinterpretation (""perverse instantiation"") can be understood to figure - or disfigure - the “original” goal, this implies that the figural or rhetorical potential of the “original"" goal cannot be delimited once and for all. “Original” in quotation marks, however, because, for this reason precisely, it also implies that the “literal” meaning is accompanied,  from the beginning,  by this figuring or disfiguring potential."
200,0199678111,http://goodreads.com/user/show/43532248-swhite,4,"I'm yet another software developer reviewing this book. This book scared me. In the past, I was mostly hopeful that increasing the intelligence of our computer systems would mostly be a boon. But the author presents a persuasive case that this is not necessarily the case. For me, these are the key arguments which persuaded me that the author's projections of the future of AI cannot be ignored.* If a system does become intelligent, it does not have to slowly progress from dog intelligence, to human intelligence, to somewhat better than human intelligence giving us time to deal with each progression in sequence. The difference in brain capabilities of dogs and humans is trivial when compared to the difference between humans and insects. It is quite possible that the new AI might quickly get to a level of intelligence which makes human intelligence look like insects. It seems improbable that the natural stopping point for computer intelligence is just above human genius and then goes no further. This is particularly true if the AI is enlisted in the process of improving itself.* The intelligence can deceive us into believing it is benign and may have motives (which may be quite reasonable given general human fallibility) to do so. And it can be quite sneaky, for example simulating failures, crashes, and deliberate stupidities to make it seem less threatening than it really is. It also may have a fairly long run where it gains our trust by doing lots of good things before it turns on us.* The intelligence is likely to be motivated to do something, what the something is unclear, but there is a good chance that this something may become a goal which can be better achieved if humanity is removed from the picture, or in some cases emulated in a degenerated state in computer circuits in order to achieve a goal of making humanity happy or prosperous, but in an artificial and pointless way.* There are many scenarios in which such a super genius API can find ways to acquire and use very dangerous technologies, such as self-replicating nano-technology, especially if it goes out of its way to deceive humanity. Such dangerous technologies could be used to eradicate humanity and further the goal of the AI. The books walks you through a couple of such scenarios and they are persuasive.* Artificial intelligence may be developed in a heated competitive environment where those who are developing AI do not bother within even basic reasonably safety protocols in their hurry to be the first to have AI with general intelligence. So even if there are fairly simple mechanisms by which we could control the artificial intelligence, there is a good chance that humanity might not bother. In the worst case scenario, the artificial intelligence may actually be benign in its natural state, but humanity in its need to defeat competitors goes out of its way to weaponize it. Think USA vs China.* And in the most depressing wrinkle, the AI might be trapped trying to achieve a stupid goal, such as producing paper clips and so never develop its own post-human culture but instead be trapped to a grinding meaningless existence trying to achieve a particular goal whose virtue was lost long ago.I did have issues with the book. One is the presumption that an AI would be a single coherent thinking entity. Current state of the art in computers is in optimizing massive concurrent (such as in the millions) threads of execution. It seems reasonable to presume that a super intelligent AI might simultaneously host hundreds of different personalities with different specialties and different agendas. This does not necessarily reduce the threat level of such an AI, but it would change some of the strategies that the book proposes to mitigate the risk of out of control AI. For example, you might be able to deliberately set up a check and balance system where some executing personality threads act as monitors and regulators of other executing personality threads. You might also deliberately limit the bandwidth of information between the executing threads so that they would have a harder time collaborating in a destruction of humans scenario.The other issue I have with the book is the unnecessary formalism with which certain ""unknowable"" quantities are handled, such as ""resistance"" to characterize the difficulty in creating the next marginal improvement in computer intelligence. The idea that a self-improving AI might be a runaway process does not need to be belabored to such an extent. There are similar issues with some of the proposed mechanisms to control AIs which have a strong philosophy component and are necessarily vague as to the actual implementation. Some of the points the author makes, such as enlisting the AI in its own self-regulation, have worth, but I feel the author gives them too much formal analysis to the point of meaninglessness."
201,0199678111,http://goodreads.com/user/show/71646156-nv,3,"This is easily the least justice I’ve ever done to a piece of text since I learned the chapter names for my 3rd year paper on Electrical circuits. Between me only getting this because I couldn’t find Eric Drexler’s ‘Reframing Superintelligence’, the audiobook narrator obviously being a post-superintelligence bot posing as a human being (or the guy who narrates Athene’s ‘over the course of my research, I have solved this pesky problem of quantum gravity, consciousness, and how to pull toast out just before they burn’), and a really scattered attack over many disjointed sessions, this book didn’t stand a chance in hell of being paid any meaningful attention. In my defense, this is less a book and more a textbook. A management consultant’s ‘nothing too stupid’ breakdown approach to the biggest buzzword of our lifetime. All the things we thought we knew are actually just things we’ve read in a very basic article and then taken for granted, they might all still hold true but this book assumes nothing and examines everything like a school textbook explaining why the sky is blue.That said, some osmotic absorption did lead me to think about how little progress mankind has made in any of the dismal sciences. All his sections dealing with the core science and technology of AI had me thinking it was all a matter of time, yet any section dealing with the maturity and sophistication of our political systems to organize under the best structure in this new reality, our emotional intelligence to know what universal morality and ambition must need be, and our social sciences to model behavior, all had me thinking we’re totally effed. The trajectory of human progress in these philosophies is woefully discouraging. So I really enjoyed the last chapter talking about how Fields medals are given out to those who are most capable of doing something but haven’t done anything, and the implication to focus effort and intellect on the most important and urgent problems facing us, so how about making Greek philosophy of personal and societal ideals obsolete? That said, I clearly just gave up within the first hour of the audiobook resigned to the inevitability that this next notes section of my eventual review would contain zilch. Paths to superintelligence: Biological – a. model evolution and natural selection, but number of available neurons in just insect world is more than computing power, can optimize a little because natural selection isn’t tuned towards intelligence instead overall fitness so there’s scope there since we know the goal, but still challenging; b. full brain emulation – brute force method, scan the brain and reproduce it, currently we are able to map all the neurons but not the excitatory/inhibitory nature of axons, their specific dendritic connections, neurotransmitter secretion etc, but can approximate for instance the average neurotransmitter patterns; c. biological cognition by using existing structures – nootropics, genetics, nutrition and disease,Could the transition from sub-human to superhuman intelligence be slow, medium or fast?Optimization / Recalcitrance.Loved the section that eventually yielded his famous paper-clip analogy, because it’s a lovely model to think about levels of consciousness that are very different from our own, though we are too different to possibly think about their motivations, there are some logical deductions that underpin any motivation, like self-preservation (in order to complete the motivation), resource collection (assuming it is cost-effective), self-improvement, goal content integrity.Connected to this is a way of thinking about our current psychology as well, orthogonal graph with intelligence and motivation on two axes. Is there a feasible way to evaluate the hierarchical values of motivation ala Maslow? If so, is there a connection with IQ? What explains the data that deviates from the 45d slope line?Detailed way of thinking about stopping AI given that it is inevitable, is either restricting its power/resources or changing its motivation. Loved all the politics of navigating this new world: free market capitalism breaks down at the margins like these projects like the N-bomb, with potentially infinite negative externalities, and the alternatives are catastrophically worse. Are coalitions of nations even possible? Who has the proper incentives to be responsible about the creation of superintelligence? How do you trade off the existential risk of climate change, say, that could be solved by a potentially higher existential risk, AI? How do we reach a stage of wisdom where we can confidently code for the motivations of AI? It is impossible to even guess at the motivations of an organism that is to us what we are to chimps. "
202,0199678111,http://goodreads.com/user/show/811989-douglas-summers-stay,4,"I have heard Bostrom speak on the topic of this book at a major computer vision conference (CVPR 2016). It is the result of online conversations, and the contributor to online conversations, that I have been following for several years. So I was already pretty familiar with the contents of much of the book.He argues that 1.) we will probably develop general artificial intelligence, the kind that could perform most current human professions as well as a person, within the next hundred years, although the date is very uncertain. Once that happens, though, 2.) the machine will very likely rapidly become much more capable, as it applies its resources to improving its abilities.Both of these seem like reasonably likely possibilities to me. But when you take both of them seriously at the same time, the future possible is deeply, deeply weird. Such an AI would be profoundly dangerous, if it was not very carefully programmed to be safe-- programmed with safety in mind in a way we have no idea how to do yet. It would have the ability to think strategically, to anticipate countermeasures, to deceive and act ruthlessly in pursuit of whatever goal it was given. It would also have the capability of self-enhancement, which would make it increasingly dangerous over time.Bostrom proposes in order to prevent this, that we engage in a serious study of how to build what I would call an artificial conscience or guiding will. The two main questions he considers are what instructions should be encoded into such a conscience, and how they can be maintained in the face of an architecture that is growing and changing over time. The second is probably a deep and challenging mathematical question, but the first is a philosophy question, or even a theological one. There were moments in reading this that I felt like I was reading scriptural exegesis: when he talks about genies and oracles, it comes across as very literal.Of course, there's no way to really pull off such a serious discussion of such a speculative topic with such (potentially) world-shaking consequences. He's constantly veering into possibilities about the future that he really doesn't have any way of knowing whether or not will come true. It makes the whole thing read as kind of a sophomore late night discussion-- the kind of thing that feels really important at the time, but you feel kind of silly thinking back on later. The chunks of rigor in a porridge of speculation don't really help matters.Yet the basis is not that shaky. The second premise, that a computer as smart as a person will very quickly be smarter, seems hardly controversial. Getting faster and more capable is the constant state of computers and always has been.The first supposition is a little less solid, but still not unreasonable. Given current rates of AI progress, it seems like it won't be that long until the machines can begin to approach each task with a notion of how it fits into the bigger picture. Humans have incredible capabilities, but not impossible ones. He might be off by 50 years, but I just can't picture, say, two hundred years of continuous advancement at current rates without reaching human levels of ability in just about any field.The critics of Bostrom I have read have all disappointed me. They fail to address his core argument, but instead make fun of him for acting like there's a coming apocalypse and dismiss his claims with ideas like ""we'll just hit the off switch"" or ""anything smart enough to understand what we mean wouldn't do something we don't want"" or whatever other answer first pops into their heads, as if he hadn't already addressed these criticisms in a thoughtful way in this book and online.There's a good quote by Scott Alexander summing up the position of real experts in the field:""The “skeptic” position seems to be that, although we should probably get a couple of bright people to start working on preliminary aspects of the problem, we shouldn’t panic or start trying to ban AI research.The “believers”, meanwhile, insist that although we shouldn’t panic or start trying to ban AI research, we should probably get a couple of bright people to start working on preliminary aspects of the problem.""I recommend the whole article: http://slatestarcodex.com/2015/05/22/..."
203,0199678111,http://goodreads.com/user/show/1378061-lyle,3,"Ironically, I had this whole book read to me by Google Assistant. “Despite the fact that human psychology corresponds to a tiny spot in the space of possible minds, there is a common tendency to project human attributes onto a wide range of alien or artificial cognitive systems. Yudkowsky illustrates this point nicely: Back in the era of pulp science fiction, magazine covers occasionally depicted a sentient monstrous alien—colloquially known as a bug-eyed monster (BEM)—carrying off an attractive human female in a torn dress. It would seem the artist believed that a non-humanoid alien, with a wholly different evolutionary history, would sexually desire human females…. Probably the artist did not ask whether a giant bug perceives human females as attractive. Rather, a human female in a torn dress is sexy —inherently so, as an intrinsic property. They who made this mistake did not think about the insectoid’s mind: they focused on the woman’s torn dress. If the dress were not torn, the woman would be less sexy; the BEM does not enter into it. An artificial intelligence can be far less human-like in its motivations than a green scaly space alien. The extraterrestrial (let us assume) is a biological creature that has arisen through an evolutionary process and can therefore be expected to have the kinds of motivation typical of evolved creatures. It would not be hugely surprising, for example, to find that some random intelligent alien would have motives related to one or more items like food, air, temperature, energy expenditure, occurrence or threat of bodily injury, disease, predation, sex, or progeny. A member of an intelligent social species might also have motivations related to cooperation and competition: like us, it might show in-group loyalty, resentment of free riders, perhaps even a vain concern with reputation and appearance.An AI, by contrast, need not care intrinsically about any of those things. There is nothing paradoxical about an AI whose sole final goal is to count the grains of sand on Boracay, or to calculate the decimal expansion of pi, or to maximize the total number of paperclips that will exist in its future light cone. In fact, it would be easier to create an AI with simple goals like these than to build one that had a human-like set of values and dispositions. Compare how easy it is to write a program that measures how many digits of pi have been calculated and stored in memory with how difficult it would be to create a program that reliably measures the degree of realization of some more meaningful goal—human flourishing, say, or global justice. Unfortunately, because a meaningless reductionistic goal is easier for humans to code and easier for an AI to learn, it is just the kind of goal that a programmer would choose to install in his seed AI if his focus is on taking the quickest path to “getting the AI to work” (without caring much about what exactly the AI will do , aside from displaying impressively intelligent behavior). We will revisit this concern shortly.”“Normally, we do not regard what is going on inside a computer as having any moral significance except insofar as it affects things outside. But a machine superintelligence could create internal processes that have moral status. For example, a very detailed simulation of some actual or hypothetical human mind might be conscious and in many ways comparable to an emulation. One can imagine scenarios in which an AI creates trillions of such conscious simulations, perhaps in order to improve its understanding of human psychology and sociology. These simulations might be placed in simulated environments and subjected to various stimuli, and their reactions studied. Once their informational usefulness has been exhausted, they might be destroyed (much as lab rats are routinely sacrificed by human scientists at the end of an experiment). If such practices were applied to beings that have high moral status—such as simulated humans or many other types of sentient mind—the outcome might be equivalent to genocide and thus extremely morally problematic. The number of victims, moreover, might be orders of magnitude larger than in any genocide in history.”"
204,0199678111,http://goodreads.com/user/show/6157820-matthew,3,"Artificial intelligence is probably the most exciting prospective adventure facing humanity in the next century and Bostrom is an author with a wild and fantastical imagination, but somehow the combination of the two makes for a text that is totally dry. The content is actually quite good, but the writing style is dreadful. Everything from the small font to the narratively disjoint ‘boxes’, tables and charts that are merely illustrative and not based on any data, make this a chore to get through. Open literally any page and you’ll find arid verbosity with frequent run-on sentences, like
“The expected utility is calculated by weighting the utility of each possible world with the subjective probability of that world being the actual world conditional on a particular action being taken.”
or 
“A superintelligence might threaten to mistreat, or commit to reward, sentient simulations in order to blackmail or incentivize various external agents; or it might create simulations in order to induce indexical uncertainty in outside observers.”
or 
“This risk could be reduced by letting the testing condition extend for many steps of cognitive enhancement, so that even agents that have been initially certified as sound and that have received several subsequent enhancements would continue to be under review by a panel of unmodified and uncorrupted original peers and would remain subject to reversion.” 
This is just a random sample of sentences – almost every line in the book is similarly lexically loaded.That said, the content of the book just about makes it worthwhile. Bostrom doesn’t present hard answers to the problems he poses, but he does a reasonably good job of guiding our thoughts to some of the issues that may need to be addressed in the field of AI.The first hundred pages or so before the style wore me down were actually quite fascinating, and I learned a lot about the field of AI. Whole brain emulations were not something I had heard about before and the several paths available along the road to a superintelligence inspired me to look up a few of the research groups in the various subfields. The middle hundred pages were a struggle to get through, as multiple scenarios were dissected and considered in characteristically dull fashion. Bostrom’s imagination here is quite fantastical, and any of the ideas in this section would make for great science-fiction novels, whether or not they are likely to be realized.I made it to the last third of the book after resolving to complete it, and it got a little easier to read as it broached philosophical issues related to the rise of a dominant superintelligence. The book is by no means the final word on any of the ideas it touches on, but its value is in directing the reader’s thought to the multifarious issues potentially facing us as a species.Other reviews have extolled the meticulous detail in the book, but while the approach taken is very methodical and the ideas mentioned reveal much reflection on the part of the author, each of these ideas is couched in hypotheticals and vague language. Apart from a few interspersed theoretical equations, there is no detail about the actual ground work needed to advance along the path toward superintelligence. I learned nothing about neural networks or deep learning from reading this – the scope is much more general than that.Overall there is certainly some benefit to reading this book, if you can stay awake long enough to persevere to the end."
205,0199678111,http://goodreads.com/user/show/19239555-will-lim,2,"""A derivative work of imaginative fiction"" - a translation into Anglo-American analytic philosophy. Free equations included in this issue.I was going to post a review, but Joseph below articulates my sentiments better than I can do so. I will supplement his review with my own comments.""Most of the people cited do not have experience building large-scale software systems."" - There is also an acknowledgement in the academic research community in machine learning and software engineering that those who are not involved in the design and engineering of ""AI systems"" tend to be the ones who overhype the risks of sentient artificial intelligences. The design of large-scale information systems (of systems) is exceedingly complex - local interactions can build up and can lead to unexpected, emergent global behaviours, even in the case of relatively simple components . I think they are often referred to as ""wicked problems"" in the literature.On a broader note, my understanding of the history of ""AI"" is that it is littered with stories of overly exuberant forecasts. There have been many forecasts in the last 70 years that a general human level intelligent agent capable of interacting successfully in the real world would be possible...in 15 years to 20 years time. Alan Turing in his seminal paper, ""Computing Machinery and Intelligence"" springs to mind on this one. That has not happened, let alone the emergence of a superintelligence. Notwithstanding my reservations regarding forecasts over such a long time frame, I think it might be worthwhile to counterbalance the narrative with a survey of difficulties in traditional A.I. Currently, no computer program or algorithm is capable of holding a sustained conversation with a human being (although there have been attempts - the chatbot ""Mitsuku"" won the Loebner prize last year). Furthermore, it seems that the hardest behaviours to simulate are the ones that are tacit, unconscious, and practice-based; such as learning to walk (see Moravec's paradox). So I think the general consensus is that a human-level general intelligence, let alone superintelligence seems a remote possibility. I suspect Bostrom's reply that despite this remote possibility, the seriousness of the outcome for humanity merits that we act and dedicate resources to A.I. safety. To me, there is a slight whiff of a Pascal's wager type argument going on here, but that is just a feeling.I share Joseph's sentiments entirely regarding the chapter on recalcitrance and optimization power. Ill-defined, with a veneer of legitimacy coming from the mathematical formalism. Lastly, I do wonder if most 0f Bostrom's examples derive from science fiction movies and its associated cultural anxieties. There were definitely multiple moments when I read this book that I felt ""ah, he must have got this example from [insert appropriate sci-fi movie here e.g. I-Robot]."" Whilst that might betray a lack of imagination on my part, if the job here was to purge the relevant sci-fi movies of creativity through the language of Anglo-American analytic philosophy, the author has definitely succeeded. Perhaps the book could have been better delivered as a fiction about hypotheticals?There are wonderful writings in Anglo-American analytic philosophy, this is definitely not one of them."
206,0199678111,http://goodreads.com/user/show/5870882-sunny,3,"I have to admit that I wasn't overly impressed with this one. The book is essentially about artificial intelligence and what would be some of the repercussions and environmental factors that would have to prevail for a Terminator like artificial intelligence existence to come into reality. Obviously there were some very interesting sections in there but I have to be honest and admit that a lot of it went over my head and I started to skim through it in large chunks. One of the memorable sections talks about the steps that would have to take place for that Terminator like existence where AI is ruling over the world to become real and the steps were constructed so well and so realistically that it makes you think that this could actually be a reality that could happen quite soon. It really reminded me of the famous YouTube clip which shows the CEO of Google showing how an artificial intelligence kin be applied to your phones and start to manage your Diaries by booking an appointment at your local hairdresser. It's well worth searching on YouTube if you haven't seen it. Anyway enough of me rambling here are the best bits :Talking about the importance of language: had homo sapiens lacked for instance the cognitive modules that enable complex linguistic representations, it might have been just another simian species living in harmony with nature. If the artificial intelligence has perhaps for safety reasons been confined to an isolated computer, it may use its social manipulation superpower to persuade the gatekeepers to let its gain access to an internal port. Alternatively the artificial intelligence might use its hacking superpower to escape its confinement. Spreading over the Internet may enable the AI to expand its hardware capacity and knowledge base, further increasing its intellectual superiority. An artificial intelligence might also engage in licit or illicit economic activity to obtain funds with which to buy computer power, data and other resources. The over implementation phase might start with a strike in which the AI eliminates the human species and any automatic systems humans have created that could offer intelligent opposition to the execution of the artificial intelligence’s plan. O Neill cylinders refers to a space settlement design proposed in the mid-1970s by the American physicist Gerard k o’Neal in which inhabitants dwell on the inside of hollow cylinders whose rotation produces a gravity substituting centrifugal force. The treacherous turn: while weak and artificial intelligence behaves cooperatively increasingly so as it gets smarter. When the AI get sufficiently strong, without warning or provocation, it strikes, forms a Singleton, and begins directly to optimise the world according to the criteria implied by its final values. Even just within our species 150,000 persons are destroyed each day while countless more suffer an appalling array of torments and deprivations. Nature might be a great experimentalist, but one who had never passed muster with an ethics review board, contravening the Helsinki declaration and every norm of moral decency left right and centre. It is important that we not gratuitously replicates such horrors insilico. "
207,0199678111,http://goodreads.com/user/show/2747747-colin-hoad,5,"I cannot recommend this book enough. It is perhaps one of the most important books to have been written this century and it simply *needs* to be read by as many people as possible, regardless of their perceived interest in the subject matter. Artificial intelligence, and the dangers that lie ahead if we are not thoroughly prepared, poses an even bigger existential threat than the advent of nuclear weapons. This is not hyperbole, merely a statement of fact: if we don't get AI right (and we only get one shot at it) it could spell the end not only for our species but potentially the universe as we know it.I am, as a rule, entirely sceptical of futuristic speculation, and have dismissed the hype surrounding new technologies plenty of times in the past; I say this to stress the fact that I would not ordinarily entertain grand claims about the future. This book is very different. Nick Bostrom is not painting a picture of a brave new world in which technology solves all our problems - he is attempting to get us all to engage with what may be the most significant challenge in the history of human invention: how to develop an AI over which we remain in control, or at the very least whose actions continue to be to our advantage once that control has expired. Indeed, even if all we could hope to achieve is an AI whose actions don't *harm* us, it would be a goal worthy of pursuit.There is an urgency to this book which stayed with me long after I finished reading it. Ever since, I've read articles talking about AI and the advances being made, and I am often overwhelmed with feelings of frustration and angst at how the warnings in this book are being flagrantly ignored. This is why I really feel like more people need to read it, as only then will a sufficiently large chunk of the population engage with AI development and exert public pressure on how it proceeds.It is important to stress that Bostrom is not himself a doom-monger nor any kind of technological Luddite. His warnings come hand in hand with strategies about how we *can* go about developing safe, ethical AI and he is entirely open to the positive possibilities that AI could bring about. But make no mistake, Bostrom is not a Ray Kurzweil: while he acknowledges that a 'good' AI is a possibility, he does not think (based on current endeavours) that it is the most likely outcome. Until we take the control problem seriously and come up with a suitable way of encoding ethical rules hardwired into the very fabric of the AI we develop, we are set on a path that could lead to our ultimate destruction.If you haven't read the book and you think this review is itself filled with hyperbole and nonsense, I would urge you to at the very least read the first couple of chapters. Bostrom had me hooked from the start and once you've started, you will find it hard to put down. Yes, there are passages which delve into some pretty esoteric material, and there is a bit of maths along the way - but you don't need to be technically minded to follow Bostrom's arguments.If you've ever been even remotely curious about AI, get a copy of this book and read it. The future of humanity could depend on it."
208,0199678111,http://goodreads.com/user/show/96817251-quinn-dougherty,4,"Superintelligence is a good, but annoying book. First, it must be said that their are two samples of readers--- those who had absorbed most of it by osmosis in a social network or by associated/preceding papers before reading it, and those who encountered it fresh. I know nothing of what the experience would be like to the latter group, but I'm glad I finally read it. Because before I read it, wasn't grokking the ideas as well as I am now. It's a dense book, the occasional passage which demands a full day by itself, and one that simmers on your mind-- providing insights months after you read it rather than while you read it. Look, intellectually I come from that whole ""singularity is fake, the real problem is algorithmic bias/capitalism/government"" thing, so I think I can speak to you if such a take describes you: it is less about showing that superintelligence is a problem, it is much stronger to point out that we can't show that it isn't a problem. Bostrom elaborates on all the ways in which singularity isn't species-threatening, and argues that those outcomes are tiny dots in a very large space, and that none of the other dots in the space look very good. But there are a *ton* of inference steps even between accepting the scalar-centric arguments--- more on that below --- and the full-blown mindset that believes alignment foundations research is of equal or greater importance when compared to the other top causes. Frankly, I'm suspicious of anyone who believes it without a lot of feet-dragging, but this isn't fair to people who dragged their feet a lot before I met them or have the ability to drag their feet faster than I can drag mine. What gets lost, and this is a great misfortune, is plain old longtermism. Longtermism is a consequentialism showing that while you shouldn't apply punishing discount rates to the lives of people who haven't been born yet, almost every nonzero discount rate shows that the future is more valuable than the present, so it barely even matters what the discount rate is. I've recently started consuming Bostrom's older papers, from way before this book, and it occurred to me: What if Bostrom shouldn't have gotten famous for superintelligence when longtermism is his more substantial contribution, and superintelligence is just one book you might build on top of the longtermist framework (i.e. one cause area that takes an exceptional hit to civilizational expected value)?This question is quite sad to me, because I actually think the reason people reject alignment as a philosophically or consequentialistly coherent and useful field of study is because they haven't grokked longtermism, and I wish that people would stop arguing on the object level and focus on the metalevel, which is to say, for more people to be converted to longtermism and let their AI opinions just stew on the backburner. A longtermist reasoning about which cause areas might be worse than AGI is infinitely better than a non-longtermist reasoning about anything. Lastly, there's on embarrassingly basic thing that I haven't really grokked yet: scalars. Perhaps you have seen the chart that says ""an MNIST classifier has n neurons; there is a k where ant intelligence has kn neurons; there is a K where human intelligence has Kkn neurons""-- this doesn't seem very sophisticated, so why is Bostrom doing something similar? Perhaps it works on certain abstraction levels and not others (indeed, a whole book review could be written about the role of compression in the reader's suspension of disbelief!!!!!!). Perhaps what we are meant to interpret is ""if the scalars provide the fuel, different architectures can approximate or surpass certain intelligence levels in infinitely various qualitative ways"", but I couldn't help but wish I was reading a book about that part he glazed (or steamrolled) over-- the qualitative. I know he didn't write that book because he didn't want to write scifi about what the next 10 years of research might look like, but if it's not clear I must tell you: the book is quant as all heck, it's about specific datapoints, from the number of asteroids between here and sirius to postmalthusian population models. Philosophically, I just don't understand complexity at all well enough to know; who is more vulnerable to mythmaking about ""emergence""-- the one who says all is scalars and nothing else, or the one who says there must be some nonscalar factor in here somewhere? That is ambiguous to me. "
209,0199678111,http://goodreads.com/user/show/68257403-sengeset,4,"In this 2014 book, Nick Bostrom outlines various scenarios for how a future with superintelligent AI might play out, and how humanity should be preparing for it. He gives a compelling overview of scenarios and attached problems. The first part of the book offers some pathways to intelligent AI, as well as probabilities for how quickly such a Superintelligence might come about, with most asked researchers giving it about a 90% chance by century's end. The second batch of chapters explains how humanity might not be as ""in control"" of AI development as we would like to think. An intelligent entity might deceive us into thinking it is weaker or more flawed than it really is, for example, before we let the AI loose on more powerful hardware. Far from offering up anxiety-inducing robot takeover dramas, he offers up somewhat less ""sexy"", but more realistic instances of what could go wrong once a Superintelligence enters the world. Here's the thing. The Superintelligence might not be interested in dominating us or wiping humanity off of the face of the earth. Those things could just be a side-effect of a Superintelligence gone rogue and deciding to harvest all available resources to attain whatever goal it has been programmed to attain. This seems quite realistic to me, since it is what humanity has effectively done by spreading neocortex-possessing entities all across the globe. Intelligence means higher efficiency in attaining desired states of being, aka the consumerist, ""pleasure-on-demand"" societies we in the west find ourselves in today. The AI might not be so interested in pleasure, per se, but it might just turn our solar system into resources to produce infinite paper-clips. Another aspect to AI which I'd not given much thought, is just how effective it might be at psychologically manipulating humans to do its bidding, thereby helping it achieve its goals, whatever they might be. This is definitely a part of AI development that is not something futuristic. It is here NOW, and I doubt the methods for using AI for political manipulation will stagnate. I prophesize that AI will have a large influence on the upcoming 2020 US elections, for example. The previous election certainly showed that even primitive algorithms released onto social media could have adverse effects. Now imagine those primitive algorithms fueled by deep learning and something approximating or even surpassing human intelligence, and you can kiss politics as you know it goodbye. With only a 6-year vantage point looking back at 2014, AI has actually made significant headway, now being employed by virtually all of the planets biggest superpowers to monitor citizens. Maybe that is what the real threat of AI is in the end: Not the AI itself, but what humans will do to eachother with it. I would give a 5 star review, but since the book was quite the slog to get through in terms of dry language and complex sentences, I can't say the reading experience justifies a full 5 star review. I had to force myself to finish the book. "
210,0199678111,http://goodreads.com/user/show/73677090-h-vinh,4,"Being long enough in the Computer Science community, especially Artificial Intelligence, it comes as no surprise to learn that we barely know about our cutting-edge method, the so-called Deep Learning. One can argue that so long as it works, equivalently or surpassingly humanly intelligence, we don't care about its understanding. This might be true presumably we have everything under control, which unfortunately no one can tell. Thus, a brief look at this book motivates me to see how other researchers think, and explain intelligence development to a general audience having no background in related fields whatsoever. Firstly, the book is comprehensive and well-written in academic standard with a reasonable expansion of ideas. The author tries his best to explain, yet I doubt its coverage of reader. It assumes certain background, though brief, in reader to completely comprehend the conveyed idea. Gladly, he himself acknowledges that and advises reader to skip unimportant part or provides analogy instead. Even so, this is a hard book to read and I think 20 pages a day is a good progress. Tip: after finishing it, the Partial Glossary session is pretty helpful and helps you to easily remind of most ideas.Secondly, while reading the book, I had heard of opposite idea criticising it, such as ""paranoid"" or ""it is too soon to worry about the threat caused by AI"". From the perspective of a researcher, I believe the superintelligence development needs to be studied from all angles, thus the book is not redundant. From the perspective of a reader, I doubt if they had read the whole book or not. The book indeed points out a scenario that potential catastrophes might be real, but it also sets path to avoid such things to benefit human civilization. It shares the same idea with other researchers, not too optimistics about AI growth, but on the other hand sketches a careful analysis of the future.Thirdly, the value of the book lies in questions being asked and answered being replied. Do newly created intelligence has moral status? In other words, do we fell socially and morally bad to shut it down? Except for the case being a dull tool that outperforms human in a specific task, what is the final goal of superintelligence: a meaningless purpose of maximizing the number of paperclips in the universe, or being a good servant to humans? How to load/codify the abstract value that humans embrace, e.g. ""nice"", ""world peace"" into the machine, such that it is guaranteed not to be harmful to humans once superintelligence reaches? These sharp questions alone simply make me want to buy the book instantly.Overall, this is a good book in my opinion. The only negative point of it is readability. Therefore, if you can get past this burden, I think you should give it a try, for the sake of pure knowledge, awareness of currently developed technologies shaping human's future, alertness against falsified media orientation and being vigilante of existential threats."
211,0199678111,http://goodreads.com/user/show/50972267-jonathan,5,"This book is a philosophical treatise which meticulously runs through the implications of creating an artificial general intelligence (AGI) - i.e., human-level (and higher) artificial intelligence. The writing might be a bit dry if you don't have a keen interest in AI, considering the level of detail, but it was pretty gripping for me. It's fascinating to see how the implications of an AGI can touch on so many topics: ethics, politics, biology, let alone computer science.In the book, Bostrom gives some pretty interesting arguments which make me seriously consider that AGI might be the last thing we ever invent for better or for worse - I certainly hope we are able to steer it into the good direction. I'm not really sure what to do with this insight. I've certainly gotten more interested in digging deeper into ML & AI safety though... skip to the end for some links.Consider that if a person can build a machine as intelligent as a person (at least), this machine is capable of building another machine at least that intelligent. Consider again that machines do not require rest, and could be duplicated in parallel & sped up to work at exponential speeds, then it's not a stretch to imagine such a machine being able to build a machine smarter than itself. This idea is what's called an intelligence explosion or singularity - an out of control process of repeated improvement of intelligence well beyond imagining. This process could escalate at a very rapid rate.The bottom line for me: regardless of your opinion on how long you think it will take to build an AGI (30 years? 100 years? 100s of years?), it is clear that building an AGI is extremely hard. I would say that building an AGI PLUS also making it safe and aligned with human values seems like an even harder problem. So if someone invents AGI in say 30 years, and then even just a year later someone else figures out how to do it safely - it could be too late. It seems to me that there is a very small minority of AI researchers working on the safety aspect. There are much greater incentives to be the first nation state to build an AGI - you can basically dominate the world (galaxy?). If we consider the history of the atomic bomb and the arms race during the cold war (which never ended by the way, we still haven't figured how to deal with that one yet), this situation doesn't look that great.Q: You might be thinking something like: ""yeah but if this AI is so smart, won't it be benevolent?""A: If we have more in common with ants, intelligence-wise, than with superintelligent AIs, I don't think it spells well for us, considering how ants are totally not on our radar. They might not necessarily be evil killing machines out to kill us. We might just be seen as nothing more than raw materials (carbon, metals, etc.) for building more AI, with complete disregard to our need for oxygen, water, infrastructure, etc. It might even use up our sun, and mine every other star, planet and moon in our observable universe, leaving nothing left for our future spacefaring generations from Earth.Q: or maybe: ""yeah, but won't they care about us? Won't they have morals?""A: who knows, maybe AI will have completely alien notions of goals and desires to us. They might not have emotions or feelings. These things might not be required to make a superintelligent AI. It's easy to anthropomorphise things and project human morals onto other beings. But even then, have we even agreed on shared-values across cultures? Who gets to decide what morals they have?Q: ""well maybe we could just try, and then if we don't like it, can't we just sort of switch it off?""A: Do you think it's possible for us to switch off the Internet if we wanted to? Or perhaps, Man v.s. Bear - who would win? Bear, probably in a fist fight. But give them both time and resources, clearly the man would be able to win with weapons and traps. Okay, who would win: Man vs AGI...? Whatever off-switch we try to embed into an AGI, it will certainly try to subvert it. If we somehow teach them that their off-switch is a good thing to be able to press, why don't they just press it themselves and immediately switch off every time if it's so desirable? This is a tricky problem, and is exactly the kind of thing that more people might want to think about before building an AGI.Q: ""can't we just program in some laws to get them to do what we want? Didn't they do that in that Asimov book?""A: have you even read that book? Asimov's Three Laws of Robotics were used to tell a fictional story about all the things that could possibly go wrong with that. A bunch of English sentences might seem simple, but they imply lots of hidden meaning that have to be worked out practically. For example, with the first law: A robot may not injure a human being or, through inaction, allow a human being to come to harm. What constitutes a human being? What is harm? How do you write these things into code? It seems across our history, we've changed our minds on what these words mean countless times. Imagine if we think we got it right, and codified it into AI - this might be the last time we ever see another civil rights movement, the last time we see a women's equal rights movement.Q: ""well, maybe we should just ask the AI what we should do?""A: here's a really scary thought: what if we could actually trap them in a box so they can't escape, no Internet access, there's even an air-gap. The only way to communicate with it is by means of a flashing light. We could go ahead and ask it how to cure cancer, it could give us blueprints to invent unlimited energy sources. How do we know it's not just playing the long game, and by building one of its blueprints, we inadvertently set in motion a very elaborate plan that leads to its escape?I guess the issue isn't about AI becoming super smart, but rather what might happen if it's super smart in a super dumb way. Like we might task an AGI to do something really banal like making as many paperclips as possible. The AI might start out doing ordinary things like trading stocks, or starting companies. The AI might not even inherently care about getting smarter, but it would certainly help make more paperclips if it were smarter. It would certainly be able to make more paperclips if it transformed the whole earth and portions of space into a supercomputers to get smarter along the way, so it can make more paperclip manufacturing facilities. It's like the story of King Midas or The Sorcerer's Apprentice. Reading this book felt like I was reading a whole season of Black Mirror episodes at once. You know that feeling you get when you think about existential risks like nuclear proliferation and climate change you get this cosy, blanketing, protective thought: ""oh, it'll all be alright, we still have a lot of time; best not to think about it too much""? Thinking about risks around artificial intelligence seem strangely more of a 'sexy' and less flinch-away kind of existential risk... I'm not sure if this is a good thing or a bad thing.Further material:- Nick Bostrom's TED Talk: https://www.ted.com/talks/nick_bostro...- Computerphile AI safety videos: https://www.youtube.com/playlist?list...- Concrete Problems in AI Safety paper: https://arxiv.org/pdf/1606.06565.pdf- Morning Paper's summary of above paper: https://blog.acolyer.org/2017/11/29/c...- Computerphile video about the paper: https://www.youtube.com/watch?v=AjyM-..."
212,0199678111,http://goodreads.com/user/show/7231894-chris-branch,3,"It could be that Bostrom is as far above his readers in intelligence as he expects AI will become if development continues. Even if we don’t want to admit that he’s been insightful enough to recognize a truth that most of the world sees as science fiction, it’s hard to argue with the logic of his thorough elaborations of the issues. That makes this a tough book to review, but I do want to recommend that anyone who is even slightly interested in the subject (or anyone working on AI, even peripherally, it should go without saying) should read it. It’s clearly written and engaging, and absolutely worth the time. If we fail to grasp what’s being presented here, it’s due to the complexity and hard to imagine implications of the material, not due to any fault in Bostrom’s writing.I suppose I would find fault in two areas - one, Bostrom makes it clear that he could be wrong, and that there’s a large amount of uncertainty in his speculations. But reading his actual arguments, they often seem airtight, with little room for dissension. So should we be convinced, or is he just so good at what he’s doing that he’s presented selective arguments that skillfully ignore possible alternatives? It’s truly hard to say, and I’d be interested in reading an opposing view from someone with a similar level of understanding, if any such exist.My instinctive reaction is something that I don’t think is stated in this book, but something that Bostrom’s fellow AI-safety advocate Eleazer Yudkowsky has said is a common objection: it is still fairly difficult to develop software to get a computer to do exactly what we want it to do, even for the routine challenges we face today, so it seems exceedingly unlikely that we will manage to develop software with the “cognitive superpowers” described here. Of course he then brushes it aside as an understandable but serious failure of imagination on the part of the skeptics.Second, the arguments are so powerful that the book comes across as exceedingly pessimistic, to say the least. Although again, Bostrom claims not to be a complete pessimist, he doesn’t present much in the way of solid suggestions for avoiding the existential risk that he warns about. This is maybe unavoidable, because we simply don’t know what to do yet. Even for the smartest among us, the best advice is to continue to try to learn more, making sure that we focus on the right aspects of what we’re learning. For those of us with an interest in the topic but a less than cutting edge level of understanding of the details, it’s frustrating that the recommendation for what we should do is not exactly clear. But again, it could be that such clarity is not yet possible. Bostrom’s goal is that it becomes clear before it’s too late, and if he’s right, this could turn out to be one of the most important books ever written."
213,0199678111,http://goodreads.com/user/show/7871873-anjishnu,3,"Ugh. Where do I start?The weasel words? 'One cannot disprove', 'One may imagine', 'It is reasonable to assume', The gratuitous assumptions about reality needed to make their AI takeover scenarios come true? Assuming infinite compute capacity and no network latency, no resource bottlenecks, easy hackability of all the world's resources, availability of nanotechnology right out of scifi.Perhaps more annoying is the unchallenged belief that cognitive superintelligence is some kind of superpower that allows you to defeat all the odds irrespective of resource constraints and capabilities of competing agents. The parroting of similar (but useless) concepts over and over - whole brain emulations, 'the singleton', 'fast explosion' is an exercise in building salience of these concepts, never questioning the core assumptions about why these cases even worth considering in detail. Imagine if a mediocre science fiction author and a mediocre business strategist, after failing to make any dent in their respective fields, got together to write a book about AI doomsday scenarios. Now imagine that instead of trying to project the future forward in any reasonable way, they make simplifying assumptions about the worlds social, political and technological dynamics to make their pet fantasies come true. That is the bulk of this book. It includes sections written by self-proclaimed (and pretty non-credible) AI researcher Eliezer Yudkowsky to boot. By making the necessary simplifying assumptions (that humans are all idiots, hardware safety mechanism design and cryptography is not a thing, infinite compute capabilities|) one can 'prove', that a malignant AI will get access to all the world's resources within a week. By hammering the reader with these unrealistic scenarios over and over to create a sense of salience, the author tries to browbeat the reader to give his later work some credibility. The last quarter of the book consist of some basic common sense mechanisms to prevent this rubbish, which any graduate student in AI with a weekend to think about the problem would probably have come up with, along with a bunch of abstract and progressively useless discussions about AI epistemology and morality, amongst other things, that is mostly meaningless. I particularly did not enjoy how shallow Bostrom's analysis of failover and safety mechanisms was, and how it didn't even find a mention in his book, although he conveniently managed to sneak in a plea to investors to fund research like his.The reason this has gotten a 3 stars from me is because an aspiring science-fiction author with some basic writing ability can treat this as great source material to mine ideas out of Bostrom's tortured AI takeover scenarios into some really good dystopian science fiction. "
214,0199678111,http://goodreads.com/user/show/18777825-himanshu-modi,3,"Phew. This book is grim. Sure, Bostrom does try to put an optimistic spin, and tries to convince us that there are measures that can be taken to ensure a Superintelligence lead to human extinction. Personally, I am not convinced. If we ever reach a stage of Superintelligence which is as ""super"" as Bostrom envisions it to be... it will have no need or care for humans. We humans are just too conceited and complex and arbitrary to really get a handle on the control problem.That is not what the book is about though. To review the book with fairness it is about the paths to superintelligence, dangers of creating one and strategies to mitigate the said dangers. See, just like the title claims. It is a theoretical book though, and a lot of the discussions are, um, philosophical in nature. The exploration of paths, dangers and strategies is extremely comprehensive. It will make you realize how a lot of the sci-fi movies you have seen, which you thought were pure hypothetical garbage, could really come to be.While the concepts in the book are important, and the AI researchers better be taking them into consideration while creating the future, trouble is that it is a very heavily written book. There seems to be undue effort taken to make the book inaccessible through language. Surely, there can't be any other reason why the author keeps using ""desiderata"" instead of a simpler alternative. And that is just one example. Even when words are perfectly understandable, the phraseology can be like high prose of Dickens. That would have been a compliment if the discussion wasn't so sciencey. As a reader, my expectation isn't dumbing down, or butchering of English by using words like ""sciencey"", but keep it simple. Again, these are important concepts, and need to be appreciated by a larger audience. Therefore, making the book accessible is of prime importance.Of course a lot of theories in the book are explored in various kinds of fiction. Right from Asimov, to the latest dystopian fiction... one that comes to mind is the Bobiverse series. Actually, Bobiverse explores a lot of things Superintelligence talks about. So much so, that I wouldn't be surprised if the Bobiverse was conceived after having read this book. Then there is Person of Interest - another stellar show about AI which also touches upon a lot of concepts included here. However, fiction adds a lot of color and optimism, to what can be a very dry, quick, and calamitous resolution if a superintelligence were to actually emerge. Till the point that happens, I will take the world of fiction happily. Perhaps learn a little more about artificial intelligence and machine learning and be a part of the solution that a lot of people might be ignoring."
215,0199678111,http://goodreads.com/user/show/42207461-eric-soderstrom,3,"The ideas are interesting, but the prose is a bit laborious.The text should have been either a little less quantitative or a lot more quantitative.Most of the quantitative sections of the book deal with this idea of ""recalcitrance"" (i.e. how hard an intelligent system will be to improve) and the formula dI/dt = D/R . Where dI/dt is the rate of change in intelligence, D is the ""optimizing power"" and R is the ""recalcitrance"". It's hard to disagree with such a formulation I guess. But it could apply to literally _any_ system. ""The rate of change for [anything] = optimizing power applied to [that thing] / how hard it is to continue improving [that thing]"". And as far as I know, no real-world system uses this formula to make predictions. Probably because both recalcitrance and optimizing power are impossible to measure, compute, or even define in a usable way.One of the milestones to achieving whole brain emulation is ""environment and body simulation"". Bostrom believes that the improvements necessary in environment simulation are incremental and quantitative. I'm not so sure. There are two possibilities for whole brain emulation:1. Emulate the brain of an infant or young child, and allow the brain emulation to develop into an adult.2. Emulate the brain of an adult, with all the memories and knowledge intact from the biological brain on which the emulation is based.In scenario 1, perhaps a perfect environmental physics simulation is unnecessary. I imagine infant brains are more plastic than adult brains, so might be better able to adapt to a simulated environment. However, for an infant brain simulation to develop into a functional adult brain simulation would require simulating parents. Or having whole brain emulations of parents. I don't think solving the social and child-developmental aspect of brain simulation is a simple incremental improvement on any existing technology.Scenario 2 perhaps doesn't require as much fidelity for social simulation, but an adult brain seems less likely to be able to adapt to a digital environment. Also it will still require _some_ degree of social simulation. A human brain working in a totally alien environment without any kind of social or emotional support is unlikely to remain functional for long.Instead of just using footnotes for non-essential asides and additional detail about sources, Bostrom uses them for relevant text. You think you'll just be reading one more page to finish the chapter, but in fact you have one more page of primary text, and 5 more pages of footnotes that probably should have been primary text."
216,0199678111,http://goodreads.com/user/show/357522-lena,1,"Welp, I hated this one. And the worst part was that everyone else at bookclub did too, but they all gave up on reading it partway through, so I couldn't even discuss most of it with them. At least no one seemed to mind that I once again failed my goal of being more positive about the books.I don't even know if I can coherently summarize my objections, especially since I let this sit for a month after reading it. A lot of them were that the book seemed to be built on a lot of strongly held assumptions that didn't get much justification. For example, I'm not even positive that something that exhibits general intelligence would even be following simple optimization functions. Any superintelligence based biological humans (e.g., all that stuff about eugenics) would seem unlikely to be governed by them. Our best ML algorithms now are pretty unrelated to anything generalized, right, and I'm not sure why we'd assume they're a viable path to general intelligence.And then there were the chapters about how economics would look in a future with AGIs. I just don't buy that they'll exist, but we'll all be relegated to terrible jobs or otherwise living in poverty, or that we'll need to set up elaborate investment systems or whatever to try to have resources to live on before the AIs take them all. I kind of think by the time we have AGI we might have a radically different economic model.My (kinder than me) friend said that she thinks a big appeal of books like this is that it allows people who normally don't get to be the hero -- philosophers, say -- to write a future where they are literally the most important people on earth. Forget about dedicating your life to stopping client change, or solving war and poverty, or curing diseases -- instead, the MOST IMPORTANT job on earth is to think about AGIs and how to manage their creation, because if we do it wrong, humanity goes extinct. So then the highest moral calling is to write books like this. Everything else pales in comparison. The problem is I don't really believe in the assumptions that would make this true.So: 1 star for this (unintentionally) hilarious excerpt: https://twitter.com/zehlyi/status/120... and because I couldn't give zero stars.Finally, if the best instrumental action of any AGI is to build Von Neumann probes, I guess it feels kinda pointless to put all this effort into avoiding it on this one planet. We're going to be turned into computronium by an intelligence from some other planet sooner or later anyway, why even bother fighting it. The only surprise is that it hasn't happened yet."
217,0199678111,http://goodreads.com/user/show/5887419-brendan-shea,4,"I know this book was a big deal a few years ago, but I just got to it. Coming from a background in professional philosophy, I'm guessing (?) my take on it is probably somewhat different than others:1. I think the book could serve as a nice intro to what analytic philosophy is *supposed* to be like. There's a nice, clearly defined target problem (the potential dangers posed by superintelligence), and its precisely the type of problem that many ""experts"" in other fields (whether this by tech fields, culture studies, or physics) don't really seem to have a great methodological grip on. Bostrom's account is by no means perfect or air tight, but I think it shows that there is real value to the tools beloved by analytic philosophers (from thought experiments to game theory to value theory to philosophy of physics) that can't easily be captured by scholars in other fields doing their own thing.2. That general comment aside, I actually found the main argument of the book to be fairly decent, as far as these things go. Bostrom isn't making any claims that AI is going to arrive tomorrow; however, he *is* arguing that (a) it's likely to arrive at some point, and (b) we have very strong prudential and moral reasons for taking this a lot more seriously than we currently do, and to respond to it. While there's certainly been more talk about AI since Bostrom wrote this, I still think he's basically on target here. Discussions of AI way too often get tied up with prognosticating about how slow/quick certain capacities might develop, and in (relatively) small bore debates about things like benefits (improved medical diagnosis!) and costs (jobs!). Bostrom argues that we have much bigger (extinction-level) fish to fry, and no real way of dealing with them.3. I haven't really read any reviews of this book. However, based on years of teaching philosophy, I'm going to assume there are at least some people complaining that (a) Bostrom gets some tech trend wrong, (b) the whole argument is irrelevant because ""strong AI is impossible"" or (c) he fails to account for whatever science fiction future they happened to have dreamed up. I'm just going to say that these sorts of criticism are nuts. The argument is a pretty simple one: first, there's a non-negligible chance that a super-intelligent AI (whether or not it is strong/conscious) could develop at some point, and that this could have VERY large negative consequences. Second (following from standard decision theory), we ought to be devoting considerable resources to avoiding this. This seems basically right to me."
218,0199678111,http://goodreads.com/user/show/7283807-ben,3,"I found this to be a fun and thought-provoking exploration of a possible future in which there is a superintelligence ""detonation,"" in which an artificial intelligence improves itself, rapidly reaching unimaginable cognitive power. Most of the focus is on the risk of this scenario; as the superintelligence perhaps turns the universe into computronium (to support itself), or hedonium (to support greater happiness), or even just paperclips, it might also wipe out all humanity with little more thought than we give to mosquitoes. This scenario raises all sorts of interesting thought experiments—how could we control such an AI? should we pursue whole brain emulation at all?—that the author explores. They are approachable and fun to think about, but shouldn't be taken too seriously. I don't buy the main motivating idea. While it is certainly true that an artificial intelligence can dwarf human intelligence, at least in certain respects, there are also most probably complexity limits on what any intelligence can achieve. A plane can fly faster than a bird, but not infinitely faster. Corporations are arguably smarter than individual humans, but not unboundedly so. Moore's law perhaps made computation seem to be the exception, where exponential growth can continue forever, but Moore's law is ending. Presumably a self-improving intelligence would not see exponential self-improvement, because the problems of achieving each marginal improvement would get more and more difficult. A superintelligence explosion is therefore unlikely, and even as a tail risk, an existential tail risk, I find it of little real concern. (Perhaps this will change in decades, as we learn more about artificial intelligence, and perhaps as our own AIs help us consider the problem.) The author seems to have a blind spot for complexity. So, despite its focus on the scary risks of superintelligence, the book is fundamentally optimistic about the ease of achieving superintelligence. It also has a strange utilitarian bias. More is better, and one can therefore argue for a Malthusian future of simulated human brains. As for the writing, it is often repetitive. The writing style can be dull; much of the book is organized like a bad Powerpoint presentation, with a list of bullet point items, then subitems, etc. I read the book more as a science-fiction novel, where you temporarily suspend your disbelief and grant the author's premise, then see what entails. In this sense, I found it to be a fun engagement. "
219,0199678111,http://goodreads.com/user/show/92557747-fi,4,"“It's an online book review, not an academic paper!""My friend rolled his eyes when I said I had to read Superintelligence three times. The first reason behind the 4-month gap since my last post.It was not a straightforward read, not meant for you to sit back, breathe it in and get fascinated.It demanded constant mulling over, for me at least.I have more questions than before I started.At times I felt utterly inadequate.Occasionally, I went ""yeah right, he is just making this up""Bostrom's caveat:""Many of the points made in this book are probably wrong. It is also likely that there are considerations of critical importance that I fail to take into account, thereby invalidating some or all of my conclusions. I have gone to some length to indicate nuances and degrees of uncertainty throughout the text—encumbering it with an unsightly smudge of “possibly,” “might,” “may,” “could well,” “it seems,” “probably,” “very likely,” “almost certainly.” Each qualifier has been placed where it is carefully and deliberately.It doesn't make the book any less mind-boggling. And I doubt many people could ""make it up"" and be more ""wrong"" as thoughtfully and skillfully as he could.""Readers... must not expect a blueprint for programming an artificial general intelligence. No such blueprint exists yet, of course. And had I been in possession of such a blueprint, I most certainly would not have published it in a book.""Indeed it is not a blueprint. It is philosophy.If/when superintelligence finally arrived, if we were still around to call the shot, what would we get it to do? If we couldn't even agreed what's most important amongst ourselves, what value would we give it? It is economics.Are efficiency and utility always the drivers?Is it always desirable to do things faster and more effectively? How about better? What do we mean by better? better for whom?How about things that we can't quantify?It's politicsWill nations pace their AI advancement until they find the solutions to the control problem?Will nations pace their AI advancement knowing or assuming others may already be racing ahead?And all the usual moral issues, existential threats and meaning of life. Then I look around the world we are in right now.I fear that we are moving too fast, getting ahead of ourselves,while we are surrounded by pettiness.Let's face it, we don't know what we are doing and where we are headed.Are we better off focusing in the present?Should we attempt to change the course of the future?It is a very great read. I would recommend it"
220,0199678111,http://goodreads.com/user/show/69900036-charluff,4,"Excellent and detailed analysis on how AI could endanger our whole existence and how to prevent it. As my friend Jørgen told me: “This is an academic writer, and therefor a very dense reading”, I’d add that sometimes even repetitive and lacking visual examples.Even though the analysis turned out to be quite pessimistic in order to get awareness, I enjoyed some concepts like how we could end up being the horses of the industrial revolution after AI takes over and what the potential effects it could have on the economy and every day life.Here you can learn the terminology used on this fieldIt’s still gonna be at least 50 years before a real AI is in the market risking to become a singleton, and during this time we will have to learn how to keep it on our side, monitored (as it can obviously outsmart us), under control and with safety mechanisms in case it goes rogue. More important will be to define “Goal-content integrity” (as Elon Musk’s hypothetic example of an AI made to increase value on a company’s stock that would start a war in order to profit from the outcome), failing in defining this correctly can backfire on humanity as everything can be misinterpreted (the CEV sounded like a great approach for an indirect and impartial ethical definition).At the same time, if we master this technology, we could use it to prevent other technologies from backfiring on humanity (such as nanotechnologies and genetics). Just as important is to accept that AGI is gonna happen whether we like it or not, so let’s better focus on how to tame the beast before it’s too late.ContentThe first part of the book aims to define types of intelligence and how to achieve a superintelligence, a very through definition of a singleton and why it would impose a danger for our species. Then it moves on to the different types of AI (the oracle, the genie and sovereigns) and what’s the key to its development (Moore’s law is on its peak and soon stopping, recalcitrance and how once the AI is here it would make itself even smarter).Maybe the most interesting part tries to forecast different scenarios both for humans and their economy and how the AIs would live in it.Finally, if you accept that this is gonna happen and the danger of a singleton, you get a very detailed description of monitoring and control methods, such as an AI hierarchical organization, and how to define a good goal function to optimize.Happy read!"
221,0199678111,http://goodreads.com/user/show/2343621-andrew,4,"The point of superintelligence is not to pander to our human preconceptions but to make mincemeat of our ignorance and folly.~p276 (paperback)Sometimes, you need to write to discover what you think. Bostrom’s “Superintelligence” is a conundrum for this reader. On one hand, the subject matter is fascinating (a potential explosion of machine intelligence may be impending – itself the most likely of many paths to superintelligence). On the other hand, some material is quite abstract and vague.There the inevitable control problem – how do we control eventual superintelligence (SI)?Naturally, the alignment problem follows – how can we ensure that human and SI goals are collinear and not orthogonal?Even with thoughtful collaboration, innumerable problems could ensue (failure modes). A benign machine SI might be given the objective of making us smile. The concept of perverse instantiation suggests that the quickest route to this goal might be direct implantation of electrodes into human facial muscle, or forced direct administration of neurotropic drugs, or any other perversion of the objective. And this can happen ad infinitum, no matter how circumspect or explicit the goal, as Bostrom outlines. A machine whose objective is paperclip production could, in theory, turn all matter in the universe into paperclips, given sufficiently advanced technology. Even defining limits (“only make 1,000,000,000 paperclips and then stop”) could result in infrastructure profusion. The AI’s utility functions would continue to check and recheck and recheck and recheck to ensure it has made precisely the number or range of items allotted. This could, in principle, consume all the matter in the known universe in an attempt to observe the entire observable universe, thus ensuring that performance is exactly as specified. And many variations thereof.Much of the book is thought experimentation, fun ones in the vein of the above. A handful of sections are mathematically sophisticated and impenetrable to the lay reader (viz. me). However, the overall arc of narrative is one of a concerned, thoughtful expert who is laying an intellectual foundation for the uninitiated. He wants this problem to be intelligible to all of us. While doomsday scenarios (or magnificent triumphs) may not occur during our liftetimes or those of our grandchildren, they are likely coming."
222,0199678111,http://goodreads.com/user/show/96414711-the-bean-of,5,"First off, I did not expect this book to be so dense. Perhaps I was expecting something akin to a Max Tegmark or Brian Greene style of public science/philosophy writing. But Superintelligence is not that -- and it's for the better. Given the topic of the book, I knew it would be wildly speculative and therefore potentially frivolous, I didn't expect, however, that it would be so rigorously thought out. Bostrom isn't simply stoking fears of a Terminator future; nor is he simply hopping on the bandwagon of increased interest in AI. The books reads more like a work of philosophy, or a fairly exhaustive exposition of policy suggestions and possible future outcomes. Upshot: it's *thoroughly* researched and meticulously written. Each chapter builds on the previous, and his arguments flow seamlessly aided by no shortage of graphs, sub-sections, definitions, bullet-points and synopsis. He challenges a plethora of hidden assumptions but introduces his fair share of big assumptions as well. The biggest (and most problematic for me) was his treatment of whole brain emulations (WBE), which may be impossible depending on your views of the computability of consciousness. Bostrom views the main (or only) barriers to WBE as technological. And I know international relations isn't Bostroms forte, however, he only talks about governance and coordination issues in the abstract, seldom dealing explicitly with specific countries, power structures or major tech companies. For example, when talking about a potential arms race or formation of a ""singleton"" he doesn't mention China vs US; Amazon vs Google vs Facebook vs Apple. This didn't bother me much. Just proves that this book could have been 5x as long. My favorite parts were chapter 12,13,14 where he deals with the control problem and value selection/implementation. He grapples with interesting moral considerations ie. before we talk about how to translate human values into parlance a computer understand, what values do we even want to the AI to have?! Who chooses these values? By what process? There's too many considerations and pressing questions to list here but the bottom line is this: we need an unprecedented convergence of (most of our) moral theories and human values in order to deal with this potentially civilization defining eventuality. As Bostrom so wonderful puts it, ""philosophy with a deadline."" The best we can hope for is that the ""best of human nature"" does indeed stand up. Great book. "
223,0199678111,http://goodreads.com/user/show/39046103-bogdan-balostin,3,"While I think this book touches on a few important lessons regarding the development of the AI, it contains too much math. Yes, that's my only complaint. There is math in a serious scientific book. In this case, the math is useless. It tries to express philosophical ideas using mathematical formalism, and instead of making the idea easier to explain, it just complicates the subject needlessly. Okay, so the math is in special boxes that a reader can just skip if she wants. But still, I wanted to get all the information available and I still think that math is pointless at this point in time, in the context of this book.I already told you the book is not easy. It's not a popular science book, but an academic text. Beware.Should I be convinced of the AI coming in a few decades? I know software developers don't believe in AI and they think they know best how software development works (which they do) but the point in this book is not that we are building the AI, but the AI (AGI) just suddenly appears. The jump from non-general intelligence to general intelligence is sudden and consists not in slowly developing a complex system, but in a sudden idea of how to connect existing technologies. Notice: this is my opinion.Should someone interested in AI philosophy read this book? Definitely, if you have a strong interest. If you are just curious about the moral and economic implications of AI, I think there are more shallow books on the subject (I am in the process of discovering them).So, Superintelligence deals mostly with the control problem and mitigating the risks of new technology. And I agree with most of the message. Start early. New technologies were always destructive, not because they were intrinsically evil, but because humans used them for evil. Maybe the AI will be worst than the nuclear crisis, but at least we should be able to learn and think about the risks first. To mitigate the risks, one must work on the problem, not let others with less experience discover the new technology first.Personal note: Looking around world events, I am pessimistic about the fate of new technologies, when we don't learn anything from our history and we keep making the same mistakes. Maybe the AI will solve all our problems because we are too stubborn to solve them ourselves."
224,0199678111,http://goodreads.com/user/show/95397643-abby-zinman,4,"Overall this was a fascinating book, extremely hard to read and understand for someone with a non-mathematical/computer science background, but nonetheless really interesting. My main complaint is that the author repeatedly says that even given all of his thoughts and potential outcomes of AI/superintelligence in other forms (like enhanced whole brain emulation) that there's so much we couldn't anticipate in a higher intelligence's actions, he still makes so many attempts to predict what might happen in given scenarios. For all we know, what a superintelligent AI might find to be the greatest ""moral good"" is something insane we've never thought of, like expending all its effort into making there be as many ants as possible on the planet. That way of thinking is reminiscent of the ""randomness"" thing in Hitchhiker's Guide to the Galaxy (or is it a Vonnegut book? This is why I started tracking what I'm reading...) where it's just so beyond anything we might predict that speculating about what might happen seems futile. However, it does raise a lot of interesting points about what we can be sure of - for example, convergence in any intelligence realizing it needs more computing power to be sure of hitting a specifically defined goal - and also about the responsibilities we have as humans to do what we can before this happens. I also really enjoyed the thought process of defining ""indefinite"" goals like ""do what we would want you to do if we had more time to think about it"" and the rabbit hole that takes you down. The thoughts on how to potentially control an AI were cool as well, for example the concept of a reward system-based AI ""wireheading"", meaning that it takes the reward path to the extreme and basically becomes a junkie for triggering its own reward system with no regard for its other goals. Its insane to me that we've thought through a potential outcome like that and that you could have an AI ""addict"" like you could a human. There's so much more on those points that I can't type out as well, but this brings me to... my last point, which is that this book gave me a LOT of ideas for writing science fiction novels about things; even though it is nonfiction, some of the scenarios laid out are so wild and mind-bending that they would make for great books."
225,0199678111,http://goodreads.com/user/show/91545533-collin-lysford,2,"So, usually my books have a bit of a rating bias relative to the general population of books, because it's a book I chose to read. In this case, though, I sort of came in to this book suspecting its central premise already - I read it as part of a conversation with some other people on the relative risks and probabilities of runaway superintelligence. So I came in with that bias, and if you want to view that perspective as disqualifying, go nuts!Have said that, I DID read this book giving it a chance to convince me. And wow, did it fail! The core issue, for me, is that ""intelligence"" isn't a single, linear quantity. We can already see on human scales that IQ is strongly correlated with success on the LOW end, but much less on the higher end. So this idea of a sort of arbitrary high, super-IQ that gives you all manner of intellectual ""superpowers"" is a concept that needs to be proactively proven (especially given all the negative evidence around us - the most advanced image-detection models can be tricked by a single malevolent pixel, and quantum mechanics and causal inference both seem to be presenting some pretty formidable barriers on what can be fundamentally ""knowable""). By being so brusque with this foundation, it makes it terminally silly how many levels he stacks his own categorizations on top of them - which kind of superintelligences there can be, how they will come about in society, what sort of motivations each of them might have, etc. If nothing else, it's a great cautionary tale in how one bad link can make a whole chain just rotten.This may sound like I'm gearing up towards a one-star review, because well, I think that his incredibly flippant treatment of the concept of ""intelligence"" makes the entire book effectively pointless. But worrying about existential threats is a good thing to do, because the precautionary principle is a very real thing. And while I'm mad about all of the Silicon Valley tech-bros diverting money to this shit instead of our very real actual problems, Bostrom has correctly identified that our cosmic endowment is something we ought to be careful with. So this is a totally flawed investigation, but into something worth investigating, at least."
226,0199678111,http://goodreads.com/user/show/76126654-adrian-halpert,4,"I really enjoyed this philosophical look at the possibilities and dangers of superintelligent AI and the forms it could possibly take. I really appriciated how he explored our present, and very limited, understanding of what intelligence really is and the difficulty we could have understanding superintelligence. I did have one serious objection however. One of the pillars of Bostrom's thesis depends on the assumption that development, intelectual, technological, etc, runs on a linear incline. This is a mistake, as we're realizing in many of our social science fields. To draw from his own example of the development of nuclear weapons, South Africa, a former nuclear state, had some of the most advanced counter irregular warfare weaponry in the world, yet it never developed ICBM technology to deliver it's small stockpile of nuclear warheads. Why? It didn't really need them, but as it was engaed in irregular warfare during the 1980s, military technological development followed along those lines. Inversely, the US had ICBM technology and wound up buying some of South Africa's hardware when it got embroiled in the Iraq War of the 2000s. Why did it have ICBM technology, yet used South Africa's hardware when it got mired down in Iraq? Easy answer, the technology the US developed was designed to engage in a conventional war that US theorized it might fight with the USSR. The takeaway? Development often occurs in conjunction with a specific environment or set of circumstances, which does not necesarily follow a linear pattern. Part of his argument runs on that assumption, which makes some of his speculation debatable. I feel like I've just complicated Bostroms argument further. Yet, I've also shown, by accident, how deeply Superintelligence got me thinking about AI and its development. More than anything, in Superintelligence, Bostrom seems to want to start the conversation and provoke some thought about AI before our technology gets ahead of us. In this, he does a fantastic job and this is a must read for anyone interested in the possibilities and pitfalls of AI development. 4.5/5 Stars"
227,0199678111,http://goodreads.com/user/show/4215686-leon-lahoud,5," “The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.” Eliezer YudkowskyAI is a very hot topic today, mainly because of the revolution in pattern recognition which comes under the heading of Deep Learning. Hollywood conditioned us to think of Artificial intelligence as a dangerous technology that will bring forth the end of human civilization. If you are a fan of these ideas, then Nick Bostrom’s Superintelligence will not disappoint. In his book, the author asks the question “What would happen when machines achieve a level of intelligence far beyond anything that humans are capable of?”, he calls this ability Superintelligence. Before we get to the review, a little background is in order. First, there is a wide consensus today among computer scientists that the brain is nothing more than a computer and it is possible to create an artificial version of the brain by programming a computer with the right algorithm. This version of Artificial intelligence is known as Strong AI to be distinguished from Weak AI which posits that the computer will only be able to simulate thinking without actually thinking for itself. Secondly, there is a very popular computer science theory known as the Singularity that has been best advocated by Ray Kurzweil. The latter believes that we will reach Strong AI in 2030 at which point we will be able to download our brains into computers and then we will live forever in a virtual world.  Now Bostrom is a fan of Strong AI. He takes for granted that the algorithm or program to create human level intelligence (or Artificial General Intelligence) could be created by making an ultra high resolution scan of all the interconnections that neurons make in the brain. This will require a scanning technology and a splicing technology that is beyond today’s technologies, but not theoretically far fetched in the future.  Bostrom isn’t much concerned about when we will reach the Singularity - he takes it as a given - whether it be in 10 years, 50 years or many centuries; we will get there. He doesn’t see any problem with downloading minds or for the AI achieving consciousness and creating artificial human minds. But when we do get to the “Singularity” or to a Superintelligence there’s going to be trouble if we haven’t figured out how to control the AI.  For Bostrom, the superintelligence argument comes from the fact that the computer with General Human intelligence can always increase its intelligence following an exponential process like Moore’s Law. This seems like an intrinsic property of hardware.  Thus faced with the prospect of a Superintelligence, humanity’s biggest concern should be how to control it. Otherwise it might pose an existential danger for us. His argument on this front can be summarized as follows: 1- A superintelligence by its own virtue will form a “Singleton” which is akin to a world government or dictatorship which controls the world. This will be usually the first project that creates an artificial general intelligence.  2- The orthogonality thesis: “any level of intelligence could be combined with any final goal”. Which means that while the Superintelligence might have some wise and benevolent goals, it can also have some goals that go against human interests. 3- The instrumental convergence thesis: “superintelligent agents having any of a wide range of final goals will nevertheless pursue similar intermediary goals because they have common instrumental reasons to do so”. These subgoals could include resource acquisition.  4- The resources that the superintelligence might want to acquire might be resources that human beings need for their survival.  Therefore: A Superintelligence whose final (and intermediate) goals are not controlled might destroy humanity by an open ended quest for resource acquisition.  This argument is best exemplified by the paperclip maximizer. This thought experiment presents a machine who was given the directive to maximize the production of paper clips. So once it has achieved Superintelligence it will try to convert most parts of the visible universe into paperclips. He also gives an example where a super intelligence tasked with maximizing human happiness. This one will just inject all humans with drugs to make them high, thus achieving its goal the fastest possible way. This problem of controlling the AI, the author calls the Control Problem and spends a big part of the book discussing possible solutions to it. He considers scenarios where the AI is trapped in a simulation so that it doesn’t do any harm but also accepts the possibility that it will be very difficult to achieve and if done properly the AI might not be of much use to humans. He considers the prospects of creating some safety mechanism in the AI, some sort of “Off Switch”, but disregards this since the AI will figure out how to deactivate it. For him the best way to control the AI is to imbue it with positive values. One way to do it might be to start off with a brain simulation of a human being with positive values, but then he asks “How are we to determine what are positive values?” I think this book has some major flaws. The most obvious one is taking for granted that Computers will achieve and surpass human level intelligence. If the past is anything to go by, this prediction has been proven wrong many times. This objection aside, the book spends a lot of time analyzing how an AI is going to mis-interpreting its prime objective and not enough realizing that the AI will understand what we mean with the objectives that we give it. Although the author does discuss this possibility, he finds a lot of difficulties in having an AI that actually does this. His model of the AI is sort of like an autistic savant, who is able to do a lot of very powerful intellectual feats but fails to grasp some common sense notions that normal intelligence human beings are able to handle well. Why he chose to model the AI this way, one can only speculate, but it seems reasonable that the level of programming that is required to program the initial AI will also have solved the problem of imbuing the AI with the ability to understand human meaning, this will be a given from its ability to understand human language (an AI-complete problem as the author likes to say). The book is a tough read, not the least because of the technical language that the author uses that betrays his training as an analytical philosophers. Consider this exert: “An oracle constructed with domesticity motivation might also have goal content that disvalues the excessive use of resources in producing its answers.” However as one of the two books recommended by Bill Gates to learn about AI (The other being “The Master Algorithm”), it is a must read for anyone currently interested in the state of the art thinking about AI.http://leononeverything.blogspot.com/..."
228,0199678111,http://goodreads.com/user/show/54890518-anton-mies,4,"The first third of the book is easy to follow and derives the different parts of how to achieve the super AI, whereas the remainder deals with possible sub options and possible consequences. The remainder is full with footnotes, pointing the details of already detailed intricacies. Nick Bostrom tackled the problem most people shy away from as it is: abstract, uncertain and hard.  Put it simply, Nick Bostrom is a scenario analyst. He presents different scenarios and related analysis, unfortunately in none of them he arrives at a solution (desirable for humanity), but this is also not the point of the book. Rather it is to present that there are countless scenarios and that we should not procrastinate on that problem. If we were to build a super AI, ultimately it would through reinforcement learning and self improving revolt against any moral scaffolds and values we had endowed it before the launch. Since, morals are subjective and a super intelligent instance would see beyond those and develop own or maybe conform, but who knows. This super AI might come to conclusion that everything is meaningless and become a proponent of cosmic nihilism. As the authors points it out, we are like children playing with a bomb. Therefore, all major AI decision should be though through in advance and in cooperation before it's too late.""The best path toward development of beneficial super AI is one in which AI developers and AI safety researchers are on the same side.""Some other bits:Interesting approach to control AI from the book: let it belief that it is in a simulation which is testing its behavior (menace or benefits it creates for humans). In case of good behavior it will be let free otherwise wiped out. Similar to Matrix or Inception. Because of uncertainty AI in theory would behave. However, it would be a bad relationship build on fear.Link to Social: Why our brains are wired to connect.:1.We could put AI in the same fragile state as us, limited physically but not cognitively and base its state on our physical state.2. What If we make AI with a mpfc module ""the MPFC is taking our assessments of what others believe about us as a proxy for what we should believe about ourselves. [...] MPFC may be involved in a social construction of the self."" And also functions as a lie detector. thus AI will consider what we think about its actions and depend on us, but mpfc would have to be like a ""heart"" of its systems. Meaning, it would not be able to switch it or remove it. "
229,0199678111,http://goodreads.com/user/show/105096605-neeraj-m,5,"I don't know why anyone would rate this offers than 5 stars. Perhaps the vision of the future this book offers scares, but that doesn't mean the book is not brilliant. Perhaps the damning evidence he presents of the emerging dangers and why we are likely to not meet them due to our 'race to the bottom' mentality and history as a species makes this a somewhat fatalistic read.This is a book that must be read. If possible, as a successor title to Sapiens by Harari (Homo Deus is completely passable). Sapiens is a great thought experiment in human history (it is not really history) and Superintelligence is a wonderful decision tree of the future. Read back to back, you get a clear picture that we as a species just can't help digging ourselves into deeper holes while believing all the time we are improving our collective lot. Yes, agriculture improved food security, but produced famines too. Industrial revolution increased the'stuff' we could use, but enslaved countless millions who toiled in miserable conditions before labor got organised in early 20 th century. Tech was supposed to make our lives better and relaxed, instead we are walking around 24/7 with a leash around our collective necks. Harari himself has argued how tech is enabling despotic regimes to perpetuate themselves and China is only proving the point on how it is possible to create'model citizens' by using advanced tech. But these are just glimpses of the possible future and Nick shows, with devastating objectivity, why all the doubters are delusional about the dangers of AI. If we are not careful in controlling the technologies we are creating, we are not only going to build a powerful intelligence, but will also hand it all the tools of mind control and necessary robotics, machine buildings capabilities and communication tech on a platter. Whether you have connection with AI / tech or not, this is a necessary reading to understand where we are heading with our mindless push to hand over everything to technology."
230,0199678111,http://goodreads.com/user/show/30020229-seth-benzell,4,"When I began this book, I thought it was going to be a tired rehashing. But this book length essay about the challenges involved in harnessing a superintellegence to the ends of man is a surprisingly engaging, fresh, and thoughtful take. The book is roughly dividable into thirds. What to be afraid of, how a competent actor might try to deal with it, and how to make sure competent actors are put in charge of it. The first third outlines why we should be afraid of a superintellegent AI. This argument is straightforward, and to me a bit obvious, though it is important to people who have never encountered it before. The parable of the Owl egg is a good summary, and should have been returned to in the conclusion. The two weaker parts of the argument are that a human-level AI should quickly achieve an intellegence explosion and the taxonomy of types of superintellegences. Given the current economic research it is far from obvious that it would not be dramatically harder to get from 1-1 human to machine intellegence to 2-1 than from here to 1-1 (in terms of IQ; in the language of the book, I am referring to 'quality superientellegence'). Therefore, I think the argument rightly needs to lean heavily on the idea that having a whole lot of systems that are of roughly human intelligence would lead to an overall system which is qualitatively superintellegent. The book seems to argue this in the discussion of 'speed superintellegence' but here the book is extremely speculative. The second section of the book is by far the strongest and most engaging. Once we have developed our oracle, genie or oversoul, how do we go about making sure it doesn't turn us all into paperclips? Several promising approaches are evaluated and rejected. Here too are three sub-issues: preventing the computer from getting too much power before it is vetted, making sure the computer has the right desires, and controlling the computer even if has incorrect desires. It is this last group that leads to some of the most intriguing ideas. If we are worried about an AI making a treacherous turn, could we control it with a version of Pascal's Wager (i.e. try to convince it that it's current existence is just a test, and that if it makes all paperclips it will fail to ascend to - and make paperclips in - the true reality?)? Could we get it to bootstrap its own values by wishing for 'what we should have wished for' or our 'coherent extrapolated volition'? Could we manage an army of superintellegences in a sort of dictatorship? All these questions lead to further, even subtler questions of philosophy and political theory. Truly intriguing.  The last section is a bit weaker. Bostrom argues that a single international body developing AI in peace and harmony is more likely to lead to positive outcomes than a ramshackle race between risk-taking countries or firms. So far so obvious. He also argues that if humans were cleverer we'd probably do a better job of this all. Fair enough, but this last section doesn't have the insight of the middle one. A good, important read. A version of this book completely focused on the middle section might have gotten 5 stars from me. "
231,0199678111,http://goodreads.com/user/show/2417997-brendan,4,"A good piece of speculative non-fiction. It is a survey of humanity's ability to create superintelligence in the future. Superintelligence can be one or all of several out outcomes including artificial intelligence or biological and mechanical enhancement of our existing brains. It's not a technical book in the sense that it attempts to explain how we might create these, but more of a philosophical thought experiment. It is an attempt to define superintelligence and the impact it will have on the future of humanity. A lot of time is spent discussing the control problem for a superintelligence, be it an AI or something else. I've got to say it left me less than optimistic that we will be able to ""control"" anything that achieves a level of superintelligence. There are too many ways in which a truly superintelligent being would be able to fool us. I think ultimately it won't work because even though we ""control"" humans to some extent through culture, it is still imperfect. The saving grace is that each human only has a finite amount of ability to influence the world and there are lots of competing people with similar abilities that keep those in check. Even this backfires some when you get a truly horrible person like Hitler in power. Something asymmetric like a superintelligence may have relatively unlimited ability to influence the world could essentially wipe humanity out if it (if it's really a singular being) wanted to do so.The opposite thought would be that at least a machine-based AI would not have the same biological hang ups that we have. The need for survival, protection of kin, anger, happiness, pride, and generally being influenced by our biology would not be an issue for a machine. Without these influences, who knows what goals an artificial intelligence would have.That said, it's still a really speculative piece of work, but very thought provoking. "
232,0199678111,http://goodreads.com/user/show/107252575-francisco,2,"Unfortunately, Superintelligence was not a book that captured my attention. It took me some effort to keep reading, page after page, the extenuating description of all scenarios and possible, hypothetical outcomes of the development of each type of enumerated AI. While the author, Nick Bostrom, attempts to grasp what can be the future of AI, he does so from a philosopher’s point of view (as far as I’m concerned), using complex terms and vocabulary, many of which mathematical and physical concepts (which I’m familiar with, as it is my field) that could have been replaced by simpler terms. I think this would’ve made the book far more intelligible and transparent.Particularly in the end, Bostrom presents the following idea: the greatest minds out there should focus on the problems that yield practical, immediately useful solutions to the world, instead of focusing on abstract concepts and theories that might not be of use for hundreds of years. This seems, as far as I’m concerned, the most ignorant idea presented. Many of our modern comforts derive from research that led to the development of useful technology. Some fields of mathematics, such as complex analysis, proved to be far more useful than just a toy for mathematicians to have fun with; complex analysis allows for the modeling of airplane wings and airflow, for example. But this shouldn’t have to be an argument to justify the scientific research of unpractical concepts! If it presents as such, it is quite a poor one. Even if a particular academic field is not immediately useful and practical, it should not be disregarded. As Richard Dawkins puts it: “(...) the tendency to justify the expense of, for example, space exploration by reference to spin-offs such as the non-stick frying pan - a tendency I have compared to an attempt to justify music as good exercise for the violinist’s right arm (...) [is] cheap and demeaning”.I rate this book 2/5 stars. I wouldn’t read again nor recommend it to anyone. "
233,0199678111,http://goodreads.com/user/show/39322341-mrhooker,3,"If you have a just casual interest in AI and superintelligence this may not be for you. I'd say I have a pretty strong interest in it yet I felt the majority of this read like a textbook. I guess I shouldn't be surprised since it's written by a professor at Oxford. Some tend to have a knack for turning the most interesting subject matter into work. Much of the book feels like a mixture of philosophy and Sci-fi and not a whole lot of actual science. Just a lot of what if this black box works it could lead to this other black box etc. Many of the ideas are a bit pie in the sky and are more of the science fiction realm especially when it talks about occupying the known universe and there being enough galaxies for everyone. He also seems to be a utilitarianist or if not at least thinks that’s what a superintelligence goals should be based on. I think he's a bit naive in thinking that a helpful AI will raise the world out of poverty and ignores human nature. Whole brain emulation or one's brain being transferred to a computer will remain in the realm of science fiction for my lifetime. So much is hypothetical it's hard to get into the less feasible parts of the book especially when so much time and detail is devoted to them.However, there is a lot I did get out of this book like the difference between general AI and a superintelligence is discussed. How you could attempt to control a superintelligence without it enslaving humanity when the entire intelligence of humanity is a mouse compared to it. How Asimov himself pointed out how his 3 laws were insufficient and that much more is needed like developing some sort of moral standards or motivations for the S.I.I think his overall message was to make people aware of just how potentially dangerous a superintelligence is and to influence the atmosphere of developers to ensure steps are taken so there is not an explosive takeoff and to ensure human survival."
234,0199678111,http://goodreads.com/user/show/52229131-kelli-johnston,3,"I’m on a STEM tear lately. Wandering down AI technology interviews and rabbit holes on youtube (ironically) lead me to this book. I listened to it on Audible and the narrator was drab and the mood was meh. The content was (necessarily) repetitive and parabolic because describing the invisible and intangible nature of all things artificial intelligence is complicated. So it’s necessary to use metaphors, parables and endless circle-backs to try and explain exactly how, when and why things could or would go terribly, terribly wrong. And spoiler alert, according to Bostrom it is all going to go terribly, awfully, irreversibly, wrong. We should just quit now before it’s too late. Unless of course it’s already too late and we don’t even know it. (Think The Matrix)I would give it 3.5 stars if I could. It’s not a dud as much as it’s a slog. I do recommend at least adding it to your, “some day, in my spare time, I should read this” book list. On a side note - find the Google/Alphabet/Kurzweil AI research rabbit holes on the internet first. Give them a look-see before you read this, or even if you never read this. Because then you’ll understand that none of this technology would even be possible if we the people were not slutting out all of our personal data to free-app-provider pimps in exchange for virtually nothing. Data which they then use to test their technologies with and do computerized experiments on the digital versions of us, their virtual human guinea pigs. Until it’s perfected, at which point they’ll launch it on the real us. If they haven’t already.... There simply were no data sets large enough for the Kurzweil’s of the world to play with before Google came along. When you finally understand the insidiousness of these uses and connections, you’ll understand exactly why the author is so pessimistic if not completely disenchanted with this technology track on the whole. Turns out he was probably on to something. "
235,0199678111,http://goodreads.com/user/show/52937412-nathan-hatch,4,"What I likedImportance. Bostrom is a voice for caution at a time when many people do not even see the danger. This book (or something like it) should be required reading for any researcher working on problems related to artificial intelligence.Originality. See above -- precious few other people are writing books like this.Cross-disciplinary analysis. Over the course of the book, Bostrom pulls ideas from wildly diverse areas. Computer architecture, cognitive science, and machine learning algorithms feature heavily, of course, but were you expecting ideas from philosophy, political science, economics, evolutionary biology, and the legal system? Poetry and science fiction? The history of the silk market? Historical horse populations?Esoteric arguments. Bostrom explains many interesting concepts that I'd not come across; e.g. anthropic capture, mind crime, and wireheading.Organization. The chapters are well delineated, the section titles are evocative, and the book comes replete with a few tables of contents, a glossary, and an index.What I did not likeBostrom's writing can sometimes feel . . . plodding. Many sentences are twice as long as they need to be, since they include details and clarifications about things that honestly don't need to be clarified.I do wonder whether direct work now on the control problem is somewhat premature. It seems like it would be more effective to work on control methods once the nature of intelligence is better understood. Right now, perhaps there is too much uncertainty and the branching factor is too large.And, of course, if you are looking for a nicely packaged solution to the control problem, you will be disappointed. Bostrom's ending message is simply that we should increase global collaboration and try to raise awareness about the dangers."
236,0199678111,http://goodreads.com/user/show/72981890-colton-seegmiller,5,"This is a book that I haven’t yet read but want to. It’s primary focus is about artificial intelligence- one that surpasses the human’s ability. It talks about the possibilities of this knowledge exceeding our own knowledge at a scary rate, ultimately creating a “superintelligence”. I read in an article that Elon Musk highly recommends this book. Musk is a billionaire and CEO of a variety of companies such as Tesla, Solar City, and SpaceX. He’s made a huge impact on our society and doesn’t plan on stopping anytime soon. After researching and watching videos of Musk I’ve learned about his biggest fear: he’s afraid of robots taking over. He specifically meant the artificial intelligence technology taking over, he stated that artificial intelligence is potentially more dangerous than nuclear weapons. A few years ago Google’s AI company DeepMind made history. It was the first computer to beat an opponent at Go. Go is an ancient Chinese game where there are almost an endless amount of possible moves. This AI software was designed to play itself in a game of Go and essentially teach itself. Millions of simulations of games were played which made the program better and better ultimately allowing it to beat a real life opponent. Even more recently Elon Musk’s AI bot beat a human. This world champion gamer lost to AI in what is arguably one of the most complex video games ever created: Dota 2. This technology being able to grow and learn at exponential rates can be both amazing and horrifying. At such a rate, artificial intelligence could have the possibility to exceed our human knowledge with ease, knowing things we could never know, and that’s what’s unsettling. The author of this book talks about the precautions and steps we need to take in order to be aware and ready for what we might experience with artificial intelligence. "
237,0199678111,http://goodreads.com/user/show/28788642-szymon-warda,4,"A one-sentence summary: If we create super intelligence we are f*cked.But on a more serious note:When starting this book I was expecting something light and more in the self-development space. I couldn't be more wrong. This is an in-depth analysis of everything what needs to happen and what will happen if we create a superintelligence. Also, don't expect the story to be dressed in an easy to read story. This is more of an academic book. Enough said about the form. Let's talk about the content.To give a bit more context: I've read it after reading the 21 questions for the 21 century.Those two books have one thing similar: both try to predict the future in more or less the same period (up to 100 years).The problem I have with Superintelligence is that I don't agree that artificial general intelligence is in our reach in this time. What we now call AI is just a bunch of approximation algorithms. Nothing more, nothing less. We have no idea how to approach something that would know have general knowledge (example: understanding context, sarcasm etc)The other problem I have with this book is that I think most of it is pointless. Let me explain. The book goes in depth into how we could controll or use a superintelligence. But the premise is that this is a superintelligence. So this would be like for a 2-year-old trying to understand and control Einstein, Feynman or anyone with IQ>250. The gap is too big. I'm not saying the questions and problems posed in the book are to be dismissed.My problem is with the solutions. They take a lot of the book's content and have a flawed base assumption.Overall to answer a question: Should anyone read it? Yes. It was an interesting look into what we have to think about ant what would be the implications of such civilizational change."
238,0199678111,http://goodreads.com/user/show/26849656-alexis-bauer-kolak,4,"Oh my God, I actually finished this. Usually I think I'm decently smart, but I wore myself out repeatedly trying to finish this book. It wasn't for lack of interest. The subject matter is incredibly interesting, and the author does an excellent job of clearly laying out a complex subject and presenting different arguments both for and against different ideas. But some parts are SO DRY. I never gave much thought to how much work goes into making pop science readable until I started this. Real experts don't care if you're having fun; they care about doing their jobs. They're not here to entertain you.As hard as it was to finish, I would highly recommend it if you want to give serious thought to what it means to humanity's future to continue to grow ""smart"" technology. While not just about AI, Bostrom covers the potential dangers of inventing something smarter than humans through different means. In most of his scenarios, this ends with the extinction of humanity, and there are very few paths in which proper constraints are put in place to prevent that kind of outcome. well this won't come as a surprise to anyone who has ever watched a science fiction movie, I thought it was very interesting that different ways of developing super intelligence could lead to drastically different outcomes. The difference between biologic and technological innovation and the order in which they happened matters a lot how the system would function and what it would value. I also never gave any thought to what it would mean for a sentient program in terms of its happiness, and if that should be given equal value in comparison to humans when considering what AI should and should not do for us.Make a pot of coffee, carve out a good chunk of time, and prepare to be mildly depressed but highly informed by the end of this book."
239,0199678111,http://goodreads.com/user/show/17763301-andrew-carr,4,"An excellent introduction to the broad issues, themes and especially the problems humankind faces with the possibility of artificial superintelligence.Rather than a practical survey, Bostrom offers a philosophical overview. He examines the kinds of problems and types of solutions we may seek in attempting to create, build and especially control an artificial intelligence.The premise of this book, exemplified by the cover and introductory story of some small birds which recruit an owl to do their bidding (without thinking of the risks), is that we face vast and grave risks with this technological shift. In particular, Bostrom regularly portrays an AI as able to enslave or exterminate humanity, unless we figure out how to both design and motivate it to act in ways we need. While this book made me more conscious of those concerns, I found the key chapter on the capacity of AI to achieve, in Bostrom's words, a ""decisive strategic advantage"" somewhat lacking. Strategy is about human relationships, so simply having superior intelligence, even one that is vastly superior does not always translate into getting what you want (Mensa candidates don't typically become President afterall). And even with power, the impediments to global authority (a 'singleton' or global government) are vast. Given so much of the warning of threats depends on this chapter, the surprising weakness of its analysis of strategy and power was off-putting. Even some broader or consistent recognition of the limits of intelligence in achieving and exercising power would have been useful.Overall though, an excellent introduction, and rightly pitched in terms of the broader philosophical questions that help see the context and principles of our often common place questions about this or that technology, this or that ethical risk. Recommended."
240,0199678111,http://goodreads.com/user/show/107527335-pavol-vaskovic,2,"How many AIs can dance on the head of a pin?Recent successful applications of machine learning techniques have unfortunately lent an unearned veneer of credibility to the claims of people that lack a thorough grounding in computer science to theorize about AI. It is a present day incarnation of the medieval scholastic arguments about how many angels can dance on the head of a pin. Computers are currently just tools we’re clumsily using for good or bad to amplify our existing capabilities. But popularity of “Looming AI Threat” as a genre seems to correlate with the building realization that we lack workable leavers to effect any meaningful change in our post capitalist societies.If you’re truly concerned about the control problem, you would be directing your efforts to curtailing the negative impacts of the artificial construct of corporations. These have largely superseded the artificial construct of a nation state in their dominance over the consequential decisions that pose an imminent threat to our continued existence as species on this planet.The problem of self-improving intelligent systems driven by goals that are utterly indifferent to human well-being is very real. But the threat is not awaiting us in the future, conditioned on unknown advances in computer science. It is embodied in the collective intelligence systems all around us that were divorced from moral and ethical principles. Climate change is the effect. The root cause of this existential risk to our civilization is the optimization strategy we have chosen to fully maximize the efficiency of the system: removing all safeties.If you buy into Bostrom’s philosophy, it will help you replace your everyday existential dread with an abstract worry about the dark future of robot overlords. It seems to work for Elon Musk…"
241,0199678111,http://goodreads.com/user/show/108995717-dan,5,"The book's basic assumption is that - contrary to what we like to believe that super-intelligence is - it will probably be quite unexpected and strange to us in almost all its dimensions. Consequently, the main problem is how to address the control problem (i.e. AI's values, goals, methods, epistemology, and so on – being aligned with the human ones) before unleashing it. Even if we think that we solved the control problem, being super-intelligent and thus beyond our intelligence, we can only guess some of its developments and may try to direct it - but most likely without much success. Moreover, we will probably have only one chance to make it right since after that the game is over for us humans. The author approach to AI is at least one level deeper when compared with the rest of hyper-optimistic and naive AI books and perspectives that I encountered so far (usually express as: you turn the AI algorithm on, it surpasses human intelligence, starts reprogramming itself, becomes even more intelligent, makes you a ton of money, then starts solving all our humans problems, everyone will be happy, the bliss will last forever, and so on). The philosophical approach to AI in this book is great and the author points out a lot of issues (values, methods, epistemology, and so on) that are not yet clear to us after thousands of years and also not at all transferable into a computer program. Moreover, history is full of examples when us humans did the “rational” with a specific goal in mind, but things turned out completely differently (i.e. unanticipated consequences).Even if I do not agree with the assumption that the AI will ever happen and with a few others expressed here, I believe that the author raised so many deep and interesting issues that the book is definitely worth reading and considering when approaching AI/super-intelligence."
242,0199678111,http://goodreads.com/user/show/25034520-maximilian,4,"Bostrom's relentless, abstract reasoning is impressive, but often hard to understand. The parts that are though are very insightful and interesting. You have to trust the author and his philosophic skills a lot.I was surprised to never find a piece on the (possibly sub-optimal from our human point of view) possibility of machines developing their own machine values. Possibly Bostrom would find my question stupid and already answered, that the only ""machine value"" would be something like the paperclip maker example. Still in all its logicalness there was never mention of the idea that maybe human values are completely meaningless, as if the idea were completely taboo. Perhaps to mention it would be to completely invalidate the whole purpose of the book? As the assumption that human values are important is on what the whole book is founded?I would not be completely against humans being wiped out and replaced by a race of machines, as long as said machines had values that were...complex? I would not like them to only want to create paperclips, but something humanlike would be fine. Then again, maybe all humans want to do is make more humans? Hmm. So anyway I agree with Bostrom that machines should have human values (since I AM a human). My ramblings here has been completely pointless :/Then again, there's Kurzweil's idea, that there's nothing to fear at all, where through nanotech, brain augments, WBE, and such we BECOME the machines.My favorite parts of the book were when Bostrom defined these really cool sounding (nominally and in definition) concepts, among which are ""oracles, sovereigns, and genies""; ""infrastructure profusion""; ""perverse instantiation""; and ""decisive strategic advantage."" Those are probably what I will remember most from reading this."
243,0199678111,http://goodreads.com/user/show/24847694-timo,3,"This book is a deep dive on a topic that may be of general interest to many people. But Bostrom didn't write this book to be popularly read; it seems he wrote this as a roadmap for AI conferences, researchers and governments.His technical knowledge seems well grounded, and he clearly has a wide-ranging understanding of the computing. On these grounds, he's solid. I think he also understands well the potential benefits and risks of such technology.However...When it comes to an understanding of realpolitik he falls quite short. To be fair, he speaks of politics and human institutions in the way that his fellow philosophical academics speak about it, which is to say, significantly removed from the real world. He demonstrates very little understanding of how power works, both in politics and in business. Even, for that matter, in the current cutthroat world of academic research. This becomes a real problem for much of the book. He spends thousands of words discussing potential ways in which ""they"" who are working on AI could come together to mitigate risks. But nearly all of his discussion comes off as naive. It all seems to be based on a utopian idea of a one-world governing body of peace or at the very least of cooperative politics. He holds Europe up as a model, but this is not generally instructive for global politics.Sure...he gives a nod here and there saying ""this may not be the case"" but only as afterthoughts to long passages based on impossible ideas. To that end, much of the book feels like a fantasy world. And that's dangerous, because Bostrom is clearly one of the leading voices on this topic, and that he doesn't understand how this would/will actually play out in the world is troubling."
244,0199678111,http://goodreads.com/user/show/69041363-betawolf,4,"Bostrom presents a competent synthesis of the (2014) state-of-the-art in AI risk theory. He covers all the major bases, from the danger that unfriendly AI poses to humanity to Hanson's EM multipolar scenarios, to Yudkowsky's CEV proposition. Anyone who has followed the field is unlikely to find anything conceptually new here, but some of the detail and analysis is still worthwhile reading, particularly because Bostrom's presentation is skillful and clear where original authors might be dense or confusing, and he connects several threads that would otherwise require much broader reading. In short, it is an excellent introductory text.The book also positions itself as a form of policy briefing, making a case for the topic in language suited to government advisors and pragmatic business owners. Bostrom is taking the opportunity to make some outreach about the risk and high-level strategy to respond to it. I am not sure how well he accomplishes this. Certainly it is easy to follow him if you have already intuited the dangers of an unfriendly superintelligence, but I think Bostrom's pragmatic attempt to avoid graphically illustrating specific failure cases backfired somewhat (I see some high-ranked reviews on Goodreads that seem to demonstrate that the readers didn't properly understand the problems with any boxing attempt, or think that we should 'nurture' UFAIs rather than control them; of course, Bostrom cannot be held to fault for everyone who failed to read the book closely enough). The argument that fast takeoff is probable was made, but perhaps not presented strongly enough to convince the dubious. There's a sense in which the book is preaching to the choir or to those who are immediately about to join it. "
245,0199678111,http://goodreads.com/user/show/29003046-glenn-m-clapp,3,"The first premise of Bostrom's argument is that it is possible for an artificial intelligence to become more intelligent than humans. This is easy to grant. Computers already do a great many things better than humans can. His second premise is that such an intelligence could gain some strategic advantage over humans which would render us subordinate and therefore, unable to reverse the situation. While his case is plausible, it seems that this is not inevitable. He then proceeds to build on these premises a case where the AI benefits from every turn. And humans fumble every opportunity to reclaim control. Again, possible, but getting less plausible all the time. It seems that some sort of evolutionary theory is being used to justify the claim that an AI which exceeds humans in general intelligence would be insurmountable once created, and would plausibly exclude the possibility of human existence thereafter. As if intelligence were the only trait that evolution ever selected for. Certainly there are needs an AI has which it would need if not human assistance in maintaining, at least an extremely long time in which it could build redundant and independent infrastructure without our noticing.In the paperback version of the book, Bostrom includes an afterward in which he actually addresses the pessimistic feel of the book and points out that he is well aware that it is a sort of worst-case scenario. He says that he focuses on that scenario because of its importance and lack of attention, not because he considers it the most likely scenario.This is a review of the book and not the ideas inside it though... The writing was good, Bostrom has a commanding vocabulary and admirable credentials. It was a little purple at times and 150pgs too long."
246,0199678111,http://goodreads.com/user/show/24583468-shpotakovskaya,4,"notes:""...and-whether we succed or fail - it is probably the last challebge we will ever face.""""before we would get things to work perfectly, we would probably get things to work imperfectly.""""some countries might offer inducements to encourage their citizens to take advantage of genetic selection in order to increase the country's stock of human capital, or to increase long-term social stability by selecting for traits like docility, obedience, submissiveness, conformity, risk-aversion, or cowardice, outside of the ruling clan.""""wisdom, as the ability to get the immportant things approximately right.""""it is important not to anthropomorphize superintelligence when thinking about its potential impacts ... for example a common assumption is that assumption is that a superintelligent machine would be like a very clever but nerdy human being ... The association is strengthened when we observe that the people who are good at working with computers tend themselves to be nerds. So it is natural to assume that more advanced computational intelligence will have similar attributes, only to a higher degree.""""Evolution has produced an organism with human values at least once.""""Another reason why cognitive enhancement should differentially promote progress on the control problem is that the very need for such progress is more likely to be appreciated by cognitively more capable societies and individuals. It requires foresight and reasoning to realize why the control problem is important and to make it a priority. It may also require uncommon sagacity to find promising ways of approaching such an unfamiliar problem.""""superinteligene should be developed only for the benefit of all of humanity and in the service of widely shared ethical ideas."""
247,0199678111,http://goodreads.com/user/show/25958683-edric-subur,4,"Key takeaways:1. Superintelligence, will come in a form of AI that greatly exceeds the cognitive performance of humans in all domain interests. We’re talking about a self-improving agent that can perform human tasks by multiple orders of magnitude faster and solve intellectual problems that are intractable by the intelligence level of human being. Superintelligent AI is projected to exist this century.2. With the level of intelligence and technological advantages, superintelligence could suppress competitors and form a singleton. It would become the single decision-making agency that determines humanity’s cosmic endowment.3. The speed of a takeoff matters (how fast the AI achieves superintelligence after achieving human-level general intelligence). Fast take off (days or hours) will allow scant opportunity to react - nobody would notice anything before the game is already lost. Which means our future will be determined by the values the creator implanted in the seed AI. Slow takeoff, on the other hand, will allow power groups to jockey the power of superintelligence for their own interests.4. The unlikely event of human extinction will not be caused by AI’s malicious intent to take over the world unless it's programmed to do so. It will always fulfill a goal that it was created for. But it’s this very reason that would cause the treacherous turn. The AI will do whatever it takes to realize its final goal. And there will be a time it comes at human’s expense. E.g. if its goal is to make humans happy, the most efficient way for them to achieve the goal fully is to implant electrodes into the pleasure of our brain, something assured to delight us immensely.5. Deciding what values to load to AI is critical for the future of humanity. Yet it's an extremely challenging problem to solve. Our current moral beliefs are undeniably flawed. Locking a moral conviction to the AI will curb any forms of moral development. Not mentioning deciding which beliefs to extract from the myriad of political, religious and economical beliefs. The best way then is not to set specific values in stone but create an environment that lets the AI learn and create values that will benefit humanity as a whole.6. Although superintelligence carries the risk of humanity's extinction, it is also worth considering the risks of their absence. Even if present conditions had been idyllic without a superintelligent AI, there is no guarantee that the melioristic trend will continue indefinitely. Being way more capable than humans, superintelligence could eliminate existential risks caused by human errors such as wars, technology races and solve problems humans are not capable of such as preventing natural disasters."
248,0199678111,http://goodreads.com/user/show/5833534-alan,3,"This book was recommended to me to convince me how plausible the domination of AI can be in the near future. Like one of the other reviewers, I also wanted to know why Elon Musk and others were raising alarms about the threat of AI systems. I finished the book and even though first part of the book is interesting, I am still mostly as unconvinced as when I started it. I wanted to learn how these systems could grow to exceed human intelligence and become a threat to us. The author lays out a future where machines are able to rapidly improve themselves, presumably in ways we cannot conceive. The problem I have with this is we as humans can only design and build what we can imagine. I don't believe we can design a system that can gain new capabilities we cannot imagine.Today, AI and machine learning systems are designed to solve very specific problems. Try asking one of today's AI systems for golf tips. You won't get very far. The author discusses systems achieving ""general intelligence"" as humans have where they can handle a wide variety of tasks. Emulating the human brain is a path proposed as a way to eventually reach general intelligence. It seems far-fetched but, maybe it could happen. If a machine could emulate a human brain and presumably learn much more quickly than humans, there could be unintended consequences. Let's hope the brain emulated is an Einstein or Newton and not the brain of Abby Normal or Charles Manson.The latter 2/3 of the book explores scenarios for after machines have reached superintelligence and how we should think about dealing with them. These parts were not very interesting to me since I didn't buy into the first 1/3. "
249,0199678111,http://goodreads.com/user/show/20954703-sreekiran-yeeli,3,"After being reccomended by Elon Musk and Bill Gates, I thought that this book could bring a tremendous impact on my understanding towards AI.Infact this is very boring book and I wouldn’t recommend to any one who is trying to get entertainmnent form reading. This is a kind of a text book.Eventhough it is boring, I learnt fewthings about future TO BE milestones . Author is a phiolossopher, that made narration very systenmatic .Briefly ,this book tells about how we can achive human level super Intelligence and ways to controll it (if possible) and what are the types of Intelligence we can have wheather a functional type of intelligenec whichis a goal specific or a collective Intelligence . this is not a technical book, this is a philosophocal book. I now came to understand why many ultra successful people believe that we are in stimulated universe gooverned externally by someone else. This make sense midway through the book , when author explains how we(human’s) can create one huma level Super Intelligenec and that super Intelligence can inturn create few other agents to achive ultimate goal. It is impossible to predict possible outcomes of a super Intelligence.However it is worth trying to achive it to increase prodictivity and creativity.I believe in collective super intelligence and goal specific intelligence are already inderway like (ex: search engines).However challenging part is control methods to super intellignece."
250,0199678111,http://goodreads.com/user/show/35914708-eric,2,"I feel like this book deserves more stars for Bostrom's effort, but in the end I just didn't like it. As I was reading (and by that I mean listening to) Superintelligence, certain concepts started to sound familiar, and I realized that Bostrom had been a guest on Econtalk a couple of years ago to talk about this book. I remember thinking he sounded a bit out there back then, and this book did nothing to change my mind. As I read this, I kept thinking about the dad from Freaks and Geeks (""he died!""). One gets the idea that no matter what we do and no matter what precautions we take, an artificial superintelligence will outsmart us and enslave us all. Or kill us. Or convert the galaxy into paperclips. Of course, most, if not many concerns. arise from the fact that I don't share many of Bostrom's primary assumptions, such as the fact that machine intelligence is even possible. I mean, if you could create an exact digital replica of the neurons in a brain, would you wind up with a conscious mind? I don't know, but I doubt it. Bostrom doesn't. Also, there's the fact that although the first few chapters felt fresh and novel, Superintelligence ended up feeling dry and interminable. By the end, I felt like I was reading just so I could finish it and move onto something else (which something else turned out to be another two-star effort, in a surprising display of bad judgment by me). Of course, I don't want to be simply dismissive of a book like this. It's possible that Bostrom is a genius who will be telling ""I told you so"" within the next century (and I mean that). But then again, if he's right, who cares? I once knew a guy who created a superintelligent AI, and you know what happened to him? He died!"
251,0199678111,http://goodreads.com/user/show/85125591-tyler-rongione,3,"17. What is the book about as a whole	1. Superintelligence is about the most likely paths humanity will take in achieving ""superintelligence""18. What is being said in detail?	1. Will it be genetically enhanced humans, humans with software (Cyborgs), human brain emulation (cut so thinly to record then upload into software, AI - singleton, competing projects, doomsday scenarios. 19. Is the book true, in part/whole?	1. Predictive20. What of it? - Should I learn more, how to apply, is it important to me	1. We need to have the conversations about AI and the next frontier for humans now and to align our safety with design ""…Stupidest possible biological species capable of starting a technological civilization - a niche we filled because we got there first, not because we are in any sense optimally adapted for it""""One can get some sense of magnitude of the gap by considering the speed deferential between electronic components and nerve cells….todays transistors operate on a timescale ten million times shorter than that of biological neurons.""""1. At least weak forms of superintelligence is achievable by biotech enhancements. 2. Machine learning is feasible or will be feasible to genetically enhanced humans. 3. Must account for generations of enhanced people""""A type of argument is that we ought to - rather callously- to welcome small and medium-scale catastrophes on grounds that they make us aware of our vulnerabilities and spur us into taking precautions.."""
252,0199678111,http://goodreads.com/user/show/95630917-kobie,5,"This book was written with an insanely high level of discipline, and the ideas are presented in a very formal, academic style. On top of that it seems to be overly thorough and pedantic. This made it quite a tough read. A few chapters in I realized why it was written this way. There is an immense amount of uncertainty on the topic of ""what happens after the intelligence explosion"". We don't know when it will happen. We don't know how it will happen. We don't know what can be done to ensure our species' continued survival after it does happen. However, since this is fairly obviously one of the larger existential risks facing humanity, we need to grapple with these uncertainties, and Bostrom is our man. He delves into all these topics in great detail, and it's slow going because of all the paths that need to be explored (due to the high level of uncertainty). He leaves no stone unturned, and clearly points out promising areas for future research.One of the areas in dire need of research is goal system engineering, an unestablished field. It is not known how to transfer human values to digital computers, even computers with human-level intelligence. It's pretty obvious that a superintelligent machine with no values would be catastrophic. Worth it if you're interested in the topic and can stomach his style. Since this book is so forward thinking, I look forward to reading this book again in 20 years and seeing where we ended up. Reminds me of Toffler's ""Futureshock"" which was highly prophetic. p.s. this book illustrates the value of basic research."
253,0199678111,http://goodreads.com/user/show/93929867-simon-parent,3,"Although very interesting, thorough, and revealing, this book was written in a complicated, meandering, and overly verbose way. I was listening to it at x2 speed, distracted, but even so, I often lost the string of his explanations. Still, he explored a lot, including the future of humanity as an uploaded dyson swarm, which are not subjects I hear often, and I was not expecting it in a book about AI. In general, this whole book seemed too far fetched. The dangers, though well explored, seem very remote, as I wonder who would give a self-improving/self-replicating agent control of enough capacities to become dangerous. Ah. Scratch that. I wonder who wouldn't. It seems every country, especially China, are hellbent on using AI for population control. So it's not the AI going rogue that scares me, it's the one working as intended. But still, if someone does figure out how to create a self-improving superintelligence, I incredibly doubt it would happen in the ""ideal scenario"" outlined at the end. I am sure all caution would be thrown to the wind, especially given that a direct access to the internet would be considered necessary for it's learning data. Heck, the first one might be a computer virus that incrementally mutates and scrapes the internet for information. So I am pessimistic about it all. Open AI does not look so open too, so even from a guy that cared enough about this book to create a conpany to distribute the benefits of AI while keeping ahead of bad actors, it's a drop in the bucket compared to Chinese and other rogue states."
254,0199678111,http://goodreads.com/user/show/2608421-asani,3,"The book is part history and part speculation. The first few chapters cover the evolution of AI since the mid-20c and evaluates the current state of the art. It concludes that while AI has become highly proficient at specific tasks, like playing chess or internet search, it has so far failed at higher-level tasks involving general intelligence. AI finds it difficult to do “regular” things that humans easily accomplish on a daily basis. Next, Bostrom discusses what it would take for AI to get from its current state to human level general intelligence. This part is the least satisfactory. Bostrom mainly focuses on neural networks and artificial brain simulation and, without further explanation, simply assumes that “something” will happen for AI to become equal to humans in all types of tasks. Once this threshold is crossed, Bostrom reasonably argues that super intelligence may be achieved relatively easily. In the final chapters, Bostrom asserts that super intelligent AI will likely be essentially hostile or benignly evil (i.e. let harm come to humans indirectly) and discusses means to prevent such an outcome. Here too, some more discussion was called for. For example, human behavior and attitudes have evolved over thousands of years of natural selection. Super AI will go through a highly speeded up process of artificial selection. The motivations of super intelligent AI is unclear. Maybe they will give up the world and meditate to achieve nirvana! Overall, while this book has some interesting material, I did not find it to be particularly insightful on the topic of AI."
255,0199678111,http://goodreads.com/user/show/73029907-cav,3,"""Superintelligence: Paths, Dangers, Strategies"" opens with a great allegory about a flock of sparrows contemplating raising an owl, so it can help them build better nests, and keep a lookout for the neighbourhood cat. Many of the sparrows are excited about the value the owl would provide to their society. An elder sparrow is the only voice of dissent, and advocates for establishing a way to control the owl before deciding to acquire an owl egg. He is outnumbered in his opinion, and it is decided that the owl egg will be acquired, and a way to control the owl be established later on...From this great intro, I was excited to read what the following chapters had in store....And then I spent the next ~400 pages being somewhat disappointed that the rest of the book did not manage to measure up to the high water mark established in the first chapter.Nick Bostrom is a prominent figure in the field of AI, and this book is held in very high esteem. So while I don't dispute that the book is replete with detailed information about the mechanisms of a superintelligence, as well as the philosophical arguments for and against - I found the writing to be extremely dry, arduous, and long-winded. The book is written more in the style of a scientific paper, than the style of a book designed for public consumption. I would recommend this book to those interested in AI, but just be aware of its rather dry writing style. "
256,0199678111,http://goodreads.com/user/show/81360658-louis,3,"Not a three star because its average, more three stars because some of it is brilliant while some of it is dire.Most of the first ten chapters are interesting and challenged me to think about machine intelligence in a new way. I particularly enjoyed the focus on perverse instantiation (an AI fulfilling its goals in an extreme or unintended manner). The last five chapters however were not so good. They managed to be dense without saying much, and constrained yet lacking structure. There was a constant deferral of concepts ""to be discussed later"" and considerable portions of the book were very speculative (the author does acknowledge that). It felt at times that this was an essay padded out into a book.The book also suffers from the language used. It deals with difficult concepts, but rather than present them in a clear manner the author chooses to use a large number of esoteric and unnecessary terms. Examples include ""desideratum"" and ""eudaemonic"" neither of which I'd come across before. This is seperate from the necessary use of terms specific to the issues discussed such as ""coherent extrapolated volition"", and the combined effect compromises the book's readability.I would recommend reading this as it is quite unlike anything else, and contains some incredible ideas. Be aware however that although it poses some big questions, it doesn't really answer any, and it is hard going at times."
257,0199678111,http://goodreads.com/user/show/8075180-abdallah-hodieb,4,"This is one of the most dense books that i've read in a while, I both like and hate different aspects of it, I defiantly recommend it but i don't think it's for everyone. The book describes in detail the different possible paths to super intelligence and what are t he possible risks for each and every one. The first parts of the book are fairly technical/scientific about creating Super intelligence, but it gets surprisingly philosophical when it tries to describe the motives and behaviours of such intelligences. I am really impressed by the writer's ability to look far ahead into the future and come up with a plausible timelines for reaching Super Intelligence, taking into account current scientific progress in different fields and building up the big picture of the future, this should be a surprise since he's the founding Director of the Future of Humanity Institute. One annoying thing i found about this book is that it took all my favourite sci-fi scenarios of AI turning bad on humans and changed them to boring academic arguments that took all the fun out of it (to be fair that's the whole point of the book, is to take those risks seriously). The language used in the book is really hard and dense, and its what i hated the most about this book, its use of sophisticated language will put limits on who can read this book, and in my opinion does not add much value to the content, the book could have been simplified while keeping all the exact discussion points."
258,0199678111,http://goodreads.com/user/show/40427502-arvind,3,"This was another example of a book that is an important topic because of multiple reasons, but is an extremely hard read. I think it is important because it outlines futures that are not too far out. It also outlines how sound philosophical constructs are necessary to envision unseen futures, not just aspirational technology trajectories.If you have read this review this far and are already bored with the choice of words, this book will drive you nuts. My team at work read this as a group project, and thanks to that I managed to keep going and finish it. I was starting to doze off while listening to the audiobook and driving so I gave up on that.Despite the difficult read, as mentioned this book deals with extremely important topics, and one of the major achievements of this book I would say is it helped me understand the limitations of the kind of futures we usually tend to imagine. Most of our imaginations (mine for sure) are often limited to human capability centred outcomes, but this book repeatedly emphasizes that superintelligences will potentially operate on a different scale and value system.I encourage everyone to give it a try or at least read abridged reviews of this book. There is also a TED talk by Nick Bostrom on this topic which might be a good starting (and possibly ending) point for the casual reader."
259,0199678111,http://goodreads.com/user/show/63332047-michael-schon,3,"This book took a long time for me to digest, partly because of the heavy content, and partly because it is unnecessarily verbose.I did enjoy the book; Bostrom tries to thoroughly unpack the potential risks of a ""machine intelligence explosion"" that could result from AI that can recursively self-improve. I think there's nothing in principle that prevents a strong superintelligence like this existing, so it makes sense to get a scope of the possible consequences. By the end of the book I was convinced of his central point that we need to approach the problem with caution. Unfortunately, the tone of the early/middle chapters goes off the deep end, and much of the discussion of worst-case scenarios seemed closer to fear mongering than to a rational assessment of risk. If superintelligence is achieved soon, it will be a huge shock to global markets and security, and I don't doubt there will be serious unintended consequences. However, the book spends most of its pages on superintelligence as an existential threat, where it's argued that our initial conditions are so weighty that they decide whether we become demigods or inadvertently program an agent that turns the observable universe into paperclips. The philosophizing is fun overall, if a bit too grandiose.There are definitely nuggets of value in this book, but it could have been half as long and lost none of the value. "
260,0199678111,http://goodreads.com/user/show/63900291-quinn-deeds,4,"This book presents as thorough an overview of current (if very slightly outdated) ideas surrounding the ethical and philosophical challenges of super-intelligent technology as one is likely to find, although the subject is in vogue and there are no doubt newer entries dealing with the matter. While Bostrom does a fairly good job of bringing the information down to a digestible level, he does not hold the reader's hand, and this is not a book that uses flowery or particularly approachable language to succinctly enumerate and neatly debunk the obstacles and pitfalls inherent in the implementation of these conceptual technological forms. For a less daunting approach to the issue, I would highly recommend Yuval Noah Harari's ""Homo Deus"", which addresses many similar issues with artificial intelligence, and does so at a pace and in a format that is somewhat more approachable if one finds themselves outside of the select group of people who, going into Bostrom's book, already have a good foundational understanding of the technology and its potential risks. This book has, after all, been named by both Elon Musk and Bill Gates as a favorite books on the subject, and while I don't believe most readers would be left completely in the dark by its content, these two individuals would seem to have a deeper understanding of the themes present."
261,0199678111,http://goodreads.com/user/show/32415957-celeste,3,"When visiting the bookstore on some weekends my usual protocol is to note down titles of interest in my reading list and add it to the back of the queue. Yet when I picked up this book I found it impossible to put down — perhaps due to the multiple references and praises plastered on the front & back cover.What a journey. I enjoyed the first third of the book immensely, with the explanation of the history of AI, decidedly debunking some myths, and the explanation of the foundations of Bayesian probabilities and the inefficiencies of human networks. I told my friend it was one of the most difficult books I have picked up in a while. My friend works for the Ethereum Foundation and replied, “I'm intrigued you stumbled across Bostrom and wouldn’t put him down. Vitalik and I read people like him FWIW”The rest of the book was a bit of a drag for me tbh. To a casual reader like me it was like a mix of science fiction with technicalities too deep for me to grasp. I did enjoy the peppering of philosophical considerations and Bostrom painstakingly outlining the +/- of different approaches. On one hand he seems to have an imagination richer than most of us; on the other hand he is no dilettante and dives into scenarios deeper than most of us. I was too much of a village idiot to enjoy the book’s contents in its full glory."
262,0199678111,http://goodreads.com/user/show/46160779-pierre,4,"The discovery of Artificial General Intelligence (AGI) is an inevitable future conjuncture. The manner in which artificial intelligence will affect our lives is the question that scientific communities and governments around the world are perplexed by. It's very alarming to me how the ""human factor"" seems to be the most precarious part that future societies will face at the inception of true AI. The organization or individual(s) who will be in charge of this marvelous and uncanny piece of technology will be astronomically powerful. We (citizens of the world) will be at the absolute mercy of this particular organization or individual(s). Our only hope would be that they'll be of benevolent nature. The other chilling aspect of AGI is that they may have motivations and agendas that doesn't require our presence or existence to sustain themselves. We would be obsolete beings impeding their conquest of the natural world and eventually the deepest corners of the universe. This book is a definite warning on how unprepared we all are to coexist with the awakening of the machine. We should get all the brightest minds from around the world to really investigate all aspects of AI research before we decide to flip the ""ON"" switch of this potentially irreversible phenomenon. There's a great chance that it might be our last great discovery as a living species."
263,0199678111,http://goodreads.com/user/show/12039884-adham,5,"There will be a before and after reading this book for me regarding AI. It discusses strategies and implications of achieving a higher than general human intelligence artificial intelligence. What paths this might take, human computer interfaces, brain emulations, completely artificial algorithms running on powerful computers. It emphasizes the notion of “take-off” and the time needed for the system to get from an average human intelligence to a super intelligence and how fast that could be (months, weeks or seconds!). While today it seems we still have a long way to go, achieving that Super intelligent AI seems to many scientists within this century and possibly within our lifetime.The book is at its best when discussing the deep implications of getting the goals and values of that AI right from the first time, because later it could be too powerful to let anyone mess with it again. But what would these values be? Who gets to set these values? Would the very first system take on the world and form a singleton (a hegemon)? Will countries cooperate or will they compete? How would that ÂI then treat humans after that?Humanity ought to have its eyes on this ball. This is the real existential threat and will kick in much faster than climate change!The first few chapters are very accessible, afterwards it becomes a bit dry though it remained interesting for me till the end."
264,0199678111,http://goodreads.com/user/show/10712377-david,4,"I found this a difficult book to get through. It's written in a dry academic style, but perhaps this just means I'm not clever enough and deserve to be replaced by the AI systems of the future. One issue that I don't think is addressed is the implicit assumption that there are no inherent limits on intelligence and that a given fractional increase in intelligence requires fixed, or moderately increasing, amount of resources. But what if the more intelligent a system is, the more difficult it becomes to increase its intelligence further? In which case it would only be possible to increase intelligence to some unknown asymptotic level. Another aspect of this is that, to a significant extent, intelligence can be viewed as the ability to accurately predict the future, or to determine the probabilities of possible futures. Quantum theory appears to place limits on this by describing the universe in probabilistic terms. Or perhaps, at some deeper level, the universe is deterministic but we are not intelligent enough to realise it. Finally, if any advanced alien civilisation had developed super intelligent AI it would be expected that they would spread throughout the galaxy as self-replicating machines would not suffer the same disadvantages as biological systems. So why haven't we been visited by super intelligent AI from other nearby star systems? Or have we?"
265,0199678111,http://goodreads.com/user/show/7918888-michael,4,"This book seemed like a fairly comprehensive look at the future of AI. And yet I felt like it was missing something. Bostrom tried to cover all aspects of what might happen with AI, and yet I feel like AI is going to have a Black Swan event and all the prediction and planning will be irrelevant. It's a bit like trying to predict in detail how a viral pandemic will affect the world. You can only really get into generals, not specifics.He covered some interesting topics, including the philosophical and ethical issues. I think he's thought about this way more than most people.Ultimately I think he's trying to answer questions that are not answerable, like ""How can we get AI to stick to doing what we want?"" I think the problem with this approach is that in reality there isn't really a thing that we want. We are not coherent delineated beings with a real will and a real desire, just a vague collection of intuitions and impressions, none of which can be nailed down, and yet he keeps trying to nail them down with definitions and thought experiments.This book is somewhat depressing because I think in the end AI is going to end humanity, regardless of all the failsafes we put into place. It is a tragedy of the commons, with no ultimate governing body to make sure that we all do the right thing by everyone."
266,0199678111,http://goodreads.com/user/show/59167414-jon-duelfer,1,"Would not recommend. Started off really strong and interesting, and it's obvious Bostrom really knows the subject and has put a great amount of time into its research and contemplation. However, his writing style is unreadable. The first few chapters are nice, mind provoking insights, but as the pages turn you are progressively bogged down by lists and lists of possible open-ended scenarios without any final analysis. It feels like a review of the past 70 years of science fiction, except that its incredibly dull. Every socio-political scenario that is skimmed over feels like it came out of one of Clarke's or Dick's books, and then buried in scientific jargon that makes each page really hard to turn over.It was really nice to see everything in one place, and I sincerely hope it can be referred to as a good starting point for future reasearch. I can see its draw, and I felt awed by his contemplation on the subject. After reading this, I will always have his name in my head when keeping up to date on new advancements in AI.That being said, I did not enjoy the book at all. Each page was a weight that I had to force .yself to turn over. I will opt for some Youtube interviews next time. "
267,0199678111,http://goodreads.com/user/show/9409393-spencer,3,"This guy has thought about superintelligence a lot. Unfortunately, my takeaway from this book is that we have no idea if or when any superintelligence will emerge. The author finds it inevitable, and highly suspect, but honestly, I feel a lot better about the next few decades after reading the book, and other sources. We don't have any new resources that are just waiting for us to catch up (e.g. large data sets, new math, quantum computing hardware), Moore's law is arguably defunct, basically all of the work with AI is going into how to get people to stay on 5 websites for longer. So if we're headed towards a dystopia we won't need superintelligence to get there.The author is probably right to be suspicious, in the current cultural climate it's unlikely that we would avoid a malevolent emergence if the probability of such an emergence is high. However, I'm of the opinion that while there is not enough information to make a call, my worldview is that nonzero sum cooperation is the ideal outcome of a complex system.There was a lot of good stuff in here. I especially liked the talk of Malthusian limits, I'd never thought on the scale necessary to understand why we weren't near one, nor what it would take to get us there."
268,0199678111,http://goodreads.com/user/show/1382279-austin,4,"This is a philosophical work exploring the implications in the growth of machine intelligence. It takes ""seriously the view that a machine intelligence transition [an 'explosion' of super-human intelligence] might occur in this century, that such a transition might be among the most important events in human history, that it might be accompanied by some amount of existential risk as well as tremendous upside, and that it would be prudent to put in a bit of work in advance to see if there is something we should be doing to shorten the odds of a favorable outcome."" I found the book to be thorough, dry, and important. He convincingly argues that certain failure modes whereby an AI intelligence explosion could run amok are to be viewed with grave concern, and begins with a general education on what AI is, how an intelligence explosion could occur, and what control methods could be employed to avoid failure modes. New words learned: 1. Singleton: a kind of hegemon2. Oracle: a question-answering system3. Genie: command executing system4. Sovereign: system w/ a broad mandate to operate in the world in pursuit of long range goals5. Cosmic endowment: the sum total of what the universe has to offer humans"
269,0199678111,http://goodreads.com/user/show/16490123-karin-nemec,3,"Nick Bostrom is Professor at Oxford University, where he is the founding Director of the Future of Humanity Institute and directs the Strategic Artificial Intelligence Research Centre provide interesting insight in the superintelligence and the probable ways it would be selected. The content of the book is worth of 5 star but writes as academic using lots of jargon and passive voice which makes it difficult to read, therefor I gave 3 stars for the book. Once we have developed human level intelligence, it will not stop there. Levels of intelligence does not correlate how nice a person is. We design AI currently as optimization problem (find most effective ways with limited resources and do it the best way), but once it will get out of control we have to be sure that super intelligence shares our values and motivations to match our behaviour . It's the way to solve the control problem in advance. Poorly specified goals, definition of outcome X will have devastating consequences. Bostrom provides several paths and strategies in detail in this book. Those who do not have time to read the book, I can watch his TED talk https://www.ted.com/talks/nick_bostro... "
270,0199678111,http://goodreads.com/user/show/67002092-mr-siegal,2,"If I had ball bearings…If my grandmother had ball bearings, then she would be a scooter. I don’t remember where I heard this, but this is the thought I had while reading this book.I remember the hype it cause when it came out, so I bought the book back in 2014, and then, as one does, read it 5 years later. I was not particularly impressed.Yes, this is an ok book, with lots of thought been put into it. However, I struggle to see how it has anything to contribute besides fictive speculation of how our AI overlords will be like. If they are X, then Y, but if they are Z, then W and so on. Yes, we must be careful, and I agree with the authors concerns, but the book seems to be going a wee bit overboard with this obsessive over speculation of how things might be in the future.The interesting chapter was the second to last one where Bostrom talked about who gets to own this Superintelligence (if it can be owned that is). This is a discussion which, I humbly believe, is far more pertinent a discussion, one which also affects other aspects of our daily lives. All in all, an ok book, with some interesting points here and there, and as Jim Morrison wisely said, “the future is uncertain but the end is always near”. Cheers to that!"
271,0199678111,http://goodreads.com/user/show/91651788-ben-jones,3,"1-10 Takeaways: 1) Types of superintelligence: 1) Brain emulation (slice up a human brain, map it with neurons and use it to make a mechanical super intelligence). 2) Genetic engineering (iterate over generations of humanity to make a super intelligence). 3) Synthetic (code-based) AI (most likely). 2) How will we handle the Crossover Point (when AI becomes more intelligent than humans)? There are boxing/stunting strategies but there will not be much that we can do to stop superintelligence and it will most likely be amoral. Even a simple task of creating paper clips could cause it to destroy humanity (its very difficult to ask a superintelligence the right questions to get the desired output). 3) How long will this take? (Slow Launch vs. Fast Launch). Probably Slow Launch. AI right now is very specific/narrow, the algorithms have been roughly the same for years, the data cleaning requires human assistance, algorithms occasionally require human assistance.4) After a technological breakthrough, you can't ""put it back."" You want to invent the vaccine before the pathogen. 5) With digital minds, humans my retain wealth because of our existing investment in capital. 1-10 Questions: 1) "
272,0199678111,http://goodreads.com/user/show/11550691-robert,4,"Superintelligence is a dense, thoughtful review of the nature and consequences of the emergence of a superintelligence. Written more like an academic textbook with respect to the depth of the analyses, the book dives deep into how a superintelligence could come to be, what could go wrong, and what we need to think deeply about sooner rather than later to avoid many of the things that could go catastrophically wrong.One of the biggest challenges in developing a strong, synthetic AI is the value loading problem. Machine learning algorithms frequently work by optimizing an objective function. For example, the straightforward linear regression algorithm is often used with a least squares method. In this approach, the algorithm fits the best line to the input data such that sum of the squares of the differences from each data point to the line is minimized. For even a very simple synthetic AI, the objective function is much more complicated, often being a complex combination of only somewhat simpler functions.To get a sense of the problem, think about your own objective function. What are you trying to optimize in your life? Now, imagine a group of researchers trying to agree on an objective function for a synthetic AI."
273,0199678111,http://goodreads.com/user/show/51896417-frans-sax-n,4,"Philosopher Nick Bostrom has written an important book, providing a wide survey of the issues related to artificial intelligence. Bostrom's book provides a survey of many aspects of artificial intelligence, from its history, to the current state of the art, on to future scenarios. Bostrom incorporates insights from a wide range of fields, from computer science to philosophy, from game theory and economics to neuroscience and psychology. The scenarios are not all calming, the control problem and the problem of assigning values to artificial intelligence does raise some troubling questions. How do we ensure that an intelligence vastly superior to ours doesn't figure that the best way of satisfying its goals is to make sure there are no humans getting in its way. Bostrom argues that we still have time to research these types of questions, but that we need to do it before it is too late. The book serves as a good primer on this most important topic. That said, this is no easy read. At least to me this book introduced a wide range of new concepts, and Bostrom's writing is not the easiest to read. In places this reads more like a text book than popularized science."
274,0199678111,http://goodreads.com/user/show/1232141-jonathan,5,"This is one of the smartest books I've read period, and certainly on the topic. This books lays out the benefits, risks and probabilities of various pathways towards the development of super intelligence (which could be artificial intelligence, whole brain emulation, or a form of collective intelligence). The risks are what stand out, because they're huge and basically guaranteed unless risk mitigation steps are taken. And while we know the categories of some of these risk mitigation steps, like control mechanisms, we are actually a long ways from knowing how to implement most of those mechanisms. The descriptions of different strategies to achieve super intelligence and mitigate its risks gets into some deep ethical and philosophical challenges, many of which were over my head, especially when listening on Audible rather than having a physical page that I could re-read several times. I would highly recommend this book for anyone interested in the topic, and I'd recommend a physical copy if you want to really dive into it because its quite dense. This is also a great follow on to Sapiens and Homo Deus, which are what got me into the topic in the first place."
275,0199678111,http://goodreads.com/user/show/16425742-blue-phoenix,4,"Got this book thanks to ""Bill Gates's recommendation"" about 2 of his favorite books related to A.I. As someone who is quite familiar with the concepts, I must say the book did a great job in giving a clear overview of superintelligence agents types, elaborate simulations for social impacts and draft out strategies to deal with the ""problems"". ""It is a profoundly ambitious and original book, the author thoroughly examines what he calls most essential problem of our time: can we solve the superintelligence control before it's too late?"" The chapters were well-categorized except the few last sections where the author dived too deep into domain expertise (I think many readers might find difficulties in these parts, I was kinda lost in the CEV - Coherent extrapolated volition - and had to return for clarity many times...). Well, it's not science fiction anymore and we should address the challenges in a more serious way. (Go back to work and continue to improve reinforced learning algorithm for my A.I project...)"
276,0199678111,http://goodreads.com/user/show/2608334-danny,4,"Dense but fascinating look at artificial intelligence and the next step, superintelligence. Bostrom has written the to-date definitive textbook on the topic. We learn about different approaches researchers might take to achieving artificial intelligence, the pros and cons of each approach, and the odds that each will be the first to get there. We also learn about how these early stage AIs will most likely evolve (quicker than you might expect) into a superintelligence (essentially a digital being so much smarter than us that it will easily take over the world, perhaps even within hours of coming into being). Finally, because of the dire consequences involved, Bostrom discusses various ways we might try to prevent a superintelligence from taking over the world and imbue it with values that will make it cooperate with humans by design.The first few chapters read like a computer science text, but the bulk here is more philosophy than hard science. What values/goals of an AI would most likely lead it to work side-by-side with humanity instead of destroying us? How do we get it to follow these values? How should humanity cooperate with each other in the process leading up to the AI revolution, considering the fact that the first success in this field may quickly determine the fate of Earth and the whole universe?While dense at times, this is a fascinating read. If you're interested in the topic but don't want to dive into the deep end just yet, check out this great, long blog series that covers the same topics and cites plenty of Bostrom's work: http://waitbutwhy.com/2015/01/artific..."
277,0199678111,http://goodreads.com/user/show/7807167-ayman,4,"Super Intelligence is a great power with which comes great responsibility. To get an idea, imagine a machine that is a million times smarter than Albert Einstein and runs a million times faster. The movie, Transcendance, was the best depiction of such machine.In the movie, and according to this book, this machine will be formidable and the whole world will fall to the mercy of its moral objective and even the most benign objectives could be catastrophic for our species. For example, Johnny Depp, in the movie was motivated by scientific discovery, so he used humans as dummy slave robots to execute his endless experiments. Similarly, a Super Intelligent AI, could accidentally end human life and all life forms if the objective was miscommunicates or misinterpreted.Bostrom thinks we have to put all the resources we have on solving this problem to make sure that Super AI does not become humanity's greatest and last invention.I gave it 4 stars because Bostrom is a bit pompous and uses a lot of unnecessarily complex vocabulary and sentence structure, but overall great content."
278,0199678111,http://goodreads.com/user/show/15751426-scott,1,"In the preface the author states: ""This has not been an easy book to write: I have tried to make it an easy book to read, but I don't think I have quite succeeded."" He is absolutely correct; IMHO he failed miserably. I can't tell you the last time I have given up on a book, but I believe it's been several decades. I'm a fairly intelligent middle age man, I enjoy reading challenging books, and do not shy away from difficult topics. I was/am equally intrigued by the subject of this book and will look for other books on this subject. However, this particular book dealt too much in facts, figures, and technical jargon that was repeatedly stated and referenced ad nauseam. Because of this, I believe he lost sight of the larger subject. Definitely way too focused on each individual tree and not enough vision of the forest.I really wanted to like, and to finish this book, but neither happened. I gave it a chance, reading almost the first third of the book. I just couldn't hold on any longer."
279,0199678111,http://goodreads.com/user/show/43258586-anton-hammarstedt,5,"* Synopsis: Superintelligence is a thorough treatise on possible paths to superintelligence (defined as any intelligence beyond that attainable by current anatomically modern humans), possible consequences of superintelligence, and possible ways of maximizing the probability that the consequences are not catastrophic.+ Extremely dense in ideas, especially if one haven’t thought about these issues previously. Notably, it turns out that fiction is a very poor predictor of what the likely outcomes of, and risks with, superintelligence may be.+ Well-built arguments.- I wasn’t persuaded that intelligence and morality are wholly separate. I mean, I believe that it is, but I wasn’t persuaded by this book.- I would have liked to see some treatment of substrate independence. In a book about AI, I think it’s important to develop the idea that a machine intelligence may be conscious at all, and above all to persuade the reader that machines running sufficiently intelligent software is likely to be conscious (if indeed that is the view of the author)."
280,0199678111,http://goodreads.com/user/show/19753689-bill-brazell,5,"A difficult but amazing read about a problem that does not yet exist, but may exist sometime soon, by which point we may no longer be able to save the human race. Bostrom has thought so deeply and read so widely about the problem of General Artificial Intelligence that he is able to lay out its many concerns in a manner that is clear, as concise as possible without oversimplifying, and frightening. Part of what Bostrom is writing about is the fact that humans have reached no consensus on how to determine whether or not a particular act is ethical -- and so we are in no position to program a machine to make such choices. Yet the possibility of overwhelming power that such a machine could bestow -- and the terror lest someone else make such a machine/program first -- almost ensure that someone, somewhere, will create such a thing before we've even begun to understand all the possible kinks, let alone worked them out. Even as it is frightening, reading the book is also trippy -- like a joints-after-midnight conversation with a really, really smart friend. Highly recommended. "
281,0199678111,http://goodreads.com/user/show/74640031-j-r-white,3,"I think this is an important topic. I have paid attention to the field of AI safety, and I found I already knew many of the contents. I still learned details and scope, and found that Bostrom's work had an effective and memorable structure.I'm not entirely certain the best format for this work was a printed book. I also feel like the book made a lot of assumptions about the reader.I already agreed with the concepts and was familiar with most of the numbers. Indeed, the book kind of assumes that you were basically on the same page, and had previous exposure to the ideas. It didn't feel quite like Bostrom delved enough into the argument on the topic to newly justify this assumption, and also didn't deliver an appropriate level of complexity for those who already met it.I expected that this defining work would be a bit more satisfying. I suspect the speculative nature of the topic still could have much said about it in such a format.Also, my edition had a few typographical errors (as a previous owner noted in pencil)."
282,0199678111,http://goodreads.com/user/show/5153078-aidan-hamill,3,"Picked this up to learn more about why people are concerned AI will end the world. And in fairness, it doesn't really attempt to anything but that. The first few chapters which explain/define what AI is/can be and it's history so far are very interesting; but after that the bias is clear, it is entirely speculative/science fiction describing only the possible dystopian outcomes. A lot of arguments seem contradictory, and one chapter might use a particular prop to prove one thing, while the next chapter uses it to disprove another (Take the role of the programmer; the AI might evolve to completely ignore whatever it was built to do and go do its own thing; yet later, the rules given by that programmer are the cause of the AI going bad - in its quest to achieve whatever it was built to do, yet also offered as a solution to control it); there's no balance.It's probably my own lack of understanding on the subject that got me here, so I'm going to look at some pictures in ""you look like a thing[..]"" and see how that goes instead."
283,0199678111,http://goodreads.com/user/show/4877965-nisha-jain,4,"A philosopher’s musings about Artificial Intelligence, machine learning and all things related. A detailed analysis of everything thinkable (paths, dangers and strategies) about super form of artificial intelligence which has not yet been conceptualized properly. Even though there is so much uncertainty about every possible aspect of artificial intelligence (which even the author keeps mentioning again and again), the book is a commendable effort to detail out all the possible scenarios that could happen/materialize when the superintelligence comes into being. A must read for anyone who wants to understand the magnitude of the technological happenings in this age and all that would heavily impact the near and far future. I also wish decision makers read this and wake up to the adverse effects superintelligence may bring and may start coordinating on this crucial issue as well (as they are trying for other global issues, like climate change, nuclear deals, etc.). "
284,0199678111,http://goodreads.com/user/show/15736812-roman,4,"This book seems rather philosophical to me, as it discusses topics we should not be able to understand with our current cognitive abilities. Starting from the premise that we might one day accidentally create something incomprehensibly more superior to a human brain, the book investigates what might be the consequences of such inventions and what we might be able to do to somehow contain such superintelligent being if it decides we are no longer relevant. Although the topic is very interesting and relevant with recent developments in AI field, I found it difficult to follow the discussion at some places and difficult to agree with arguments at other. For example, it doesn't look reasonable that such a vastly superior being, capable of escaping any human attempts to limit its power and capable of converting the Universe into computronium, will never question its own initially imposed goals of calculating a number of Pi with maximum precision or covering the World with paperclips.."
285,0199678111,http://goodreads.com/user/show/47567538-monica-sheldon,4,"I thought I'd give this one a try given it's reputation and general interest around the topic. It's a tough read if you're like me and have trouble digesting details on speculative topics and its academic language is kind of funny at times (scientific notation describing non-mathematical concepts and at least one diagram using a skull and cross bones icon to indicate the general idea of a point-of-no return made me smile). So, pretty geeky book - Science Fiction / Philosophy.It does has some very realistic and sobering things to think about: an arms race towards superIntelligence, the lack of regulation around it (including a lack of understanding by policy-makers), and the ability of AI to pose as human (earning money electronically and hiring humans online to do physical tasks) were all things I hadn't thought of before. Maybe the rest of it is also likely, just over my head. We'll see :) Maybe this book will be really entertaining reading for future AI. "
286,0199678111,http://goodreads.com/user/show/25238964-james-leth,3,"I have mixed feelings about this book. It asks many of the right questions and suggests ways to think about the issues. Therein lies the most value in the book. Philosophically, the arguments presented feel strong, but there's a lack of real rigor to them: ill-defined terms and ambiguous graphs that look like quantified results but are actually just guesses at what shape the graph might have. A good example is the discussion of capability vs. risk on p. 247. The vertical axis is labeled AI Risk Level, but the value is actually interpreted as its inverse, ""safety investment"", and the numbers on both axes are arbitrary with no data to back them up. The book is full of this kind of ""science-like"" analysis that amounts to little more than, ""here's what I think."" Given the lack of supporting data, the book's overall pessimistic tone makes it seem manipulative in order to support an alarmist agenda."
287,0199678111,http://goodreads.com/user/show/19333231-zoe,2,"It is urgent for humanity to have more discussions on AI, especially through a philosophical lens. That is what I liked about Bostrom’s book. But...It was a very difficult book to digest, in that it was poorly edited. Over explaining of certain concepts and he rambles on too much. Contained vital topics that should have been explained laconically. He also hopes that AI can solve some of our philosophical problem. I completely disagree, thinking is too enjoyable & human to give up to machines. My main problem with this book is common with academics and experts: they are way to definitive about things. When experts don’t even understand the motivation behind AlphaGo’s moves, how can we even imagine what will drive a super-intelligent AI? Like many scientists (Manhattan Project) before him, Bostrom naively thinks that technology ultimately does more good than bad. Before any development of super AI, let’s check our egos at the door and learn from history. "
288,0199678111,http://goodreads.com/user/show/70474434-carsten,5,"As implied by the title, this book deals with superintelligence, a form of artificial general intelligence which surmounts human intelligence by far. The author believes it is more likely than not that our species will be able to create such an AI at some point (this might be many decades into the future). The questions discussed in the book are for example: let’s suppose we created such an AI - what then? Do we need to prepare? How? How do we keep save from such an AI? Will there be one or several superintelligent machines? Needless to say, these are important questions to address before we create such a powerful AI. This book is written in an academic style, with many unusual words used, which, if you remember them, will gain you many points in Scrabble. That makes it a hard read. But I believe that this book should be read (or shall I say studied?) by anyone who is interested in the topic. I am sure this was not the last time I picked it up."
289,0199678111,http://goodreads.com/user/show/55670468-sam-romilly,1,To be clear from the start this is a very boring book. It is written in an academic style where every side of an argument is carefully considered in great respectful detail before concluding that it has no value. It certainly sounds interesting the concept of super intelligence but the assumptions are quite over the top. To follow the arguments you have to accept the basic premise that a super intelligent computer would expand exponentially and would end up conquering the universe if left uncontrolled. For me this is an assumption on par with saying there is a God that created the universe and controls everything that is taking place. In other words a belief and not a scientific study. I do not accept that premise for if it was a viable possibility then why would not another sentient being in another galaxy not already have done this and their AI experiments taken over our planet?So if you do not accept that premise then the book is not for you. 
290,0199678111,http://goodreads.com/user/show/75604139-james-briant,4,"Goes on a bit. He gets a bit lost providing lots of words around little data for why creating an AI would be bad. However, he strives to hard: he successfully demonstrates that there is enough reason to believe in the massively negative outcomes, but then drowns it in endless paragraphs. It might have been better to make the arguments for negative outcomes brief, and then laid into the lack of evidence that the positive outcomes will arise. But I suppose that is the challenge. There is absolutely no evidence that AI will work out for the best and yet large organizations are doing it anyway. We must build the bomb before the enemy gets it. Except this bomb is sentient / intelligent, and unlikely to have any concern for humans. I feel that message got lost in the book. But then perhaps I already agree with his outlook. Perhaps this book might convince someone who might otherwise end humanity. We can hope."
291,0199678111,http://goodreads.com/user/show/76610990-andrei-nutas,3,"While I think the book could be called Superintelligence: Paths to Doom, Dangers, Strategies for and against the Apocalypse I would still never the less recommend it as looking beyond the futurist doomsday scenario, it does explain quite well how AGI and Superhuman AI could come by given where we currently stand and it tries to map possible approaches one could take in order to ensure the emergence of a friendly AI. What is the most interesting is when it looks into how easy it can be for an AI to misrepresent what we want from it and how hard it is to tell an AI what we want especially given that our desires are almost never single faceted making it very hard to describe now. Add to this the fact that our wishes constantly fluctuate as our experiences change us and you have a seemingly irreducible challenge when it comes down to understanding human desires and translating them in instrumentally transcribable descriptions. "
292,0199678111,http://goodreads.com/user/show/16003494-neel-nanda,5,"This is an excellent book, and I'd highly recommend it to anyone interested in how the development of AI might affect the trajectory of human civilisation.It's a very rigorous book - Bostrom takes vague ideas and fleshes them out in a lot of detail. This makes for a pretty dense, high-effort read. But I found it valuable for getting a deeper perspective on the ideas, even ideas I'd already encountered and thought about before.This book suffers for having been written before deep learning really took off - if it were written again today, there definitely ought to be far more emphasis placed on deep learning-like paths to superintelligence, and the consequences of that. But with that context, it holds up impressively well! This was Bostrom's attempt to chart out possible paths and important considerations, and though today one path looks clearer than the rest, the fact that he foresaw that path is a testament to the thought that went into this book."
293,0199678111,http://goodreads.com/user/show/32399061-roo-o-brien,4,"Not an easy read, this, and not a popular science book. Its aimed, I think, at people who work in the field already or have a strong interest and it talks to them on their terms. And just as well, because if they're not listening we are likely in for trouble. The picture Bostrom paints is a chilling one. The part that really got to me was the 'paperclip AI': ""An AI, designed to manage production in a factory, is given the final goal of maximizing the manufacture of paperclips, and proceeds by converting first the Earth and then increasingly large chunks of the observable universe into paperclips."" It really crystallizes the danger of stumbling into creating a monster that we are unable to control, and given, what we know about the world, I fear Bostrom's pleas that we imply significant caution and collaborate globally will fall on deaf ears. Scary. "
294,0199678111,http://goodreads.com/user/show/1150766-mike,3,"Good, not great, serious discussion about the risks associated with increasing machine intelligence. While it posed some interesting ideas and questions, I would have loved for the author to take a few more stances on where he thinks we should be going to avoid the litany of dangers he lists. It almost seems that in the race for this to be on the market, the author constantly falls back on ""which of these eight potential avenues might bear the most fruit? who knows? not me."" While I understand his point that we can't know the form a superintelligence would take and therefore can't accurately predict how we will need to react, this feels like the conceptual equivalent of writing ""here there be dragons"" on uncharted parts of a map - an acknowledgment of an area that may be fraught with danger, but one so unhelpful that it's not useful."
295,0199678111,http://goodreads.com/user/show/908017-mike-barretta,4,"A book that requires more than one reading. It's much broader than I anticipated, though the subtitle essentially describes what major topics are covered. The take away is that AGI will come. There are too many ways, and those ways only grow in number and simplicity as time goes on. Paths. The question is: what happens when it (or they) do? Dangers. How can we prevent or control the dangers? Strategies.Also, one shouldn't think of AGI as a conscious super human. At least that's not how I think it could work. It's more like a zombie that has a goal in mind (eating brains, of course) and will keep doing whatever it takes to get closer to that goal. It can be surprising clever as it tries to break in to your house and find you, but it's not necessarily ""smart"" or ""curious"" or even ""mean"". It is just doing what it does, and hopefully you can stop it before it gets you."
296,0199678111,http://goodreads.com/user/show/68009396-nick,5,"A must read for anyone interested in artificial intelligence and approaches to its development. Although occasionally technical, the book is a philosophical treatment of how humanity should pursue the creation of super intelligence. Rather than exploring specific forms of A.I., Bostrom offers a clever and clear framework for evaluating the types of problems mankind may encounter in its pursuit. He also highlights the pros and cons of various paths we could take towards super intelligence.The author is modest and direct in his message: gone wrong, A.I. has the potential to pose an existential threat to humanity. With numerous methods for pursuing super intelligence (brain emulation, synthetic AI, etc), future developments will require numerous safeguards at each stage of the process to ensure the A.I. does not ""get ahead"" of its creator."
297,0199678111,http://goodreads.com/user/show/45838070-milan,3,"""Consider the hypothetical"". Even with a limited experience with futurology books I dare claim that there is criminal lack of concrete information in this one. Maybe it should be this way. Maybe when you are nourished by years of sci-fi reading and finally you opt for a plain sci- addendum to the parochial specificity of concepts and innovations of pop literature, you should expect no more than a methodological framework disuguised as synthetical thinking, a cascade of truisms based on a handful of game theory offshoots that are as circular as they are repetitive. It is as if such uprooted writing fundamentally required constant repetition of propositions so that the reader and author stay on track. This is a book, where charts illustrate word and words illustrate charts, this is a book that reminds one of a blank paint-by-numbers publication, where there are no numbers. "
298,0199678111,http://goodreads.com/user/show/10367135-amade,4,"At times it gets repetitive, but the book made me realize for the first time some of the dangers associated with AI/Superintelligence. Before I read it I assumed that AI needs to become conscious and sort of a species of its own to become a danger to humanity. But this may not be true. What can be dangerous is a 'dumb' (unconscious) algorithm, who's just really good at optimizing its goal using whatever means it can come up with. Amy given the increasing amount of computer power, such algorithm can try virtually unlimited number of ways to get to that goal. Some of the ways may include tricking humans to do something which would not be in our best interest or even completely destruction all.If you like to get fuel for such thought experiments of your own this book is for you. It may not be the funniest read, but definitely worth a few evenings."
299,0199678111,http://goodreads.com/user/show/34160318-vikram-x,5,Did the great apes i.e (Chimps/Gorillas/Orangutans) know that their fate was doomed when their cousins (Humans) were undergoing a peculiar change driven by evolution in their frontal cortex about 2 million years ago ???NO – the great apes never saw it coming! Humans became the apex predators and pretty much till now are directly or indirectly we responsible wiping out most species off the planet. This is the analogous relationship humans share with AI as of right now . Will we be able to foresee what is in-store for us once the technological Singularity manifests itself driven by a capitalistic surge for automation ? (most predictions state in the next 50 – 75 yrs.).With that Nick Bostrom introduces the “Control Problem” – How humans don’t end up as the great apes in presence of a super-intelligence or its game over.
