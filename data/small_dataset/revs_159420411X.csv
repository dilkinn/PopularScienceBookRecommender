,isbn,user_link,ranking,review
0,159420411X,http://goodreads.com/user/show/1139239-michael-austin,5,"Nate Silver has done an incredible (and, quite possibly an unpredictable) thing with _The Signal and the Noise_: He has written an extremely good book when he didn't even have to. Nothing is more common than for someone like Silver--a media phenom with a strong platform (his 538 blog) to phone a book in to cash in on his 15 minutes. I have probably read two dozen books in the past five years that do exactly this. But _The Signal and the Noise_ is a much more substantial book than, say, _The Black Swan_ or either of the _Freakonomics_ offerings. It is a wide-ranging, in-depth look at the ways that we are wired to make predictions (and the reasons that these are so often wrong).Silver ranges over a variety of prediction environments: baseball, chess, poker, the stock market, politics, weather, and terrorist attacks to name the most interesting. Throughout it all, he reminds us that human beings are pattern-seeking animals and that we are just as likely to build patterns where none exist as we are to find the correct patterns and harness their predictive capacity. Predictions work best when they are 1) probabilistic (i.e., express a range of possibilities and assign probabilities for each); 2) when they use as much information--both statistical and analytical--as possible; and 3) when they are continually revised to account for new information.As logical as these sound, human nature seems to drive us in three opposite directions: 1) we seek predictions that are definite and can be acted upon (i.e. ""Obama will beat Romney,"" or ""it will rain tomorrow""); 2) we gravitate towards methodologies that seem to discover a magic bullet formula that guarantees success; and 3) we feel compelled to stand by our predictions even as they become increasingly unlikely. Seasoned prognosticators play a long game. Under the right circumstances (a poker game, for example), a strategy that produces only a sightly better prediction than random chance can produce huge dividends.Perhaps most surprisingly, Silver is a great writer (or, at least a great explainer). As an English major with very little grounding in statistics, I could still understand everything he said. Even more importantly, his narratives are interesting. Who could have predicted that from America's most famous stat-geek? "
1,159420411X,http://goodreads.com/user/show/913238-justin,2,"I had read most of this book with a fair degree of equanimity - finding some faults, but also a lot of good information in it. Then I'm jarred out of complacency by a sudden shot from nowhere, in which he says that David Hume, one of the greatest philosophers of the 18th century, is simply too 'daft to understand' probabilistic arguments. Without any introduction to the subject, he claims Hume is stuck in some 'skeptical shell' that prevents him from understanding the simple, elegant solutions of Bayes.What makes this so painful to read is that it shows Silver has never even taken the time to read Hume, at least not more than the two paragraphs he used to cite his sources. If he had even kept on for five more pages he would have found that Hume was defending the very type of probabilistic arguments that Silver said Hume was 'too daft' to understand. Nate seems to have given a cursory glance to a single page of Hume's work - ""SCEPTICAL DOUBTS CONCERNING THE OPERATIONS OF THE UNDERSTANDING,"" without even bothering to proceed to the very next section - ""SCEPTICAL SOLUTION OF THESE DOUBTS,"" in which Hume lays a rational foundation for belief in the absence of certainty.In fact, the entire 'Enquiry of Human Understanding' can be read as a treatise attempting to supplant abstract and questionable a priori proofs, with more sensible arguments grounded entirely in the test of experience and probability. By brushing Hume aside so casually, Silver spits in the face of his own philosophical progenitor - a man who helped plant the foundations for the sort of thinking that Silver now takes for granted.I am sure the vast majority of readers will roll a bemused eye at my anger over trivial details like this - but not only does it show that Silver very often doesn't take the time to understand his sources (see Michael Mann's critique of Silver's presentation of global warming), but Silver's casual remarks could easily turn a lot of readers off to Hume before they've even read him. Trendy books like Silvers are far more popular than classic works of philosophy, and new readers are likely to take Silver's description as an accurate portrayal of that daft, old skeptic, David Hume. "
2,159420411X,http://goodreads.com/user/show/3486245-charles,4,"The Signal and the Noise is a very interesting book with mixed success: 3 1/2 stars, were this permitted. I found it somewhat difficult to review; however, my entire book group – without exception – had similar opinions. I would encourage you to view this as a group opinion.At its best, TSANTN is interesting, illustrative, educational, and provocative. And many chapters – including banking, the weather, volcanoes, elections, and poker – were exactly that. Four stars, without hesitation. The problem is that some chapters – including baseball, terrorists, and the last several – were dull. Either too long or too scattered or just not interesting. (Again, this was the unanimous opinion among my group.)Nate Silver is a wunderkind polymath, who has scored resounding successes in statistical applications to baseball, poker, and, most recently and most impressively, politics. He emphasizes that huge bunches of data are the tools needed for predictions and that there are huge bunches of data out there. He calmly points out that some things are predictable and are predicted, using various methods with resultant various success. Some things that are predictable are not predicted accurately, exactly because the wrong tools or approaches are used. He equally argues that some things are not predictable, and when predicted, have, predictably, low success. Poor predictors often share the characteristics of ignorance of facts, inappropriate application of basic probability analyses, and, especially, overconfidence. Forecasts are made more inaccurate by overfitting – confusing noise for signal.His grasp of applied math and statistics is refreshing. His application – although, perhaps not the explanation - of Bayes theorem is lucid. His writing style is casual, more impressive considering the subject material.As has been noted by others, the number of typographical errors is unacceptable. An even greater editorial error is letting the author ramble on (again, in some chapters). Liberal use of both a sharp red pencil and an X-Acto knife would have improved this book.So, overall, I really liked some parts. This is why I gave the book a 4-star review. (Most of my book group ended up awarding only 3-stars). But, overall, after a few strong opening innings, the precision of text and purpose waned. In the beginning I did not want the book to end; by 2/3 of the way through, I was more than ready."
3,159420411X,http://goodreads.com/user/show/7213075-ted,5,"4 ½ stars.Nate Silver is probably best known as the statistician who confounded the “experts” by predicting the results of the 2008 and 2012 U.S. Presidential elections. As a matter of fact, his web site (https://fivethirtyeight.com/) actually did much better than the average pollsters and media with the 2016 election as well. I was following the writing on the site right up to the night of the election. Entering the final few days, 538 was giving Trump about a 1/3 chance of winning, while most others were saying that the election was a foregone conclusion. And on election day, the 538 article which pointed out early signs that Hillary could be in trouble was so accurate that I had given up for her before 10 pm that evening.And, despite any negative impressions I may leave below about any issues I previously had with Silver's writing, or his style, the last few years, in which he's developed his own web site, together with the interactions he's had will the commenters and other statisticians that he's hired, have made his writing a model of clearness and conciseness. He also (nowadays) is very careful to refrain from making rash statements about probabilities, usually listing many reasons why the ""odds"" being quoted could be risky bets.Anyway - before Silver's election triumphs he was known to a less wide, but no less fervid, audience as a sabermetrician who, starting in 2003, contributed predicted statistical ranges of performance for major league baseball players to the Baseball Prospectus.In The Signal and the Noise, Silver discusses issues related to these foundations of his reputation in the second and third chapters. To me, the chapter on political predictions was fascinating, the chapter on baseball less so – this despite, or perhaps because of, the fact that I’ve been a keen consumer of sabermetric literature almost since Bill James brought it into the mainstream in the late 1970s.On balance I found the book, in terms of insights offered and simple interest, much closer to the political chapter than the baseball chapter – thus the high rating.I have to confess, however, that I certainly had my expectations lowered by Silver’s Introduction. This impressed me as an attempt (possibly at the urging of an editor?) to present a “Big Theme” context to the book which was described not only disjointedly, but in a manner that makes Silver look like a poor writer, which he isn’t at all. (view spoiler)[For example, many statements that I would think are pretty much common knowledge are footnoted. To be fair, Silver does have a habit of putting comments in addition to source information in his footnotes. Where I believe he often errs is in not needing a source for a statement that is pretty non-controversial; in these cases the comment could just be inserted into the text and the footnote dispensed with. There are other cases, such as “(The printing press) was a spark for the Industrial Revolution in 1775 …”, in which simply removing the year from the statement removes the need for a footnote. (hide spoiler)]The “Big Theme” that Silver talks about in the Introduction is that of Big Data inundating humankind, starting with the invention of the printing press and culminating in recent decades in the spread of powerful computers (to both hold and analyze previously unimaginable amounts of data) and the world wide web, which makes this data not merely available to almost anyone, but overwhelmingly so.But Big Data is only briefly mentioned in the book, and is brought up again in the Conclusion in a correspondingly unenlightening manner. In fact, the book’s first and foremost theme is simply expressed in the book’s title. The difficulty in handling large amounts of data is separating the signal from the noise. The theme, expressed in this manner, is handled more or less brilliantly throughout.Once past the Introduction, the book immediately improved. Silver seemed to quickly find his comfort level in treating one area after another in which we attempt to make predictions, with varying success. Besides the chapters on political forecasts and baseball, there are discussions of the economic meltdown of 2007-8; weather and earthquake predictions ; economic forecasts; infectious disease (flu) forecasts; gambler’s bets; top-level chess; poker; investments; climate forecasts; and terrorism.The great majority of the chapters I found very interesting. Silver writes well, and can clearly get across his points. He shows convincingly I think how these fields differ from one another, and how the problems they have with making successful predictions and forecasts vary from field to field, depending on a variety of elements.I approached the chapter on climate prediction with some trepidation, wondering if Silver was going to somehow take the position that it was all baloney. Thankfully no, and his conclusions about climate forecasts are along the lines of “well the forecasts of warming so far have had a rather mixed record”. So he feels there is a case to be made for some skepticism regarding the accuracy of the models, and thus of the forecasts being produced by the models. He doesn’t doubt for a moment the science involved, or the ultimate warming path we are on, but cautions against believing that we have a very good handle on how fast the warming will occur under different scenarios of additional heat trapping elements being added to the atmosphere.But what Silver doesn’t analyze, here or anywhere else in the book, is how the aspect of risk should be accounted for in making predictions, or in acting on the predictions that we do make. I suppose this may be a bit off the track of what he’s addressing in the book. But it’s one thing to forecast the likelihood of my house burning down (very small), or of a young healthy person needing vast amounts of medical care in the next 12 months (also very small). It’s quite another to use those forecasts to conclude that in neither one case nor the other is spending money on insurance a good idea.Most of us realize that because of the catastrophic consequences of these very unlikely events, buying insurance is rational. In the same way, it seems to me that ignoring climate change forecasts until “more evaluation” of these forecasts, and thus more fine tuning of the models, can be done, is a tremendously risky thing to do, and cannot really be rationally justified.I’ll wind up with a brief mention of an aspect of Silver’s thinking that I found more interesting than anything else. That is his interest in, and application of, Bayesian reasoning or inference. Silver is quite obviously much taken with this, and he does a good job (in my opinion) of explaining it. He doesn’t really introduce it until his chapter on gambling, where he shows how it can be used to make probabilistic forecasts using several interesting (non-gambling) examples. In almost every chapter following this he refers to the way that Bayesian reasoning can be used to strengthen forecasting and to overcome some of the difficulties of predicting in that area."
4,159420411X,http://goodreads.com/user/show/155663-david,5,"This is a fantastic book about predictions. I enjoyed every page. The book is filled to the brim with diagrams and charts that help get the points across. The book is divided into two parts. The first part is an examination of all the ways that predictions go wrong. The second part is about how applying Bayes Theorem can make predictions go right.The book focuses on predictions in a wide variety of topics; economics, the stock market, politics, baseball, basketball, weather, climate, earthquakes, chess, epidemics, poker, and terrorism! Each topic is covered lucidly, in sufficient detail, so that the reader gets a good grasp of the problems and issues for predictions.There are so many fascinating insights, I can only try to convey a few. At the present time, it is impossible to predict earthquakes, that is, to state ahead of time when and where a certain magnitude earthquake will occur. But it is possible to forecast earthquakes in a probabilistic sense, using a power law. Likewise, it may be possible to forecast terrorism, because that too, follows a power law! (Well, it follows a power law in NATO countries, probably because of the efforts to combat terrorists. But in Israel, the tail of the curve falls below the power law, likely because of the stronger anti-terror emphasis there.) The accuracy of weather predictions increases slowly but steadily, year by year. Ensembles of computer model runs are part of the story, but human judgment add value, and increases the accuracy. Weather forecasts issued by the National Weather Service are unbiased in a probabilistic sense. But weather forecasts by the TV weatherman are very strongly biased--the weatherman over-predicts precipitation by a significant amount.Nate Silver shows that the people who are most confident are the ones that make the worst predictions. The best predictions are those that are couched in quantitative uncertainties. Silver shows how Bayes Theorem can be applied to improve predictions; it is all about probabilities. And I just love this footnote,
A conspiracy theory might be thought of as the laziest form of signal analysis. As the Harvard professor H.L. ""Skip"" Gates says, ""Conspiracy theories are an irresistible labor-saving device in the face of complexity.""
"
5,159420411X,http://goodreads.com/user/show/5210022-julie,4,"The Signal and the Noise by Nate Silver is a 2012 Penguin publication. More Information, more problems-This book was recommended by one the many books related emails I get each day. I can’t remember what the particular theme was for its recommendation, although I’m sure it had something to do with how political forecasting data could fail so miserably. Nevertheless, I must have thought it sounded interesting and placed a hold on it at the library. Many of you may be familiar with statistician, Nate Silver. His blog/podcast, ‘fivethirtyeight’, is quite popular, featuring talks about polls, forecasting, data, and predictions about sports, and politics, and was even carried by the NYT at one point. I admit I was not familiar with his work until now. However, after reading this book, I think I will keep a closer eye on his website. This book examines the way data is analyzed, how some predictions are correct and why some fail. “The Signal is the truth. The noise is what distracts us from the truth.” I’m not one to put my trust in predictions or polls. I don’t bet on sports teams, and I’m even skeptical about the weather forecast. With the polls and the media thinking they had the most recent election forecasted, I think people are warier than ever. That may be why there has been a renewed interest in this book. The first section of the book, takes a look at the various ways experts make predictions, and how they could miss something like the financial crisis, for example.Silver does speak to political predictions. Thinking like the ‘fox of the hedgehogs’, the biased of political polls, the media’s obsession with things the public doesn’t care about. Remember, this book was published in 2012, so, apparently, the media didn’t learn their lesson. (Silver predicted Obama’s win over Romney much to the chagrin of ‘Morning Joe’, and more accurately predicted the outcome of the most recent election, closer than most)“The fox knows many little things, but the hedgehog knows one big thing”.The second portion of the book is where Silver really excels: Baseball statistics. Now, this section really appeals to baseball fans, which I am not. But, it also would appeal to those who understand math and complicated Algorithms. Again, not my thing. I tried my best to understand this section, but just could not get into it and because it was not a topic I was well versed in, much of it went over my head and frankly, it was boring to me. So, I gave up on this section and went to the next. Weather: This section, which deals with prediction of major weather events, such as hurricanes was very interesting. Weather forecasting not only has an effect on safety, but on our economy as well. Many times, forecasters get things right, and many lives are saved, but at times, they get in right, but things are not as bad as predicted, such as the recent blizzard expected to hit NYC. Yet, as frustrating as that may be, erring on the side caution, still might be a good thing, and remember, many weather forecasters, those working behind the scenes, are not being paid exorbitant fees. Just think about the times when you made it out of the path of a tornado, and be thankful for these guys, who must decipher an incredible amount of data and unpredictable patterns, and they must deal with the human element on top of that. Raw data doesn’t always translate well to the average consumer. For example: What does ‘bitter cold’ mean to you? But, there has to be an honesty in forecasting, too. Television ratings can come into play, too, unfortunately. This was my favorite section of the book. Earthquake predictions, economic forecasters, sports betting/gamblers, or anyone or anything that depends on statistics, data, or formulas is examined in this book. It’s all interesting, for the most part, although, math equations and other information laid out went over my head. The author recommends Baye’s theorem, which I understood on one level, but was overwhelmed by it most of the time. But, I did find the book fascinating, informative, and chock full calculations juxtaposed against unpredictable elements that could not be foreseen, or against patterns in plain sight, were ignored, all mix together to prove why predictions and forecast often fail, but also, what makes them work!Although, this book centers around events taking place throughout the economic crisis, and is a point the author often refers back to, the last point in the book of ‘what you don’t know can hurt you’, reminds us that history can repeat itself, that there is always the element of improbability, the unfamiliar, the unknown, and what we can learn from it in order to make better, more informed decisions in the future. 4 stars"
6,159420411X,http://goodreads.com/user/show/9266018-ilya,2,"This book was a disappointment for me, and I feel that the time I spent reading it has been mostly wasted. I will first, however, describe what I thought is good about the book. Everything in this book is very clear and understandable. As for the content, I think that the idea of Baysean thinking is interesting and sound. The idea is that, whenever making any hypothesis (e.g. a positive mammogram is indicative of breast cancer) into a prediction (for example, that a particular woman with a positive mammogram actually has cancer), one must not forget to estimate all the following three pieces of information:1. The general prevalence of breast cancer in population. (This is often called the ""prior"": how likely did you think it was that the woman had cancer before you saw the mammogram)2. The chance of getting a positive mammogram for a woman with cancer.3. The chance of getting a positive mammogram for a woman without cancer.People often tend to ignore items 1 and 3 on the list, leading to very erroneous conclusions. ""Bayes rule"" is simply a mathematical gadget to combine these three pieces of information and output the prediction (the chance that the particular woman with a positive mammogram has cancer). There is a very detailed explanation of this online, no worse (if more technical) than the one in the book. If you'd like a less technical description, read chapter 8 of the book (but ignore the rest of it).Now for the bad. While the Baysean idea is valuable, its description would fit in a dozen of pages, and it is certainly insufficient by itself to make good predictions about the real world. I had hoped that the book would draw on the author's experience and give an insight into how to apply this idea in the real world. It does the former, but not he latter. There are lots of examples and stories (sometimes amusing; I liked the Chess story in Chapter 9), but the stories lead the reader to few insights.The examples only lead to one conclusion clearly. If you need to be convinced that ""the art of making predictions is important, but it is easy to get wrong"", read this book. If you wonder: ""how can we actually make good predictions?"", don't. The only answers provided are useless platitudes: for example, ""it would be foolish to ignore the commonly accepted opinion of the community, but one must also be careful to not get carried away by herd mentality"". While I was searching for the words to describe the book, I have found the perfect description in Chapter 12 the book itself:Heuristics like Occam's razor ... sound sexy, but they are hard to apply.... An admonition like ""The more complex you make the model the worse the forecast gets"" is equivalent to saying ""Never add too much salt to the recipe"".... If you want to get good at forecasting, you'll need to immerse yourself in the craft and trust your own taste-buds.Had this quote been from the introduction, and had the book given any insight into how to get beyond the platitudes, it would be the book I hoped to read. However, the quote is from the penultimate chapter, and there is no further insight inside this book."
7,159420411X,http://goodreads.com/user/show/137566-kate,5,"I'm going to do this the Nate Silver (Bayesian) way. Kind of. Prior ProbabilityInitial estimate of how likely it is that I will buy Nate Silver a drink: x = 10% (This may seem high, given that he is a stranger who lives in another city, but I did rely on his blog during the past two elections, so I'd at least like to.) New Event -- I read Nate Silver's bookProbability that I will fly to New York and track him down and thrust a drink in his hand because this was a great book and I am impressed. y = 50%Probability that I will stay home just remember to check FiveThirtyEight more often instead. z = 30%Posterior ProbabilityRevised estimate of probability that I will buy Nate Silver a drink, given that his book was illuminating and enjoyable: xy/xy + z(1-x) = 15.6%.Feel free to check my math. "
8,159420411X,http://goodreads.com/user/show/17420799-olive-fellows-abookolive,3,I was expecting a lot of data but this was...a LOT of data. 
9,159420411X,http://goodreads.com/user/show/4195304-dewey,1,"I wanted to like this book as I enjoy reading Silver's blog. The majority of chapters in this book are inferior rehashes of arguments and anecdotes from other authors. See Moneyball, the Information, Fortune's Formula, A Random Walk, The Theory of Poker etc. etc. The book is clearly intended to capitalize on the popularity of his 538 blog, which as John Cassidy of the New Yorker just articulated overemphasizes the use of Monte-Carlo simulations to come up with inanely precise projections of a tenth of a point of who will win the Presidential election. While heuristics and Monte-Carlo style simulations may provide details given the parameters included in the model; Silver's assumptions about the usefullness of one poll over another; and the averaging of prediction markets generally reach similar conclusions to what basic common sense would dictate. I happen to believe just as some people inevitably beat the market by looking at past historical data without actual acumen, Silver's model seems to have been successful. The self-aggrandizing by Silver of his own skill at Poker, political forecasting, sports betting etc, seems to belie his own understanding of Bayesian theory and at times reach nauseating levels. I don't care to know his own personal income from limit poker or his player tracking system used by baseball prospectus. The books dabbles in many areas and is truly compelling in none of them. While not an awful book, a curious reader would be better served by reading separate books on area's of interest including book's that offer a stronger statistical background and less ""pop culture"" examples.I do not recommend this book to anyone.See more @Timeisrhythm.wix.com/home"
10,159420411X,http://goodreads.com/user/show/8686221-wen,5,"Another classic on statistics. This one focused more on real-life applications; sports, politics, finance, weather, climate change... I assume those who had basic statistics would enjoy it more. it was about weeding out noises from the data, and zooming in on signals which will improve the quality of the predictions. All easy say (or read) than do :) Here is my prediction...okay more like a hunch: machine won’t be taking over the sorting task mentioned above before humans safely land on Mars. Let’s see how I did.This was my second read of the book as part of my recent series of refreshers on statistics and data analysis. I felt I appreciated Silver's approach to the problems more this time, hence I added one star."
11,159420411X,http://goodreads.com/user/show/35276649-amin,4,"""فارسی در ادامه""My actual rating would be 7/10. In general, it was an interesting and insightful read, although I have mixed feelings about some of the chapters and concepts, and sometimes the pretentious tone of presenting ideas. Let's start by two weaknesses:At some points it seems good prediction looks like a 'hammer' to see all the problems as 'needles'. So, all the problems can be interpreted as the failures of prediction. To me it does not sound very scientific (in a Popperian sense): an 'out-of-sample' situation for Silver is close to what Talib uses to explain 'antifragility'. Or the concepts of hedgehogs and foxes are interesting, but the implications are black and white, in a gray word.Furthermore, there is too much detail and bla-blas on some of the topic such as baseball and basketball players in America, which makes the book boring or too Americanized! without a good understanding of the main points which makes some chapters very journalistic. However, it tries to highlight the importance of statistics, and the way facts less quantifiable and accessible for everyone contribute to unique predictions.The second and the more analytical half of the book was more interesting to me. Ideas such as the changing mental model towards predicting (advantages of humans over computers and the role of the chaos theory), statistical errors in everyday life (overfitting and taking noise for signal), the necessity of creating incentives for good predictions (especially in the fields of politics and economics), complexity of predictions for resolving collective and global problems, the necessity of understanding context (and still having good theories), the story of human/machine rivalry (materialized in Kasparov/Blue Deep match), the necessity of being familiar with the basic principles of human action (to live in an uncertain world), misuse of prediction in financial markets, journalistic side effects of making prediction a popular science and the necessity of understanding the world as a gray zone (in contrast to black and white, or impossible/certain situations) are among the interesting ideas for further investigation through different chapters.در کل اثری مفید و خواندنی بود. گرچه فصلها و جزئیات علمی و کاربردی شان با هم تفاوتهای چشمگیری داشتند. نیمه دوم و تحلیلی تر کتاب جذابیت بیشتری داشت، از این بابت که مفاهیم مهم و کاربردی را ارائه می کرد. مواردی مانند خطاهای آماری انسان در محاسبات، تفاوت یا رقابت انسان با کامپیوتر در پیش بینی، نیاز به آشنایی اولیه با علم پیش بینی در زندگی روزمره، اهمیت توجه به زمینه هر موضوع برای پیش بینی صحیح و غیرهاما دو ایراد: اول اینکه به سبک کتابهای پرفروش علمی برای عموم، مثل کتابهای گلدول و نیکولاس طالب، مفهوم اصلی کتاب که پیش بینی صحیح است مثل چکشی است که هر چیزی را میخ می بیند و راه حل اصلی را در پیش بینی صحیح برمی شمرد. از دیدگاه پوپری این رویکرد را من خیلی علمی نمی دانم و بیشتر برایم جنبه تجاری دارد. نکته دوم جزئیات فراوان و شاید غیرضروری در برخی فصول است که وجهه ای آمریکایی (مثلا در فصول مرتبط با بیسبال یا بسکتبال) به کتاب میدهد یا برای خواننده ای که خیلی به موضوع خاص فصل علاقه دارد جذابیت بیشتر داردجزئیاتی درباره برخی مفاهیم و فصول:(view spoiler)[آغاز علم پیش بینی آنجاست که بشر دریافت برای تغییر آینده غیرجبری باید آن را پیش بینی کند. اما در دنیای پیچیده و واقعیتهای موازی، ""نظریه آشفتگی"" نشان داد با افزودن اثر روابط غیرخطی و پیش فرضها نسبت به وضع سیستم در گذشته چندان هم نمیشود نسبت به پیش بینی ها مغرور بود. بعلاوه، در پیش بینی های روزمره، مثل هواشناسی، کلی نگری انسان به کامپیوتر ارجحیت دارد و از طرفی سودآوری و منافع مادی بر پیش بینی صادقانه اولویت داده میشوندبعضی از معضلات روز را میشود با خطاهای آماری توضیح داد. مثلا ساختن فرضیه ای که بیش از حد با آمار تطبیق کند، بیشتر مصرف رسانه ای یا آکادمیک تا توضیح واقعیت. وقتی خطاهای محاسباتی و نویز هم وارد محاسبه شوند و عدم قطعیتها بیرون بمانند، نتیجه میشود دادن جوابی خاص به مساله ای کلی که بین مردم هم پخش میشود. بعلاوه معادلات و فرضیه هایی که همه اتفاقات را توضیح دهند، پیچیده ترند و توجه بیشتری جلب میکنند؛ حتی اگر غلط باشندچرا حوزه اقتصاد بدترین آمار پیش بینی را دارد؟ شاید از یک طرف دادن پاسخ مشخص به سوالات مبهم در آن داغ است، مانند آمار برای آینده. از طرفی وقتی سیاستمدار دست روی متغیری میگذارد، اعتبار آن متغیر برای تحلیل اوضاع پایین میرود و انگیزه هم فراوان است که پیش بینی جهتدار تولید شود. با حجم بی نهایت داده، میتوان بین هر دو متغیری ارتباط یافت. بنابرابن نیاز به نظریه های خوب و ایجاد انگیزه برای پیش بینیهای واقعی بیش از همیشه استمسائلی که می توانند مخاطرات جهانی ایجاد کنند، مثل بیماریهای مرگبار، ملتها را خواه ناخواه به هم نزدیک میکنند؛ برای فهم راه حل. اما پای انسان که به پیش بینی ها باز شود، پیچیدگی تحلیلها آن قدر بالا میرود که درک بشر کفایت نمیکند و انسان باید بجای تحلیل و محاسبه، راه حلی خلق کند که پای خودش را از محاسبات بیرون بکشد. بنابراین، مدلها بیش از ظرفیت محاسبه، به هوشمندی انسان احتیاج دارند تا مسائل پیچیده راحت تر فهمیده شوندبیشترین اطمینان به اعتقادات را کسانی دارند که بر روی آنها قمار میکنند. تجربه هم ثابت کرده که موفق ترین آنها کسانی هستند که بجز دانش فنی لازم، فهم خوبی از کانتکست پدیده ها دارند. این گونه است که تفکر بر پایه احتمالات لاپلاس، راه را برای نظریه بیز در مقابل رویکرد فیشری در آمار باز میکند. بر اساس رویکرد بیزی، از هر اعتقادی شروع کنیم و بر اساس داده های جدید آن را اندکی تغییر دهیم، نظراتمان به حقیقت همگرا خواهد شدیکی از جالب ترین داستانهای پیش بینی، تقابل انسان و کامپیوتر در شطرنج است که در مسابقه معروف کاسپاروف با دیپ بلو زیبایی خودش را آشکار میکند. جدا از اهمیت خلاقیت و تفکر استراتژیک بشر در مقابل سرعت محاسبه و تفکر تاکتیکی ماشین، نکته مهم فهم چگونگی ""فکر کردن"" یک ماشین است که محدودیتها و نیت سازندگان آن را آشکار میکند و برای زندگی واقعی هم پیامد دارد.واقعیت نه چندان خوشایند این است که در حوزه های مختلفی، ضعیف ترین افراد منبع درآمد بخش وسیعی از افراد متوسط هستند. یعنی صرفا در بخشهایی که رقابت وجود دارد و بازی برنده و بازنده است، ضررهای بزرگ افراد با مهارتهای پایین جا را برای بهره مندی عده زیادی همراه میکند، بدون اینکه چندان خوب و ماهر باشند. بنابراین در چنین دنیایی آشنایی با حداقلهای یک زندگی فکری ماهرانه، دیگر نه یک امتیاز، که یک ضرورت استفرضیه اولیه در تجارت این بوده که معامله صورت میگیره تا هر دو طرف نفعی ببرند. اما واقعیت بازارهای مالی حکایت از تفاوت در دیدگاهها و پیش بینی‌های گاه متضاد دارد و هر روز آدمهای بیشتری پیدا میشوند که فکر میکنند می‌توانند خرد جمعی را شکست دهنداصطلاح ""نفرین برنده"" (کسی که بالاترین ارزش گذاری را برای یک کالا میکند، بالاترین قیمت را برای آن میپردازد؛ عموما بالاتر از ارزش واقعی) نقطه شروع خوبی است برای تامل در رفتار قبیله ای در بازار، یا سرخوردگی بعد از خرید (هیرشمن)، حبابهای بازار، صحیح بودن قیمت ها یا فرضیه بازار کارا. به قول معروف بازار مالی یک جریان اصلی دارد که اقتصاد را به پپیش میبرد و یک جریان هیجانی و سریع دارد که محل بازیهای مالی استدر مورد مساله مهمی مثل گرمایش زمین، تنها بر روی جنبه های خاصی از واقعیت - مثل اثر فعالیت های بشر - اجماع وجود دارد و ترجیحات سیاسی و ارزش خبری منجر به فراگیر شدن دیدگاهها میشود. اما از طرف دیگر جنبه های مهم و عموما فنی هستند که برای عموم جاذبه کمتری دارند، مثل اینکه بر مدلهای کامپیوتری محاسبه گرمایش زمین اجماعی وجود ندارد، درحالیکه بر روی نتایج این مدلها قبلا اجماع داشته ایماگر ذهنمان را به اشتباه عادت بدهیم تا دنیا را دوقطبی بفهمد، قطعی و غیرممکن، ضروری و بیفایده، دوست و دشمن، آن وقت در مواقع ارزیابی و پیش بینی آینده احتمالا یا دچار اطمینان کاذب به یافته ها هستیم یا ندانستن نادانسته ها در جهل مرکب سرگردانمان میکند. پیچیدگی دنیا و واقعیت هایش از حدود وسط میگذرد، جایی که تحلیل و نگرش و بینش انسان بیش از هر جای دیگر به کار می آیند (hide spoiler)]"
12,159420411X,http://goodreads.com/user/show/9032502-mike-mueller,4,"I followed Nate Silver's blog (FiveThirtyEight) closely during the run-up to election day 2012. His premise was simple: grab every public poll possible, attempt to correct for pollsters' known biases, and produce a forecast based on the result. Somehow no one had thought to do this before. Silver simply crunched the numbers and nailed the outcomes in every state. Meanwhile, pundits, bloggers, and assorted blowhards made predictions based on nothing but gut feeling and partisan hackery, and they mostly missed the mark (often by a wide margin).I was looking forward to reading more about his methodology in this book, as well as his take on the principles involved in making predictions from noisy data. In this regard, I wasn't disappointed. Silver does a good job of laying out the rules of the road:* It's easy to mistake essentially random fluctuations for a meaningful pattern, and in some contexts (say, earthquake predictions), this can have devastating results.* Having a well-formed, testable theory is better than just looking for any correlations you can find in your data set.* Always make predictions and update your probability estimates like a good Bayesian. Your predictions should approach reality as you continually refine them.* Watch out for biases in yourself and in your data set.* Often overlooked: make sure incentives are aligned with the results you would like to achieve.Also, some specific interesting facts:* Making a living at poker is really hard. Without any really bad players at the table, it's nearly impossible for anyone but the top players to turn a profit.* The efficient market hypothesis doesn't hold up to scrutiny; however, even though the stock market has discernible patterns, it may not be possible to exploit the patterns and consistently beat the market.* Weather prediction has gotten a lot better in the last couple decades, even though most people think it hasn't.* Both earthquakes and terrorist attacks follow a power law distribution.If you're a stock trader, scientist, gambler, or simply someone who wants to form an accurate picture in a noisy environment, there's something in this book for you. It's nice to see this kind of clear-headed, rational thinking becoming sexier recently. The book is also well cited, which helps give weight to some of the more counterintuitive claims.There was a missed opportunity to spend some time on results from the medical research industry. It's well known that publication bias and other factors result in misleadingly positive results for new treatments, which ultimately go away after independent researchers attempt (unsuccessfully) to reproduce the results. It seems like a pertinent, prototypical case of finding patterns in noise, one which could have been instructive.A final note: Silver is not the best writer; his prose is uneven and occasionally downright awkward. His casual style works fine for a blog, but here it diminishes the impact the book could otherwise have had. This is his first published book, and it shows. There are also a couple glaring mistakes that make me think he needed a better editor."
13,159420411X,http://goodreads.com/user/show/1045774-mehrsa,3,"Some interesting parts, but it's really hard to take this superforecaster seriously on political forecasting--you know what I mean? And I am sort of over the moneyball theory too. I mean, it was useful a few years ago to break free from ""gut feelings"", but I think the pendulum swung too far into just cold data and needs to swing back into the world of humans and fat tails and Trump getting elected. "
14,159420411X,http://goodreads.com/user/show/5926288-cameron,5,"This is a really amazing book - a must read for anyone who makes decisions or judgement calls. Even before I had finished the book it caused me to look at some of the assumptions and bad forecasts I was making as well as recognising ""patterns"" as noise.There is nothing ""new"" in this book, just well established and solid methods applied well and explained very coherently. The writing is excellent, the graphics helpful and the type not too small. There are plenty of footnotes (relevant to the page), but I didn't bother with the references at the back. All up it was not at all the onerous read I was expecting from the size and nature of the book.What I particularly liked was that it agrees with many of my ""hunches"" and ""gut feels"" (that seem to work out mostly) but more importantly puts theory that I can put to the tests and use more widely. A few points raised really made me feel chuffed and not alone (a little cleverer than most): The misuse and misapplication of Occam's razor; Overfit of models onto data; Fisherian statistical significance (particularly in medical science). There was only one ""low"" point; chapter 11 on free markets, ""If you can't beat'em..."", kind of got off course. It started out as a slightly irked, though legitimate, response to a smart ass comment about a free market betting pool being a better predictor than his 538 website. It then went into stock market trading and but didn't go far enough into the information inequalities with market making for my liking. The end conclusion (two streams - indexed investment on signal trading and short trading on the noise), I agree with.A final point on my bad predictions: of the last 4 books I have read I have judged reading time and effort on size and been wrong 3 times - twice with small novels that were philosophically challenging and unpleasant to read and once with this behemoth of a book that was breeze to read! "
15,159420411X,http://goodreads.com/user/show/47417894-laura-noggle,3,"Meh, I was hoping for more.Interesting at points, but the main message gets swallowed by the noise—almost too much random content. Basically, it's hard to predict stuff. Be careful what predictions you trust, most of them will be wrong a good portion of the time.The end. "
16,159420411X,http://goodreads.com/user/show/4739424-mal-warwick,5,"An eminently readable book about how experts make sense of the world (or, more often, don’t)Statisticians rarely become superstars, but Nate Silver is getting close. This is the guy who writes the FiveThirtyEight.com blog for the New York Times and has correctly predicted the outcome of the last two presidential elections in virtually every one of the 50 states. But Silver is no political maven weaned on election trivia at his parents’ dinner table: he earned his stripes as a prognosticator supporting himself on Internet poker and going Billy Beane of the Oakland A’s (Moneyball) one better by developing an even more sophisticated statistical analysis of what it takes to win major league baseball games. And, by the way: Silver is just 34 years old as I write this post.The Signal and the Noise is Silver’s first book, and what a book it is! As you might expect from this gifted enfant terrible, the book is as ambitious as it is digestible. Written in an easy, conversational style, The Signal and the Noise explores the ins and outs of predicting outcomes not just in politics, poker, and sports (baseball and basketball) as well as the stock market, the economy, and the 2008 financial meltdown, weather forecasting, earthquakes, epidemic disease, chess, climate change, and terrorism.Fundamentally, The Signal and the Noise is about the information glut we’re all drowning in now and how an educated person can make a little more sense out of it. As Silver notes, “The instinctual shortcut we take when we have ‘too much information’ is to engage with it selectively, picking out the parts we like and ignoring the remainder, making allies with those who have made the same choices and enemies of the rest.” What else could explain why Mitt Romney was “shell-shocked” and Karl Rove was astonished by Romney’s loss in a presidential election that every dispassionate observer knew was going Obama’s way?Silver asserts that “our predictions may be more prone to failure in the era of Big Data. As there is an exponential increase in the amount of available information, there is likewise an exponential increase in the number of hypotheses to investigate . . . But the number of meaningful relationships in the data . . . is orders of magnitude smaller. Nor is it likely to be increasing at nearly so fast a rate as the information itself; there isn’t any more truth in the world than there was before the Internet or the printing press. Most of the data is just noise, as most of the universe is filled with empty space.”Sadly, it’s not just in politics that bias clouds judgment and leads to erroneous conclusions. “In 2005, an Athens-raised medical researcher named John P. Ioannidis published a controversial paper titled ‘Why Most Published Research Findings Are False.’ The paper studied positive findings documented in peer-reviewed journals: descriptions of successful predictions of medical hypotheses carried out in laboratory experiments. It concluded that most of these findings were likely to fail when applied in the real world. Bayer Laboratories recently confirmed Ioannidis’s hypothesis. They could not replicate about two-thirds of the positive findings claimed in medical journals when they attempted the experiments themselves.”In general, Silver’s thesis runs, “We need to stop, and admit it: we have a prediction problem. We love to predict things — and we aren’t very good at it. . . We focus on those signals that tell a story about the world as we would like it to be, not how it really is. We ignore the risks that are hardest to measure, even when they pose the greatest threats to our well-being. We make approximations and assumptions about the world that are much cruder than we realize. We abhor uncertainty, even when it is an irreducible part of the problem we are trying to solve.”There’s more: Silver relates the work of a UC Berkeley psychology and political science professor named Philip Tetlock, who categorizes experts as either foxes or hedgehogs (in deference to an ancient Greek poet who wrote, “The fox knows many little things, but the hedgehog knows one big thing.”). Hedgehogs traffic in Big Ideas and often hew to ideologies; these are the people who talk to the press and are frequently found on TV talk shows. Foxes are cautious types who carefully examine and weigh details before reaching conclusions. Not surprisingly, Tetlock found that “The more interviews that an expert had done with the press . . . the worse his predictions tended to be.”In other words, Be afraid. Be very afraid. If the people who supposedly know what they’re talking about often really don’t, how can the rest of us figure out what’s going on?"
17,159420411X,http://goodreads.com/user/show/23417467-rick-presley,3,"Nate Silver does an excellent job demonstrating the different domains where statistics plays a part. More importantly, he describes why methods that proved successful in one domain are inadequate or inappropriate to another domain. The best part about the book is that he doesn't resort to math to explain these differences. The problem with the book is that he fails to take the lessons from previous chapters and apply them to subsequent chapters. I think this may have explained his hubris in mis-forecasting the 2016 election outcome. I did hear an interview with him that said his stats weren't wrong. If 2 out of 3 scenarios had Hillary winning, then 1 out of 3 scenarios had Trump winning. I think this illustrates his discussion on the difference between likelihood and probability. I would recommend this as a primer on stats for the non-mathematician, but I would caution that there are sprawling passages of boring stuff that you'll want to skip over. "
18,159420411X,http://goodreads.com/user/show/6100646-brian-clegg,4,"It was really interesting coming to this book soon after reading The Black Swan, as in some ways they cover similar ground – but take a very different approach. I ought to say straight away that this book is too long at a wrist-busting 534 pages, but on the whole it is much better than its rival. Where Black Swan is written in a highly self-indulgent fashion, telling us far too much about the author and really only containing one significant piece of information, Signal and Noise has much more content. (Strangely, the biggest omission is properly covering Taleb’s black swan concept.)What we’re dealing with is a book about forecasting, randomness, probability and chance. You will find plenty about all the interesting stuff – weather forecasting, the stock market, climate change, political forecasts and more, and with the exception of one chapter which I will come back to in a moment it is very readable and well-written (though inevitably takes a long time to get through). It has one of the best explanations of Bayes’ theorem I’ve ever seen in a popular science book, and (properly to my mind) makes significant use of Bayesian statistics.What’s not to like? Well, frankly, if you aren’t American, you might find it more than a trifle parochial. There is a huge section on baseball and predicting baseball results that is unlikely to mean anything to the vast majority of the world’s readers. I’m afraid I had to skip chunks of that. And there’s a bizarre chapter about terrorism. I have two problems with this. One is the fawning approach to Donald Rumsfeld. Nate Silver seems so thrilled Rumsfeld gives him an interview that he treats his every word as sheer gold. Unfortunately, he seems to miss that for much of the world, Rumsfeld is hardly highly regarded (that parochialism again).There is also a moment where Silver falls for one of the traps he points out that it’s easy to succumb to in analyzing data. On one subject he cherry picks information to present the picture he wants. He contrasts the distribution of deaths in terrorist attacks in the US and Israel, pointing out that where the US numbers follow a rough power law, deaths in Israel tail off before 100 people killed in an incident, which he puts down to their approach to security. What he fails to point out is that this is also true of pretty well every European country, none of which have Israeli-style security.I also couldn’t help point out one of the funniest typos I have ever seen. He quotes physicist Richard Rood as saying ‘At NASA, I finally realised that the definition of rocket science is using relatively simple psychics to solve complex problems.’ Love it Bring on the simple psychics.Overall, despite a few issues it was a good read with a lot of meat on probability and forecasting and a good introduction to the basics of Bayesian statistics thrown in. Recommended.Review first published on www.popularscience.co.uk and reproduced with permission"
19,159420411X,http://goodreads.com/user/show/1927322-dave,3,"Silver's gone 99 for 100 on predicting the state winners of the last two presidential elections. Here he goes something like 7 for 13, very good in parts, solid in some, and misfires in others. It's well-researched, mostly objective (but by no means totally), but it rarely covers anything I didn't already know. If you've read Michael Lewis's The Big Short and Moneyball you can skip chapters 1 and 3 and if you've ever had a class that proves pundits are not any more accurate forecasters than the population at large you can skip chapter 2. In addition, Silver loses his way with the climate change chapter as subjectivity overcomes math and the piece covering his online poker career in lifeless, as I expect it would be for anyone who's not a fan of the game. Silver's at his best covering the weather (temperature predictions and hurricane landfall site predictions have decreased their margin of error by significant margins in the last few decades; trust the National Weather Service and not your local newscaster for the most accurate forecast), earthquakes (impossible to predict), and the Bayes theorem, which he champions as the best model by which to life your life and conduct your business. As we learn that it's nearly impossible to beat the stock market over the long run without the benefit of inside information, it becomes clear that the best thing a reader with sound statistical analysis ability can take away from this book, other than making the Bayes theorem a default operating method, is to take that skill and apply it where the analysis to this point is weak. The stock market, baseball, poker - they've been covered, but if you can separate the signal from the noise as the availability of big data overwhelms our ability to parse the useful pieces from it then you can gain a competitive edge in your industry. It's good advice and there are some solid parts of the book, but for such a successful guy there was not much groundbreaking material here. If I weren't a completist I would have read only the chapters that started going somewhere in the first few pages, as the correlation between the first five pages was .92. The exception is the chapter on chess, which was fast out the gate, but faded down the stretch, especially as Silver ignored the fact that Kasparov's loss to Deep Blue was in part triggered by the unfairness of the latter's team getting to see the former's recent matches, but not the other way around. So,yes, Silver's political forecasting is exceedingly accurate and his writing is hit or miss."
20,159420411X,http://goodreads.com/user/show/6023167-jonathan-mckay,2," The Prior Before reading this book, I thought there was a 70% chance I would rate this book 3 stars or higher.  The Signal Silver's chapter on Poker was interesting both from the perspective of statistics, but also about poker tactics and the metagame. I wish this were the core of the book. Also, the explanation of Bayes' theorem was solid, as was the chapter on stocks. The Noise  Everything else. Superforecasting is MUCH better when talking about predictions, and much more engaging. Shiller's book Irrational Exuberance is better on stocks, even Rumsfeld's biography Known and Unknown: A Memoir is better when talking about politics. It felt like Silver took a lot of shortcuts and made claims about causality in multiple areas without sufficient evidence.  The Result Read chapters 8, 10, and 11. Skip the rest. Better yet, just skip this book and read Superforecasting. That's 77% of the chapters that are below three stars for me. So let's run some Bayesian inference, with the hypothesis that I would give this book >= 3 stars. P (Hypothesis given evidence) = P (Evidence given Hypothesis) * P (Hypothesis) / P (Evidence).27 = .3 * .7 / (.77)Now there is only a 27% chance of >= 3 stars."
21,159420411X,http://goodreads.com/user/show/175283-patrick-brown,4,"This was a fun read that tickled the nonfiction part of my brain in pleasant ways. It felt a bit repetitive in parts, and I found myself wondering how various chapters (such as the chess chapter) related to the whole. In the end, I'll take from this book the need to think probabilistically in life, and Bayes' theorem, about which I knew little. The chapter on terrorism was an excellent ending to the book, as it not only tied the concepts together, but it also made apparent the stakes in predicting the future. The McLaughlin Group, for instance, gets to keep coming back each week, even though their predictions are laughably bad. When you're trying to guess whether a terrorist might nuke New York...well, you kind of have to be more right about that.Still, I'm not sure this book quite added up to the sum of its parts. For instance, after reading about the super-skilled sports gambler, I didn't have any better idea how he did what he did than I had before reading the chapter. Perhaps he wouldn't tell Silver his secrets, I don't know. I doubt my predictions will get much better from having read this book, either (though I wonder whether that was the goal of the book or now). I'd still recommend it to anyone with a love of charts, a thirst for interesting data-driven nonfiction, or anyone looking for something to shake up their reading list with something a little different."
22,159420411X,http://goodreads.com/user/show/1836077-lightreads,3,"Eh, underwhelmed. A survey of prediction and predictive tools, starting with failures and moving on to successes. Nothing particularly new or interesting here, and I think Silver knew it. It’s not like the premise that the strength of a prediction depends on the accuracy of the data is revelatory or anything. A lot of survey nonfiction like this can be saved with interesting collateral content. This book tours over a dozen topics, but I didn’t find much new or compelling or even particularly complex in the subjects I know something about (the efficient market hypothesis, political polling, the spread of infectious disease), and more damningly I was never engaged by his writing on subjects I don’t know much about (the weather, sports betting, baseball. Oh my God, so much baseball.) I guess what I’m saying here is that the book format reveals all of Silver’s weaknesses as a writer, and there are many. The nicest thing you can say is that when he’s really on a roll, he’s workmanlike. And that’s okay! He doesn’t have to write brilliantly, he can just keep doing statistical modeling. (Better him than me – I disliked stats so much, it doesn't actually qualify as math in my head.) Just, turns out I prefer him doing stats in 1000 word articles and in person, where he comes across much better."
23,159420411X,http://goodreads.com/user/show/5015851-ms-pegasus,4,"Yes, this book is by that guy — Nate Silver who correctly predicted the winner of the 2008 presidential elections in 49 out of 50 states. That might seem off-putting. The credentials portend a heavy tome on statistics. Those fears are quickly allayed. This book is entertaining as well as informative. Silver offers solace to those frustrated by information overload. Over-simplification on the one hand and brute-force data crunching on the other can both lead to serious errors. Of the latter he writes: “The numbers have no way of speaking for themselves. We speak for them. We imbue them with meaning. ...Data-driven predictions can succeed – and they can fail. It is when we deny our role in the process that the odds of failure rise. Before we demand more of our data, we need to demand more of ourselves.” This is a book that provides a context as well as explanation for something called Bayes's Hypothesis. Silver begins by considering the many recent instances of blatantly failed prediction. These include the 2008 housing bubble, the collapse of the Soviet Union, and the Fukushima disaster. In all of these examples he probes the multiple reasons behind human error. Among these is our very human imperative to interpret through patterns. Vision and taste, for example, are perceptions derived from the brain's ability to discern pattern. In a similar way, we try to make sense of events affecting our lives. Unfortunately, all too often, we are unable to separate significant data from insignificant data. In the data-rich field of economic forecasting, it's all too easy to develop models that overfit the data, accounting for insignificant and significant data points indiscriminately. A dense layer of possibly random correlations is captured in a convoluted skein of calculations fed into a computer to generate a “pattern”: “The wide array of statistical methods available to researchers enables them to be no less fanciful – and no more scientific—than a child finding animal patterns in clouds.” A second major source of error is emotion. Experts are frequently wrong because they simply don't want to look bad. He cites the participants of the McLaughlin Group. An outlandish prediction which proves true will be remembered. If it's false, people tend to forget. There is a built-in incentive to grandstand, making outlandish predictions. Scholars may have the opposite incentive: It's safer to stay within the consensus rather than risk looking foolish. Silver also points out another dichotomy. Some experts are so wedded to a pet theory or model that they are incapable of recognizing contradictory data. He characterizes such people as hedgehogs; their opposite are the nimble minded foxes, always seeking out new information and willing to try out new frameworks for fit. Finally, he cites an innate tendency to ignore frightening signals. “Human beings have an extraordinary capacity to ignore risks that threaten their livelihood, as though this will make them go away.”Along the way, he redefines the problem of forecasting in today's world. We live in a world of complex and dynamic systems. A promising forecasting model must allow for adjustment through feedback. Context is always important to separate independent from dependent data points. For example, during the housing bubble, the rating agencies did not recognize that the playing field for issuing mortgages had shifted drastically. The assumption that each mortgage default within a given tranche was independent was the basis for their overly optimistic credit ratings. A corollary of this is that qualitative information must be included in the forecasting process. The problem then becomes how to quantify qualitative data. Finally, we live in a world of uncertainty. Failing to include uncertainty in forecasting calculations is a form of denial. In other words, there is a lot of noise and a sparsity of signal. How can uncertainty be expressed and used in the forecasting process?It cannot fail to astonish most readers that Silver cites weather forecasting as one of the more successful efforts in forecasting. First, meteorologists work with hypotheses that describe how weather systems work. Second, there is an enormous amount of data. Third, the models are constantly being improved as new data either affirms or disproves the latest prediction. Silver also discusses a technique called agent-based modeling, used to predict the spread of epidemics. Incorporated into the model is a sim-city of human behavior parsed by demographic details down to the minutest level.In Chapter 8 Silver finally introduces Baye's Theorem. In addition to his own examples, he uses the classic example of how the rate of false positives in a sample of mammograms affects the actual probability that a positive test accurately predicts the presence of cancer. Rather than repeat the explanation here, I have added some useful websites in the notes section. (The reason I do this is that the more ways a math problem is explained, the likelier it is that understanding will eventually come. I admit it. I didn't understand the formula itself until I had worked through several of these alternative explanations. My favorite is the one that used decision trees). It's amusing that Silver chooses as his first example a scenario in which a woman finds a stranger's underpants in her husband's bed. Using Bayes's Theorem, he gets the probability down from 50% to only 29%! Imagine the beleaguered husband giving this explanation to his wife!Bayes's Theorem is all about conditional probabilities: There is an assumed prior probability, and a resulting posterior probability. The general idea is that even if the prior probability is a wild guess, it will be refined by repeated recalculation of the formula by applying new data successively. The result isn't a prediction – it's only a probability that a proposition is true. It's a technique for modulating new data to align its importance with older data. It's a reminder that uncertainty arises not just from the numbers we collect, but from the innate complexity of the events we are attempting to study. The method is contrasted to the more familiar bell-shaped curve assumptions of frequentism. Silver's varied interests are reflected in this book. He provides examples from Kasparov's chess match with Big Blue, and an interview on poker strategy with Tom Dwan. These examples serve to illustrate the dynamic properties of applying Bayes's Theorem. Anyone interested in either of these areas should definitely take a look at Silver's commentary.Will this book leave you an expert on Bayesian Theory? By no means. The book is designed to whet your appetite. Silver concludes with the final consolation: “Prediction is difficult for us for the same reason that it is so important: it is where objective and subjective reality intersect.” NOTES:Silver's formulation of Bayes's Theorem: (Prior Probability x Probability of specified event) / (Prior Probability x Probability of specified event) + (Probability of specified event being not true) x (1 - Prior Probability). Additional websites that explain Bayes's Theorem:https://www.youtube.com/watch?v=aGnVj... This is a video explanation using a decision treehttps://www.youtube.com/watch?v=E4rlJ... This is a classroom video which includes a decision tree explanationhttp://betterexplained.com/articles/a... This is a really detailed text explanation covering Bayes' Theorem step-by-step with interactive calculation boxes."
24,159420411X,http://goodreads.com/user/show/35482263-gumble-s-yard,4,"Book about prediction by the author of the 538 political blog, which became particularly famous in the 2012 presidential election (after the book was written) due to the author's high confidence in an Obama victory due to polling evidence in marginals. The author was prior to 538 spread over two jobs - online poker (until it was made illegal in US - see below) and baseball stat evaluation (where he developed his own site which he sold to a professional site for which he then worked). The book's central themes are the importance of Bayesian stats (as opposed to Fisher type confidence intervals based only on data) as the optimal blend of expertise and data and the difficulty of distinguishing the true signal from underlying noise which can either obscure the signal or create false ones. He continues various areas in turn - all of which have their own forecasting issues, which are often very different leading to his third point the difficulty of drawing hard and fast rules around prediction. Generally an interesting book – more a compendium of ideas and so lacking the really big idea/takeaway – which seems deliberate due to the last point.In respect of the financial crisis, he identifies various failures of prediction (housing bubble, rating agencies, failure to see how it would cause a global financial crisis, failure to realise how big and deep recession would be) which he largely ascribes to over-confidence and inability to forecast out of sample events. In political forecasting he claims his ability think probabilistically, revisit and alter past forecasts and look for data consensus means he outperforms what is a poor level of competition (biased and unscientific political pundits). For baseball again he initially competed against simple rules of thumb but sees the real skill in continuing to combine the best of stats with properly incorporated qualitative information to continue to look for edges. Weather forecasting he sees as largely a success story especially when you account for bias (for example to over-predict bad weather as that is less catastrophic an error) and allowing for chaos theory which makes precise long range forecasts difficult. Earthquake forecasting by contrast has had almost no success (here he talks about over fitting). For economic forecasting there are lots of challenges (Uncertainty principle type ideas such as Goodhart’s law, self-fulfilling prophecies so that talk of a recession causes one, natural biases of commentators including either not wanting to go away from herd or being deliberately provocative) not least the sheer noisiness of economic data. For infectious diseases he discusses self-cancelling prophecies (epidemic warnings change behaviour in a good way) and although it’s a challenging area he believes practitioners in this field (perhaps due to their Hippocratic oaths) are more thoughtful about their predictions. In chess he discusses in detail the psychology of Kasparov’s defeat by a computer – an error it made in a losing position convinced him it could think more deeply than it could as well as where humans are better or worse than computers and how blended programmes are very strong. For Poker he takes the view that the Poker players are very natural Bayesians, adjusting their knowledge both as cards appear and also assessing chance of different hands by an intuitive posterior analysis based on how they think their opponents would act with different hands. He also takes the view that he standard of opponents is key to if you can make money. For stock picking he discussed the efficient market hypothesis (especially with transaction costs) and the psychology of bubbles. For climate change he discusses healthy scepticism and also his conclusion that scientists are a lot more seekers after the truth than politicians. For terrorist attacks he discussed power laws to extrapolate to major attacks (which actually dominate costs and deaths) and the importance of lateral and imaginative thinking around threats."
25,159420411X,http://goodreads.com/user/show/1185911-gina,4,"Reading Nate Silver is like exhaling after holding your breath for a really long time. I found FiveThirtyEight back in the primary days of 2008, when it was Hillary and Barack fighting it out, and it became apparent that not one of Hillary's advisers to whom she was presumably paying lots and lots of money were as smart or observant as Nate Silver (or Obama's advisers). One of my favorite tweets ever (I don't read many tweets) came from Ken Jennings on election morning of 2012, something along the lines of ""Obama could still lose this thing if too many democrats write in Nate Silver with little hearts drawn around his name."" He had Obama with a 90% chance of winning. And while you could find plenty of other people calling it for Romney or Obama, they are for the most part just talking heads that don't actually care about reality. When Nate Silver gives you a 90% chance of something, it means that nine times out of ten it is going to happen, and one time out of ten it won't, nothing more and nothing less. You don't have to spend energy paying attention to which station it is on and who he is catering to. He caters to reality, which is surprisingly novel. Finding someone who can do this feels like, as I said, exhaling. Of course he has biases, etc, but his job is to be aware of them. This whole book is about why making accurate predictions is extraordinarily difficult. Sometimes apparently impossible, as in the cases of trying to beat the stock market over the long term or predict earthquakes. Sometimes made extremely difficult by humans' strong tendency to not accept the truth of things that don't serve our ends, as in the case of the financial collapse of 2008 (which first chapter in this book is the absolute best summary of that whole fiasco I have ever read). Sometimes the message of people willing and able to make careful, thoughtful predictions with honest margins of error, as is the case with many climate scientists in relation to global warming, is hijacked by politics and agendas. (The chapter on climate change was also exceptionally good, and the people who are criticizing Silver for being a climate change denier or for giving legitimacy to deniers' views have very poor reading comprehension and/or are so blinded by their own religious belief in their version of climate change that they cannot accept the reality of how hard it is to make accurate predictions.) The chapter on his era as a successful online poker player was very entertaining and reinforced why I do not have the stomach to be a gambler. All that being said, be forewarned that most people will find this book extremely boring. It is in the vein of Malcolm Gladwell, but about three times as long and dense (and therefore more substantial). Also, I sadly did not feel like I had gained a very deep understanding of Bayesian thinking by the end, which is unfortunate since that is one of the main points of the book. Surely that is partly my fault, but he could have been more clear about it. At any rate, I think the chapters on the financial collapse and global warming should be required reading for everyone, and the rest of it for those who are interested. "
26,159420411X,http://goodreads.com/user/show/267189-todd-n,5,"I finished this right before the 2012 elections, and I should have written my review before then so that I could convince more people to read this book when Nate Silver got more than Internet famous for a few weeks.At its core, this is a book about how to think -- a very important topic in these times when anything less than complete certainty is viewed as a weakness and the usual response to a disagreement is to double down on the position no matter how ridiculous.In contrast to this, Mr. Silver frames issues in terms of Bayes Theorem, which is a centuries-old mathematical formula for determining how probabilities change as new information becomes available. Of course, for this concept to have any bearing on how one thinks, one would have to (1) think in terms of probabilities instead of certainties and (2) modify one's views based as new information comes to light. So, maybe 5 to 10% of Americans?Mr. Silvers points out that a good way to test the validity of a model (or a person) is to see if it gets more accurate as more information is made available. (I would add this corollary: If more information makes a person a worse predictor of reality, then that person is an asshat.) But he also warns about ""overfitting"" a model by forcing it to fit noise instead of signal.There are so many interesting topics covered in this book such as the prediction of weather (has gotten a lot more accurate despite the jokes), prediction of earthquakes (no progress despite throwing those Italian scientists in jail for failing to predict an earthquake), the economics of poker sites (without large number of dummies to feed in money they are unsustainable), and hurricane landfall predictions (getting much more accurate, as we saw with Sandy). It also has the most intelligent discussion of global warming that I have encountered.I read some of the other reviews that complained that this book is somewhat meandering, which I can see. But I really didn't mind at all. I have enough interest in the general topics of prediction, probability, and Bayes Theorem that I found the occasional digression illuminating rather than distracting.Very highly recommended."
27,159420411X,http://goodreads.com/user/show/1041137-kristen,5,"I picked up The Signal and the Noise from the library because I thought it would be slightly boring. I've been having trouble sleeping lately, and I wanted something that would be distracting without being too stimulating. For a while it was hitting that sweet spot, but then it took a turn toward the unexpectedly awesome. Nate Silver is great a explaining things and illustrating them with compelling stories. That's what I was assuming the book would be. But what I was not expecting was the extent to which The Signal and the Noise embodies a philosophy of living. Silver is a proponent of thinking probabilistically, which means making predictions and decisions based on the most likely outcome, given the data you have. Sometimes probabilities are effectively 100% (will the sun rise again today?), but often they are not (will a hurricane hit? does my opponent have a better poker hand than me?).This book spoke to me, because I'm actually kind of bad at dealing with uncertainty. I like to think that I can control every outcome, even knowing logically that that is not possible. The Signal and the Noise is a good reminder that even when you make the best decision possible based on your data, you might be wrong. The chapter on poker was my favorite in this respect, and provided a nice metaphor for how to take a more philosophical approach to the limited predicability of life:When we play poker, we control our decision-making process but not how the cards come down. If you correctly detect an opponent's bluff, but he gets a lucky card and wins the game anyway, you should be pleased rather than angry, because you played the hand as well as you could. The irony is that by being less focused on your results, you may achieve better ones."
28,159420411X,http://goodreads.com/user/show/3605371-susan-visser,5,"I really enjoyed the book, Nate's talk, and meeting him in person. The book is about predictions and goes through many world events that we can all relate to and discusses the signals and noise that went on around these events. You'll recognize the 2008 US election, the large earthquakes, especially in Japan, swine flu, both the one in the 70s and the more recent epidemic, economic meltdowns, 911, Pearl Harbour, stock market fluctuations, and much more. Throughout these stories, we learn about what the predictions were and why they failed or succeeded. Nate gives advice on how the predictions can be improved in these particular incidents, but gives the reader advice on how to create accurate predictions in similar situations.One of the most amazing things you'll learn in the book is that weather predictions is one of the best success stories. Most of us think that weather forecasters are the worst at their jobs, but we're not thinking about probability as we should.You'll learn about Bayes theorem of probability and how to use it in fun things like winning at poker!I enjoyed the book very much and encourage you to read it!"
29,159420411X,http://goodreads.com/user/show/39797-rose,3,"This book had so many parts that really captured my attention. The chapter on chess was particularly fascinating. Nate Silver did a great job of compiling vignettes about humans and our inability to see the signal through the noise.On the other hand, this book is simply a series of vignettes. And while I love that they are told in a way that conveys the point, I didn't feel like each chapter I was continuing on a journey or growing from point to point. It was just a series of points, tacked on. I like Steven Jay Gould's books of scientific essays, but I know going in that that is what I'm getting into -- a set of essays.And then there's his problem with the word ""literally."" I realize that there are many who feel it is grammatically correct to use ""literally"" to mean the exact opposite. I do not agree, but despite where you fall on that debate, you have to admit that he overuses it to the point of literally driving me out of my mind. I'm honestly shocked that this verbal tic got through an editor. I would have probably forgotten about it if it had been every once in a while, but geez! For example, on page 276-277, he says, ""literally"" three times in the span of seven sentences. Literally.""[A chess opponent must] execute literally 262 consecutive moves correctly... unless a computer can literally solve the position to the bitter end, it may lose the forest for the trees... Literally all positions in which there are six or fewer pieces on the board have been solved to completion.""No matter where you stand on the grammatical rules around ""literally,"" you have to admit that this tic literally adds nothing to the text and should have been caught in editing."
30,159420411X,http://goodreads.com/user/show/13158409-el-kiablo,3,"Nate Silver is clearly trying to do the ""unusual analysis of normal occurances"" thing that Freakonomics did, although his topics are a bit bigger and his discussion is a little more numbers oriented. Unfortunately, it's also less engaging. He's not a terrible writer, per se, especially given how data driven he is, but he's also not particularly compelling. A lot of the subjects he covers are potentially interesting. The problem is that the academic tone combined with his often abstract subject matter left a lot of the subject matter blending together in my mind. Yes, weather patterns are different from sports betting, but the underlying problems are the same (or at least he presents it that way) and he examines them using the same tools, and in the end, it all felt a bit same-y. It's great that he found so many examples that fit his thesis, but at a certain point they seem redundant.I don't imagine that a lot of this material is going to stick with me, in no small part because unlike say Malcolm Gladwell who will argue against conventional wisdom, Silver is often arguing for common sense things such as ""it's hard to predict terrorist attacks"". As such, if I ever do think about the topics Silver talks about there won't be much reason to bring his name into the debate.Still, there were some bright spots in the book - particularly the climate change chapter, which discussed the issue with an apolitical clarity you rarely see in the media. Also, there are people who I would recommend this book to, but they would have to be statistic minded people. For people who just want a semi-smart airplane read I'd recommend something else."
31,159420411X,http://goodreads.com/user/show/2199404-andy,1,"UPDATE December 2016: Nate Silver's predictions for the 2016 U.S. election were horribly inaccurate (http://www.politico.com/story/2016/11...), confirming my contention below that his blah blah about predicting--even in his narrow field of expertise--is hot air. Downgrading from 2* to 1*. Original review from 2012 below.The thing that bothered me was the lack of depth about how to make real world accurate predictions about substantive matters. On the question of flu pandemics, for example, there are multiple books that have been written just about the 1976 Swine Flu Fiasco. Silver summarizes all this in a few pages and then launches into some mathematical abstractions. And he misses the key point--which is what caused the Great Pandemic of 1918? If you don't know that, you should not go about trying to predict future deadly pandemics.It doesn't matter what equations you have, you have to actually know stuff. Once you know stuff, entire categories of hypothetical futures go to zero probability right away without even doing any math.There are numerous excellent reviews here on Goodreads going over various positives and negatives of Nate Silver's book. Please consult those for more details."
32,159420411X,http://goodreads.com/user/show/798356-hien-le,1,"Probabilistic predictions. Bayesian Reasoning. Over-fitting. Wisdom of the crowd.There, I just saved you 560 pages of boring reading. The book is all fluff and very little meat:1) historical narrative (at one point gives the most boring recollection of a Lakers Playoff run)2) boring encyclopedia on who was wrong when predicting some event at some time or another3) hindsight analysis on why certain models and predictions were wrongI found myself seething with rage while reading this because I would look at entire paragraphs and only see one sentence of actual content. I learned all too quickly to expect that Silver would go meandering around like a senile storyteller before just re-iterating a point made in a previous chapter or repeating the same thought with increasingly less coherent or relevant examples.There's one diagram that shows Bayes' formula and explains a practical application to understanding false positives and false negatives in cancer screenings but that's about the only time you'll feel like you've actually learned something. It's about as shallow as your average TED talk but drags on for ~3-400 pages then the rest of the book is a vomit of footnotes and references."
33,159420411X,http://goodreads.com/user/show/5917740-nilesh,2,"The Signal and The Noise has a charming way of introducing a handful of elementary statistical tools while exploring their strengths and weaknesses. However, there is not much new for anyone who has handled data or come across the basic statistical theories.The most obvious shortfall is that the examples used are often long and distracting. More importantly to me, the conclusions drawn are simplistic (end result driven) and at times even wrong. Take this example: the book spends enormous time indirectly showing how a prediction of 90% success means 10% failure too. Or even with repeated attempts, the success rate should not be assumed 100% and this is how ""bets"" must be placed. Yet, it glorifies someone who stakes life savings (successfully) on extremely long odds (6:1) which at best was well-priced (only with hindsight or in the eyes of the one betting).The book panders to the usual biases. It tries to show - rightly - that the predictors in earthquake sciences know the probabilistic nature of what they are doing and hence are right, humble and glorious but not those in financial markets or policymaking. It often assumes that those who got their bets wrong - like in financial markets - are simply ignorant, vain and foolish rather than simply unlucky. Effectively, the book is too quick to arrive at conclusions on successes and failures based on basic facts as they became available post the final outcomes (there are some exceptions but only for the self-achieving purposes - for example the person who got Italian earthquake forecast right based on some superstitions).The convenient examples used help make the standard conclusions rather than properly analyzing ex-ante versus ex-post. A book about the differentiation between the signal and the noise should have spent less time on the eventual outcomes and more on the good and bad practises in the process of prediction.All that said, the sections on the Bayesian probability discussions are excellent, but there is too much relatively meaningless or simplistic surrounding this. The author also misses the trick in mostly ignoring the changed role of statistics in the age of massive data and quantification. The book should also have at least addressed what should/could and could not be quantified (say the quantification of relative preferences and pitfalls for instance) even if it decided to stay away from the time-varying or non-polynomial correlations. The fact that a book containing the word ""noise"" in the title had almost nothing on various levels/types of noise is another way of showing its populist nature.All in all, the book does not live up to the claims on the title."
34,159420411X,http://goodreads.com/user/show/3948872-laura,3,"A love song to Bayesian reasoning on par with http://xkcd.com/1132/. Proposes an answer to a question that baffled me during the most recent economic collapse; why so many people driving the crisis maintained, with all apparent earnestness, that no one saw it coming while the record reveals that LOTS of people did. Lots of people spoke movingly about housing bubbles. Lots of people suspected that collateralized debt obligations were far riskier than rated and some were vastly overvalued. But the rating organizations Had Done The Math. They Knew The Risk. They Advised Their Clients. And They Caused A Whole Lot of Human Misery. Because, Silver said, they didn’t get that risks could be cumulative; that it was quite possible a hiccup in the market could make people all around the country miss mortgage payments. And if the risks were cumulative, then the math is wrong. Goes through a variety of forecasting and prediction fields and assays an analysis of what fields do it well, what do not. My attention wandered during the sports and gambling sections, but was riveted during the discussion of weather and earthquakes. He suggests that a little sharper analysis of the data would have shown that a 9.1 earthquake was quite foreseeable in Fukushima, and why the authorities thought it wasn’t. The story of chess playing computers was great fun. I think he gave a wee bit too much attention to climate skeptics. But I truly enjoyed his observation that “A conspiracy theory might be thought of as the laziest form of signal analysis. As the Harvard professor H. L. “Skip” Gates says, ‘Conspiracy theories are an irresistible labor-saving device in the face of complexity.’” 417. Great last line. “May we rise from the ashes of these beaten but not bowed, a little more modest about our forecasting abilities, and a little less likely to repeat our mistakes.” (454). Good bus book. "
35,159420411X,http://goodreads.com/user/show/1308238-gerald,3,"I really enjoyed this - it was actually what I expected, but I'm not sure if its what everyone expects.Firstly, don't bother unless you're a complete statistics geek (I am). But of statistics, there's loads and loads, from baseball to poker to chess to 9/11 (and terrorism) to weather to earthquakes. So many great graphs and use of statistics tests to prove or disprove something.Mostly its about Bayesian theory - how and when its used, along with what it could be used for. What it's not is a Nate Silver memoir - though its at its most engaging when he's telling personal stories.. his starting baseball prediction spreadsheets on Excel when he worked as a graduate at a big accountancy firm.. or how he fell into Poker (which caused him to leave the aforementioned job). And a lot of the rest of the time, it is a little bit too unfocussed (and could have been significantly shorter).The other thing it's not, is Political. Despite his importance to modern American politics, Nate Silver takes a very detached, statisticians view to everything, talking gleefully of Obama's win (but only because he called it so closely) but interviewing Donald Rumsfeld about terrorism. However, the great surprise for me was the chapter on climate change, over which he states openly a 'healthy scepticism'. I would have labelled myself as a climate change agnostic rather than skeptic, but the key fact here is one I share - that those who have very strong views either way probably don't know enough science to prove anything themselves (it seems possible that no-one does) and probably have a political, emotional or business reason which is a greater influence on them. The propaganda that comes out about climate change is far weightier than the facts.Right - off to test my newfound chess and poker skills."
36,159420411X,http://goodreads.com/user/show/4809742-benjamin,5,"I approached this book a bit fearfully. One television personality, while plugging Silver, recently described his work as a triumph of arithmetic, and any description of statistics as a magic bullet or of one's choice of statistical methods as being self-evident should viewed with caution. Thankfully, Silver is a careful and thoughtful writer, and one of his central theses is that prediction and statistical inference are difficult. He covers varying successes and failures in fields such as baseball, meteorology, seismology, politics, economics, finance, climatology, and national security. He frames discussions of future improvements and limitations in terms of Bayes Theorem, poker, chess, and the Pareto principle.Perhaps Silver's starkest examples of the importance of grounding one's predictions in reality are when he draws an explicit distinction between statistical forecasting (of the type done by people who do not need to understand the underlying dynamics) and prediction based on models (for example, in the case of meteorology).This was a great read.my favorite quote: ""New ideas are sometimes found in the most granular details of a problem where few others bother to look. And they are sometimes found when you are doing your most abstract and philosophical thinking, considering why the world is the way that it is and whether there might be an alternative to the dominant paradigm. Rarely can they be found in the temperate latitudes between these two spaces, where we spend 99 percent of our lives."""
37,159420411X,http://goodreads.com/user/show/1072582-kaethe-douglas,5,"First things first: skip the introduction. It's more boring than any other section, and all it tells is what the general outline of the book is. You can get that from the contents.This is a book which is very well-researched, and well-reasoned, with apt examples. The net result is that what Silver is saying seems self-evident. Forecasting is hard, forecasting accurately is harder. The National Weather Service gets it right, the McLoughlin Group gets it horrifically wrong, but earns ratings. By the time you get to the end you'll be saying ""of course"" to everything Silver writes, because he's built just that substantial a case.If someone had told me, ahead of time, that this was a book explaining how Bayesian reasoning can make forecasting better, that it talked a lot about baseball and poker, I wouldn't have read it, probably. Don't let that dissuade you. In real life we are constantly making our own or interpreting the forecasts of others, and Silver explains how we can do that better. It's not flashy or gimmicky, but it's truly useful advice we can all apply to a greater or lesser extent.Good stuff, both as to content and style.Library copy."
38,159420411X,http://goodreads.com/user/show/2542668-rachel-bayles,3,"While not terrible, I simply didn't find this book all that interesting. There isn't much here that you don't already know. It's not that it's badly written, and he clearly did his research. But it drones on without every jumping out and grabbing the reader. In general, I'm a fan of the NY Times writers, and have read several books by them which read more as a series of columns, rather than a cohesive whole. So I'm not against the concept per se. But Mr. Silver strikes me as someone who has yet to make the transition from blogger to writer.He also has an awkward habit of inserting random quotes from people who he's interviewed, which don't add much to whatever point he's trying to make. It's off-putting to have someone who has access to feel like they need to market themselves as someone who has access. It smacks of someone who is too in awe. All that being said, his faults are probably those of any younger thinker, and he is clearly trying to use his brain to come up with new ideas. In ten years, when his thinking matures, he might be brilliant. But buy this in paperback, or skip it entirely, and wait for his next one."
39,159420411X,http://goodreads.com/user/show/11004626-gwern,4,"(excerpts) An excellent popular (easy to read) overview of a variety of statistical topics, with a good focus on not fooling yourself with overfitting. Silver, somewhat like Meehl, is a subjective Bayesian decision-theorist in fundamental outlook and approach to analysis, but a methodological pluralist, which makes some of his work a little confusing: he is judging things by how they approximate a proper fully Bayesian decision analysis (as is necessary for betting and other applications of forecasting), but this is not always explicit and he can't compare the implemented methods with the gold standard (because they're too difficult to implement, which is why he falls back to more conventional methods). And some of the technical aspects are a little weak (the Hume discussion comes to mind), but what do you expect, Silver's a busy guy."
40,159420411X,http://goodreads.com/user/show/19521288-vadim-smakhtin,2,Boring
41,159420411X,http://goodreads.com/user/show/63614091-a-mig,4,"The book meanders through the world of Prediction, between forecasting of natural hazards to stock market prediction via games like baseball, poker and chess. Overall an entertaining read.On a personal level, since two of my articles are cited in there but only my PhD co-advisor was interviewed on those (part 5, refs. 55. Mignan et al. 2007 and 56. Mignan et al. 2006), I could clarify a few things:ref55. 'Bowman's technique, like Keilis-Bork's, was highly mathematically driven...' - It was my third paper at the end of my PhD and I had departed from the complexity theory of Dave Bowman by then, so it does not relate to the complexity view discussed in the present book nor to 'Bowman's technique'.ref.56 'a paper that [Bowman] published in 2006 also suggested that there was a particularly low risk of an earthquake... Just a year later..., a series of earthquakes hit exactly that area... It was devastating to Bowman's theory.' A good start for my academic career : ) I didn't feel the same as my co-author did. Having done the bulk of the analysis, I could see that jumps in the data had the potential to create signal where there was none (false positives) and hide potentially real signals (false negatives) - although it could well be that there was no signal at all. In complexity theory you were supposed to keep all events in the 'signal' (including the noise) which led me to start considering non-complexity approaches! As a compromise, I added all times series in an appendix for the reader to judge by himself. I continue working on seismicity physics and statistics to this day and I still believe that the process is more 'complicated' than 'complex'.So when Silver writes 'If success in earthquake prediction has been almost nonexistent for millennia, the same was true for weather forecasting until about 40 years ago. Or it may be that as we develop our understanding of complexity theory-itself a very new branch of science-we may come to a more emphatic conclusion that earthquakes are not really predictable at all.', I answer: complexity is one theory, others might work better, future will tell! Obviously, as of now, no theory has demonstrated that earthquake prediction is possible or that it is impossible."
42,159420411X,http://goodreads.com/user/show/74750514-boykie,3,"I came to this book via multiple recommendations/citations.It got to the point whereby I felt I must read the book.Also, it didn't help that the idea of signal and noise (in terms of what to focus on and what to ignore) kept popping up in discussions.Sadly, I found this book to be rather impenetrable. The heavy emphasis on baseball was also rather off putting as I have no knowledge whatsoever on baseball and baseball statistics.I also found the book to drone on and on to the extent I kept realising I had lost the thread of the conversation.I have no doubt Nate knows his stuff and is writing is in simple clear English - no unnecessary jargon, however, I simply failed to follow along.About halfway in, I had to consider my opportunity costs and made the executive decision to drop the book and move onto something else.It's probably one of those that I'll be better off returning to at some point in the future."
43,159420411X,http://goodreads.com/user/show/350233-casey,4,"3.5 stars, but I'm rounding up because I like stats. The first half of this book is fantastic: it outlines the issues that cause people to make terrible predictions. Across many fields, people are not so good at prediction, for a number of reasons. Silver fights the idea that having enough data means that predictions will be great. Data is noisy, and just adding more noisy data isn't going to allow computers to magically find signal. There's a reason why statisticians say ""Garbage in, garbage out.""The second half of the book, which focuses on the so-called solution, is where everything falls apart. According to Silver, the answer is Bayes theorem. I don't take much issue with this: Bayes theorem is elegant and useful, particularly when it comes to making inferences about the world. There's a reason why you can't go to a conference on cognition without hearing a talk on Bayesian learning: it's a good idea, and it seems to work.The problem is that Silver seems to confuse Bayesian statistics with ""thinking probabilistically"" when the two don't really mean the same thing. I think probabilistically when I get on a flight (or walk past the lottery ticket counter without buying one). The probability of a safe flight is close to 100%, whereas the probability of winning the lottery is close to 0%. This is not Bayesian, it's just a simple computation of some event of interest divided by total events (e.g., safe flight / all flights (safe and..., well, not so good)).Bayes theorem is different. Most simply, it's stated as follows: P(A|B) = [P(B|A) * P(A)] / P(B). As you may or may not be able to discern from the equation, it allows you to compute the probability of some event A occurring given that some event B has occurred, using the probability that B occurs given A, the probability of A on its own, and the probability of B. In simple terms, imagine that I'm waiting on the elevated platform for my train. When I get to the platform, I don't have much reason to believe that my train will go express and skip my stop. However, I wait for awhile, longer than I should have to, and the train doesn't come. I can compute the probability that the long wait signals that I'll get skipped using Bayes theorem, and it's possibile that my brain has been doing something Bayesian throughout my many morning commutes. Give it a little input, and Bayes will let me know if I'm better off taking a cab.Throughout The Signal and the Noise, Silver will profile someone who's making a lot of money betting on sports, or making a lot of money playing poker. Then, he'll say ""And this is Bayesian! This proves that we should all be using Bayesian stats all the time!"" He doesn't really explain how any of these people are applying Bayesian stats, or profile anyone making good predictions using a non-Bayesian approach. I like Bayes, but Silver is going to have to do more than say ""Look! Bayes!"" to convince me that it's the panacea for prediction.Silver also points out that conventional methods for hypothesis testing produces far more false-positives (usually called Type I errors) than they should. This is true, and most scientists are aware that they need to make some changes in the way they analyze data. Silver, of course, thinks that we should all be using Bayes, although he doesn't speak about any of the other methods that can reduce Type I errors (including simple methods, like reporting the effect size). At one point, Silver equates science with Bayesian thinking, while completely missing the utility of a well-designed experiment.I do recommend Silver's book, particularly for non-scientists who are interested in statistics. However, it's best taken a series of interesting stories about prediction, and not as a fully developed theory for how most predictions should be developed."
44,159420411X,http://goodreads.com/user/show/7743315-jeanne,5,"Nate Silver's The Signal and the Noise: Why So Many Predictions Fail – But Some Don'tis not a quick or easy read. It is a fascinating one, though, and with some patience, not difficult either. Who knew that a book on Bayesian statistical reasoning and prediction could be this much fun?There are several reasons to read this book. Silver is a geek and proud of it. He made me proud of my geeky side (and wish it were further developed). Silver sheds light on a wide variety of fields – the stock market, baseball, chess, poker, elections, and climate change, to name a few – helping us uncover what we know and, more importantly, what we don't know. In the process, he uncovers larger, more important patterns about knowing and predicting. You will know both more and less than you do currently after reading this book.What will you know? You'll know more about how people tend to think and understand the world. One example:[One] instinctual shortcut that we take when we have “too much information” is to engage with it selectively, picking out the parts we like and ignoring the remainder, making allies with those who have made the same choices and enemies of the rest. (pp. 3-4)(He wasn't talking about politics at that point, although could have been.) Silver described foxes and hedgehogs, an idea first attributed to the poet Archilochus, then to Leo Tolstoy. Hedgehogs believe in big ideas governing the world, that things are easily predicted and understood. I'm a fox, though: I believe that things can be described and understood, but I'm distrustful of simple, un-nuanced answers. While many people celebrate hedgehogs, Silver is kinder to us foxes, who are better at forecasting. (Foxes are more likely to like The Signal and the Noise.)We live in a world that likes easy answers; we like people who make quick and bold predictions (hedgehogs). Accurate predictions require both scientific knowledge and an awareness of our typical biases and mistakes. As he concludes, we need ""the serenity to accept the things we cannot predict, the courage to predict the things we can, and the wisdom to know the difference"" (p. 453). Easier said than done.Silver concludes that one reason we have difficulty making accurate predictions is that we have difficulty distinguishing the signal (the true pattern) from random noise. We are pattern-seekers and look for and discover patterns in short-term variations. Predicting well (or better) requires that we recognize the limitations to our abilities to predict and learn from our mistakes."
45,159420411X,http://goodreads.com/user/show/67951314-phrodrick,5,"Possible audiences for The Signal in the Noise:The Technical Reader, Trained Statistician - this is not a sophisticated analysis and you will be disappointed in its relative lack of advanced discussion.The General Reader with a passing interest in larger issues - there is much happening in the mathematical world around you that impacts you in ways you may not appreciate. This book is very much for you and will benefit you in ways detailed below.Data Crunchers - people employed to use large amounts of data but not fully trained in the ways of statistical analysis ,needing a better appreciation of how to make your data sing. This book is custom-made for you.Nate Silver has written a very readable and well documented introduction to the subject of Bayesian statistics. This method was originally developed in the 1700s. It has been lost, found, refuted, suppressed, and re-derived several times since then. The Signal and the Noise champions Bayesian analysis in the context of a larger isue. The larger discussion is the near impossibility of making predictions about the complexities of our world armed only with the ever-growing databases that populate the 21st century. While heavily favoring baseball and gambling as examples in probabilistic thinking Mr. Silver presents the case that we can improve the value of our forecasts when we employ Bayesian mathematics.Of the several books I have recently read describing the history of Bayesian math and it's ability to forecast some issues better than classical frequentism, The S and N contains the best and simplest description of the formula driving Bayesian analysis. It is virtually the only formula in the book.Nate Silver has been politicized. In recent years he has refused to call elections based on political preference and in this book he does not reflexively deny the theory of climate change. Therefore he is anathema to the politically driven. Deep in the footnotes of the last chapter there is a dig against Al Gore but evidently this is not enough to establish him as a neutral voice. Nate Silver's is a neutral voice. To the degree that he has been successful in calling a number of recent elections it is because he is better at numbers than those who put the demands of political left/ political right in front of all else."
46,159420411X,http://goodreads.com/user/show/5148911-devika,5,"As the rate of information growth exponentially exceeded our understanding of how to process it, we entered a dangerous territory. This is a fantastic book about building the right prediction habits in order to filter through the signal (the relevant data) and the noise (or the irrelevant). Nate Silver has done an excellent job outlining our inherent biases that explain why some predictions fail while others don’t. Silver raises a very insightful point – data cannot possibly obviate the need for theory because without theory there is no way to interpret data. He then further discusses implications of prediction in different areas—forecasting weather, earthquakes, financial bubbles, chess (my favorite bit), poker, politics, and terrorist attacks. The essential and most recurring issue is our tendency to over fit the noise in the model without looking at the underlying structure of relationships between the independent and dependent variables. This is a double whammy as the model looks better on paper but performs even worse in the real world. He reminds us that evolutionarily we are wired to seek patterns for survival. It helped us avoid predators back in the day. This becomes problematic when we start building patterns where none exist. If we start acknowledging our bias, and understand that we are approaching the world with Bayesian approximations that we seek to update and improve with increased understanding to arrive at the greater truth, we will notice our predictions become more accurate. Since we can never be purely unbiased, our predictions will never be completely accurate but they can get close enough. Highly recommend this to everyone! Definitely going to re-read this myself. "
47,159420411X,http://goodreads.com/user/show/26188-jafar,3,"This is a good book on the role of probability in natural sciences, social sciences, games, and sports. Silver examines areas where we've done poorly in our predictions (e.g. economy, earthquakes, infectious diseases, global climate change), where we've made improvements in our predictions (e.g. weather forecast), and where we can do fairly well due to the availability of good data (e.g. sports, games, elections). The chapter on how Deep Blue beat Kasparov is fascinating, but a single paragraph about baseball makes me want to skip an entire chapter. Silver argues that due to the probabilistic nature of many branches of social and natural sciences, the centerpiece of our analysis should be Bayes's theorem. Now, Bayes's theorem is mathematically straightforward. It offers a way to calculate a conditional probability, and you can find its proof in any statistics text book. The problem is how to interpret this theorem and where to apply it. Silver says that we should use it in every every situation where the outcome is uncertain to refine the probability of what we're trying to predict. Bayes's theorem, however, requires us to know the probability of an event before we weigh in the new evidence. Understandably, some people have serious philosophical problems with this approach. Is this the way to do science — basically, to have an opinion about a theory before we conduct an experiment and gather new evidence? Silver doesn't really get into this discussion."
48,159420411X,http://goodreads.com/user/show/5038680-eric-lin,5,"Pretty fantastic series of case studies about things we make predictions about. He's refreshingly honest about how his success stems from making better predictions in areas where there wasn't a lot of previous work already done. It seems he leaves the complex modeling to others, and spends most of his time observing general trends, and making macro-level forecasts.Some parts of the book were kind of very ""Nate Silver"" - his initials are embossed on the front cover, he name drops constantly, mentioning that he had lunch with this famous forecaster, or how he had a running bet with some other scientist. I suppose some people enjoy that kind of detail, but I really didn't care about who Nate Silver has lunch with. It wasn't really that annoying, it was just strange, since he could be so simultaneously humble about how lucky he has been with some of his forecasting successes. I actually wonder how different this book would have been if he had written it after his incredible success predicting the 2012 election.Anyway, this book was super interesting, and I think anyone reading this book will come away with a better sense of how we make educated guesses about the world around us."
49,159420411X,http://goodreads.com/user/show/145381-brian,5,"(5.0) Fantastic walk through use and abuse of statisticsBest book I've read all year! ;) I'm glad he didn't bathe himself in glory over his presidential and congressional predictions. Appreciate the discussions that use examples such as sports, medicine, weather, climate, poker, the stock market to illustrate his points. Just a very intelligent book I wholeheartedly recommend to all."
50,159420411X,http://goodreads.com/user/show/84491-mark-hiew,5,"This book is well-written, prescient and entertaining. Silver mixes data, logic and narrative to tackle substantial issues such as 9/11, climate change and the 2008 recession in a balanced, thoughtful manner. He demonstrates the importance of understanding heuristics and discounting the majority of predictions one sees in the mainstream press."
51,159420411X,http://goodreads.com/user/show/5914461-bruno-teixeira,1,"A huge waste of time. if you intend to read this, let me give you a brief overview: Nate knows baseball, poker, climatology and geology. Aside from lots of BS, his advice to differentiate noise and signal is to be bayesian. unless you want to waste your time reading about those subjects without any insight on forecasts, there is nothing useful here."
52,159420411X,http://goodreads.com/user/show/12885473-darian-onaciu,5,"One line summaryExcellent book about how forecasting works within various fields and why it sometimes doesn't, riddled with how a Bayesian approach can help us to better assess probability.Who should read this book?Anyone who wants to better understand how to predict the probability of events happening. "
53,159420411X,http://goodreads.com/user/show/12053415-luciano,4,"Rich in case studies, meticulously researched and easy to read. Bayes theorem can't save the world, though."
54,159420411X,http://goodreads.com/user/show/20317165-alejandro,1,For a book that talks about distinguishing between the signal and the noise this book sure does have a TON of noise. The whole book seems to be extremely repetitive and pedantic. 
55,159420411X,http://goodreads.com/user/show/1500718-ryan,4,"Published in 2012, Nate Silver's The Signal and the Noise provides a readable overview of predictions, forecasts, and Bayesian reasoning.If I understand his overview of Bayesian reasoning, it works like this. If you have a hypothesis, take the chance that it is true, the chance that something else might explain it, and then consider prior probability. Forecasters rely on big data to do this, but I bet most people just ""eyeball"" it. Prior probability is important. I know many Americans worry the government is coming for their guns; given that has never happened, they should update their priors and conclude it's unlikely to happen. But I wonder if our priors sometimes blind us to unlikely events. If we're ""eyeballing"" the odds, we probably think there's a 50/ 50 chance of pretty much anything happening. If we individually engage in Bayesian reasoning, we should also worry that we're bad at it. One takeaway from this work is to think about your priors a lot and always be revising them, but maybe also don't put too much stock in them. There is an overview of climate models and forecasts that I found useful to read. One scientist states that we'll never see CO2 levels drop, nor will our children, because CO2 stays in the atmosphere for so long. In spite of the coronavirus economic slowdown, atmospheric CO2 levels have continued to rise. We need to cut emissions, year over year over year... A reading of this chapter will also highlight how difficult it is to create forecasts from climate models. One forecast overestimates warming because Europe sees the forecast and reduces its emissions, so CO2 concentration is a bit lower than expected. Is the forecast wrong? (FWIW, since 2000, annual CO2 emissions have been through the roof.) Silver notes at one point that climate scientists can only explain our current climate in their models if they rely on increased CO2 emissions, something that I don't think I'd read before.Other notes... Silver distinguishes risk from uncertainty. He distinguishes accurate predictions from precise ones. Some predictions can become self-fulfilling prophecies and others can instigate a mitigating response and become self-canceling. Our minds are wired to find patterns and ""when disaster strikes we look for a signal in the noise."" Poker players make money on the weakest player, but if you eliminate the fish, most of the sharks go hungry.Silver is sometimes less than charming. He uses Bayesian reasoning at one point to suggest that there is a 99% chance that Lindsay Lohan will be arrested again, a needlessly unkind joke. His account of interviewing experts is dry and cliche, always written like, paraphrasing, ""I dropped by his office on a crisp autumn day for two hours."" But I otherwise learned a lot. I see a lot of skepticism of the media and of reports about models or else an enthusiastic embrace of them. I suspect part of what we're seeing in those reactions is ignorance about what forecasts are, how they work, and what to make of uncertainty and risk."
56,159420411X,http://goodreads.com/user/show/83951658-paras-dahal,3,"I went into the book looking for some practical advice on how to avoid pitfalls when making predictions and working with data in real-world scenarios- given the author is a revered political forecaster, but quickly realized the book is aimed at a lot more general non-technical audience. Initial chapters have quite interesting real-world stories on how forecasting and predictions play out in real world, which slowly gets tedious and boring in the later chapters and fails to culminate to the point he is trying to make. But the zeitgeist of the book is to urge its readers to think about the world in a probabilistic, or more specifically, Bayesian way-update personal beliefs with new evidence as they arrive to gradually be more and more correct in one's predictions. In that regard, I think the book succeeds, but it's not clear how a general audience can adopt Bayesian thinking in the quotidian aspects of their lives."
57,159420411X,http://goodreads.com/user/show/51602586-dominika,3,"As far as an introduction to prediction and probability goes, this is pretty great. Nate Silver goes through a lot of different sciences, from sports predictions to meteorology to epidemiology, so you learn a bit about the broad topic and then you learn about these specific fields as well. I was going to chastise him a bit because he used the most basic of epidemiological models and was criticizing it, but admitted that the experts realize this issue. I think unless you're interested in this topic, this might be a bit dry for most people. It's not bad, but I feel like it verges on textbook without the techniques that make it more usable. "
58,159420411X,http://goodreads.com/user/show/108638268-jason-bailey,4,"By harnessing insights from many worlds — poker, chess and baseball, but also meteorology, gambling and terrorism — Nate Silver deftly shows how probability is often misunderstood and how people are often overconfident in their own abilities, including the ability to make predictions. The book's foundation is Bayesian statistics, which suggests that people get closer to the truth by gathering more evidence that either reinforces or challenges prior assumptions. A chapter on virology is particularly timely now, during a pandemic that has killed far more people than swine flu, Ebola or SARS — the diseases of interest when the book was published in 2012. Silver explains why ""self-fulfilling and self-cancelling predictions"" make accurate modeling even more difficult.An accessible book that is engaging throughout, it reminds us that identifying the relevant signal among noisy data is incredibly easy in hindsight but otherwise incredibly difficult.(Caveat: I have done some freelance work for FiveThirtyEight, Nate Silver's current project.)"
59,159420411X,http://goodreads.com/user/show/26584756-isabelle-bradbury,4,"3.5 stars. This book was a mixed bag of sorts for me- I was captivated by the chapters about weather predictions, natural disasters, and terrorist attacks, yet found myself bored through the multiple long chapters about sports and politics. It’s very well written but the commentary at times was a bit much for me, when I really just wanted the data. I did learn a lot and noticed this gradually turned into my nightly audiobook that I used to help me fall asleep (interpret that as you may). "
60,159420411X,http://goodreads.com/user/show/14665026-lukasz,2,after reading Tetlocks' book this one doesn't seem to bring anything new to the table. Only good thing about it is bringing uncertainty in other than political context. There was too much storytelling for my taste and not enough technicalities and insights (especially for someone who read Talebs or Tetlocks books).
61,159420411X,http://goodreads.com/user/show/60590286-surya-bhupatiraju,5,"It's actually so fun reading about Nate Silver's sobering process of predicting the world, and I learned something from all of his well-chosen examples, from credit agencies rating mortgage-backed securities to poker to earthquake and terrorist attack prediction. Two of my takeaways are 1) in general, trying to predicting the future is very, very hard but can be hugely fruitful as an exercise in really understanding it, and 2) betting markets are really interesting! I think there's a world in which I would've done more of these things and enjoyed it more than I thought I did."
62,159420411X,http://goodreads.com/user/show/2527265-david,4,"The Signal, the Noise, and the Nation's Capital: A TriptychPanel One: Nate Silver is Hari Seldon.My long-suffering wife and I flounce about the landscape here in The Nation's Capital, putting on airs to the effect that our political/social/philosophical views are derived from the great thinkers. De Tocqueville! Max Weber! John Locke! After many years of marriage, we finally came clean to each other on our deepest, darkest secrets. Lo! they turned out to be the same: our political/etc. views were really derived from the science fiction books that we read during impressionable youth.Most relevantly in this case, Isaac Asimov's Foundation Trilogy caused us to form the opinion that history had a direction. The direction was not only discernible, but even predictable. Like the first great genius of the Foundation, Hari Seldon, we believed (actually, in our case, merely hoped) that history was reducible to one very large mathematical equation. Know the equation, know the future. Know the future, predict the future. Predict the future, alter the future. For the better. Just like Hari Seldon.Nate Silver is Hari Seldon! Save us, Nate Silver! Save the Galactic Empire!Panel Two: A Brief Guide to Pretending You’ve Read Nate SilverBaseball may be The Nation’s (and Nate Silver’s) idea of the National Pastime. Here in the Nation’s Capital, the favored pastime is pretending to be smarter than you are, see Panel One. In the spirit of being all about giving back, I offer three bits of jargon from this book, which you may feel free to use to project the mistaken impression that you have read it.1) out of sample (p. 43) - When an event is ""out of sample"", it is a square peg you're trying to force into a round hole. Silver's example: if you are normally a safe driver, but currently drunk, you cannot claim that your previous safe driving record earns you the right to drive home. Your altered state is ""out of sample"", rendering your previous safe (when sober) driving record irrelevant.I think this would be a useful phrase to pronounce contemptuously when dismissing someone else's illustrative example.2) overfitting (p. 163) - ""The most Important Scientific Problem You've Never Heard Of"", according to a subchapter headline. Definition: ""the name given to mistaking noise for a signal"", and also ""an overly specific solution to a general problem"".This would be a potentially useful repellent to those pesky people posing practical workable solutions that might contradict a lovingly-held ideology.3) Pareto Principle of Prediction (p. 312) - ""... getting a few basic things right can go a long way"". Learning 20 percent as much as an expert predictor will make you 80 as successful as the expert predictor. A variant on the law of diminishing returns.I'd like to champion this phrase as an alternative to the cliche of ""low hanging fruit"", a favorite here in the Nation's Capital, but which I've always found a distractingly obscene mental image.Panel Three: Why Wasn't I Informed?Although happily unburdened with mathematical aptitude myself, I attended a school with a spectacularly successful mathematics program, some of which rubbed off on me. Later, I perhaps unwisely went to graduate school, during which time I was compelled to engage in wanton acts of social science research, along with the inevitable statistics courses which accompany. Even though my attention wandered from time to time, I'm pretty sure that this Bayesian reasoning that Silver is nattering on about never, not even once, made an appearance. How is that possible? The strongest single impression that I got from this book was: ""Damn, this would have been a useful thing to have learned."" An organized, analytical method to quantify and improve our assumptions about the world seems like something the world could use more of.It would have certainly provided a happy escape for all whose theses depended on finding something, anything, that would yield the magic p<.05, the doorway to perfumed land of publication. In any event, I'm glad that Silver and likeminded have rescued and rehabilitated this obscure intellectual trend. I look forward to the happy day that we will see political pundits panhandling on the streets of the Nation's Capital as a result."
63,159420411X,http://goodreads.com/user/show/42320745-emily,3,"This is a long time coming; I started reading this in 2016, got too caught up in forecasting politics, and put it down until about a week ago. Some chapters I really liked, others I slogged through, depending on how interested I was in the subject that was being forecasted. I was surprised by that, because I expected to enjoy the majority of the book due to my love of stats, but my enjoyment really varied. Silver has some really lovely explanations and overall I think the book is quite good. Would have given it 4 stars if I had been interested enough to read it in a more reasonable period of time!"
64,159420411X,http://goodreads.com/user/show/195166-bobscopatz,5,"I think this is book is not only important, but it is very well written. Nate Silver has taken a complex subject and, through apt analogies and clear examples, made it accessible to just about everyone. The level of math understanding required to get the message of this book is pretty much at a high-school level. He uses equations sparingly, and usually only to show how simple the relationships really are in a Bayesian analysis. For the most part his treatment is not at all focused on the mathematics, but on the logic--also simple and easy to follow in most cases.The feature I think I appreciate most about the book is Silver's use of common examples to illustrate a point, followed by a more in-depth treatment of the main subject of the chapter. This gives readers an understanding of the principles and how things should work out in the more complex story he unfolds.I have one very minor quibble. I hope that Mr. Silver takes another look at Chapter 11 before the next edition goes to press. For some reason I noticed an uncharacteristically large number of typos and wrong word choices having crept in there. But, to me, the most important thing (albeit relatively minor in the grand scheme of things) was his mistake in describing the Muller-Lyer illusion. He gets it backwards which of the two lines appears longer as becomes really obvious when the famous illusion is presented at the top of the next page. This isn't really central to his point (he ventures into an explanation of the psychology of stock trading and tries to make a link to perceptual biases), so I didn't rate the book lower because of it. It jumped out at me but probably won't bother too many other folks. Just chapter 11 feels like it was rushed through and not given the same thorough review as the other chapters--which contain few (if any) editing errors.Nate Silver tells us up front that he's going to try to convince us that Bayesian analysis focusing on conditional probability estimates based on prior and updated information is a much better way to analyze many (most) things that we routinely subject to typical inferential statistics. Along the way he makes a good case for the utility of Bayes' theorem in making judgements of causal (versus correlational) linkages. He also does a good job of making it clear what should count as real prediction (as opposed to model fitting using existing data). He comes at this as a professional gambler and a professional making predictions.Ultimately, the bottom line is that if we demanded the kind of truth and disclosure in predictions, the political world would be in a better place. We could recognize when people are so biased as to deny that any evidence will ever change their minds on a topic. We could see clearly how accurate a prediction is likely to be and, ultimately, how accurate it turned out to be.This is a good and important book."
65,159420411X,http://goodreads.com/user/show/901342-jacob,4,"This is pretty good, especially for a nonfiction book about statistics and probability. I'd avoided it a bit, anticipating it would be dry, and then scanning through the pages I got excited that it was going to be really engaging. It turned out somewhere in the middle. Silver is at his best when he's talking about his own experience (for example, his own playing in the chapter on competitive poker), but he's gone and interviewed and researched for this book and it's helpful when he talks about his experience, for example interviewing Donald Rumsfeld. Besides the fact that I'm interested in prediction (it's the whole point of science), this book touches several topics that are also interesting. To me weather forecasting, for all that it is the biggest success story in the book, is unfortunately not one of them.The ones that do interest me include the stock market, which I would argue is inherently unpredictable precisely because predictions tend to be self-negating, as they are in almost any human realm. However, it's not as efficient as Eugene Fama would imply; for one thing, it depends on people to beat the market in order to be efficient. Another is earthquakes, which are apparently extremely unpredictable. It makes me feel a little better, especially considering the recent NYT article about how the Pacific Northwest is due for a big one any time. Several places are, and yet they keep not happening.The poker chapter was very interesting too, even though I don't really play. But my biggest interest was in the chapter on global warming. Silver clarifies that the only real consensus is that the Greenhouse Effect exists and higher CO2 levels contribute to it; the predictions of future climate have demonstrated themselves to be off so far, and the best predictors of climate for the last 25 years are models which aren't accepted. I'm a little surprised that this chapter doesn't discuss climate effects which are not known yet and that might end up compensating for the warming effect. Silver makes a lot of the heating effect from increased evaporation contributing to clouds which will further trap heat, but won't those same clouds also block some energy from making it to the Earth in the first place?In addition, the discussion of prediction markets is a bit weak. Silver claims at the outset to be in favor of them, but seems to think they need some serious time to mature before they become at all useful. It would have been nice to have him make a call on what kinds of things they will tend to be more useful for."
66,159420411X,http://goodreads.com/user/show/6098128-huw-evans,4,"Nate Silver has an enviable reputation in the world of predictions. Over the years he has made significant impact on the world of Baseball, poker and political punditry. Using Bayesean analysis he has looked at the reasons that we are not very good at predicting important things like the weather, the market, politics, even terrorism. This book does not cock a snook at punditry but looks to see why the experts often end up with egg all over their faces. Remember how we smirked at the crestfallen faces on Fox News when Obama won his second term? Before the smirk fades do we remember with equal clarity the excuses and devastation on CNN when Hillary lost to Trump?This raises the first problem of inherent bias. If you hold a particular position then anything that reinforces that position is good and anything that points away from it tends to be ignored. If you believe you are the King of Poker, every winning hand reminds you that you are indeed the King. Every losing hand was a fluke, a bad draw but not your fault. If you beat the Market you are a player until the Market beats you. Those people with a more flexible approach to thinking are more likely to be able to alter their view and question it. The physicist Richard Feynman used to argue that if your theory was not supported by the evidence then your theory is wrong and not the other way around. Just because things do not go your way does not mean that the rest of the world is wrong.In its simplified form Bayes theorem states that p(A\B) = p(B\A).p(A)/p(P) which is the only time, I hope, that a mathematical formula will ever find its way into a book review. In the middle of all this is a key component, the inherent bias that you already possess. How likely do you personally think a particular event is going to happen? Too many people would appear not to consider their inbuilt views as part of their prediction process. Apparently doctors are very good at Bayesean analysis; without realising the mathematics it is part of differential diagnostics when they consult with patients. For this we should all be grateful.This book is written inevitably with an American slant. I do not profess any interest in poker, one of his major chapters, and have to admit that the whole analysis was as tumbleweed. However, the core theme of rethinking your position based on facts is one that should become the mainstay of any educational process. Being able to separate the truth from opinion would seem to be the core of element. "
67,159420411X,http://goodreads.com/user/show/1362376-avolyn-fisher,2,"Maybe I'm 7 years too late. Maybe I've read too many other data books, or have too much of a background in data given that I work in the field, but I just did not enjoy this book.I found it incredibly hard to focus as the chapters nearly put me to sleep. Nate chooses topics that admittedly 'cannot be predicted' such as the stock market (or we would all be rich), political outcomes (don't we know it, as Nate himself, had since found himself wrong about the 2016 election after writing this book) and a myriad of other topics (with the mandatory cliche examples like baseball sprinkled in) to set the stage in the first 8 chapters of this book on why predictions fail. This felt like a softball pitch to himself to set up the back half of the book, which came far too late in my opinion, to finally discuss what to do about these bad predictions to make them better. And yet his theories don't actually apply to the first half of the book because the first half is again, things that are just very volatile and hard to predict by nature.What compounded matters was that he chose to discuss each topic in painstaking detail. Nate was as swept away by the minutiae as inaccurate forecasters are caught up in their inaccuracies. And his book held the irony of Alanis Morissette's 1995 song, Ironic (which actually wasn't about irony, how ironic!) in that he seemed to fall into many of the traps that he called out in others. He mentions a caution around making broad generalizations around a small sample of occurrences but then again many of his examples are just that. He talks about not being too convinced by hindsight predictions, but his critiques are all hindsight responses with a flavor of 'but now we know otherwise' as if that is supposed to be his wise upper hand on those who were inaccurate. He gets so detailed that I couldn't really tell at various points if the details were in support of his point or in conflict with his suggestions. And sadly, I think he had the opportunity to be much more interesting as I've read other books that discuss Nate Silvers methods and they were fascinating! But he only really talks about his methods when he talks about his baseball scoring from his days at KPMG, the rest of the book is a painful rehashing of the work of others.Data books that I think are more worth your time over reading this one:Superforecasting by Philip TetlockEverybody Lies by Seth StephensWeapons of Math Destruction by Cathy O'NeilEverydata by John H. Johnson"
68,159420411X,http://goodreads.com/user/show/4651295-jason-furman,5,"The Signal and the Noise was a really great read. It's one of those books that lets you annoy your friends by tediously repeating facts, many of which they already have picked up from reading the book, reading reviews, or other tedious friends. Like when the Weather Channel says it is a 30% chance of rain it rains 30% of the time, but that when it says a 10% chance of rain it really means a 3% chance of rain (they would rather people be pleasantly surprised). Or that earthquakes are unpredictable. Or that minor league baseballs players are difficult to predict and give a good advantage to very good scouts, while in the majors it is different. Or that a particularly great professional sports better will win 56 percent of the time. Or other examples drawn from the areas Nate focuses on: political predictions, elections, the macroeconomy, financial markets, epidemics, earthquakes, terrorism, baseball, chess, and poker.There is a deeper and more important set of lessons in the book to anyone that has not been sufficiently exposed to Bayesian methods. None of that was new to me, but it is still interesting to read and should be mandatory reading for anyone who has not been exposed to it before.I take some issue with the presentation of economics. Nate is completely right that macroeconomic forecasting has a terrible track record, and does not even appreciate how terrible its own record is. But he doesn't seem to recognize that there is a lot more to economics than macroeconomic forecasts. And, at least as much of the fault with those forecasts lies with the people demanding and using them as with the people providing them.And I'm not quite as impressed with Nate's election forecasting--anyone relying just on public polls taken in the days before the election would have correctly picked 49 or 50 of the 50 states in both 2008 and 2012. Nate did not have any magic, but he did have much better perspective on the uncertainty in the forecasts and how to read/interpret the significance of movements well in advance of election day.But those are quibbles, this book really deserves wide readership, probably starting with all of those who rushed out to buy it after the election and still have it sitting on their shelves."
69,159420411X,http://goodreads.com/user/show/3030419-fraser-kinnear,4,"Thoroughly enjoyed it.The heart of the book lies in the ""Less and Less and Less Wrong"" chapter, which details Bayesian probability (and how it compares to Frequentist methods). Silver argues Bayesian thinking is most productive format we have for making predictions, and what explains we have been doing wrong in various fields (by pointing out fallacies in heuristics, non-Bayesian thinking, etc)The Bayesian focus on asserting a prior probability, testing that against new events to produce a better (posterior) probability and then iterating reminds me very much of another book I had read earlier in the year - Eric Ries's Lean Startup. Ries's manifesto, which is very much in-vogue in the tech community, feels very Bayesian now: Place a small bet with a minimum viable product, test the results, then iterate. The cheaper and faster (i.e., more efficiently and frequently) a company can iterate their product, the more successful they will be.I don't think Silver or Ries are really talking about anything new, but considering how popular both of these men are today, I wonder if this won't be philosophy that transforms many more disciplines over the next few years.Two other unrelated thoughts on the book:Looking at the frequently utilized end-notes, it was clear Nate Silver had a LOT of fun researching this book (he mentions having performed over 100 interviews, and there were a lot of really cool sounding scientific papers that he referenced).The best explanation for Nate Silver's magic came from someone I heard describe him on Charlie Rose recently. Nate Silver, like Michael Lewis, recognizes the value of of narration and providing context and color for what are traditionally very dry or academic ideas. Silver isn't quite as good of a story teller as Lewis, but he is better at focusing on the concepts."
70,159420411X,http://goodreads.com/user/show/7094406-rohan,3,"It is surprising that the Book is so close to SuperFreakonomics in terms of kind of topics it talks about. Whether it is Economics, Weather Forecasting, Global Warming and of course the things Nate Silver is most well known for Baseball and Politics. And yet, it was quite different from it.Interestingly, Author starts off by using 'Big Data' quite a few times and believes that our intelligent use of Big Data Technologies are our best bet of extracting Signal from a noisy data in future. In this book he talks about variety of things that I mentioned in the paragraph above this one and also about Hurricanes, Earthquakes, Stock Markets, Poker and Chess. My favorite chapter of the book was the chapter on Chess when he talks about the famous match between Gary Kasparov and the IBM's Deep Blue Supercomputer.It is very clear that Nate Silver believes in numbers and Statistics a lot to estimate the probability of the right outcome. He in fact dedicates quite a few pages talking about using Bayesian hypothesis to do effective forecasting and make an informed and accurate predictions. There are quite a lot of good points in this book which were presented in a very simple, straightforward and non-technical way and for that reason I would recommend people to give it a read once. It is a very readable book and even though I (and Mr. Taleb of Black Swan fame) would not agree with the whole idea (and effectiveness) of making predictions in complex and dynamical systems, I would still say the book is worth the read. Nate Silver (and his fans and readers) must realize that all it takes is one bad prediction to cancel out all the right predictions he has made over the years especially during elections."
71,159420411X,http://goodreads.com/user/show/3088726-mark-schlatter,5,"One of the best nonfiction reads I've had in a while. What I love about this book is that Nate Silver is smart enough about predictions to be humble about them. He's not touting Big Data or the idea that with enough computer power we can predict great things. He's talking about why the successes he and others have had were possible (the ""low hanging fruit"" of baseball stats and politics, the rising emergence of weather prediction through atmospheric modeling) and why predictions in some areas are so difficult (earthquakes and recessions). He's selling nothing more but informed (and patient) prediction, and it's still a great read. Rarely did I get through 20 pages without reading a point or seeing a graph that rocked me.A few highlights for me:1) Hands down, this is the best explanation I've read for Bayesian statistics (and the best argument against frequentism I've seen). I had never really appreciated the philosophical distinctions before (even having taught a stats class), but by focusing on the sources of error, Silver makes a compelling case.2) Wonderful coverage of power law principles, including where you might expect them (earthquakes) and where you might not (terrorism).3) A great explanation of overfitting (and the overconfidence it engenders) as opposed to probabilistic forecasts.4) The vast coverage of the book. I was expecting baseball and politics. I wasn't expecting discussion of climate change, day traders, Pearl Harbor and 9/11 conspiracy theories, economics jokes, Donald Rumsfeld, and the poker bubble. And I would have read more --- the breadth wasn't a distraction, but a great way to show the many facets of forecasting."
72,159420411X,http://goodreads.com/user/show/10270148-mac,4,"I enjoyed this book for many reasons. It is full of perceptive insights, wide ranging areas of exploration, informative examples, and vivid analogies. Nate Silver's argument for being probabilistic (instead of deterministic) when making predictions is comprehensive and persuasive; his emphasis on using Bayesian reasoning is thorough and again persuasive. So I found the book informative, useful, and interesting. However, the real strength of this book is not the collection of insights and thought provoking information. Rather it's the quality of the writing. In someone else's hands, this book could have been a morass of confusion. Here, the writing is well done on many levels--the overall structure and organization are clear and directly articulated in the Introduction. Also, there is a clear flow from chapter to chapter, section to section, and sentence to sentence. And Nate moves smoothly among different levels of abstraction, often supporting abstract explanations with concrete examples or clever analogies. In fact, the writing is so well done, I didn't notice for a long time that I was understanding complicated ideas without re-reading paragraphs and sections. Having visited Nate's website many times, I would have predicted his book would be well written; now my prediction is confirmed.Is there a downside? As sometimes happens with Nate, he can include more than I want to know about a subject so the book could be a bit tighter, a bit more concise. But overall, his book is informative, well written, and (let me not forget)entertaining too. "
73,159420411X,http://goodreads.com/user/show/19964120-matt-asher,3,"Some good insights here, as well as a very good chapter on IBM's chess playing computer. Most mainstream accounts spin this narrative as ""computer finally vanquishes man at chess."" The real, full story is much more complicated and shows just how much the competition between Kasparov and Deep Blue was really between one man and a team of programmers who made continual adjustments to their algorithm. The section on rating a shortstops fielding abilities was equally excellent.On the downside, Silver makes a central assumption that is dubious, perhaps dangerous. Of all the new (digital) information being generated, he says that ""most of it is just noise."" A strong claim, not backed up by evidence, and not explored deeply, as one might expect from a book with this title. Nowhere does Silver consider that one person's noise is another signal, or discuss the idea that the shape of the ""noise"" might be deeply informative. Silver also tosses off some inherently dubious claims (bolstered by a footnote, but clearly meaningless). For example, in a parenthetical he says that ""weather has direct effects on 20 percent of the nation's economy."" Clearly, though, the size of the effect that weather has on the economy is entirely dependent on how you define the terms and how you look at the question; it could be 100% if you consider that on any given day, anyone's work could be affected by extreme enough weather. Claims like this one, published in a matter of fact way a statistician, give the false impression of meaning and precision that doesn't exist, and is dangerous to presume. "
74,159420411X,http://goodreads.com/user/show/60222708-nati-s,5,"There are certain kinds of questions that all of us wrestle with. The nature of these questions is such that there is no clear way of finding an answer to them that can be undoubtedly verified. Our best, or perhaps our only, option is to use information that we have available and infer based on it the answer that is most likely to be correct. It is up to us how we interpret the information to come up with the answer. Only after the True answer is revealed, can we really know the veracity of our interpretation with which we inferred our original answer. This book is about how to best use information we have at our disposal to deduce the most likely answer. Information tends to be messy and complicated, making it easy for us to mistake the noise for a signal and incorrectly infer an answer. The book uses several examples from poker, chess, the stock market, earthquakes and terrorism to show us how one can think probabilistically in order to make the best possible forecast. I loved how the book did a very good job at explaining Bayesian reasoning. A way of thinking which leads to a high probability of capturing Truth. I found it to be written intuitively and accessible to everyone. It clarified the need for critical thinking, the importance and need for the greatest phrase in science: “I do not know”.Here’s a nice quote that in my view summarizes the main message:""If you hold there is a 100 percent probability that God exists, or a 0 percent probability, then no amount of evidence could persuade you otherwise."" I give it 5 stars."
75,159420411X,http://goodreads.com/user/show/2339052-liz-s,5,"Would definitely recommend this to anyone interested in stats and forecasting, especially as it relates to how we make policy. Silver's voice tracks his writing in 538 closely: thoughtful, smart, engaging, funny, etc...I think it's an exceptionally clear and well-written book, especially given the dense topics involved. But this will probably still feel like it's too deep in the weeds for many readers who just aren't interested. On the other hand, in some chapters, I felt like Silver had room to come down with more math and less ""here's a vague approximation of what the stats are doing.""In general, the book's earlier topics (baseball, meteorology, financial crisis) felt a little tighter than the later wandering into climate change. In any case, I think this book is good evidence of why Silver is good at what he does - he thinks about complexity, draws from his background as a well-trained statistician, tries to analyze why his models fail, is reasonably skeptical of overly confident predictions, etc. Portions of the book are lengthy asides into seemingly minute trivia that end up playing a large role in a given prediction's variation -- and that kind of attention to bad predictions is exactly why Silver does a better job than most. "
76,159420411X,http://goodreads.com/user/show/731872-kathleen,4,"When all of us were counting down to the elections last year, one name kept popping up on my radar, that of Nate Silver and his blog about the statistics of the 2012 elections. Nate has a real gift for telling stories to go with the numbers, and is not hesitant to tell the unvarnished truth about what they say, whether they favor one party or the other.Turns out that Mr. Silver has more than just a gift for interpreting polling data - he's been around number crunching for a long time. He quit his first job as an economic consultant at KPMG to play poker professionally, and developed his baseball player performance forecasting system, called PECOTA.But as I said, he has a real gift for telling the stories that underly the numbers, and making the science of prediction accessible to the non-statistician. I found his chapters on weather prediction and earthquake science fascinating, and I even really enjoyed the chapter about sports prediction. (This says a lot because most of the time if the sport doesn't involve carrying a stick, hitting a puck, and wearing skates, I could care less about it.) He lost me a little when talking about economic forecasting, but overall I found this a good read."
77,159420411X,http://goodreads.com/user/show/2077405-tara,3,"This feels more like a 3 1/2 than a 3, but alas, these crude voting systems.Nate Silver sets out to follow two paths about forecasting: one, a kind of survey of the different ways people attempt to forecast (or predict) the future based on information; the second, an explanation of how we can better predict and use the vast amounts of data being generated. The first path is probably the better developed of the two, as he takes a wide variety of topics and makes them interesting and relevant. Through these tales of forecasting in weather, earthquakes, poker, baseball, betting, and stock markets he weaves a basic background of how to make predictions. Think of it as an introduction to statistical analysis, and especially an introduction to Bayesian thinking. If you approach this is a very interesting popular science book, then you won't be disappointed. I'll admit I might have had too much hope that Silver would delve deeper into the mechanics of better forecasting. He does something almost as good, however, in that he provides copious amounts of references so you can do the research if you're inclined. If you're not, then there's still a lot of fun to be had."
78,159420411X,http://goodreads.com/user/show/6317395-omar-leal,4,"First off I should probably say that I am a big Nate Silver fan. I’ve been reading his 538 blog for sometime and to be honest, to a fault, his blog sustained me during stretches of the Presidential race when things looked bleak. Disclaimers aside, for a book about predictions/statistics this is a very enjoyable read. The pace is good and the math descriptions are more explanatory than rigorous, so those who may consider themselves math challenged should not be afraid of this book. To summarize I would say this book describes the efforts to apply the scientific method to prediction. Some fields have had more success than others and the examples Mr. Silver uses ran the gamut from weather to poker and earthquakes to terrorism. Mr. Silver has a good sense of humor and provides lively examples and commentary concerning each of the topics. I found myself drifting a bit during the poker portion, but aficionados may find it much more interesting. Overall I thought it was an excellent book by someone who is quickly becoming a standard bearer for predictions and statistical analysis of various topics."
79,159420411X,http://goodreads.com/user/show/744272-john,4,"I would probably give this one 3.5 stars if I could, but I'm erring on the positive end of that mid-point because, in general, it was a book I enjoyed reading. My complaint, such as it is, is that for long stretches the book feels more like a series of anecdotes or case studies than a single coherent argument. Those individual stories are all really interesting, so I didn't really mind that too much, but Silver DOES clearly want to make a larger argument of some kind, and it doesn't come through as clearly as it could. At the same time, when he gets to the portion of the book where he is doing more to make that argument, it starts to feel repetitive— Bayes, probabalistic thinking, revise your assumptions, etc. I also thought that he was comparing some quite dissimilar kinds of phenomena— most explicitly, earthquakes and terrorism— and suggesting that predicting them is similar to a degree that felt wrong to me.All that said, I enjoyed reading it quite a lot, and learned a lot from it about many different things— the amount of research that went in to this, and the range of things he covers, is pretty impressive."
80,159420411X,http://goodreads.com/user/show/6397437-douglas,4,"If my girlfriend hears me say ""Nate Silver"" one more time, she just might lose it. And I'll admit, I'm kind of a junky for Silver's FiveThirtyEight blog. But how can you not be? The guy writes sensibly and accessibly on difficult and complex topics, and when he makes a prediction, he usually gets it right. The Signal and the Noise is Silver's book on the science of prediction, and it covers a number of subject areas (politics, economics, sports, and weather to name just a few) where people use statistics to make predictions--some good, others not so good. In each of these subject areas, Silver breaks down the data and tries to distinguish signal from noise for his readers, but that isn't really what the book is about. Instead, Silver's goal is to get his readers thinking about the world probabilistically, with a truer sense of the things we know and the things we don't. As our world becomes increasingly data-saturated, the ability to think in this way will only become more crucial, and I'm about 85% sure of that."
81,159420411X,http://goodreads.com/user/show/1717479-terry,4,"Bottom Line: Really well written book on the role statistics and modeling play in the modern world.The book talks about a lot of systems, but I feel the three take away points on which Silver has a unique perspective are:1) The profusion of large data sets has made finding insight harder not easier. Large sets tackled with standard statistical tools lead to a lot of false positives as one creates synthetic variables. Understanding and a good base model are required to bring this into focus. That skill set isn't there yet.2) Predictions need to have confidence intervals or ranges to them. Point indicators are useless these days.3) We need to shift thinking from frequentist to Bayesian predictions which means sacrificing the notion of a perfect experiment and being comfortable with creating Bayesian priors.The above are reasonable and his discussion of areas where prediction will improve vs. those where it will not is quite good.If you want an insight into the tools of modernity, please read."
82,159420411X,http://goodreads.com/user/show/222151-ed,5,"I loved this book. It is one of the most interesting and intelligent books I have read for years. And I found his writing style very comprehensible. Indeed his explanation of Bayes Theorem which has eluded me for years finally made the penny drop. His chapters cover not only political and economic forecasting, but weather, climate, sports, earthquakes and many other areas. His examples are brilliantly illustrative and he is rightly cautious on the whole area of forecasting, risk and uncertainty. He also gives us many tools to improve our own forecasting but again without over selling. There is no snake oil in his approach and the improvement he offers is modest and requires we think hard and differently. I particularly liked his explanation of the current state of climate science forecasting and also his suggestion that economic forecasts include the same plus/minus probability span that political polls already include. I also understand indirectly why his 538 political blog was so successful: it is grounded in deep understanding of the subject of forecasting. "
83,159420411X,http://goodreads.com/user/show/970632-mitchell,3,This book is currently being discussed at work chapter by chapter. I'm not really sure why this one rather than some other book was selected. It's kind of a survey of multiple topics. And certainly a simple enough book that us less math-y types could follow it. But it just didn't grab me. And didn't have enough new-ish content. It did do a good job on getting across some ideas that I've had trouble with in the past including Bayes Law and Power Law Distribution and Double Log plots. But it also reviewed stuff that I was more familiar with. So not bad but not on par with say Freakonomics: A Rogue Economist Explores the Hidden Side of Everything or Moneyball: The Art of Winning an Unfair Game. 3.5 of 5.
84,159420411X,http://goodreads.com/user/show/9217628-michael,4,"Definitely worth a read to get a sense of the art and science of forecasting from the mind of Nate Silver, the Election forecaster behind the 538 blog.The book starts a bit slow, but picked up for me when he dug into sports betting, political punditry, poker, chess simulators. The quality of the chapters is a bit uneven where some are gripping while others are a bit of a slog. At 440+ pages, it required some strength of will to get through it, but happily, I just did. It's not overly quantitative, and the writing is good if not gripping.You come out of the book with a better understanding of Bayesian probability, and forecasting in it's many incarnations. The stuff on weather, earthquakes, and climate change were interesting as well, but the book lacked an overarching structure that tied the different areas together more coherently.For folks who like Nate Silver (or #drunknatesilver for that matter) add a star as I feel like I know the guy better having read this."
85,159420411X,http://goodreads.com/user/show/2028832-kathleen,5,"Simply put, this is the book to read if you want to understand this xkcd comic. More thoroughly, this is a book about finding patterns in the massive amounts of data that we're collecting as a society these days and using that to make predictions about everything from earthquakes to terror attacks. It is about understanding how personal preconceptions can influence our perception of patterns where they might not exist, and it introduces tools to combat that. While I don't particularly want to live in the exaggerated metaphorical world that Silver describes--one where everyone walks around wearing sandwich boards full of predictions that are mathematically revised when two people meet and compare notes--I do think our world would be a much better place if people considered this perspective a little more. This is a useful, interesting, well written book. I highly recommend it to everyone. "
86,159420411X,http://goodreads.com/user/show/14468-joyce,4,"Only Nate Silver could get me to read a book about probability and statistics on my day off. I appreciate his clarity and his agile use of examples. He walks through Bayes's Theorem by calculating the probability that your spouse is cheating on you given that you find a strange pair of underwear in your dresser drawer (29%). In a long chapter on poker -- he played professionally -- he patiently explains Texas hold 'em, hand rankings and probabilities, and what you can know at each stage of play about your opponent's hand. Very lucid.Covers Kasparov and Deep Blue matchup in minute detail. Baseball, political punditry, weather forecasting, climate change, professional gambling, search relevance tuning -- this book lays open up a probabilistic world. Tons of material in here -- notes, graphs, great writing, & good quotes:""All models are wrong, but some are useful."" George E.P. Box."
87,159420411X,http://goodreads.com/user/show/118030-stephanie-sun,4,"""The numbers have no way of speaking for themselves. We speak for them. We imbue them with meaning. Like Caesar, we may construe them in self-serving ways that are detached from their objective reality.""Although a few of the experts that Silver meets with could fit in in a Michael Lewis menagerie of quirky geniuses, for the most part this feels like an anti-Michael Lewis book: boring hard-working left brainers hard at work on the boring but important work of parsing out minutiae that the rest of us are happy to think about in bold reductive stripes or not at all.Other than that I have not much smart to add. This is really good. Nate Silver is an impressive young man, and our nation is lucky that someone so conscientious and intellectually spirited has so prominent a voice in mainstream culture."
88,159420411X,http://goodreads.com/user/show/504424-bup,5,"In an imaginary world where people self-motivate to become better people, and to spend each day moving toward substance and moving away from fluff and appearance, this book would be read by every person in the world.That's frustrating, but at least the people who do read this book can make the world a little better. Seriously. Reading this book will make the world a better place.Understanding that employing probabilities is wise, and that a demonstration of confidence in knowledge is usually a sign of self-delusion, can help everybody.Silver looks at baseball, weather, earthquakes, terrorism, poker, chess, Bayes' theorem and a few other things but makes it all easy to follow. No hard math.Thank you, Nate Silver - you're making the world better."
89,159420411X,http://goodreads.com/user/show/620387-joanna,5,"Really enjoyed this book, which is not surprising since I've enjoyed much of Nate Silver's work at fivethiryeight. I read this book with a book club, but missed several of the meetings discussing it, which is a shame because it's a great book to talk about with others. Silver does an excellent job explaining how predictions work, and walking through examples of different types of predictions. The recommended Baynesian analysis makes a lot of sense and seems like it should have broad applicability for a lot of forecasting. The book is well-researched and footnoted and many of the footnotes contain sly humor along with the references, so it's worth flipping (or, since I read this on kindle, linking) back to read them. This is serious stuff, but Silver makes it broadly accessible."
90,159420411X,http://goodreads.com/user/show/5780263-audrey,3,"Largely catered to an American audience. The author claims to be a ""fox"" -- taking many approaches to one problem. However, he then attempts to make big, overarching conclusions like a ""hedgehog"" believing in governing principles about the world. The book discusses predictions related to baseball, US elections, chess, weather, global warming, hurricanes, earthquakes, poker, the financial crisis, and terrorist attacks. Quite focused on disasters or hobbies that are relevant to a certain type of readership. At the end, his main conclusions are: correlation does not equal causation, don't follow the herd, make lots of forecasts, and don't fall into bias. Sorry to ruin the ending for you..."
91,159420411X,http://goodreads.com/user/show/844382-josh,4,"Perhaps a bit longer than it really needs to be to get the point across, but it's all good stuff. Lots of interesting, clear-headed explanations about some of the probability-driven applications in modern life. Nate Silver is really good at not only understanding the math, but also the human psychology. The two don't always play nice together: people often want black or white in a world full of grey."
92,159420411X,http://goodreads.com/user/show/3132060-andrea-james,3,"I thought the author did a good job of making statistics accessible and understandable through entertaining anecdotes and examples. It probably helps if you have an interest in baseball, poker and medicine (maybe that's why I found the examples entertaining).It was nicely written and easy to follow and had some useful reminders such as the importance of not thinking that something is improbable just because we do not know much about it or understand it."
93,159420411X,http://goodreads.com/user/show/9862564-erika-rs,5,"Although this book is structured as a series of case studies, at its heart, it is a philosophy of how we know the world and how we can know it better. All of our knowledge of the world comes through forecasting, sometimes formal but more often not. We make predictions, see how they turn out, and then make more predictions. Ideally, we get better at this process over time. However, as humans, we are systematically bad at this. The chapters go in to many reasons why. The key takeaway, is that if we want to be better at making predictions, we need to make predictions in a structured way and analyze if and why they succeed or fail.Our models have optimistic assumptions. In dynamic systems, these assumptions are often such that if (when) they are violated, the model is dramatically off, not just a little off. This error of modelling is illustrated using the subprime mortgage crisis that led to the 2008 recession.Our predictions are worse when we see the world through the lens of one big, overarching idea. We generally are better at making predictions and have a more accurate picture of the world when we adjust our models and predictions frequently in response to new data. We do better when we think probabilistically instead of in binaries. This is illustrated by the inaccuracy of most political predictions. Political pundits tend to see the world through ruling narratives which blinds them to the nuance of reality.Formal models can be powerfully predictive, but the best predictions will be those that combine the output of models with human judgment -- when that human judgment can be separated from the biases that we have when interpreting data. This is illustrated by baseball, where modelling led to huge successes in finding players... until everyone was using models, in which case, thoughtful combination of models and judgment started winning out.A key element of getting better at forecasting is determining how to evaluate predictions. Since predictions are inherently probabilistic, we cannot just judge them based on their accuracy. A prediction that is right for the wrong reasons is worse than one that is wrong for the right reasons. When evaluating forecasts, we should consider their accuracy (how often are they right) and their honesty (was the forecast the best that the forecaster was capable at the time)? In the long run, evaluating honesty is likely to yield more improvement than evaluating accuracy. Silver illustrates this in the domain of weather forecasting.Our ability to model well depends on our ability to determine what data matters. When the data is noisy, we may overfit it, leading to models which look good on paper but which do not have predictive power. They do not separate the signal from the noise. This is illustrated in the domain of earthquake predictions.Forecasts are often presented as certain, without confidence intervals to show the range of likely outcomes. Even when confidence intervals are given, they tend to be too narrow when compared to actual outcomes. This is especially true when the data itself is complex, noisy, and evolving. This is illustrated in the realm of economic predictions. Models build upon purely statistical predictions by trying to draw some causal connections between the input and the output. A model can provide deeper insights into the predictions that it makes, such as generating ideas for how to prevent the spread of disease (the example domain in this chapter). However, it is important to understand the limitations of a model and the context it was designed for. A model which predicts how a disease will spread through a whole population on average does not give insight for how it will spread through communities within that population.We can get better at predictions overtime, and we can come to have beliefs that are more true over time. As long as we do not start from a position of absolute certainty that something is true or false, even false beliefs can eventually be corrected as long as we are honest with ourselves about updating the weight we give our beliefs as we get new data. This is formalized via Bayes' formula, which is a fairly simple formula where beliefs are updated based on the weighted probability that the observed data would occur if the belief were true and if it were false. This is explored via gambling.Computers have vast computational power which allows them to evaluate possibilities more quickly and more accurately than humans (and with less emotional attachment to certain possibilities). Humans are better at looking at problems holistically and picking out patterns. These complementary skills imply that it is unlikely that computers or humans alone will make the best judgment. Instead, they can supplement each other, with computer models informing human judgment and teaching humans to be better. This is illustrated in the domain of chess programs.Some domains are highly probabilistic. These domains teach us that having the right process is more important than getting the right answer. In fact, in probabilistic domains, a forecaster is guaranteed to be wrong often, so being too results oriented can prevent them from getting better. Instead, a good decision making process will be what helps a forecaster do better in the long run. Another attribute of highly probabilistic domains is that often a few key skills can have a large impact. Further work gives a much smaller edge, but an edge which is critical in domains where better prediction skill determines success. This is illustrated with poker. Prediction markets, including the stock market, are a way to try to predict the true probability of an outcome. Aggregate predictions are generally better than individual predictions. Even the best forecasters tend to only be sometimes better than the aggregate. This is captured in the efficient-market hypothesis: the idea that you cannot beat the market. However, a stronger variant of the efficient-market hypothesis is likely false. It is hard to beat markets, but that doesn't mean that they are always right. Aggregated predictions are still based on individual predictions, and when those are flawed, the aggregate will still be wrong (but less wrong). Common sources of inaccuracy in markets are overconfidence, herding (linking predictions that shouldn't be linked), and short-term incentives. One thing to note, however, is that some amount of noise is necessary for a market to work. If everyone made exactly the same predictions, there would not be a market -- there would be no one willing to take your bets. Sometimes, strong models back predictions. If these models are based in well understood principles (especially if they are based in the physical sciences), then they can provide ways to make predictions that are outside the realm of observed data. However, we need to be aware of the various sources of uncertainty: there is the uncertainty of the model itself, the uncertainty about initial conditions (the data fed into the model), and temporal uncertainty (predictions are generally more uncertain the further they are in the future, especially since we do not know what the future will hold). The strength of models and the importance of understanding the uncertainties they introduce is illustrated in the realm of climate change.Often, the problem isn't not having a signal, it's not being able to find the signal among the many available signals. When this happens, it is easy to ignore important signals which do not fit into our preconceived notions of how the world can behave. This is especially true since we tend to assume that that which is unfamiliar is also unprobable. We also tend to think that something is impossible if we've never seen it before. However, if something is a larger version of something we see regularly at common scales, we should never assume it is impossible. This is illustrated in the realm of terror attacks which, like earthquakes, tend to follow a power law distribution.The key takeaways across all of these examples:- Think probabilistically- Clearly set your prior probabilities- Make predictions and improve- Realize that we tend to believe that we are better at predictions than we really are"
94,159420411X,http://goodreads.com/user/show/4754628-luke-echo,1,Started reading this but...I just find it difficult to take someone so obsessed with baseball very seriously. I mean baseball stats?.... who really gives a shit. Does this book actually get interesting?
95,159420411X,http://goodreads.com/user/show/37311791-santhosh,5,This is a really amazing book - a must read for anyone who makes decisions. What I took from this book is the necessity to think randomly in life.I really enjoyed this - This book was actually what I expected. I give this book a 5 star.
96,159420411X,http://goodreads.com/user/show/5862823-brian-eshleman,4,"He selects good examples from the every day to the earthshaking, not to make the reader abandon the practice of prediction, but to challenge him or her to hold predictions loosely and subject to change with new information."
97,159420411X,http://goodreads.com/user/show/545361-ayndri,4,"Excellent read. It was a helpful refresher of how Bayesian statistics is applied in different fields to accumulate knowledge. Cuts through the hype about prediction to explain how complex it can be, but also how paying attention to some core fundamentals can get you a long way. "
98,159420411X,http://goodreads.com/user/show/2206295-joel-goldman,5,"A highly readable, fascinating explanation of the power and pitfalls of prediction. Eye-opening."
99,159420411X,http://goodreads.com/user/show/10687934-joe,5,"Really cool. Like something of Gladwell, if Gladwell actually provided strong sourcing for every claim he made. Great intro for someone who knows next to nothing about statistical inference."
100,159420411X,http://goodreads.com/user/show/146952-grumpus,4,"The perfect nerd (of which I am) book.Stats and probabilities about poker, baseball, and more. Plus predictive modeling. To be that good. Dare I dream?"
101,159420411X,http://goodreads.com/user/show/4489591-matt,3,"Some good data, a few good insights, but far less gripping and useful than the introduction might lead one to believe. Might I suggest a co-author for the sequel?"
102,159420411X,http://goodreads.com/user/show/2996713-nicholas,5,I can't remember anything but I felt much smarter after reading it.
103,159420411X,http://goodreads.com/user/show/4545006-dan-nordyke,4,HAPPY NOW MIKE???
104,159420411X,http://goodreads.com/user/show/6169728-max-nova,4,"As a guy who makes his living off of stats and predictions of complex systems, I knew that Silver’s “The Signal and the Noise” had to be on my list. It’s a popular treatment of complex topics like chaos theory, bayesian stats, risk vs. uncertainty, and the dangers of forecasting outside of your sample. Silver puts his argument in a historical context and uses a host of modern examples to make his points, including the recent housing crash, Moneyball, Deep Blue vs. Kasparov, and counter-terrorism. He also throws in earthquake prediction and epidemiology.All in all, an entertaining book but not one with any mind-blowing insights for anyone who has been doing stats for a while.A few interesting excerpts below:################If there is one thing that defines Americans—one thing that makes us exceptional—it is our belief in Cassius’s idea that we are in control of our own fates. Our country was founded at the dawn of the Industrial Revolution by religious rebels who had seen that the free flow of ideas had helped to spread not just their religious beliefs, but also those of science and commerce. Most of our strengths and weaknesses as a nation—our ingenuity and our industriousness, our arrogance and our impatience—stem from our unshakable belief in the idea that we choose our own course.There are entire disciplines in which predictions have been failing, often at great cost to society. Consider something like biomedical research. In 2005, an Athens-raised medical researcher named John P. Ioannidis published a controversial paper titled “Why Most Published Research Findings Are False.”39 The paper studied positive findings documented in peer-reviewed journals: descriptions of successful predictions of medical hypotheses carried out in laboratory experiments. It concluded that most of these findings were likely to fail when applied in the real world. Bayer Laboratories recently confirmed Ioannidis’s hypothesis. They could not replicate about two-thirds of the positive findings claimed in medical journals when they attempted the experiments themselves.But this book is emphatically against the nihilistic viewpoint that there is no objective truth. It asserts, rather, that a belief in the objective truth—and a commitment to pursuing it—is the first prerequisite of making better predictions. The forecaster’s next commitment is to realize that she perceives it imperfectly.The most calamitous failures of prediction usually have a lot in common. We focus on those signals that tell a story about the world as we would like it to be, not how it really is. We ignore the risks that are hardest to measure, even when they pose the greatest threats to our well-being. We make approximations and assumptions about the world that are much cruder than we realize. We abhor uncertainty, even when it is an irreducible part of the problem we are trying to solve.Nobody saw it coming. When you can’t state your innocence, proclaim your ignorance: this is often the first line of defense when there is a failed forecast. But Sharma’s statement was a lie, in the grand congressional tradition of “I did not have sexual relations with that woman” and “I have never used steroids.”Human beings have an extraordinary capacity to ignore risks that threaten their livelihood, as though this will make them go away.Uncertainty, on the other hand, is risk that is hard to measure. You might have some vague awareness of the demons lurking out there. You might even be acutely concerned about them. But you have no real idea how many of them there are or when they might strike. Your back-of-the-envelope estimate might be off by a factor of 100 or by a factor of 1,000; there is no good way to know. This is uncertainty. Risk greases the wheels of a free-market economy; uncertainty grinds them to a halt.An American home has not, historically speaking, been a lucrative investment. In fact, according to an index developed by Robert Shiller and his colleague Karl Case, the market price of an American home has barely increased at all over the long run. After adjusting for inflation, a $10,000 investment made in a home in 1896 would be worth just $10,600 in 1996. The rate of return had been less in a century than the stock market typically produces in a single year.There is a technical term for this type of problem: the events these forecasters were considering were out of sample. When there is a major failure of prediction, this problem usually has its fingerprints all over the crime scene.Tetlock’s conclusion was damning. The experts in his survey—regardless of their occupation, experience, or subfield—had done barely any better than random chance, and they had done worse than even rudimentary statistical methods at predicting future political events. They were grossly overconfident and terrible at calculating probabilities: about 15 percent of events that they claimed had no chance of occurring in fact happened, while about 25 percent of those that they said were absolutely sure things in fact failed to occur. It didn’t matter whether the experts were making predictions about economics, domestic politics, or international affairs; their judgment was equally bad across the board.On the basis of their responses to these questions, Tetlock was able to classify his experts along a spectrum between what he called hedgehogs and foxes. The reference to hedgehogs and foxes comes from the title of an Isaiah Berlin essay on the Russian novelist Leo Tolstoy—The Hedgehog and the Fox. Berlin had in turn borrowed his title from a passage attributed to the Greek poet Archilochus: “The fox knows many little things, but the hedgehog knows one big thing.”... Foxes, Tetlock found, are considerably better at forecasting than hedgehogs.Academic experts like the ones that Tetlock studied can suffer from the same problem. In fact, a little knowledge may be a dangerous thing in the hands of a hedgehog with a Ph.D. One of Tetlock’s more remarkable findings is that, while foxes tend to get better at forecasting with experience, the opposite is true of hedgehogs: their performance tends to worsen as they pick up additional credentials. Tetlock believes the more facts hedgehogs have at their command, the more opportunities they have to permute and manipulate them in ways that confirm their biases. “When the facts change, I change my mind,” the economist John Maynard Keynes famously said. “What do you do, sir?”You are most likely to overfit a model when the data is limited and noisy and when your understanding of the fundamental relationships is poor; both circumstances apply in earthquake forecasting.As the statistician George E. P. Box wrote, “All models are wrong, but some models are useful.”Fisher’s interests were wide-ranging: he was one of the best biologists of his day and one of its better geneticists, but was an unabashed elitist who bemoaned the fact that the poorer classes were having more offspring than the intellectuals. (Fisher dutifully had eight children of his own.)It is often possible to make a profit by being pretty good at prediction in fields where the competition succumbs to poor incentives, bad habits, or blind adherence to tradition—or because you have better data or technology than they do. It is much harder to be very good in fields where everyone else is getting the basics right—and you may be fooling yourself if you think you have much of an edge.There is fairly unambiguous evidence, instead, that insiders make above-average returns. One disturbing example is that members of Congress, who often gain access to inside information about a company while they are lobbied and who also have some ability to influence the fate of companies through legislation, return a profit on their investments that beats market averages by 5 to 10 percent per year,33 a remarkable rate that would make even Bernie Madoff blush.Efficient-market hypothesis is sometimes mistaken for an excuse for the excesses of Wall Street; whatever else those guys are doing, it seems to assert, at least they’re behaving rationally. A few proponents of the efficient-market hypothesis might interpret it in that way. But as the theory was originally drafted, it really makes just the opposite case: the stock market is fundamentally and profoundly unpredictable. When something is truly unpredictable, nobody from your hairdresser to the investment banker making $2 million per year is able to beat it consistently.This book advises you to be wary of forecasters who say that the science is not very important to their jobs, or scientists who say that forecasting is not very important to their jobs! These activities are essentially and intimately related.The dysfunctional state of the American political system is the best reason to be pessimistic about our country’s future. Our scientific and technological prowess is the best reason to be optimistic. We are an inventive people. The United States produces ridiculous numbers of patents,114 has many of the world’s best universities and research institutions, and our companies lead the market in fields ranging from pharmaceuticals to information technology. If I had a choice between a tournament of ideas and a political cage match, I know which fight I’d rather be engaging in—especially if I thought I had the right forecast.The substantive variables fall into about a dozen major categories: growth (as measured by GDP and its components), jobs, inflation, interest rates, wages and income, consumer confidence, industrial production, sales and consumer spending, asset prices (like stocks and homes), commodity prices (like oil futures), and measures of fiscal policy and government spending. As you can see, this already gives you plenty to work with, so there is little need to resort to four hundred variables."
105,159420411X,http://goodreads.com/user/show/61555049-boy-blue,3,"I'm absolutely stunned that there's no mention of Francis Galton and his Wisdom of Crowds. Several times Silver uses Galton's exact phrase ""the wisdom of crowds"". Let me quickly enlighten those who aren't sure what I'm talking about How much do you think this cow weighs?Well it turns out that if we average everyone's guesses we'll get incredibly close the right answer. Even closer than if we just ask a single expert. In many ways this is the method that Silver advocates with all of his stuff, essentially aggregation. There's been plenty of modern approaches to this studyThe next question is would a village of cow farmers be better than the goodreads crowd. Would a group of cow biologists be even better. The answer is probably but it doesn't really matter because the crowd of amateurs on average is better than an individual cowherd. When applying this to political prediction Silver finds few ""experts"" who are worth their title. In fact in most fields he examines Silver finds the pros lacking. In many cases Silver is just advocating aggregation and therefore the wisdom of crowds. In fact often it's the wisdom of the wisdom of crowds. Meta-wisdom.I also think there was a missed opportunity here to recognise that often the signal is the noise. If we look at Galton's cow experiment for example and we imagine everyone shouting out their predicted weights, that's all noise, but if we average all of their results we get a signal. In that case the signal actually IS the noise.Silver's love of baseball has turned Billy Beane into his messiah. I mean the man is brilliant but he was the forerunner and there have been improvements. Look at Theo Epstein's relentless search for ""the right"" data and deep understanding of the draft or expand more on the blend of scouting reports and data.I found the book weakest when digressing to biography, Silver's own poker predilections or his baseball buzz felt amateurish next to some of the other stuff he was talking about. Although I shouldn't knock him for his enthusiasm or his ability to bring statistical analysis into the mainstream. I guess his main skill is much like his idol Billy Beane, he can talk to the common man and make them understand.Some great little facts in there as well. How many of you know about ""wet bias""?Silver's twin pillars are aggregation (as mentioned above) and Bayesian Statistics, specifically Bayesian Inference. Here's Baye's formula here.For those who just glazed over. This formula essentially allows the person predicting things to keep updating their predictions and making them more accurate. See that's the real lesson here, true statisticians, people like Nate Silver don't make a prediction and stick to it when the data turns against them. They're mercurial and they move with the prevailing winds. That's to say they keep adjusting their predictions right up to the event. Some may feel cheated to learn that. We wanted a fortune teller right? Well Silver is arguing that Statisicians are sexier than the oracles at Delphi and while I disagree, I definitely think they're more useful.With that we find the answer to whether you'll like this book or not. If you are the sort of person who becomes annoyed when you find out how a magic trick works and that it wasn't that complex at all then don't read this book. If however, you're the sort of person who becomes more interested by the magic trick when you find out how it works and see that the person has a lot of skill and you could learn that skill too then this book is for you."
106,159420411X,http://goodreads.com/user/show/33428641-andre-stackhouse,5,"I've listened to FiveThirtyEight's politics podcast for years now and I honestly still have difficulty recognizing Nate Silver by voice. It's a great podcast but Silver just doesn't stand out much. There's no real reason to believe that a book about predictive math and statistical models would be sexy, but it turns out that math is really cool and accessible when the focus is shifted away from the ""how"" and onto the ""why.""Every sentence of The Signal and the Noise is laden with the colorful thoughts of a careful and restless intellect. Silver has a way of taking familiar debates, histories, and stories and infusing them with new perspective derived from methodical exploration of the uncertainty of events that their previous narratives never acknowledged.Topics range from analysis of the Great Recession of 2008, to counter-terrorism intelligence, to the inner life of poker players, and the differences between predicting weather, global climate change, and earthquakes. It seems likely that Silver hits a hobby horse or two for most readers, but his passion for the chosen case studies has a way of making even the most distant subjects riveting.For me the most brilliant section was Silver's take on the famous chess matches between Grandmaster Garry Kasparov and IBM's Deep Blue, a topic I've watched several documentaries on and heard Kasparov's firsthand account. I was skeptical there was much to add to this story, but found he gave deeper insight into both the human and machine side of that fight humans lost just over two decades ago.While the first game of the 1997 rematch is often described as an easy win for Kasparov, Silver goes into more detail about Kasparov's brilliant winning chess strategy than Kasparov typically does himself. And while it's often said that Deep Blue won the tournament essentially by outlasting an exhausted and stressed Kasparov, Silver explains how it was actually a misread of the first game that spelled the beginning of the end for Kasparov. I've never heard the story told with such beautiful irony.This book is also a respectable challenge to the field of mainstream classical economics. While not a deep dive into behavioral economics, a recurring theme of the book is the unpredictability of human behavior and the gross, simplistic, and overconfident predictions made by economists and economic journalists. If you find yourself at a painful impasse with one of those insufferable, ""It's just basic economics!"" market absolutists, do me a favor and throw a copy of this book at their face.The Signal and the Noise is required reading for anyone trying to take a step back from the insane complexity of 21st century life in hopes of building some foundation with which to understand the world. While there is much to learn from this book, the greatest value will come from a refined thought process and a true understanding of the importance of skepticism.P.S. For those of you audiobook inclined, the narration for this one is top notch. Mike Chamberlin delivers the brainy prose with a sort of suave wunderkind quality that I thoroughly enjoyed."
107,159420411X,http://goodreads.com/user/show/106358246-aaron,4,"Good, but light on practicality or any particularly strong conviction. I'd equate it to a survey class, with an emphasis on Bayesian statistics. I suppose the lack of any particular convictions is a testament to Silver's trust in Bayes' Theorem.I have a decent math/stats background, and a relatively strong computer science background, so nothing of the math or computers I found particularly difficult, or for that matter, new at all. The book focuses on a few major areas, which I thought was a good idea -- going over too much doesn't let you focus on anything, and here at least you dive deep-ish into some interesting subjects. I've also read Moneyball, so the baseball part was a bit repetitive, but interesting regardless. Overall, the book is well written, easy to read, and imparts plenty of information (albeit mostly fun facts) plus a lot of an Intro to Statistics class.Some of the book reads particularly ironically due to Silver's failure to predict Hillary Clinton's 2016 loss in the presidential race (71% likely) -- though he did a better job of prediction than the vast majority of other predictors, it is an example of where predictions go totally wrong even with all the data and collection techniques that you want. With that in context, I would have loved more discussion about how presumably ""random sample"" data collection goes wrong -- but it makes sense that, pre-2016, this wasn't really on the radar.There's some decent discussion of computer models and how computers work through problems, but I think Silver intentionally or not misses out on some more interesting ways that computers can model existing data sets (for example, reinforcement learning) as opposed to solutions that are over-complicated and often not nearly as good as claimed (for example, Monte Carlo simulations). It's kind of weird that the main point of the book is that ""Bayesian reasoning + context = good predictions"", but then he speaks so highly of a system that's basically a giant game of The Sims. Maybe it's better than what I've seen, or maybe it's bias on one of our ends, but it just stuck out as counter to his main point."
108,159420411X,http://goodreads.com/user/show/35395038-amith,5,"A fascinating book. Before picking it up, I had a fear of this being one of those books with catchy titles that present grossly oversimplified take on a subject. I am happy to be wrong. The book deals with prediction in general, across different fields like economics, sports, weather forecasting, elections and earthquakes. The author states predictions in some of these areas have performed well than others. He highlights the reasons for these and also the fundamentals that need to be met for making a good prediction, whatever the field may be. The book shed light on the uncertainty that lurks in all those predictions that crop up in the mass media- GDP growth, Election, natural calamities which are usually presented with the highest confidence. Towards the middle, the book introduces Bayesian inference as a means of prediction. It encourages the reader to think in a probabilistic manner, tweaking his hypothesis and inference as more information come up. This is one of the biggest takeaways from the book. The last chapters deal with application of Bayesian thinking to different areas like poker, counter terrorism and climate change.Some of the chapters such as those on weather forecasting (with some discussion on Chaos Theory), chess and earthquakes are my favorites. This is not a book that is a very easy read, but it never got too complex that I had to put it down. Very informative, interesting and enjoyable."
109,159420411X,http://goodreads.com/user/show/38538651-steven,4,"Nate Silver's The Signal and the Noise: Why So Many Predictions Fail - But Some Don't tires too hard to be approachable. As a result, he doesn't talk about the main ideas underlying this book with much precision. You come off thinking, ""Gee, probabilistic thinking and Baye's Theorem are important!"" But you have no idea what these two concepts actually are or how to use them. I think How Not to Be Wrong: The Power of Mathematical Thinking does a much better job of covering similar material while being both approachable and precise. The above comment is a total shame because Nate Silver does really cool work. He knows his stuff and the examples and topics covered in this book are pretty interesting to me. In short, Silver's point is that it is very, very easy to see trends and relationships that confirm what you want to see. It's much, much harder to figure out if those trends are actually real, and if not, what is really going on. This is a concept I really like thinking about.On the other hand, I LOVED Silver's chapter on climate change. This is a topic I want to know more about. But it has become so politicized that I find it hard to separate the political junk from reality. Silver gives a great guide on how to approach a highly politicized topic and what to expect from a fair treatment versus an unfair treatment. In short, proponents should recognize the uncertainty of the models and opponents should recognize the excellent scientific work behind the theory. "
110,159420411X,http://goodreads.com/user/show/40159364-moses-hetfield,4,"FiveThirtyEight has long been a guilty pleasure of mine, so I was excited to see that Nate Silver had written a book full of his insights on statistical analysis. Silver did not disappoint.The Signal and the Noise is chock-full of excellent observations about the nature of prediction, of statistics, and of the common pitfalls that come with it all. I particularly enjoyed Silver's explanation of Bayesian statistics. Prior to reading this book, I knew Bayes' theorem as just one of many equations I had to memorize in statistics, and one of relatively little import. Silver points out, however, the deeper implications of Bayes' theorem and the flaws of other common methods of measuring statistical significance.While the book was generally very engaging, I had trouble sitting through entire chapters on topics like poker and chess, although I appreciate the points Silver made with those.Side note: this book was written after Silver had correctly predicted the outcomes of the 2008 and 2012 presidential elections, and some of his writing on the topic seems very self-congratulatory in a way that seems a bit amusing in the context of his less impressive 2016 predictions."
111,159420411X,http://goodreads.com/user/show/5285276-viet-nguyen,4,"Bought this book in 2013 and can only finished it in 2017, but at least I could finish such a good book.Nate Silver gave examples in many areas of prediction, like baseball, earthquake, weather, stock market, and even terrorism acts, and showed that prediction is hard, but in some cases there is still hope. The way to improve our chance of getting a good prediction is to apply Bayesian thinking: accept that we have prior biased, but refine our beliefs whenever we get new information. I met Bayes formula in my Statistic course and found the formula quite simple, but didn't fully appreciate its importance. But thanks to many examples in the book, I'm convinced that applying this way of thinking is very helpful. I guess the contents might be too basic for Stat people, but having a common theme and extensive examples to support the argument makes it a nice book to me."
112,159420411X,http://goodreads.com/user/show/21161258-keith,1,"This book has a few good chapters but otherwise it's mostly a collection of bad examples, faulty logic, and poor ""experts"" being interviewed. I was concerned when I saw that the author had interviewed Paul Krugman. The author quotes Krugman as claiming he predicted the housing bubble, which of course he did because he advocated the creation of a housing bubble. The problem is he thought it was a good thing. He never thought it would pop. That point was left out. But when fools like Paul Krugman are interviewed as experts, it goes to show the author didn't do his homework. Some parts of the book are good though and I will get to those after pointing out the problems.An important point to make is that when this book is dealing with economics it's faulty to the core. Many people have predicted the 2007 bubble, the NASDAQ bubble, and others before. I know because I was one of them (in 2007, I'm too young for those before) and everyone else that understands economics saw these bubbles too. The problem is the author interviewed the so called expert economists who don't even understand basic economics. Everyone from the Austrian school explained these bubbles while the experts were clueless. The experts are even oblivious to the bond bubble we have today while it's staring them in the face. The author tries to explain the foolishness of these experts by saying economics is a soft science rather than a hard science. That's not true. Economics is a hard science. It's just that our schools now teach Keynesianism rather than economics. The government promotes this because it allows them to be reckless and irresponsible. So the heart of the matter is that the author is interviewing people that are economists in name only, yet they have no clue what they're doing. This makes their predictions look like random guessing, which it is. But had he talked to someone that understands basic economics and his eyes would finally open. There are numerous problems when it comes to mentioning disease statistics because the author assumes vaccines are effective. He also assumes herd immunity is real when it's not. He even gives an example of the Chicago outbreak of measles and says it's an example of behaviors clustered in a small group. That behavior being refusal of vaccines. The problem is that was not what actually happened. Over 90% of the kids that contracted measles had already been vaccinated. But the author interviewed a doctor on Pharma's payroll to get a false story rather than look at the actual kids that were infected. The problem most likely was a foreigner moving into the area carrying measles. It then infected both vaccinated and unvaccinated because as it's been shown again and again the vaccine isn't providing protection.There is also a point made a few times that since 2/3s (Merck's CEO went so far as to say that 90% of drugs sold today are no better than placebos) of medical studies can't be replicated that these studies are signs of noise in medical research. What the author doesn't understand is that research fraud is the way of business in medicine. So these studies couldn't be replicated not because the results were obtained by random chance, but because they were fraudulently designed to arrive at a result that would allow a drug or vaccine to gather support and sales. The author points out the problem of correlation not meaning causation but then violates this logical argument himself by saying autism coverage in the media is causing an over diagnosis of autism. That's a reasonable hypothesis but it's easy to see that is putting the cart before the horse. This is because autism rates aren't increasing in those over the age 20 and the unvaccinated. They are increasing only in young children being vaccinated. This is another example of why it's important who you interview. If you ask a doctor paid by pharmaceutical companies to explain this you get backwards logic to leave his employer off the hook. So the overall theme to the book is that the author doesn't understand the topics he's using as examples in his statistical arguments and then he violates his own statistical rules while doing so. Sometimes he has good examples such as the chapters on earthquakes, weather, and poker. I see the point he's trying to make, but in order to apply what he's saying you need to be sure you understand the subject in the first place. It does no good to track the opinions of Keynesianists ""economists"" and try to find a signal amid noise. That's like looking for a diamond in poop. Just go to the diamond mine. Learn real economics and then it all makes sense. All of the bad parts aside, the Chapter titled How to Beat Em is worth a read. I recommend reading the portions of this chapter concerning ""herding"". The author does a good job explaining this and it's a part of Wall Street every investor should understand. The end of this chapter however is completely wrong. The author thinks bubbles are caused by this herding, and some small bubbles could be. But the large stock market bubbles have all been caused by the Federal Reserve. They do this by manipulating interests rates far lower than what the market rate of interest would be. This causes malinvestment and a mis-allocation of resources that creates bubbles. The federal government also contributes to bubbles just as they did in the housing market by insuring mortgages.The last chapter is another useful chapter. This seems to be the issue he looked into the most. It's also the most applicable to the title of the book. Overall I wouldn't bother reading this book. If you have nothing else to do and the book falls into your lap then just read the few chapters that were worth it."
113,159420411X,http://goodreads.com/user/show/89665996-dennis-willems,4,"The Signal and the Noise is a book that one should read when/before working with data and predictive models. It gives fundamental insights into the way we as people work with data, correctly and incorrectly.The various topics in the book might interest you or might not, but the overall signal of the book is crystal clear and understandable. For me, with a study background in statistics and daily application of it in my work, the book could have gone in more depth on the theoretical foundations of the challenges described. However, since it does not, I can now also recommend it to people who are not interested in theoretical statistics. Although the first chapter required some perseverance in case you don’t like baseball...The signal and the noise has a stronger message connecting the topics in the book than for example Freakonomics. As we start to use more and more data, this book makes you think twice when reading news articles with statistics in then, and also think twice when you see (faux-)statistical model outputs in practice. I highly recommend it!"
114,159420411X,http://goodreads.com/user/show/75675449-michal,5,"Nate Silver, a statistician and a political analyst delivers one hell of a book here. He introduces the reader very smoothly to bayesian thinking, the core idea in his statistical understanding of the world. The book is full of notable and well-explained examples of statistical thinking's utilisation most of which the reader wouldn't believe are fit to be explained by statistics (for me the most surprising utlisation was baseball). This book does not require one to be a MSc in statistics, but a basics understandings of mathematics and probability is paramount to the full enjoyment of this book. "
115,159420411X,http://goodreads.com/user/show/75107226-steven-willmott,5,This is a great primer on thinking statistically and making forecasts. It never gets too technical but explains why even professionals in many fields often aren't thinking about things in the proper way. The examples given throughout the book are all very clear and easy to relate to. Top book of 2020 so far...Super relevant for understanding some of the projections used in the Coronoa virus news cycles. More often than not explaining why they are misleading.
116,159420411X,http://goodreads.com/user/show/59414059-nick-lucarelli,4,"Bit too long for my liking. Torn between giving it a 5 for his revolutionary use and explanation of data science in fields like sport, poker, the weather and politics that illustrates our tendency to revert to biases and how to overcome these (see: Bayes' theorem), or a 3 for the very long bows he draws and generalisations with regards to other scenarios his thinking applies too. A bit USA-centric as well for my liking but what can you do."
117,159420411X,http://goodreads.com/user/show/10412083-nicholas,4,"The book provided a number of examples of the difficulties and methods of predictions and forecasting. A very fascinating book and recommended to anyone who is interested in weather, sports, finances, and/or politics as all are discussed within. Moral of the story: it's as difficult as it is important to know the difference between knowing what we know and knowing what we think we know. "
118,159420411X,http://goodreads.com/user/show/5825281-daniel,5,"I wasn't sure at first as it seemed to start slowly, but the further I got the more convinced I became that I was reading a really important book, one that can help you wade through the bullshit life throws at you more easily and intelligently."
119,159420411X,http://goodreads.com/user/show/44407432-jonathan-lamb,4,"Really interesting book. I admit to having skimmed through a chapter or two, particularly the ones about gambling because I'm just not that interested in poker to read 15 pages about the intricate statistical probabilities of each hand. The chapter about earthquake predictions was the best."
120,159420411X,http://goodreads.com/user/show/44540664-angela,4,"Nate Silver is one smart cookie, and I thoroughly enjoyed being able to soak up some of his prediction knowledge via reading this book. I liked that Silver chose a wide variety of topics to dive into with this book - from baseball to politics to weather to poker. It gave a little something for everyone. And he really did his research when he wrote this bad boy - my Kindle shows me at 66% complete when I reached the acknowledgements, meaning there are ~80 pages of citations and footnotes. I believe this book will give me knowledge nuggets to throw out there when socializing with friends for years to come - ""hey, did you know that if the actual weather prediction is a 50% chance of rain, it will be reported as either 40% chance or 60% chance because the general population gleans the 50% probability to be too wishy-washy?"" The information in this book is very interesting, but the way that it's packaged is not-so-much. Silver is clearly a prediction guru and not a writer. At 435 pages, this book could have easily been whittled down to half that if all of the superfluous verbiage had been removed."
121,159420411X,http://goodreads.com/user/show/49696258-wpschrec,3,"It was alright. Talks about Bayes Theorem, but most of the stuff he talks about feels long winded. It's like he wanted his book to have a main overarching point (the Signal) while clouding it with lots of stories and examples that made it a bit too tedious (the Noise)."
122,159420411X,http://goodreads.com/user/show/5939972-gregg,3,"I’m a bit surprised that a book about prediction/forecasting would not be able to predict/forecast that the book would start to get redundant and boring. It’s one thing to do a deep dive, quite another to hit the Challenger Deep."
123,159420411X,http://goodreads.com/user/show/33875331-manuel,3,"nate... Nate... NATE![the book] is good (e.g., baseball, chess, and especially the weather prediction chapters)... until it isn't (the longest chapter is border-line about denying climate change)... which is sad.""Still, we are imperfect creatures living in an uncertain word."" p.328"
124,159420411X,http://goodreads.com/user/show/10726611-chunyang-ding,4,"Nate Silver writes a good book here, but not necessarily a great one. He brings to the table a careful and nuanced view of statistics, particularly in improving a lay-person's understanding of uncertainty, but sometimes feels like it misses the mark when it comes to the examples he chooses. Perhaps it is just because of the onslaught of books these days trying to create narratives out of statistical science, but talking about baseball, terrorism, and earthquakes seems to have been covered over and over and over again. On the whole, this is still quite a nice book for people who want to engage some critical statistical skills with plenty of supplementary data and charts."
125,159420411X,http://goodreads.com/user/show/39863912-jack,3,"Essentially a love letter to Bayes' Theorem, The Signal and the Noise creates some well-formed arguments against alternate methods of prediction, overfitting and more using a diverse set of examples.At times this book deserved 4 stars and at times it deserved 2. When it was good, it was a fascinating read with articulate points made through case studies. When it was bad, the writing was clunky and hard to read, compounded by the occasional typo and labeling error. Honestly, I was quite shocked to find errors in a published book, but maybe I have a bad copy?While the low points were frustrating, I appreciate that this book reminded me of the importance of Bayes' Theorem and inspired me to apply it in new methods to my professional life. "
126,159420411X,http://goodreads.com/user/show/93226580-james,5,"A humbling read, covering how forecasting relates to pandemics, national security, and climate change.. all highly relevant these days. Bayesian prediction is really interesting to learn about, and different than one would expect.Taleb Nassim has criticized some of Nate's successes and it's worth reading over after you read this book. More info: https://towardsdatascience.com/why-yo..."
127,159420411X,http://goodreads.com/user/show/17071108-emily,4,I love books like this that get relatively technical and you couldn’t really articulate a summary of it to anyone else but directionally in your head you’re still like “I get it”. makes me feel dumb but in a good way. Stats is cool!
128,159420411X,http://goodreads.com/user/show/21764521-chris,3,"My true rating for this book is three-and-a-half stars. I would have given it four stars had Silver been more concise; he could have condensed this book to half its size and still made nearly every point. That being said, I was impressed with Silver’s ability to analyze so many widely-varying case studies in prediction, and to elucidate why we are better at predicting some phenomena than others. Baseball and poker are both very amenable to prediction, while earthquakes, GDP growth, the stock market, and predicting terrorist attacks are not. The ability to make accurate predictions rests on two cornerstones: sound theory and good data. Predicting earthquakes is difficult because even though we understand the fundamental physics underlying how earthquakes are generated, we would need thousands of sensors miles underground in order to obtain the data necessary to make accurate predictions about when an earthquake will actually strike. Predicting GDP growth and the stock market suffer from the opposite problem: we have truckloads of data, but no theoretical framework strong enough to sustain its weight. One very important point which Silver emphasizes is that a deluge of data is not a panacea for prediction—in fact, without sound theory more data can actually make prediction more difficult because it is harder to separate the signal from the noise. I especially appreciate this point with respect to one of my own professional interests, seizure prediction. In graduate school I worked on a semester project in which I applied scads of statistical techniques to neural data in an effort to predict when seizures would occur. The project failed miserably, and so far no one has developed a good seizure prediction algorithm. I believe this is because we still don’t have a fundamental understanding of how seizures are generated. Until we do, seizure prediction isn’t worth the effort.Two of the most interesting topics were weather and climate change. I expected Silver to talk about how weather is inherently unpredictable due to its sensitive dependence upon initial conditions (chaos), but in fact meteorologists have made tremendous progress in predicting the weather over the last fifty years or so. They have a clever way of turning the bane of chaos on its head by running a suite of simulations, all with slightly different initial conditions, and then aggregating the results to formulate a probability of certain events occurring (rain, for instance). The chapter on climate change was one of the best in the book because it clearly separated the facts from the ideology (on both sides). We understand the basics physics of climate change, which is a major point in favor of our being able to predict global warming trends and their consequences, but at the same time we do not understand many of the details well enough for large-scale computer simulations to tell us anything meaningful. We can predict with confidence that the world will continue to warm as carbon dioxide levels increase (contrary to what many on the far right would have us believe), but predictions such as mass famine or catastrophically rising sea levels are foolish (contrary to the assertions of many alarmists on the left). That is not to say such catastrophes cannot happen—just that we don’t have enough information to say that they will with any level of certainty. Silver seems to have lost faith in the ability of the political process to forge any meaningful solution to the climate change problem: “Climate scientists might do better to withdraw from the street fight and avoid crossing the Rubicon from science into politics. In science, dubious forecasts are more likely to be exposed—and the truth is more likely to prevail. In politics, a domain in which the truth enjoys no privileged status, it’s anybody’s guess.” A pretty cynical conclusion, but given Silver’s experience in the political realm, I can’t blame him.The other chapter that I thoroughly enjoyed was Chapter 2, in which Silver eviscerated political pundits for their terrible predictions. Most people talking politics on TV are “hedgehogs”—ideologues who tend to see the world as black-and-white and are averse to admitting their errors. According to Silver, this mentality leads to very poor prediction ability. If we want to make more accurate predictions, we should strive to be “foxes,” seeing the world in shades of gray, striving to see problems from many different vantage points, and being willing to admit mistakes and correct course if necessary. In addition, we need to think probabilistically. Instead of predicting that this or that candidate will certainly win the election, we should assign probabilities to each candidate winning. Silver even goes through the basics of Bayesian probability calculations, which is fun. Overall, the main points I will take from “The Signal and the Noise” are that even in this age of Big Data, theory is important—perhaps more important than ever before. And thinking probabilistically can help improve our predictions, perhaps even in making mundane decisions such as when to go to the grocery store or which route to take home."
129,159420411X,http://goodreads.com/user/show/56596205-robert,5,One of the best science books I have ever read. I would never have guessed that I could be so engaged by a book about statistics. Thank you to Jess for lending it to me.
130,159420411X,http://goodreads.com/user/show/1993178-ilinca,5,"Not everything was as interesting as the bit about the weather, for instance, but there's a great deal to be learned from this guy, and this goes well with Moneyball, Everybody Lies and a number of books that explain how data science works for everyday things. "
131,159420411X,http://goodreads.com/user/show/97961034-michael-buckingham,5,I learned a lot.
132,159420411X,http://goodreads.com/user/show/76409884-dana-m,4,"Like Duncan Watts's ""Everything is Obvious,"" this book covers science communication and perception. Silver focuses on challenges scientists face making forecasts and biases among audiences interpreting the results. He covers the economy, hurricanes, earthquakes, terrorist attacks, baseball, chess, and climate change, noting the specific conditions that make that kind of forecasting problem different from others. It's compelling like Michael Lewis's books without lionizing individual characters, but still interesting and occasionally funny. His example walkthrough of Bayes's Theorem is whether your spouse is cheating on you, conditional on you finding a pair of unknown underwear at home. It's an example you're unlikely to forget how to re-calculate. Silver also covers Tetlock's categorization of scientists as ""foxes"" or ""hedgehogs;"" the former gather data from lots of sources and understand the uncertainty in their forecasts, while the latter make clear, rigid, oversimplified predictions"
133,159420411X,http://goodreads.com/user/show/102968154-victor-sena,4,Lots of interesting stories and opinions. But it's also too simplistic or imprecise sometimes. The last few chapters were a bit boring as well.I really liked the book though. It explains the author's mindset and tells interesting tales around predictions. I find myself referencing it's stories and studies often now.
134,159420411X,http://goodreads.com/user/show/52315211-josh,4,"Labored but insightfulSilver used real world examples to illuminate our overconfidence in our own predictive powers, but also offers some rules of thumb that are helpful in simply realizing that we’re making loose predictions with noisy data and when we may be mistaking noise for signal. However, It is a bit disarming to be reminded over and over again that finding the signal in the noise is like trying to find a specific needle in a pile of needles. "
135,159420411X,http://goodreads.com/user/show/35624237-foxtrot,3,"Interesting books, not revolutionary approach to stats but propose lots of example to explain basic concepts. All of the interesting parts could have been condensed in fewer pages though. Low intensity reading. Sometimes I felt like wasting my time and going to read directly a stat textbook would have been much more efficient.Lots of repetitive content seen through different examples. That may be good / no good depending on how comfortable you are to stats / estimation prior to reading this book.If you are a undergraduate / graduate in a scientific domain you will feel like wasting your time reading this book. All the non-mathematical infos that is valuable could be summed up in a blog post looking like the few lines afterward : (mini spoiler alert)Gist of the book : - Don't be overconfident in your estimation. Overconfident estimator want to draw attention on them through seeking THE prediction that will get them a pass on all talkshow. - Making prediction requires good data : bayesian estimation makes good use of prior knowledge to evaluate likelihood of a future event knowing prior situation. Continuously update your prior bias with the information you can get.- Making prediction in a field requires a certain level of expertise / knowledge to differentiate the signal from the noise among the overwhelming information flow. Don't be fooled in hoping to get tremendous ROI by following the herd blindly. - An estimator must be evaluated on the long term (a series of forecast) not on a single forecast.- Be careful to cognitive bias (eg : 90% chance, 99% chance, 99,9% chance and 99,999999% chance of an event happening look close to each other at a first glance but they convey highly different level of confidence)In a nutshell, you're better off reading the wikipedia page about Bayes theorem and a few high level explanation of its implication on the web. It will take you less time for the same resulting knowledge minus the detailed examples."
136,159420411X,http://goodreads.com/user/show/2385396-bradley-eylander,4,"Great book on predictability. I believe the author was trying to drive the point to the readers that if they are making decisions, try and associate a probability to their guess - using Bayes Theorem of course. I also liked the idea of people that announce their predictions to an audience should have to put money down with their prediction, that way the audience can better trust that the forecaster actually believes what he/she is predicting.Some topics were seemly drawn out, which I predict will bore most reads (such as his topic on earthquakes). The author mostly focused on where predictions fail and are difficult to predict, which made the book somewhat of a downer. "
137,159420411X,http://goodreads.com/user/show/76619267-abhishek-dalal,4,Ambitious in its coverage of topics that range from predicting terror attacks to diseases to the weather. Provides a useful framework of thinking about data probabilistically and thus getting closer to making better predictions.Much more useful than any formal stats course I've taken in school or college.
138,159420411X,http://goodreads.com/user/show/10735430-jean,3,"I hate to give Silver only 3 stars, as I follow him closely, enjoy his newsletter etc. While I overall enjoyed this book, I think it would have benefitted greatly from more judicious editing. I'm pretty interested in this material, and I found it too lengthy and dense, so I think the average reader wouldn't love it. Unlike many of the reviews, I didn't find the material to be consistently easy to follow and it also seems like Silver has a few axes to grind. I'd recommend if you are deeply interested in the subject, but don't think others would enjoy."
139,159420411X,http://goodreads.com/user/show/817196-jen,4,This is a sipping book. I think it should be read in chapter order because Silver does build on his discussion of prediction in a nice way. It also introduced me to the process of prediction in some interesting areas that helps me understand the concepts in my own are of research. 
140,159420411X,http://goodreads.com/user/show/38319939-lee-richardson,5,"# Review 2Easily my favorite book of all time, so I ended up re-reading it. Six years later, I came away more impressed. A couple observations:- I think Silver advocates for uncertainty in our understanding of the world better than anyone I've seen. This way of thinking: hedging our bets, understanding the limits of our knowledge, testing our subjective perceptions against objective reality, etc. is becoming more popular, and I think for good reason.- I didn't realize how aligned this book was with psychology, and in particular, the ideas from books like ""Thinking Fast and Slow"" by Daniel Kahneman. Silver sees a big part of his project as carving a path forward towards overcoming our cognitive biases and getting closer to the truth. You can imagine Silver as someone watching cable news, getting increasingly frustrated with the quality of the conversation. - One of the main complaints from the statistics world is that Silver mixes up Bayes theorem (a formula based conditional probability), Bayesian inference (probability interpreted as subjective beliefs) and Bayesian thinking (thinking about the world in probabilistic bets, and updating them with new information). I think it's a fair criticism, and Silver could have had a few more statisticians provide a technical review, but to me, it's a minor point against him. Given the rest of the book, it's pretty clear that Silver is a great researcher, and does his homework on the topics he writes about. So maybe, just maybe, the differences between Bayesian inference, Frequentist inference, Fisherian inference, etc. are pretty damn confusing (I would bet that 80% of statisticians don't know the differences, especially the newly minted graduates). This is a long and tedious rabbit hole to go down, and it obscures the main message: it's good to test your knowledge by making real world predictions, since predictions test our subjective perceptions against objective reality. As far as I know, there's only two ways to do this: experiments and prediction. So focusing on real-world predictions, opposed to quibbling over Bayes / Frequentist debates, seems like a more productive use of time. - He really goes after R.A. Fisher. This is warranted in some cases, especially given Fisher's stubborness against smoking and cancer, but he should have given Fisher some credit, especially for inventing experimental design.- I've met a lot of people in this book (e.g. Rich Loft at NCAR, John Grefenstette at Pitt, and now Google), during my PhD. I wonder if I was subconsciously drawn to these projects because I read this book right before graduate school.- In my opinion, one reason this book was so successful is because the writing was crisp, and the book flowed well. For a statistician / forecaster, Silver writes better than most, and I think that was a real boon to the popularity of the book. - Another reason for the books popularity is that it aligns with the Zeitgeist of our culture. It focuses a lot on the information explosion, big data, real world events such as the Flu and 9/11, political polarization, etc. It was also a bit more philosophical than I remembered, as Silver is pretty clearly advocating for a world-view based on humility and uncertainty. - Finally, Silver seems a lot less impressed the the train / test split paradigm that's popular in most practice today. He considers this time spent in ""model land"": ""Companies that really get big data, like Google, aren't spending a lot of time in model land. They're running thousands of experiments every year and testing their ideas on real customers"". So once again, Silver's focus is on testing our subjective perceptions against objective reality. # Review 1I read this a couple years ago, so this review will be pretty stale. I found this book very inspiring as someone who is pursuing statistics in academia currently. Silver is wildly entertaining as a writer, and this is the example of a book which is impossible to put down once you start reading it."
141,159420411X,http://goodreads.com/user/show/20386845-trey-malone,4,"If Silver had cut 200 pages out of this book, it would have been amazing. Instead, it tended to drag a bit in areas that are all too familiar for someone who has read a book or two in the genre. And by genre, I mean anything in the ""freakonomics"" strain of pop non-fiction in the past decade, because he covers literally everything from poker and chess to meteorology and climate change. Where he really thrives is his discussion of forecasting, and his examples of good forecasting practices. I particularly enjoyed his chapter on meteorology and the chapter that compared polling to market forecasts."
142,159420411X,http://goodreads.com/user/show/64202567-antonio-stark,4,"An amazing additional read to the ""Modern Computational Statistics"" course I'm currently taking. Statistics are everywhere in life - from your news broadcasts to stock markets to decisions in everyday life that are based on uncertainty. This book gives an amazing array of references that uses Bayesian inference methods to make informed decisions in probabilistic situations."
143,159420411X,http://goodreads.com/user/show/61557581-behrooz-parhami,5,"It has been a long time since I was so impressed with a book. Given the arrival of the age of “big data” (whatever that is) and the rise to prominence of “data science,” every literate person should read this book and heed Silver’s warnings.A statistician and founder of NYT’s political blog FiveThirtyEight.com (the blog’s name comes from the number of electors in the US), Silver rose to prominence by his baseball and election analyses and was named by Time magazine as one of the world’s 100 Most Influential People. The FiveThirtyEight blog began with a forecasting model based on averaging many polls, each weighted according to its past accuracy. It then evolved to include more sophisticated forecasting techniques.Early in the book (pp. 12-14), we learn of bias as a human defense mechanism against information overload. The human brain is remarkable; it can store several terabytes of information, according to Robert Birge of Syracuse University, yet this is only one-millionth of the information produced in the world each day, as estimated by IBM. So, we have to be extremely selective about the information we choose to remember. In his 1970 book, Future Shock, Alvin Tofler hypothesized that one way of dealing with information overload is to simplify the world in ways that confirm our biases, shedding nuances and key details in the process. As a result, rather than serving to bring us together, more information tends to push us into the familiar confines of our biases. Also relevant to the notions above is a view of judgment as lazy thinking. When you see something new, your brain goes into overdrive until you identify it and assign a noun to it (“Oh, that’s a fork”); you then relax and stop thinking. The same is true with regard to people (“Oh, that’s a Latino/feminist/Republican”). Stoppage of thinking at this point makes you miss all the nuances. https://www.facebook.com/mindvalley/v... This lust for detecting patterns according to our biases is also what makes us bad at predictions. Yet, predictions are also essential to our decision-making and achieving favorable outcomes for ourselves and those we love. It turns out that the more extreme our beliefs, the less accurate our predictions. Carrying an extreme position makes us less likely to use all the information that is available to us and more likely to make our predictions emotionally, rather than logically.It’s scary to think that we can never make objective predictions, as they will always be tainted by our subjective beliefs. However, just being aware of the problem and believing in the pursuit of objective truth (regardless of our ability to find it) go a long way toward making better predictions.The noise of the title refers to all the inessential or irrelevant information that prevents us from focusing on what is important. We read on pp. 60-65, for example, that “Political news, and especially the important news that really affects the campaign, proceeds at an irregular pace. But news coverage is produced every day. Most of it is filler, packaged in the form of stories that are designed to obscure [their] unimportance.” “Rooting for the story,” that is, hoping for a more dramatic turn, is the classic form of media bias. “Candidates, strategists, and television commentators—who have some vested interest in making the race seem closer than it really is—might focus on outlier polls.” The book has a two-part structure: The first 7 chapters, 231 pp., deal with diagnosing the prediction problem (prediction pitfalls, ch. 1-3; dynamic systems such as weather, ch. 4-7) and the last 6 chapters deal with applying the Bayes’ fix (Bayes theorem to the rescue, ch. 8-10; examples, ch. 11-13). A concluding section (9 pp.), acknowledgments (3 pp.), notes (56 pp.), and index (20 pp.) end the book. Here are the chapter titles:Chapter 1: “A Catastrophic Failure of Prediction” (the 2008 financial crisis)Chapter 2: “Are You Smarter than a Television Pundit?”Chapter 3: “All I Care About Is W’s and L’s” (success of predictions in baseball)Chapter 4: “For Years You Have Been Telling Us that Rain is Green”Chapter 5: “Desparately Seeking Signal”Chapter 6: “How to Drawn in Three Feet of Water”Chapter 7: “Role Models”Chapter 8: “Less and Less and Less Wrong”Chapter 9: “Rage Against the Machines”Chapter 10: “The Poker Bubble”Chapter 11: “If You Can’t Beat’em” (global warming)Chapter 12: “A Climate of Healthy Skepticism” (terrorism)Chapter 13: “What You Don’t Know Can Hurt You” (market bubbles)Every chapter is jam-packed with interesting observations, often accompanied by mind-opening visuals (graphics). In the rest of this review, I will cite just a few examples, in the interest of keeping my review shorter than the book itself!Figure 10-9 (p. 321) is a scatter-plot of batting averages for a number of baseball players during the months of April and May, 2011. The two variables show almost no correlation, highlighting the role of chance in batting success. A similar lack of correlation is seen in Figure 11-3 (p. 340) for stock-market fund performance from year to year. Figure 11-4 (p. 341) stresses the point that market index trends are almost indistinguishable from random walks!Silver repeatedly stresses the well-known distinction between mere correlation and causation, something that can baffle even experts. Here is one of the most bizarre examples. For three decades, between Super Bowls I and XXXI, stock market rise and fall in the US showed near-perfect correlation with whether the Super Bowl winner was from the National League or the American League (p. 185). The importance of communicating uncertainties is another key point. For example, there may be a prediction that the water level in a river with 51’-high levees will rise to 49’ (p. 178). With these numbers, area residents may feel relieved and safe. However, the prediction may have an uncertainty of plus-or-minus 9’. Knowing this uncertaintly makes a big difference in how people prepare for the upcoming storm.Another cautionary tale pertains to the dangers of overfitting. Fitting a curve through the data showing frequencies of earthquakes of various magnitudes in Japan (Figure 5-7C, p. 170) might lead to the conclusion that a magnitude-9.5 quake is nearly impossible and that magnitude-9.0 quakes occur once every 10,000 years. A more reasonable extrapolation (Fig. 5-7B, p. 169) puts the frequency of magnitude-9.5 quakes at once every 1000 years. There is a very big difference between these two forecasts! In fact, we have never observed a magnitude-10.0 quake and don’t know whether it is even possible. Predicting rare events is one of the major challenges of forecasting. In predicting quakes, we have gotten pretty good at forecasting long-term trends (Tehran, the capital of Iran, will have one major quake every 300 years) and, more recently, very-short-term trends (a quake will be coming to Los Angeles within minutes). Filling the gap between these two extreme time frames isn’t easy! Predicting terrorism is quite similar to earthquakes. Is the one-off event of September 11, 2001, really the worst that can happen (p. 432) or do still more calamitous terror attacks await us?Complexity does not necessarily make the models better. If you performed linear regression on global temperature records and the levels of CO2, you would get a near-precise prediction (within microseconds, on a laptop) of the trend of 1.5 degrees Celcius warming per century since 1990 (p. 401), even though your model ignored sunspots, the level of sulfur, el-nino effect, and a host of other parameters (whose inclusion would have required hours to run the model on a supercomputer).Mistaking short-term variations for long-term trends is another pitfall. Global temperatures since 1900 (Figure 12-11, p. 405) show an unmistakable rising trend, but within that trend, there are multiple flatlines (e.g., during the 2000s) and even downshifts (e.g., 1930s-1940s). Some people have a hard time wrapping their head around the notion that such a long-term problem might need immediate (short-term) action, even if the rising trend eases for a decade or two!Let me end my review with a final example which many of us experienced first-hand. The economic crash of 2008 resulting from a housing bubble was said to have taken analysts by surprise. However, the crash came as a “surprise” because many of them had closed their eyes to warning signs (p. 22). Quite a few people saw the said signs, but their opinions were dismissed, a classic case of skirting inconvenient truths! Similarly, the Japanese attack on Pearl Harbor was a “surprise” because people missed or dismissed a large number of warning signs (p. 413)."
144,159420411X,http://goodreads.com/user/show/5325549-glenn-myers,4,"A difficult, challenging book, rather like (as we learn) its subject. Prediction, it turns out, in many fields, is best done by people who analyse a wide range of data, who seek consensus with others, who prefer to assign a range of probabilities rather than suggest one outcome, and who revise their predictions in the light of their success: in other words, boring, geeky people. It's done worst by those who trust their gut feelings, are simplistic, and appear on TV, and who when wrong, blame it on unusual circumstances. Most predictions (done by either group) are over-confident, economic prediction being only slightly less successful than forecasting earthquakes (which is useless). The best, chewed-over, consensual economic forecasts of annual GDP growth are massively, catastrophically wrong a third of the time. If you want them to be right 90% of the time you must assume they could be wrong on their GDP numbers by a massive plus or minus 3.2 points. So the UK's predicted (?) 2.6% this year could actually turn out as a recession, or an Asian-style 5.8%: meaningless. Once a decade, actual GDP will exceed even that range (probably through a recession) And yet the government must plan using these ridiculously wayward numbers. Weather forecasting is one of the few areas that has improved over the years. All this Nate Silver describes in deep but probably necessary detail over several areas of his particular expertise (poker, baseball, politics and the stock market), as well as a few areas that he is probably not so at home with (earthquakes and climate change). A strength of the book is the statistical heft behind his assertions; a weakness, possibly, is that by the end you feel you have read a textbook rather a work of popular science. i'm not sure that's a criticism, but you may as well know what you are letting yourself in for. Also, the book makes absolutely no concessions to a non-American audience, not even bothering to explain baseball terms or what Moneyball is or was (for example), another example perhaps of editors being useless, overworked, non-existent or cowed. What are publishers for these days? It's hard to tell.  Some conclusions I came away with:1. Most forecasts of most things should be taken with a pinch of salt.2. It's (broadly) impossible or at the least not worth trying to beat the stock market by trading or stock-picking; an index tracker will put you within a cat's whisker of the best over the long term.3. Be very suspicious of silver bullets4. Not all forecasts are terrible. Some people still make big money on sports gambling, for example, ideally (and this book will help greatly) get an understanding of the forecasting record in a particular field before you commit yourself to it; some are much more reliable than others. For an example of the last point, Nate Silver has compared polling data with outcomes and one of his data points says that if someone is 20 points ahead in the polls, six months out from (in this case a senatorial) election, there is a 93% chance they will win. As I write this, the vote for Scottish independence from the UK is six months in the future. The no-to-independence people are (I think) about 20 points ahead. Nate Silver himself has said the yes camp has 'almost no chance' of winning, and he correctly predicted every state in the last presidential election. in Scotland the talk is of undecideds, shifting sands, late surges. Don't believe it. The bookies will give you odds of 4-1 for a no vote, so a little punt on a yes (if you betted, which I don't) would be a nice earner.An insightful, even indispensible, but far from easy book. "
145,159420411X,http://goodreads.com/user/show/11228-suman,3,"Even a week after completing this book, which I listened to during the less intellectually stimulating parts of my job, I'm not entirely sure what to make of ""Signal and the Noise"". On one hand, it is a thoughtfully written book that applies the statistical mindset to a variety of interesting problems -- including poker, earthquake forecasting, and baseball player projections -- and I imagine its quite eye-opening to an audience that normally doesn't view the world probabilistically. Even more inspiring is the fact that Nate Silver, who could have easily written a stupid money-grabbing book on the heels of the perfect 2012 election forecast, instead took the time to write a truthful book on the subject of statistics. On the other hand, there are some glaring flaws in statistical methodology that merit some attention. Moreover, certain topics were more superficially treated and probably could have been omitted.The book is ambitious, covering topics such as election forecasting, baseball player performance prediction, earthquake prediction, weather forecasting, terrorism, the great recession, climate science, and some other topics that don't come to mind. On average, the subjects with which Nate Silver had previous familiarity were better. There was a detailed description about a poker hand that's about as good an analysis on poker that I have read anywhere, and the chapters on baseball and political forecasting were quite good. Some others, such as terrorism, were weaker. The worst section, in my opinion, was about the great recession, because many of the assumptions made (such as the people working for rating agencies were not painfully stupid) by Nate were not backed up by evidence and were discredited in other books, such as Michael Lewis' ""The Big Short"". Most others were somewhere in between, but I do have to give Mr. Silver credit for speaking with experts in the field to better understand the problem at hand. At the very least, there are a variety of fun facts (did you know that a pre-printing press book cost, on average, $20k?) that are interesting to learn about.More maddening, however, was the intellectual rigor applied to the book. To be fair, Nate Silver is vastly more intellectually rigorous than say, a Malcolm Gladwell, in that he openly states his methodology and doesn't overstate his predictions. Unfortunately, however, he seems to be both contradictory and a bit of a doctrinaire. It is well known within the statistical community that extrapolation leads to bad predictions. Nate Silver rightly takes scientists to task for using non-robust extrapolation methods to predict infection rates of H1N1 after an outbreak at Fort Dix in 1976. Later in the book, however, Nate Silver himself predicts catastrophic terrorist attacks (exceeding 1M dead) by extrapolating frequency of current attacks.The part that bothered me more, however, is that the author is so unabashedly Bayesian that he fails to acknowledge triumphs of the other major branch of statistics: Frequentist statistics. While Bayesian thinking seems quite natural as a poker player, applications such as spam filtering, speech recognition rely on statistical models that apply frequentist approaches. To overlook these advances (which would be much harder using a Bayesian approach) and champion only Bayesian thinking is really the only bit of overreach I noticed in the book.Given what I've written, I would say, on the balance, I am happy to have read/listened to tSatN. I think a lay reader has a lot more to gain than a statistician, but there's enough intellectual meat in there to keep most everyone satisfied."
146,159420411X,http://goodreads.com/user/show/5425279-kevin,4,"I would give this about 3.5 stars. I read this book for a book club at work. Whether it was a good choice for that is another story, but overall, I was surprised by how much I liked this book. I thought it was going to be boring, irritating, partisan, pseudo-science used to try to bolster positions on topics that I didn’t really agree with. He tends to have rather agnostic positions about the topics he discusses, which makes the book decent. Books like this tend to seem to be one of two flavors: journalists like Malcolm Gladwell (the Tipping Point) who are good at presenting information and have books that aggregate a bunch of research and anecdotes into a very accessible book and researchers like Steven Levitt (Freakonomics) who write about their own research and tie in some related topics. Nate Silver is not an academic doing rigorous research like Levitt, but thinks he is knowledgeable enough to deduce whether ideas are good or bad or strong or weak. His background is in internet poker, baseball statistics, and aggregating political election polls. These do require some intimacy with math, but nothing that most math majors (provided they have some background in probability and statistics) wouldn’t have a great deal of familiarity with.I did like a number of parts of the book. Many of the things it talked about or highlighted are topics that I’ve either seen before or are not terribly unobvious, but the book had some interesting stories and provided good reminders of those ideas. Some of the ideas that were most useful include: being clear what your assumptions are and distinguishing between facts and assumptions, the need for being clear with the level of uncertainty in a prediction (the river will only rise 49 feet, just short of the 51 foot levee, but the margin of error is 7 feet!)(178), overfitting can make a model look better but perform worse (167), the importance of not being married to your initial idea if better ideas come along (the notion of a person who is a hedgehog about ideas versus a fox) (53), the importance of updating models with new information to calibrate them (244), distinguishing whether a perceived anomaly is a feature or a bug (285), how sophisticated methods often cannot compensate for bad data – garbage in, garbage out (289), agreement between forecasters is not related to accuracy (382), confusing the unfamiliar with the improbable (419), and many more. There were a few topics that he beat to death that I could have had a lighter dose of. The first thing that comes to mind is earthquakes. They’re hard to predict, and more data has not made us more accurate at predicting them. He basically repeats that over and over for 33 pages. I wasn’t that enamored with the chapters that talked about his own exploits in poker and baseball statistics. It’s what he knows, but they didn’t seem as big and important as many of the other topics covered. He also starts talking about Bayes’ theorem about halfway through the book and goes on and on about it at every turn. I felt like it could have been more tactfully integrated. Overall it’s a very readable book that honestly seems to be seeking the truth, and keep you thinking about ways that you are either deceiving yourself or others regarding your estimates of what is going on. It’s not a very technical book, but it has some good content that people who deal with data in business can probably use to help make better and more honest decisions. It’s a little long for how much content it has, but it’s relatively worth it. "
147,159420411X,http://goodreads.com/user/show/1116520-remo,5,"Nate Silver’s book is interesting and well written. He has had good success predicting baseball results and political elections, and the main theme of the book is that we should use a probabilistic approach to predictions. He hates ""certainty"", and is a HUGE fan of Bayes Theorem (see my last paragraph for an explanation). Silver reviles “forecasters” who make predictions based on what they wish will happen or what sounds best and will get them on TV. He has no respect for those who refuse to adjust their predictions when new facts come in. Extreme confidence can be achieved with multiple iterations of Bayes Theorem, but should not be arrived at in a “hedgehog” fashion. Forecasters often ignore key pieces of context, often termed “out of sample,” because they had not occurred before. The book looks at many types of forecasts, such as weather, gambling, earthquakes, elections, terrorist events, chess, and economics. He discusses modeling, and how people should not “overfit” their models to match past observations, which almost always leads to bad, over-complicated models that fit the noise rather than the signal (and won’t accurately predict the future). Silver also decries excessive reliance on technology, a la computers and Big Data. Silver argues that knowledge of the underlying data is important to make good forecasts – statistical inferences are much stronger when backed up by theory or some deeper thinking about root causes. A study he cites from the Fed found that judgmental adjustments to statistical forecasting methods resulted in forecasts that were about 15 percent more accurate. Technology-enabled humans can make better forecasts.We need to be less focused on results, which have a survivor bias but doesn’t tell us how well a forecaster will do in the future. For instance, an executive at a company who saves costs by not paying insurance may look brilliant until a disaster occurs. We need to understand why someone is successful rather than just looking at past performance.In “The Poker Bubble” chapter Silver discusses the Pareto Principle of prediction, which is the old 80-20 split – you can achieve 80% results with 20% effort, and more effort buys you less results. He says the first 20% begins with having the right data, technology, and incentives. Then you can develop heuristics (rules of thumb) based on experience and common sense and some systematic process to make a forecast rather than doing so on an ad hoc basis. Absolute predictive accuracy may not be as important as relative accuracy, being better than your competitors. (I wonder if Silver moved to ESPN because the “water level” of political forecasting had risen high enough that he no longer had an edge on his competition?) Bayes Theorem is concerned with conditional probability – it tells us the probability that a theory or hypothesis is true IF some event has happened. You have a prior probability, or initial estimate, of a likelihood of something, expressed as x. A new event occurs which. You have a probability y of it occurring as a condition of the hypothesis being true. You also have a probability z of the event occurring if the hypothesis is false. The formula is xy/(xy + z[1-x]). If you think a population you are drug testing has .5% users, and the test is 99% accurate, then the odds someone who tests positive is a user is .99x.005/(.99x.005+.01[.995])=.33=33%."
148,159420411X,http://goodreads.com/user/show/3677821-pete,4,"The Signal and the Noise (2012) by Nate Silver is a book on forecasting, statistics and analysis. Silver writes the popular New York Times blog 538 that focuses on US electoral forecasting. The book is also full of information about sport statistics, betting, using computers to play chess and other subjects. Silver also looks at the rise of Bayesian statistics as compared to a frequentalist approach. The first section starts off by looking at political forecasting by pundits and others. Here, people who ignore the betting odds and the polls are just putting forward what they want to happen. Silver quotes Tetlock’s Expert Political Opinion which shows how little we can predict about political events in general. Silver describes how his models can beat the betting odds but only slightly. Silver then looks at baseball and covers much of the ground that the excellent Moneyball covers. But Silver extends on that book and points out that today the performance models and scouts and their judgement have been melded. Weather forecasting is then described and the remarkable accuracy of a few days of weather forecasting is explained. The improvements in hurricane forecasting are well described. Silver then contrasts weather forecasting the endless failing search for some predictive skill in forecasting earthquakes. Silver then looks at the models and the forecasts for infectious disease and describes the challenges with them. The next chapter looks at how some people make money in sports betting which Silver describes as using large amounts of information and computing power to beat betting spreads by a few percent. Silver then shifts to looking at how computers were created that could win at chess and Jeopardy and then again puts forward his view that computers coupled with experts can improve over raw odds by a few percent. Continuing looking at probabilistic games the online poker bubble is described. Shifting back to looking at economics Silver looks at economics where he points out that quite a few people had pointed out that there was a housing bubble in the US but there were incentives for many people to keep the bubble going and when it collapsed being with the herd was safer than making a call that there was a problem and being wrong. The book has a really good discussion of the efficient market hypothesis and it’s varying degrees of application. Silver looks at climate predictions where he misses points made by others on the lack of data compared to weather models and also he completely misinterprets ‘hide the decline’ from the climategate emails as being about current temperature rather than hiding data that was relevant to paleoclimatology. Following that there is a chapter on surprise attacks that looks at Pearl Harbour and terrorism where he puts forward model similar to those on earthquakes that looks at the frequency of attacks that kill varying numbers of people. Finally Silver describes why he thinks we should be Bayesian, or Foxlike, shifting and changing our predictions and making them using varying data sources and respecting the odds. He also points out that making predictions and seeing where your own biases lay can improve your understanding. The book is really good. It’s a little long and a chapter or two could have been removed but the message that we should be Bayesian is put forward in an informative, fun way. "
149,159420411X,http://goodreads.com/user/show/6340193-katie,3,"I expected a lot from this book and was underwhelmed. While Nate Silver's common sense (data should drive predictions, not politics) was refreshingly novel in interviews I saw and reviews I read, rather than a revolutionary and fascinating read the book is more of a comprehensive, thorough, and at times dull guidebook to the prevalence of uncertainty when it comes to making predictions about the real world in just about every conceivable context. The occasional perceived dullness of the book is probably my fault -- I have absolutely less than zero interest in gambling or wagering of any kind, and Silver often states that he's a proponent of wagering on one's forecasts. For the same reason, however, it was gratifying to see my gut instincts about investing, gambling, weather forecasting, predictions about the economy, and other topics are pretty much in line with the facts: the vast majority of people can't beat the stock market in the long run or make a living playing poker (even though most are smart enough to know some people can win, and this is where the trouble starts), economists are way more uncertain than they let on, local weatherpeople care more about ratings than accuracy, and scientists agree more about the causes of climate change than the media lets on. There were many interesting things about this book, and everything is incredibly accessible and clearly explained (in fact, I wish there had been more math -- Silver includes only a single equation, Bayes's theorem, and explained it so well that I think he could have easily gone further). However, I felt it was too much of a basic overview and summary to be truly memorable, even for someone without much knowledge about statistics or the numerous fields addressed (elections and politics; seismology; epidemiology; meteorology; the economy, the stock market, and the recent financial crisis; baseball statistics; poker and gambling; climate change; and terrorism and security). Having read Michael Lewis's work, I feel I could have skimmed several chapters and not missed much. The main things I walked away from this book thinking about was that prediction is an important part of moving science forward, and while advances have been made in information accessibility, ""there isn't any more truth in the world than there was before the Internet or the printing press. Most of the data is just noise, as most of the universe is filled with empty space"" (250). And while they have the potential to be immensely helpful, more powerful computers aren't necessarily the answer to better predictions either, because predictions based on data without theory, context, or human insight are often meaningless. And finally, it's important to close the gap between what we know and what we think we know by considering just how much we don't know before trying to make any predictions about it."
150,159420411X,http://goodreads.com/user/show/35215676-deilann,4,"Originally posted on SpecFic Junkie.I'd been meaning to read this book for a while. Nate Silver talking about predictions, big data, and our limitations sounded so up my alley I couldn't even. But then I forgot and read other things. And can I say how glad I am that I picked it up? The mathematics in the book was pretty easy to grok and the ideas laid out are definitely worth perusing. It's also a pretty simple read, for not feeling entirely popsci. (Although, let's be honest: it's popsci.)Nate Silver, who came to fame mostly for predicting the 2012 elections much better than most other people, covers a variety of topics where we've been working at making better predictions as our technology improves. Some of the highlights include weather, earthquakes, politics, economics, climate change, and terror attacks. There's also a good deal about sports and gambling, which makes sense, considering Nate Silver got a lot of his start doing baseball predictions.The book is basically a big proponent of Bayesian thinking. And while sometimes Bayesian logic makes my head hurt, I'm not about to say that it's not a powerful tool. And I definitely think more people should be aware of it. Nate Silver also looks heavily into how our biases can impact what signal we find (true or false) and how we share our prediction with others (frex, do we explain our level of uncertainty?).I found the section on weather and earthquakes incredibly fascinating, which probably isn't surprising, considering I live in the San Francisco Bay Area. Weather is one of the few areas where more computing power and data has actually helped us create more accurate forecasts. And they're highly accurate, even if the commercial forecasts so many of us see aren't. The section on earthquakes was pretty depressing. (We're not likely to be able to accurately predict specific earthquakes ever.)The sections on politics and economics, however, were kind of terrifying. Basically, no one, not even political scientists are any good at political prediction. And economics is almost worse. Most economists can't predict a recession, even when we're in one. But instead of just pointing out those chilling facts, Nate Silver provides good reasons why this is the case, arming the reader with better ways to view those kinds of predictions.Perhaps my favorite thing about the book was the emphasis on how changing your mind is not weakness. It's the right thing to do when faced with evidence that contradicts your model of the world. Just sticking to your guns and going down in a blaze of glory doesn't help you... or anyone else.Another thing I appreciated was the sheer number of footnotes. Pretty much everything was cited.[citation needed]Oh, and the chess parts were awesome."
151,159420411X,http://goodreads.com/user/show/1266077-stephen-gallup,4,"There's no denying that mankind has, to a very great extent, taken charge of the world and adapted it to suit our purposes. Because we've been able to interpret data and apply that knowledge to achieve our wishes, we sometimes imagine we've become masters of our fates. Nevertheless, quite often our best experts in any given subject make fools of themselves while giving information to the rest of us that's worse than useless. Nate Silver's book talks about how that happens. (Essentially, he says, the causes are (a) psychological biases, (b) misguided incentives, and (c) flawed statistical thinking.)First a sampling of egregiously wrong predictions, made with great assurance by established subject matter experts:A strain of flu virus diagnosed on a U.S. military base in 1976 was thought to be highly infectious because it had spread rapidly among the soldiers there. Dire predictions of an epidemic ensued, prompting a crash program to develop a vaccine and begin mass inoculations. Numerous people developed fatal complications from the hastily-produced vaccine. Meanwhile, no cases of that flu variant were reported outside the base. It turned out to be less virulent than other strains, and had spread on the base only due to the unusually close proximity in which the soldiers lived.A renowned geophysicist warned that not one but a whole series of magnitude 9-plus earthquakes would hit Peru in 1981. This terrified the people of that country and wreaked havoc on tourism and property values. There was no earthquake.Moving to events more of us remember, about ten years ago, focusing on evidence that supported “a story about the world as we would like it to be, not how it really is,” all the ratings agencies gave their AAA score to thousands of very iffy mortgage-backed securities, at least 28% of which subsequently defaulted—a rate more than 200 times higher than predicted. In other words, “trillions of dollars in investments that were rated as being almost completely safe turned out to be almost completely unsafe.” The result was a financial meltdown of epic proportions.(This reminds me of the dot-com crash that occurred a few years earlier, and especially of the cheerleading by ""techno-utopian advocate"" George Gilder, who proclaimed stocks like Globalstar would make investors fabulously rich. Instead, people who trusted him went broke.) In addition to the false prophesies, there have also been failures to predict major events of which there had been ample warnings (the Pearl Harbor and 911 attacks, for example, or on the positive side of the ledger the collapse of the Soviet Union). In those cases, the indicators were simply overlooked, because no one seriously thought such an event was credible.(And if Silver were writing this book in 2017, he would likely also use the illustration discussed in this article: The new level of dysfunction in Washington DC is due to the fact that nobody in government seriously ""expected to be in this situation"" (i.e., with Trump as president). The article concludes, ""If we didn't expect to be where we are today, how on Earth can we know where we'll be tomorrow?"" As bloggers like to say, read it all.)If we are as smart as we think we are, how can such mistakes be possible, especially in a field like economics where the government alone generates some 45,000 economic indicators? Over an 18-year period, predictions about the economy have been drastically off the mark one-third of the time. Forecasts often fail to ""predict"" recessions even after the recessions are already under way.Silver assures us that ""economists aren't unique in this regard. Results like these are the rule."" The problem is that most experts don't acknowledge the degree of uncertainty present in every forecast. They are dealing with processes that are ""irreducibly complex."" And given all the data available, how are they to choose among all those shifting factors and interrelationships to find the ones that truly have an effect? What's an indicator and what's a coincidence? Despite conventional wisdom, the league that wins the Super Bowl has no connection with the direction of the stock market or the GDP. Worse, indicators known to have been important in the past (e.g., housing prices) may suddenly become less reliable (e.g., when the government takes steps that artificially inflate those prices).It's better to try to make predictions than to give up, but at the same time ""if you can't make a good prediction, it is often harmful to pretend that you can."" The experts need to communicate the risks inherent in their forecasts. Complex models (say, a graph with many data points) can give very precise answers, but those answers aren't necessarily meaningful. Thinking they are meaningful may lead to overconfidence. Overconfidence may then lead to arrogance and intolerance of people who don't accept the same conclusion—not to mention eventual frustration with a world that does not behave as expected. Alternatively, someone may perceive a pattern and then look for data that confirms the pattern—while (perhaps subconsciously) ignoring data that does not. Humans have evolved to look for patterns instinctively. It's a survival skill, but we are ""terribly selective"" about the information we notice and react to, and consequently we can see patterns when none are present.What is the true pattern—the signal—and what's just noise? (These terms come from electrical engineering, but they work in any context.)""When we can't fit a square peg into a round hold, we'll usually blame the peg--when sometimes it's the rigidity of our thinking that accounts for our failure to accommodate it.""Successful gamblers are different. They don’t believe in sure things, focusing instead on probabilities. They place bets only when their estimate of the odds is different from the odds on offer.In the second half of the book, Silver moves into analysis of how any of us might tackle the challenge of making a useful prediction. If everything in the universe is governed by orderly rules (which we like to believe is true), we need a way of defining the limits of our knowledge of those rules. Silver explains Bayes’s theorem, which is a formula for computing the probability of a proposition being true.To pick an example that means something to me (since my first wife died of breast cancer in her early forties), it is known that:1) The probability of a woman developing breast cancer in her forties is 1.4%.2) The probability of a mammogram falsely indicating breast cancer is 10%.3) If she does have breast cancer, the probability of a mammogram detecting it is 75%.Plugging these values into the formula shows that the probability of a woman in her forties having breast cancer, even after a positive mammogram, is still slightly less than 10%. That number is low because the initial risk factor for her age group is even lower.Note that even when the probability of something is low, if it’s sufficiently devastating we still have to address it.Of course, there are many questions in the real world that have no firm values to plug into the formula. We have our beliefs, which are largely subjective, but making predictions based on those beliefs is the way to test them. This is the way science moves forward. Silver imagines a society that functions like Intrade, with everyone advertising the odds they give to various propositions (President wins reelection: 55%, stock market crash: 10%, etc.). In this society, when two people meet, and have different forecasts, they have to reach a consensus and revise their forecasts to match, or else they should be happy to place bets. “What good is a prediction if you aren’t willing to put money on it?”Near the end, Silver delves into the bad-faith argument we as a society are having about global warming. As in so many other fields, the science is exceedingly complex and hard to model accurately. Proponents of the theory find ""when we advance confident claims and they fail to come to fruition, this constitutes powerful evidence against out hypothesis."" Nevertheless, they make confident claims anyway, sometimes in hopes of being persuasive (but again, this works only if they are right). Global temperatures did not rise from 2001 through 2011. Is that significant? Only time will tell. But in the meantime, the cause of arresting global warming has been taken up by partisan politicians who see it as a means of imposing new regulations on society—and opposed by other partisans with their own priorities, such as preserving a healthy economy and minimizing graft. Silver’s advice to climate scientists is to ""withdraw from the street fight and avoid crossing the Rubicon from science into politics."" I fear it may be too late for that.Silver’s book goes into rather exhaustive detail with multiple subjects. My personal interest in some of those subjects (baseball and poker, for example) is minimal, and my time is limited. So I don’t claim to have absorbed everything he says. Even so, this gave me a lot to ponder, and even dovetailed with a work of fiction I was reading at the same time to prompt a blog post. This is important material, of value to all of us."
152,159420411X,http://goodreads.com/user/show/18448455-adam-hewitt,3,"An enjoyable look at why we’re so bad at thinking probabilistically, and how we’d all benefit if we were better at it. I’ve been a fan of Silver’s since stumbling across his FiveThirtyEight blog back before the 2008 US election, which I’ve read ever since. This book moves on from political polling and forecasting to other areas where the data points are even more chaotic and imperfect, from weather forecasting to terrorism, and also looks at the benefits of thinking probabilistically as far as things like poker and financial markets are concerned.Silver’s book is basically an argument for us all to adopt Bayesian thinking about probability, which requires a subjective estimate of an event’s ‘prior probability’ which then gets 'updated' through a neat formula as new information comes to light or circumstances occur. The idea itself is fine, and becoming more and more widespread in the social sciences, Silver tells us – but considering its centrality to his book, I didn’t feel he did enough to explain how one goes about arriving at a prior probability in the first place. He often ends up with a precise percentage of something’s probability via a Bayesian calculation, but when you go back to look at the numbers he used to arrive at it, they seem purely arbitrary. They are meant to be subjective – that’s part of the point – but it doesn’t seem very scientific, sometimes. His explanation of this central point takes up just a few sentences.There are numerous enjoyable chapters here, but sometimes you get a sense that we’re just taking a canter through a few of Silver’s own interests – baseball, poker, the stock market – rather than reading something that’s a complete coherent whole. When he starts trying to take the methods onto more sensitive and weighty matters – terrorism, climate change, predicting earthquakes – it’s all very interesting, but it’s far from clear that the message goes much beyond ‘be sceptical and be open-minded’. Thinking back, I'm not sure the chapters on baseball and chess especially added all that much. Silver's also got a funny habit, which I can't decide is endearing or annoying, of giving us a shot of personal info about the people he interviews for the book (the meal they ate, their hairstyle, their mannerisms). Sometimes this fits in with the occasional biographical and anecdotal style. Other times, it's too jarring a change in tone and style, and comes over a bit amateurish.There was plenty of good stuff in here, and little to actively disagree with – plus, within a few sentences, Silver’s book references Steven Pinker’s ‘The Better Angels of our Nature’, which is an easy way to earn brownie points from me! But I can’t say I was completely gripped or bowled over by it."
153,159420411X,http://goodreads.com/user/show/54518540-cam-lidstone,3,"The chapter on poker is excellent which every fan of the game should read.Overall, it's a pretty good pop-stats boo, albeit too long and discursive; ironically a tad too much noise."
154,159420411X,http://goodreads.com/user/show/3436890-dennis-boccippio,4,"As with some others on Goodreads, I found this book a little hard to rate, thinking it a ""3.5"" and opting for a 4 star rating from an ""E for Effort"" standpoint. Part of this is high expectations on my part based on affinity for Silver's FiveThirtyEight election prediction work.The book is well researched and covers a nicely diverse array of example topics, including but not limited to economics, betting, sports, weather, climate, earthquakes and terrorism. The diversity keeps the interest going. A challenge here is that few of the examples were unfamiliar to me; ironically as the book is ultimately about Bayesian inference, there may be a little bit of a Bayesian thing going on relative to those most likely to buy/read and those most likely to have prior exposure and be left wanting more. The same thinking might suggest that the book is targeted more towards readers attracted by Silver's political forecasts than those with a wonkish or professional interest in prediction itself.For the latter, Silver redeems by offering something hard to find in similar popular literature, a high level synthesis across both realms and disciplines in prediction. A contrast with Kahneman's Thinking, Fast and Slow and Surowiecki's The Wisdom of Crowds (both of which Silver draws from) helps illustrate: While these two books are by no means peers (Kahneman's represents a lifetime of scholarship, Surowiecki's is more management faddish), as books, both suffer a bit from ""the curse of knowledge"" - the authors' over familiarity with the often contradictory details leaves the reader rudderless on how to apply the findings in practice.Silver, instead, takes a first step towards synthesis. This is welcome, although occasionally questions do arise about the formal correctness of mixing and matching themes and findings from very the different predictive methods (regression, classification, physics based modeling, simulation, etc) covered in the book. Absent a unifying framework to relate these methods (Silver is clearly an applied forecaster rather than a theoretician) the reader must rely on his claims to authority by experience (as well as the depth of research indicated by heavy citation) in trusting the synthesis and recommendations.Overall, Silver ends up on the positive side of the trust ledger sheet, and even for readers already familiar with the topical examples, he provides enough additional color, as well as thought provoking commentary, to make it all worthwhile."
155,159420411X,http://goodreads.com/user/show/57901-jerzy,3,"So, I'm a statistician with a fair bit of experience in the kind of modeling Silver talks about.I really liked how Silver gives broad sketches of several fields that involve making predictions, and characterizes what it is about those where we can (and do) make predictions well vs. those where we can't (and don't). It's useful to think about how (un)predictable some things inherently are, and it's practical for deciding whether or not to work on predicting them :) or, for that matter, on how to prepare for worst-case scenarios.But I can't support his fanboy-like promotion of ""Bayes's theorem"" as the solution to all ills. I certainly use Bayesian statistics myself in my work, and I agree it has potential to be used more often in many fields! *But* I don't think it helps anyone when he gives a warm fuzzy summary and claims it's the best statistical inference paradigm, without really explaining what that means or how it relates to the alternatives. I'd prefer it if people would think hard about each problem at hand and pick the right tool for the job, not ""pick sides"" over which mindset is uniformly ""best.""Compounding the problem, throughout the book, Silver says things like ""Bayes's theorem tells us..."" and I'm rather disappointed by the sloppiness. Bayes' theorem is just a simple formula using conditional probabilities. Bayesian statistics is an approach to inference and set of methods for doing statistics (which involves Bayes' theorem but also so much more). Bayesian thinking seems to be a recently-popular buzzword that basically just describes the fact that people update their knowledge as they get more data (though it's not necessarily provable that we do this in a way analogous to Bayes' theorem). These are three very distinct things, and Silver mashes them all up carelessly. It's not easy but certainly possible to keep them straight.Finally, two minor dislikes:1) Some sections were slow reading because I Just. Don't. Care. about sports statistics or poker, both of which made up a sizable portion of his illustrative allegories and case studies. I understand that other people love this stuff; I'd just prefer to see more scientific examples, like the earthquake or weather prediction chapters.2) The writing style feels like his first draft was too dry and the editors told him to add more personal details. So once in a while, the explanation of an idea is interrupted with a semi-jarring ""...as Dr So-and-so told me over a sushi lunch..."" that never quite seems to fit the flow of the book."
156,159420411X,http://goodreads.com/user/show/436346-brett,4,"I think it is likely that the Signal and the Noise may represent Nate Silver's cultural high water mark in his career. This book was published shortly after the 2012 elections, where Silver once again predicted with great accuracy the results of the vast majority of Senate races around the country, as well as the presidential election. As a longtime, if occasional, reader of his blog 538, both as an independent blog and later when it moved to the New York Times, I've always liked and respected Silver's work, and his efforts to stamp out the endless parade of silly and demonstrably untrue predictions that dominate so much of political punditry.Since the publication of the book, Silver has left the NYT and started his own website, so far to so-so reviews. He's also started appearing as a guest on television shows, both about sports and politics. To say the least, TV appearances are not Silver's strong suit. At any rate, that is tangential to whether or not the Signal and the Noise is very good. It is.Silver lays out an idea at the beginning of the book that there are two types of thinkers: hedgehogs and foxes. Hedgehogs, he says, have essentially one big idea that they believe to be true and read into their predictions. Foxes do not have a single unifying ideological idea, but do have opinions on many smaller matters. Silver contends that we should try to be like the foxes, and then follows his own advice throughout the book, hopscotching from one topic to the next but not hewing closely to any overarching ideological theme, except for the idea that we should seek to harness data when possible to help us make more accurate predictions.One way I know that this book is good--or at least that it contains some interesting insights--is that I've found myself talking about different chapters to different people on many occasions. The weather chapter in particular is a goldmine for dinner conversation, though there are several others of note as well (gambling on poker, earthquakes). The prose, if not exactly exhilarating, is perfectly acceptable, and this book is written for a mass audience, so while there is some jargon, it doesn't overwhelm the lay reader.If nothing else, Silver is doing us all a great favor by slowly helping, I hope, to bring about the end of careers for people like Bill Kristol, Richard Cohen, and the late David Broder. Perhaps someday being perpetually wrong on television about politics will have some consequences. Silver is helping us get there."
157,159420411X,http://goodreads.com/user/show/4167912-luke,5,"I had a feeling I would like this book based on my interest in baseball, economics, math, chess, and general fascination with the nature of prediction. I had no idea that all the topics, not just the ones I bargained for, would be totally engrossing.The chapters on weather prediction, earthquakes, diseases, and even politics and disaster prevention were just as interesting, if not moreso due to both their subtle differences and commonalities from the topics I was interested in, as well as their far-reaching implications.What Silver has done here has taken an extreme breadth of topics related to the world of prediction, and done a full rundown. From games like poker to very serious topics like Pearl Harbor and 9/11, anything that involves uncertainty calls for prediction. But each topic is handled with care to its specifics. You can't just take what you know about baseball prediction (data rich, easily measured initial conditions) and apply it to the weather prediction (data-rich, hard to measure initial conditions) or earthquake prediction (data-poor, easy to measure initial conditions- but are they signal or noise?).I love how Silver takes no shortcuts and eschews easy answers. He draws his conclusions for good reasons, not because they fit into his theories. He is rigorous with his adherence to the adage ""The truth of a theory is in its ability to predict"" (my words, not his) and he grades the success of prognosticators based on their results, regardless if they are Liberal/Conservative, Believer/Skeptic, Stat Nerd/From-The-Gut Scout. And in the absence of enough data (such as with Climate Predictions) or clear enough indicators to detect signal from noise (such as with economic forecasting) he does the next best thing: grading predictions on their process and acknowledgment of what they don't know and what they do.Common modes of failure (simple models, such as SIR in disease prediction and overfit models, such as recency bias in sports betting) are discussed as well as (the not-so-)common modes of success (most notably the combination of computing and human pattern-matching used in Weather Forecasts), and each example, interview, graph, and guess is as interesting as the last.This book made me want to become a professional prognosticator or work on a team to enable such activities. Or, failing that, use some of what I've learned from the tendencies of the sports-betting world to take advantage in my next NCAA College Basketball Bracket.Wonderful book that absolutely everyone should read. 5 stars and a 1 new fan of Nate Silver's."
158,159420411X,http://goodreads.com/user/show/4876444-particle-person,4,"This was a very engaging read. It seems odd to say I was staying up late to finish a book about statistical forecasting, but that is what happened. I don't think I would recommend this book to everyone, though. If you are a person who is interested in the details of how the world works, and you don't mind quantitative thinking (not math, per se — mostly interpreting graphs and thinking about what numbers MEAN) then you will like this. James Gleick fans, this is your kind of book. The approach Silver takes is to essentially give a lot of case studies of forecasts and examine either what went wrong or what worked well, or sometimes both. He looks at weather prediction, the collapse of the housing bubble (that section was particularly well-written), whether it is possible to beat the stock market (mostly not), election forecasting, baseball scouting, and climate change forecasts. The section on climate change is the most problematic, and one of the climate scientists he interviewed for that chapter, Michael Mann, objected to Silver's discussion of arguments made by a well-known climate change skeptic, Scott Armstrong, a marketing professor at the University of Pennsylvania. (Armstrong is an expert on statistical prediction but proudly denies any knowledge of the climate or of physics. Nonetheless he feels qualified to weigh in.)Mann's article on the climate chapter is here:http://www.huffingtonpost.com/michael...Returning to the book, Silver ties all these case studies together with the increasingly well-known statistical formula, Bayes' theorem. Bayes' theorem says that if you have some initial guess about the probability a thing is true, and you have some new evidence, you can make an improved guess by factoring in the evidence. This is, in fact, how science has always worked, at least in the idealized case. Silver nearly makes Bayes' theorem a creed, rather than a calculating device. His thesis is that everything in life has some uncertainty, not NECESSARILY because it's intrinsically uncertain (in the way quantum mechanics is, for instance) but simply because we always have incomplete information to work from. How can you make an EXACT prediction about the world starting from a state of INEXACT knowledge about the way the world is now? It's impossible. Therefore you should explicitly account for your biases and your uncertainty, which is what Bayes' theorem does. Or so Silver argues."
159,159420411X,http://goodreads.com/user/show/5885099-angus-mcfarlane,5,"I loved this book. I was not initially too keen on another science book club book on statistics, having recently read the drunkards walk. However, this one had little overlap with many of the stories often told about probability. Rather, Nate Silver tells some of his own story and those of other forecasters about the emotional and logical errors we make when trying to predict the future. The short summary is that we are usually far too confident in our forecasting abilities - a little knowledge is a dangerous thing - so we are often stung when these fail.The key theme of the book, the need to use our knowledge to 'condition' the probability we assign to prediction (Bayesian statistics), is repeatedly emphasized, and would be monotonous if not for the fascinating range of relevant topics Silver uses to illustrate the point. Weather gets a good rap whereas climate forecasting is given a marginal pass. The financial experts who missed the GFC a clear fail; economics generally gets a pass but only to the extent that the market is essentially unbeatable. Earthquake prediction likewise is largely almost cause apart from long term baseline trends. Silver's personal interest revolves around baseball, electoral prediction and (briefly) online poker. The former and latter cases illustrate the tension between human 'instinct' and numerical analysis, in each case the balance is about even. Political forecasting is an area Silver has lead significant improvement in, but in the US, media experts appear to be next to useless!Each of the focus areas show how easily we can look at the noise - unhelpful information - and miss the signal that meaningfully identifies what will happen next. For me, this is relevant both personally and professionally. Like everyone, there is so much stuff out there (re Internet) that knowing what the facts are is difficult, thus clear decisions (about say climate change or politics) are not easy to make objectively. Professionally, there is increasing emphasis on getting more data, but with this comes the problem of management, processing, and correctly using it - and the risk that wrong or obfuscating conclusions will prevent good decisions. As if the geological and economic uncertainty associated with mining is not high enough already!I think most people who read this will get something useful from it, but then, that may just be my overconfident bias speaking. Best to read it for yourself."
160,159420411X,http://goodreads.com/user/show/45921922-thiago-marzag-o,4,Too much baseball. But still great.
161,159420411X,http://goodreads.com/user/show/13145295-james,5,"Nate Silver's explanation of how statistical analysis works, how it doesn't, why people are terrible at assessing risk, and most of all how we should view probability and predictions. It is also, in large part, a cry for intellectual honesty among the punditocracy and a withering critique of said pundits. His explication of Bayes v. Fisher is worth the price of the e-book all by itself and should be required reading for anyone who ever plans to cite statistics in defense of an argument or make a prediction based on statistical correlation. Simply put, everything you learned in Stats 101 is probably wrong--most particularly the improbability of seemingly improbable coincidences. If there is not an underlying theory to explain a correlation, then there's a pretty good chance the correlation is merely coincidence--noise as Silver would have it. For those of us interested in national security, it has enormous implications for the way we assess branches and sequels. Will Iran develop a nuclear weapon, and if so, will they develop an actual employment capability? Ignore anyone who answers yes or no--the outcomes are possible futures with a probability of occurring that needs to be updated whenever new information comes to light. Bayes's theorem is inherently subjective, but Silver's point is that our supposedly objective measures are subjective too in the way that we choose data and build our models. John Lott's statistical cherry picking on guns and crime (which Silver mentions only in passing) is no less subjective than beginning a Bayesian assessment with a 20% likelihood (or 5% or 50%--really anything that is not 0 or 100 based) that a particular event will occur. The key is to replace that starting assumption with more solid evidence when it comes to light and not alter the model to fit the desired conclusions. The 2012 election provided a pretty good test of his hypothesis, and we can now replace the starting variable in the equation, ""will Nate Silver correctly predict the next election"" with a much higher probability based on his track record. Of course, Silver would point out that a sample group of 2 is extremely small and should not be the basis of any prediction. "
162,159420411X,http://goodreads.com/user/show/207954-scott,4,"Remarkably well-researched and fairly exhaustive, you can tell Nate Silver is a guy who obsesses over details, understanding at a gut level that's where the devil hides.He manages to hit the sweet spot of including just enough math -- reading carefully, you can follow his logic, and understand how he comes to the conclusions he arrives at. He has just enough ego to be opinionated, but you can tell he's in it for the math; what really motivates him is the epistemological rush you get when the real world seems to behave somewhat like your mathematical description of it.Most to my liking was his probabilistic approach to the world: rather than seeing the world in terms of discoverable absolute truths, he sees everything as probabilities -- always leaving a door open to things that don't fit models, or are highly improbable (but not impossible). That, and his empirical Bayesian approach to arriving closer to Truth (trial and error, using prediction as a measure of the validity of your hypothesis) makes him a man very close to my own heart. He sees the world in terms of a poker game, I see it as a craps game; but I think his metaphor may be more apt, insofar as it includes the dynamic feedback relation between humans and the world they think they perceive. Fundamentally, we both see the world as a chaotic place where you make bets based on what you think you know, and hope for the best: this basic philosophical agreement made me feel like I was chatting with a smart, generous close friend.A point he explicitly addresses later on (but could have used more discussion in the first chapter, on the causes of the financial crisis of 2008) is that first and foremost, a prediction needs to be made in good faith. If the motivation for making a prediction has little or nothing to do with what you think (or don't think) will happen in the future, then it's worthless. This is a very useful criteria for immediately evaluating a forecast -- once you start to examine predictions, especially in the political and economic fields, and begin to tease out the motivations behind those making the predictions, you can discount a lot of noise without ever needing to even look at a chart or grab a calculator."
163,159420411X,http://goodreads.com/user/show/25542229-james-mcreader,3,"A great magazine article expanded into a bookThis book is (and feels like) a really good magazine article that got filled out with some fluff and repetition until it became a book.It's good and clever - some interesting and very easy to approach explanations of why and how predictions fail - but it is painfully clear that this was about 1/2 the length and someone (the author? publishers?) decided it really needed to be longer to justify selling it. It's enjoyable, nonetheless.In terms of the content, this filling-out mostly comes in the form of repetition. Given that the gist of the book is to explain with examples the ways in which and why predictions fail, extra examples are actually quite useful in most places. The author wants to sound like a genius (let's be honest - he's building a personal brand) and so at times does stretch the facts a little bit too much to illustrate a point (especially when he strays away from his niche interest areas and moves into fields like finance that seem a little beyond him) and some of his statistics seem to have been cherry-picked to prove a correlation for the sake of the articlebook.One thing you become aware of is that the examples in the book reflect an extremely narrow area of topics that happen to be the author's specialist areas (baseball, poker and US politics plus a couple of thinly-researched areas bolted on to make it rounder) and so despite the author's best efforts to come across as a generalist expert, you do end up with the feeling that he's expanded his hobbies into a book. Things that he's familiar with are written about at length (we get a very detailed discussion of baseball batting averages without him ever explaining what a batting average is...) whilst other topics get some more wordy explanations. Nothing wrong with that per se, but it detracts from the intended impression of some overarching enlightening guide to predictions!If you expect to read this and learn how to build a hedge fund or bet on sports, you'll be disappointed.If you want to read what is, effectively, an expanded and entertaining version of what should have been a Wired article, with some clever insights that make you think about predictions and forecasts, you'll like it."
164,159420411X,http://goodreads.com/user/show/1618357-jay,4,"Ends up being a pop science book, though with denser writing and better footnotes than most. Given the title and subject, I was expecting a cross between pop science and business, since the topic – better forecasting – has an obvious use in both areas. The book consists of a large number of example topics where forecasting has been applied, including baseball (surprisingly little of baseball given Silver’s background), politics, weather (especially hurricanes), earthquakes, terrorism, swine flu, mammogram accuracy, chess, poker, and climate. By my opinion, this is a nice, broad set of examples, and keeps the interest up. He shares a weakness with most current pop-science books in that the author is a character in his book, but in this case it is restricted to mostly baseball and poker stories and a few interviews. This isn’t as bad as others I’ve read recently. Mostly the advice is the kind of advice I’d expect from such a book – think probabilistically, keep making forecasts to get better,… Silver doesn’t really go into examples of doing this in life. Overall, as a numbers guy, I enjoyed this book – these are good forecasting stories.A few anecdotes I noted:-	In baseball, the Moneyball era is long over. Instead of ignoring stats as in the pre-Moneyball era, now everyone, including scouts, use a volume of data along with personal judgment that can’t be measured. The value of the human came back into the equation.-	Weathermen are getting better, and hurricane forecasts are much better than the past given better models and more compute power. Better forecasts here are very valuable. As a business book, I look for value in providing new things to think of, or providing actionable advice. For me, “The Signal and the Noise” provided little actionable advice that wasn’t obvious (disclosure: my favorite book in college was called “Long Range Forecasting – From Crystal Ball to Computer”, so I’m probably not an average reader for this!). But it did provide some reminders of ways to think of probability that I haven’t had on a front burner, so it appeals on that measure."
165,159420411X,http://goodreads.com/user/show/553969-peter,5,"Let me say first, what I say about most non-fiction books I read: There was too much set-up and not enough conclusion. Nate Silver did a masterful job talking about fundamental predictability (found between the signal and noise) in many diverse industries and world events. But he offered little in the way of ""what's next"" and how we could all get better at predicting.BUT. What a great book! As I read through it, I gained an appreciation for just how smart Nate is, and why he was able to make such accurate forecasts at FiveThirtyEight. The guy understands system dynamics, probability, and what makes for good data ... in a lot of very different fields, in depth. And he understands and articulated well that there is still a lot of art to predictions - data models often need to be merged with experienced intuition.There were many points in the book where he surprised me with his analysis and insight. For example: local weather forecasters over-predict rain because there is a strong incentive to get sunny days right more than rainy days (no one wants their picnic rained on, but a sunny day instead of rain is a happy bonus). But the national weather forecasts folks are spot on. He had some charts detailing the exact discrepancies. And when he introduced Bayes's Theorem (a new concept for me), I was amazed at how well it could mathematically explain the probabilities we all felt on 9/11 after the first plane hit. We started with a low probability .. ""Could this be a terrorist attack? No. It must have been an accident."" But after the second plane hit we all rapidly switched to a very high probability .. ""There is no doubt this was a planned attack."" He discussed the housing crash, climate change, earthquake predictions, basketball and baseball modeling, the stock market, poker, the logic process of the Deep Blue chess program, and more. In each case, he helps you see where the signal was in the model, where the noise was, and how it either succeeded or failed in making accurate predictions.Anyone interested in logic, data analysis, or data-based predictions will very much enjoy this book. Great read."
166,159420411X,http://goodreads.com/user/show/314043-zach,3,"The challenge for Nate Silver in writing this book was to write something about probability, statistics, and prediction that is accessible to a wide, non-specialist audience without watering it down too much for the mathematically-inclined core of people who are naturally going to be most interested in his work. That is, I suspect a great many people who were eager to read this book probably have some training in related fields. I am one of those people, and I found the book to be a little too far on the generalist side of things. Anyone who already understands the term ""Bayesian updating"" already has a grasp of the major theme of the book. It reads like breezy journalism about forecasting when Silver is quite clearly capable of something deeper. He briefly covers many topics that are not particularly obscure examples of forecasting successes and failures without adding much insight - baseball, earthquakes, gambling, financial markets, climate change, terrorism, etc.One thing that will unite almost all readers of this book is that Silver really goes off the rails when he tries to talk about climate change. He wraps up an interesting and counterintuitive discussion about the successes and failures of 90s temperature models by bemoaning the fact that climate scientists have inserted them into our broken political discourse. He suggests the alternative is to...keep working on their climate models to make them more accurate. Forgive me if I think that's a horrible attitude. The conclusion to the chapter reads like whiny blog post and I'm surprised that Silver and his editor let it get published. All that said, fans of FiveThirtyEight with only a hazy understanding of probability and statistics will probably enjoy this book. It's well-written and, importantly for a 450 page book, very well-edited. Don't read this on an e-ink Kindle, by the way, because there are charts on nearly every page and they would be rendered unreadable. The hardcover has excellent typesetting and is printed on decent paper stock, so it's not a bad purchase."
167,159420411X,http://goodreads.com/user/show/5451889-emily,0,"This is a fascinating book, but Nate Silver is totally wrong about book history (Eisenstein strikes again!), and I think he is also wrong in some of his assumptions about ancient and medieval cultures and their attitudes towards knowledge and writing. There are also a few inaccuracies in the text (e.g., the definition of ""white noise"") or the graphics (e.g., the supposed state of the chess board after Kasparov's first three moves and Deep Blue's first two moves in Game 1 of their 1997 series), but this is only to be expected in a book covering so many different topics, as my own dissertation shows, and since I would guess that this book will be around long enough to go into a second edition, most of those errors will hopefully be corrected at that stage. That said, I also found some other things that bothered me: Silver could sometimes afford to be a little more careful to lay out the definitions of some technical terms early on, especially when discussing poker and baseball, with which he has particular expertise; I think he is also playing a little fast and loose with things on occasion (why should the number of patents issued necessarily be taken as an indication of scientific progress, even indirectly? could we unpack this a little, especially since he returns to this idea a couple of times?); and even if he is using the word accurately, he is still dramatically and obnoxiously overusing the word ""literally."" In sum, the book would have profited from more zealous editorial oversight. Nevertheless, as I wrote at first, this is a very interesting book (especially for those of us who like to have a little math and science in our lives), and the material is well worth some thought. Of course, the very fact that I say so bears out his point that we tend to accept evidence that supports our prior assumptions about the world, since the kind of thinking that he is advocating is similar to one of the first things that I can remember being conscious of having learned from my mother. Go forth and ponder!"
168,159420411X,http://goodreads.com/user/show/14061411-david,4,"Like most people I know, I’ve been consuming more news than any other time in my life. This appetite and occasional frustration with traditional (read: wild guesses without accountability) political commentary led me to fivethirtyeight, the site the author founded. I recently saw this book for a dollar at Goodwill and got right into it.Silver does a fine job explaining the pitfalls of forecasting, drawing from a variety of examples and interspersing anecdotes of his curious rise in journalism and the sports world. He also provides several chapters worth of describing applications of Bayes' Theorem. Make no mistake, while this book is plenty smart, it’s breezy reading. Think Michael Lewis or Freakonomics. I like to keep several books in my reading rotation at any given time and they reduce to two categories: heavy books requiring close attention read in the morning with coffee and light reading I can breeze through with a beer in the evening. This book falls squarely in the latter category. Many of the illustrations of the book will entertain anyone with casual interest in economics or stats but also baseball, poker, meteorology, and politics. Silver clearly spells out what the reader should take away from his examples and lays out rules of thumb for application. I especially appreciated his ideas on overfitting, the idea that one may provide “an overly specific solution to general problem” and while the solution matches the problem at hand, it fails to provide insight when pushed to more general applications. These big spelled out principles made me feel like I was reading a business book in a corporate environment where buzzwords abound. Surely, this book has been required reading for some organization attempting to get itself up to speed on big data. Maybe. I don’t generally like books in that category because they remind me of TedTalks but the association is forgiven here. Nate Silver’s awesome and everyone should read this book and/or fiverthirtyeight."
169,159420411X,http://goodreads.com/user/show/7052966-sofia,5,"Forecasting is one of my favourite subjects. This book covers several aspects to this: weather, politics (elections), the stock market, chess, sports (scouting and results), poker, terrorism, earthquakes.. and of course Bayes theorem, with a rather more entertaining and memorable example than the usual breast cancer case study of false positives/negatives. I loved it and will be buying the physical book to refer to (I read this on Kindle). This is not just a theoretical essay, he refers to studies and includes graphs and yet it’s quite practical and readable. "
170,159420411X,http://goodreads.com/user/show/7029980-katie-doing-dewey,5,"I love data. I thought I should just get that geeky admission out of the way since my love of this book is largely based on my love of data and the cool things we can do with it. Nate Silver is an awesome statistician best known for his model that has done a great job predicting election winners. In this book, he looks at a lot of incredibly interesting topics from public issues to sports and policy decisions to natural disasters while analyzing the common mistakes people make when making predictions about the future.I enjoyed this book so much I almost don't know where to start. It's on a topic I love; using machine learning to make predictions or classifications has always fascinated me. It's just so cool that we can get computers to make decisions too complex for us to make ourselves! And Nate Silver does an amazing job explaining this fascinating topic in an interesting and accessible way without talking down to the reader. He's clearly passionate about the topic. He draws on a rich array of sources to make his points, not shying away from the use of large words or history lessons. And the examples are relevant to everyone (especially everyone in the US, but worldwide too). Combined with the graphs and the analogies, the examples make some pretty complex topics much easier to understand.I loved how information rich the book was. I learned about many different topics and couldn't stop scribbling down fun facts to use at the beginning of this review. The pop culture references and allusions to the author's own life also made the book more engaging and made reading the book feel like having a casual but intelligent conversation with a friend. Obviously I'm a little biased in my opinion because of my prior interest in this topic, but I think anyone who loves non-fiction should definitely pick up this book.This review first published on Doing Dewey."
171,159420411X,http://goodreads.com/user/show/64121873-suze-ninh,1,"I'm going to give this book the benefit of the doubt and say that it's not for me. I couldn't focus while reading most of the book, and now that I've finished, can't even remember what this book was really about. First of all, it's disorganized. I got very excited after reading the introduction that the book would be separated into chapters discussing predictions in multiple different fields, including the economics, baseball, earthquakes etc. And then the chapter was further divided into bold catchy titles, which really felt like just a way to separate paragraphs more nicely. Somehow, all the chapters seemed to kinda merge together. The author kept circling back to the housing bubble in the economics chapter, and then maybe again in the chapter about investment? (see I can't even remember). Secondly, what is the point of this book again? Nate Silver kept referring to the elusive Bayes theorem, which at no point during this book did he explicitly explain the equation (maybe he did in the footnotes but I just couldn't get that far). He indicated that half of the book explains why so many predictions fail, which really all boils down to everything is so complicated and we are overconfident in our ability to predict, and the other half attempts to explain how to make better predictions. The second half is even more disappointment than the first. It's the same idea of how complicated prediction is, and the solution really is to think of everything in probability because you're never 100% correct. Did I really need to read a 500 page book to figure that out? I guess it's up to your judgement. Disclaimer: Also I may be biased since this book was published during his fame after the 2008 election success, and I read it after his most recent failure prediction in the 2016 election. "
172,159420411X,http://goodreads.com/user/show/3322570-moira-burke,5,"It's rare for a pop sci book to make me feel like I've become a better scientist for having read it. Computational social scientist friends: this book's for you. Like Duncan Watts's ""Everything is Obvious,"" Nate Silver's book covers science communication and perception; in this case, Silver focuses on challenges scientists face making forecasts and biases among audiences interpreting the results. He covers the economy, hurricanes, earthquakes, terrorist attacks, baseball, chess, and climate change, noting the specific conditions that make that kind of forecasting problem different from others. It's compelling like Michael Lewis's books without lionizing individual characters, but still interesting and occasionally funny. His example walkthrough of Bayes's Theorem is whether your spouse is cheating on you, conditional on you finding a pair of unknown underwear at home. It's an example you're unlikely to forget how to re-calculate. Silver also covers Tetlock's categorization of scientists as ""foxes"" or ""hedgehogs;"" the former gather data from lots of sources and understand the uncertainty in their forecasts, while the latter make clear, rigid, oversimplified predictions and are more likely to be invited on TV.Selected quotes:""Overfitting: The most important scientific problem you've never heard of""""Big, bold, hedgehog-like predictions are more likely to get you on television."" ""Harry Truman famously demanded a 'one-handed economist,' frustrated that the foxes in his administration couldn't give him an unqualified answer.""""Terrorist organizations are fundamentally weak and unstable: as is supposedly true of new restaurants, 90 percent of them fail within the first year."""
173,159420411X,http://goodreads.com/user/show/78372466-marco,3,"Some parts were truly enjoyable and interesting. Some others were inaccurate and/or questionable. Others still were severely underwhelming. The good: 1) Bayesian thinking is explained and a case through the book is made for why we could apply it into our own lives. 2) Chapters like weather forecasting, earthquakes, and others on predictions are fascinating and convincing IMO. 3) Some considerations about how wrong pundits and economists are were truly convincing and refreshing -very nice to read. The bad: Chapter 8, where Silver explains Bayesian thinking, is a crucial point of the book. I wish he did not simplify Fisher's take on statistical significance to a point of caricature. Everyone knows that the p-value is in trouble, but not (only) for the reason Silver expounded. Plus, Fisher's statistics and Bayesian reasoning are not necessarily mutually exclusive. But it seemed to me that Silver felt the urge to trash other approaches to make his point ""Bayesian thinking is the best thing ever"" Other than scratching the surface, the book does not address questions of epistemology or ontology at all. That is to say: the systems that Silver talks about show extremely wide differences in their very nature: poker, earthquakes, sports, political polling, terrorism.. abide to radically different laws of causality and Bayesian thinking only goes so far. It can be applied with some success to some systems but with far less success to other systems. I wish he used much more caution in making such distinctions instead of solving all problems with his probabilistic brush.In short: Some good ideas there, a lot of patting on the back for how good he is, some unnecessary anecdotes about sport IMO, not epistemologically rigorous as I would prefer."
174,159420411X,http://goodreads.com/user/show/126714-angela,5,"I loved this book for the same reason that I loved The Predictioneer's Game: Using the Logic of Brazen Self-Interest to See and Shape the Future and Data, A Love Story: How I Gamed Online Dating to Meet My Match. All of them lie at the intersection of math/statistics/data/modeling and psychology/sociology. While I still think Nate Silver is brilliant, after reading this book I have a better understanding of just how terrible at modeling and predicting so many other people are (& I'm talking about people who are paid for making predictions) & why it's so easy for him to look that much more brilliant by comparison.What it's really about is the use & abuse of statistics & data--what sorts of things can be predicted (short and/or long term) and which kind of can't, the most common mistakes people make when they try to use data to make predictions, and how living in the age of ""big data"" actually puts us more at risk for bad predictions.The great tragedy of this book is that the people who are most likely to read it are probably the people who already have a pretty decent understanding of data/statistics, and the people who have a less-good understanding of those things are probably the people who would most benefit from reading it."
175,159420411X,http://goodreads.com/user/show/1036274-rmn,3,"Highly recommended for anyone who believes anything ""pundits"" on tv say or anyone who doesn't look beyond the media's packaged narratives of the world. Unfortunately, those who need to read this book the most are exactly the ones who won't. And while there are hundreds of books out there like this pointing to the misuses of data and overconfidence in forecasters' own abilities to forecast, this is one of the most accessible.Silver's main points are:1. We now have a lot of data2. That data can be used to more efficiently solve problems and build forecastsbut:3. Many people refuse to use the data when forecasting and prefer to go with ""gut feel""4. Even people who use the data frequently use the wrong data5. Even people who use the right data can still interpret it incorrectly6. Even people who use the right data to build the right models still need to realize that they are just producing probabilities and not factsPutting everything together, it is possible to exploit inefficiencies in markets/businesses through data analysis/forecasting but those forecasts and models need to be constantly tweaked and tested as new information is learned (shout out to Thomas Bayes) and it is necessary to understand that at the end of the day even the best forecast is still a guess.The book is decently written, though for some reason got less interesting as it went on. Still, it's a quick easy read about forecasting for those who don't want to get bogged down in advanced statistics."
176,159420411X,http://goodreads.com/user/show/24629154-david-mario-mendiola,4,"When I read it, I couldn't put it down. I had several sleepy days at work as I was reading this. It had great insights into the problems and difficulties of prediction and every time I checked an endnote, it led to more cool discussions. I hope to read this again more thoroughly in the future. "
177,159420411X,http://goodreads.com/user/show/2490942-david,3,"This book was a disappointment to me. It starts off discussing the great housing market bubble, which was better described in The Big Short. Then goes into baseball statistical analysis, which was better described in Moneyball. Then he reviews earthquake prediction, which I already knew was unpredictable. Later, he talks about the rise of TV poker and internet poker, which would have been better not described at all (since it is of little interest to me). There were a few interesting matters on which he touched, such as how weather forecasters often have a “wet bias”, so the book wasn’t a total loss. And it reviewed some important underpinning concepts about forecasting and statistics that are worth remembering, but are often forgotten. My two favorites: (1) The instinctual shortcut we take when we have too much information is to engage with it selectively, picking out the parts we like and ignoring the remainder, making allies with those who have made the same choices, and enemies of the rest. Unless we work actively to become aware of the biases we introduce, the returns to additional information may be minimal or diminishing. (2) There is a tendency in our planning to confuse the unfamiliar with the improbable. The contingency we have not considered seriously looks strange, and what looks strange is thought improbable. What is improbable need not be considered seriously."
178,159420411X,http://goodreads.com/user/show/4368960-omayma,0,"I started this book in July, paused a while then finished it in November, on the day Trump was elected, an important event for Nate Silver in specific whose preconditions are intensively followed and circulated. At the end of the book Nate Silver wrote ""Distinguishing the signal from the noise requires both scientific knowledge and self-knowledge: the serenity to accept the things we cannot predict, the courage to predict the things we can, and the wisdom to know the difference.""And I think this is an important statement in a time where prediction/predictive analytic is a hype!In general, I liked the book and I liked how Nate Silver identified common mistakes and misconceptions that people ignore. I liked the Foxes/Hedgehogs labels for people who talk about events and predictions especially in media and data journalism. I believe his categorization makes sense, considering those who appear all the time in media and labelled as ""experts"".I also liked how he tried to simplify the Bayesian analysis with simple examples for non-statisticians. He highlighted challenging areas such as weather, earthquake, and also general ones such as Baseball or real estate. Maybe I just couldn't relate to Baseball details, as I am not so familiar with it.But overall, I believe the book covered lots of concepts, it gets people to think about critical issues and think realistically about predictions."
179,159420411X,http://goodreads.com/user/show/43874316-dylan-edwards,5,"I really enjoy Nate Silver’s way of thinking about prediction and the statistical tools we use to grade them. He prioritizes objectivity and critical thinking about data (at least in writing, I haven’t read enough of FiveThirtyEight to know if this is true of his journalism) above all else. For someone who never really understood how predictions work or how important probability is in so many different professions, I found this book to be a great introduction to these fields. "
180,159420411X,http://goodreads.com/user/show/4696922-ushan,2,"Nate Silver is a statistician and a poker player who is famous for accurately predicting election results. This fairly disorganized book is a popular overview of statistical prediction. Silver is a Bayesian and anti-frequentist; he thinks that a prediction based only on past data without a theory to provide prior probabilities will be inaccurate because it will overfit the past data. Meteorologists can now predict the landfall of hurricanes within a radius of 100 miles days in advance, which helped with the evacuation of New Orleans before Katrina; before modern supercomputers, the radius was 350 miles. For earthquakes, we are as much in the dark as centuries before. The reason for the difference is that atmosphere is well-understood, and earth's crust isn't. Likewise, we cannot predict which strain of swine flu or bird flu will be deadly: the 2009 flu pandemic killed fewer than 1/1000 as many people as the 1918 flu pandemic, but no one could have told this in advance. There are also chapters on marginally relevant subjects such as computer chess and poker; they distract from the main story. As told in many other books on the late-2000s financial crisis, the rating agencies mispredicted the default rate of mortgages during the housing bubble by a factor of hundreds because they were paid by the issuers, and had no incentive to be accurate."
181,159420411X,http://goodreads.com/user/show/11951948-paul,3,"In this modern complex world, being able to predict the future is an art, but Nate Silver has been one of the few who has taken these black arts from art to science. His credentials include baseball prediction, and more importantly the 2008 US election where he correctly called 49 out of the 50 states.In this book he takes us through many types of predictions and subjects, from weather to poker, stocks to climate. All the while teaching to look for the signal in the wash of noise. As most predictions fail, in particular those from experts, as people assume that confidence is equally to accuracy. It isn't. So he guides us through the principles of uncertainty and basic probability with the hope of improving our own predictions, because as our judgement improves we have an opportunity to overcome the “prediction paradox”.Prediction still is a young science. Silver shows that a finger in the air guess can be as good, and sometimes better than an experts option. The very best have a command of probability and and large dollop of humility, and a willingness to change as new facts become available.Overall this is a very readable book. There were some complicated sections, and there is masses of data and charts and graphs throughout the book. The are explained well, and fit the context of the points that he is making. Well worth reading. 3.5 stars."
182,159420411X,http://goodreads.com/user/show/7091767-mieke-mcbride,3,"I really enjoyed the chapter on Bayesian statistics and thought that chapter alone made this book worth reading. (That and several of the fantastic graphics.) But I also probably could've just read that chapter and nothing else and gotten the same amount out of this book... The main issue is that Silver and I just don't have the same basic interests-- I can only read so much about poker, chess, weather, and baseball (that said, the chess chapter did have s0me interesting anecdotes). This book also feels in need of an update. I felt this especially on the chapters on hurricanes, earthquakes, and climate change, given what all went down this summer, and then of course what to do with political forecasting, given November 2016. I've appreciated Silver's blogposts about the political forecasting, but I have to imagine a second book is coming. Also there were a few mentions of big data-- and how they can make use of Bayesian stats-- but given how much that has developed over the past years, I would like to hear more from Silver on this. I also couldn't stop thinking of Weapons of Math Destruction during this-- at what point do our forecasts themselves reenforce (or really trap us in) the status quo? Not as relevant to questions of earthquakes or chess, but seems important when thinking of political polls."
183,159420411X,http://goodreads.com/user/show/185285-shaz-rasul,5,"Politics are perhaps the least discussed set of examples in Silver's ""The Signal and the Noise: Why So Many Predictions fail - but some don't"" which is wholly a paen to the art of prediction and forecasting, perhaps even a love-letter to Bayes' Law.Silver's examples run the gamut from the things that he has expertise in (baseball, gambling, political polling) to those that he doesn't (climate, weather, seismology, stock markets, chess, public health, terrorism, etcetera).In all, Silver is able to find the crux of the central issues that challenge successful predictions and illuminate it either through his own exposition or through an interview with an expert.Because of it's variety, and because of the way that Silver keeps brining the readers back to the core concepts, it's an engrossing read. What works best is the interplay between the topics that Silver has first-hand knowledge of and that those that he is approaching more abstractly, more objectively, as they interweave in a way that keeps the reader from drifting away.Well worth-reading, but if you're approaching this as a political junkie only, you'll be disappointed by the limited amount of pages that politics per se gets."
184,159420411X,http://goodreads.com/user/show/1652278-susan,4,"I read this because I heard Silver on the radio before the election talking about how wrong so many electoral polls are and how right his was in 2008. And he predicted that Obama would win, which I found comforting because the news was not very comforting. The book discusses prediction, not only in terms of what it takes to have good predictions, but in terms of possibility of prediction. 100 years ago weather prediction was all but impossible; how they're pretty good. And hurricane prediction has got better and better, even in the last 20 years. Earthquake prediction, though, is another story. Right now while there's greater understanding of earthquakes, there are no reliable predictors, other than that a series of smaller quakes often predicts a bigger one to come, but even that is not always the case. I was totally bored by the baseball prediction chapter--Moneyball gave me as much info as I needed and I didn't read that but heard it discussed on the radio. However, the poker chapter was much more interesting.Definitely a worthwhile book, but like lots of popular nonfiction today, could have been a long article except that writer's don't get credited (or paid) like they do for books."
185,159420411X,http://goodreads.com/user/show/22953766-sebastian-a,4,"This is a very good book. It takes your hand and shows you with multiple examples from different angles how to look at predictions (be it by political pundits, poker players, weather forecasters, sports-betters or traders at Wall Street), what they could mean, what they definitely can not and how to evaluate how accurate they are.Of cause not all in this book is new or fantastically interesting (especially the chapters on the weather or baseball) but the topics are different enough and over a very wide range that everyone who might be slightly interested in this topic will get enough out of this book.My favorite chapters would be the ones on betting (sports, poker and the stock market) and virus epidemics.Most striking about this book is that almost none of the people who make professional predictions (on any topic) will give you a perspective on the probability of their predictions to give it context and that most media outlets take these predictions and sell them as the hard truth.This books also gives you tools at hand to evaluate given predictions -- or to be precise sharpens your general comprehension on this topic so you can say more than ""Don't trust any statistics you didn't make up yourself.""Did I mention this is a very good book?"
186,159420411X,http://goodreads.com/user/show/5942798-shana-yates,4,"A well-known and excellent book on human cognition and the ability to discern cogent information from a flood of data, and how the intersection of these topics help and hinder efforts to predict events. The book is a nice compliment to a handful of others on cognitive bias, heuristics, behavioral economics, and forecasting (see famous books by Kahneman, Tetlock, and Thaler), and though each subject area is distinct enough on its own, they bleed over into one another and understanding each, in turn, allows a fuller understanding of the rest. Silver, famous for his site FiveThirtyEight acquired by ESPN, offers a number of portraits of areas where prediction is practiced to varying success. He clearly has the most extensive knowledge of sports (especially baseball), poker, and politics -- areas where he made his name. However, I felt the book truly shined when he approached other areas like weather, earthquakes, climate change, economics, and terrorism. Using these areas that we all hear about often allowed him to highlight successful strategies to better prediction, as well as underscore the pitfalls that many ""experts"" fall for. An educational and entertaining read, well worth the time."
187,159420411X,http://goodreads.com/user/show/38576720-james-patterson,4,"This book started very slowly and I almost put it down. I am glad I didn't, because by the end I really enjoyed it. That slow start is the reason I deducted a star. A very good mix of both qualitative and quantitative analysis about all the things we try to predict, and why we are fundamentally so bad at it. Some of the ideas here were old ones but well presented, such as who a stock market model that would have generated great returns from 1960 - 1980 would have been losers from 1980-2000. Piling on to that, this book totally discredits all stock technical analysis and should be required reading for anybody who thinks that past performance is indicative of future results.Then there were ideas that were new to me, such as the fact that consensus, in many cases, may be the WORST way to get a picture of the future. The best example is economic assessments; gathering the average of a bunch of economists projections gets you...garbage. If one person predicts massive growth, and another predicts the next great depression, this is not indicative that we will have zero growth...but that's how we forecast.Recommended for all people who think they can predict the future from data."
188,159420411X,http://goodreads.com/user/show/1961462-sarah,5,"Nate Silver is an economist and statistician, who develops models to predict outcomes. He started in the baseball world and then moved to politics. He emerged on the national stage in 2008 when he (fivethrityeight.com) correctly forecast the outcome all senate races and 49 out of 50 states in the presidential race. In The Signal and The Noise, he talks about what makes good predictions in the world of Big Data and how to distinguish between valuable “signals” and distracting and misleading “noise”. He present the Bayesian theory of probability and array of issues around sorting out valuable data from the accurate but irrelevant, but for most the book he looks at fields where predictions are used; in some cases successfully and in some cases not so much. Many of the areas he investigates----baseball, politics, weather forecasting, financial markets--- are dear to my heart and I read his analysis and comments with fascination. Others, such as sports betting and earthquakes didn’t grab me as much but all in all, I was pretty caught up.This book is not for everyone and Nate Silver is definitely a wonk,. But he is my kind of wonk and I loved this book."
189,159420411X,http://goodreads.com/user/show/19131383-nick-huntington-klein,3,"A very good book on its stated topic - the realities of forecasting. The book suffers greatly when he strays from that particular topic. Very simplistic dismissals of detailed and complex arguments from top-notch intellectuals.He also has an extremely strong and often-stated opinion on the Bayesian vs. Frequentist statistics argument, which would be fine if not for the fact that everything he writes about it suggests that he does not really understand either side of the argument. He assigns a lot of good ""ways of thinking"" to Bayesianism that have nothing more than a superficial post-hoc connection to it, and often have just as much of a connection to frequentism. Mathematically, he seems to reduce Bayesianism to just Bayes's law, which could just be for the purposes of the popular book but still is misleading. He also gets frequentist statistics entirely wrong on a philosophical and operational level (starting with what appears to be an attempt at a scandalizing introduction to Fisher - haha). This misunderstanding of frequentism perhaps explains how he repeatedly uses frequentist statistics and arguments throughout the book despite his repeated condemnations of them."
190,159420411X,http://goodreads.com/user/show/3144537-adam-ford,3,"This book was disappointing, but brilliant. Silver is a public statistician, a new breed of public servant--part journalist, part scholar, part scientist, part educator. This is a much needed development in society and we will all be better off for having him around. I hope he spans dozens of imitators. But the book left me a bit wanting. It is the inherent modestness of the nature of making predictions. So much can go wrong. So much noise, so hard to find the signal. It is a bit depressing to follow Silver's survey of how poor we are a predictions in general--political pundits are singled out for not even trying to get things right (for instance). Probably the biggest thing I learned is that 90% sure means one time out of ten it won't happen. This is obvious to people who gamble or play dice games because they see it happen once out of 10 times. But for consumers of news, when an ""expert"" says there is a 90% chance of happening, we hear that as 100%. If something has a 90% chance and happens 15 times in a row, Silver revisits his model to see where he screwed up. Probabilities mean percentages."
191,159420411X,http://goodreads.com/user/show/24716402-joe-callingham,4,"Finally a general book on Bayesian reasoning that remains applicable to everyday situations without getting bogged down in the esoteric structure of the theory. Nate Silver does an excellent job at introducing some of the scientific and nonscientific areas that either heavily rely on Bayesian thinking or have been revolutionised by it. I found the chapter on trying to beat the stock market very sobering and enlightening, and will likely put to the test some of techniques outlined in the poker chapter.However, Silver does tend to repeat himself somewhat in the second half of the book, which can make someone who is aware of the caveats of theory a little bored. Additionally, while he points out the danger of overfitting models to data in an excellent way, I am surprised he does not point out the danger of exploration (which I think he falls into himself) in his chapter on terrorism. For scientists that use Bayes in their work, some of the framework Silver constructs is a little too simplistic but the novel situations in which he applies the theory it will keep you interested. Despite these shortcomings this is the best general statistics book I have ever read."
192,159420411X,http://goodreads.com/user/show/26223717-cropredy,3,"I picked up this book whilst reading ""Future Babble"" as I had enjoyed Nate Silver's ""The 538"" column in the NY Times during the 2012 election. Silver does a much better job at explaining why forecasting and prediction by so-called experts fails most of the time. I say most, because there are examples where predictions have gotten quite good (5 day weather) that Silver details. Once I started ""The Signal and the Noise"", I sent the unfinished ""Future Babble"" back to the library.Silver takes the reader through many different forecasting domains - poker, weather, baseball, stock market, earthquakes, terrorism, and more and illustrates the state of the art. As each chapter is a different problem space, the reader's interest is held. You also get, early in the book, one of the most succinct and compelling recounts of why the 2008 financial crisis happened (something that few predicted).Once done, you'll be able to discount prognosticators and realize that they mostly just like to hear themselves talk and bathe in the adulation of people who think they know something (because we all want to know the future, hence we're willing to be beguiled)"
193,159420411X,http://goodreads.com/user/show/11786941-yu,4,"Nate Silver writes in a very easy, conversational style. It's almost like sitting next to the man and listening to what he thinks about statistics. That said, the book delves into the details of a wide range of subjects to which accurate predictions are essential, and talks about the arbitrariness of the significance levels we use today. He mainly discusses the importance of it to seismology, baseball, weather reporting, poker games and terrorism. There is also a great section in which he talks about the difference between Bayesian and Fisherian probability. If you know Nate Silver for his political forecasting, this may not be the book which satisfies that interest, but the ideas that he brings up in here can be applied to any number of fields in which prediction is employed.Perhaps not quite so thought-provoking as something along the lines of Thinking, Fast and Slow, but a great read nevertheless."
194,159420411X,http://goodreads.com/user/show/418141-maire,4,"There's not much to say about this that hasn't already been said. Nate Silver has a special touch with statistical models, and he also has an uncanny ability to make it seem simple.He chooses a bunch of great predication examples to explain in detail in this book. The upside to this is that you learn a lot of fascinating things about some really random stuff (like weather prediction.) The downside to this is that it's hard to remember a lot of what you learned, because it is fairly disjointed and specific. Even if you might not come away with a useful prediction tool kit, it was a wonderful reading experience that illuminated how lots of things are done.The chapter on global warming, though, is poorly done. He forgets that there is actual science behind it and that it's not just a stats game. (Which oddly enough, he does a great job of understanding/acknowledging in several of his other chapters.) There are lots of scientists who explain the problems with this chapter online; I recommend reading their opinions after finishing this book. "
195,159420411X,http://goodreads.com/user/show/3301814-james,5,"For all the hyperbole about the book, the claim Nate Silver was the Brunel of his generation, a 34 year old Delphic oracle, I was prepared for another book that would be a fascinating magazine article that had to be stretched to meet the publishers demands and therefore into mildly interesting but incredibly repetitive. The book is firmly in the geek conquers world genre, the authors own description, but has more then enough passionate ideas to stretch comfortably in it it's extended frame. We'll written for out it illuminated with how the advent of enormous torrents of data have made predictions worse not better, how the subjectivity of forecasting is prevalent, our own biases, a pane to the weather service and Bayes. It's a book on technology, predictions, information, the economy, but at its most interesting the book is bout how we learn and what stops us from doing so. In my favourite quote from the author ""the signal is the truth. The noise is what distracts us from the truth. This is a book about the signal and the noise"". Splendid stuff."
196,159420411X,http://goodreads.com/user/show/39757508-joseph-jammal,4,"Signal and Noise is a great introduction to using probability to approach a diverse set of problems. However, the sections where Silver has greater personal experience are far stronger than the chapters where he is more of a researcher. Stand out sections include: baseball, politics and maybe finance, but weather, earthquakes and climate change are a bit forgettable. Additionally, it would have been nice if some of the probability work was explained with math in an appendix for readers that are curious about how he reached his conclusions. Nevertheless this book is a good start and very different from what I tend to read. The tone is very conversational and while it is more fun than science the underlying thesis that there is space for both modeling and qualitative approaches to understanding the world and that better results in the social sciences are achievable through multifaceted approaches is a point that I empathize with and applaud Silver for making in a book that anyone could enjoy."
197,159420411X,http://goodreads.com/user/show/3894725-john,2," I want to lie and give this three stars, but wow was it boring. I guess I've audiobooked too much of this genre of nonfiction, but I wonder if people who are rating this higher have encountered books or mid-length articles on poker, moneyball, economics, stock market, politics, or terrorism. Nate who I hoped was going to be pretty cool bores the heck out of you with massive retread of the basics in the aforementioned areas, e.g., the rating agencies's role in the WFC, and then offers only the slightest statistical/prediction insight with slight being generous in some instances. The motives of the weather media and the Bayesian breakdown did the most for me. I think he probably has a lot of interesting stuff to say, but somebody cut it or he hamstrung himself. A 250 page bleeding edge of the real world use of statistics would have rocked, instead there's sub 50 pages of that and around 500 pages of unfairly desiccated Wikipedia entries."
198,159420411X,http://goodreads.com/user/show/6200323-david-ball,5,"Wasn't sure what to expect with this book. I'd seen Silver on the Daily Show and heard about his unerring ability to predict the outcome of presidential elections, and his interest in baseball stats, but I struggled to see how how he would turn 400+ pages on stats into an engaging read (I suspect this book will be one of the most-often-bought-least-often-finished books of the year, sadly). But for those in doubt, it's definitely worth persevering; not only do you get the obligatory chapters on sports and politics, but you soon find yourself immersed into such varying topics as weather, gambling, earthquakes, chess, disease control, poker, and terrorism - you can't help be impressed with this guy's broad intellect. Did you know that you can predict the frequency and magnitude of both earthquake and terrorist events using the theory of power-limit distribution? Or that Deep Blue beat Kasparov because of a glitch in its coding. Every chapter is enlightening."
199,159420411X,http://goodreads.com/user/show/811989-douglas-summers-stay,3,"This is a book for the interested amateur about statistical prediction. Statistical prediction and machine learning are basically two ways of talking about the same thing, so I was translating everything in the book into the analogous problems in computer vision to see how it might be applied.He suggests using bayesian techniques, using a collection of independent models, using simpler models to avoid overfitting, things like that, and gives some common sense explanations of how and why they work. He used examples from the economy, from sports and poker, from terrorist attacks (frequency of attacks and number of deaths per attack make a nice straight line on a log scale) from elections and so forth.It kind of frightened me that these ideas would be new and exotic to people doing important things like evaluating housing derivatives. This is just how you have to do things if you want to get the best prediction from your data."
200,159420411X,http://goodreads.com/user/show/1983944-chris-jennings,4,"Solid analysis of predictions, probabilities and Bayesian logic in particular. At times the chapters dove a bit too deeply into the subject matter. I could have dealt with less specifics of professional poker or financial indicators of the Great Recession. But these are just my opinions. Pairing each subject matter to a specific example case study is very reminiscent of similar books by Jonah Lehrer and others, (seems like 2012 had a flood of these). It's a blueprint that obviously works with this breed of non-fiction. I was thoroughly impressed with Silver's writing style. The culmination for me was the final chapter comparing terrorist plots and earthquake predictions. Extremely thought provoking and backed up by enough data for even hardcore nerds. Easy to see why this made so many ""best of 2012"" lists. Glad I was able to squeeze it into my end of the year reading."
201,159420411X,http://goodreads.com/user/show/3856855-uchicagolaw,0,"""It is both interesting and related to many legal topics. Much of what we do in legal academia is to predict things, whether it is how courts will decide an issue to the effects of a given policy or legal choice. Practitioners make a living predicting – they advise clients on the likely effects of a course of action. Silver provides an enjoyable and interesting account of how hard prediction is, what skills and framework one should take to the task, and what the perils are."" - David A. Weisbach""Silver explains why some predictions fail and others succeed, and how to distinguish true signals from mere noise in a universe of abundant and often conflicting data."" - David Zarfes"
202,159420411X,http://goodreads.com/user/show/5581571-melissa,4,"Very interesting and easy audiobook to listen to, and also one I think would benefit from repeated listening's (every few years or so).... it's great to remind ourselves on occasion how much statistics and predictions can help us and how we can fool ourselves so easily with them also. This book contains a lot of examples, from baseball, to politics, to weather forecasting, from chess to poker to the economy....why some things are predictable with the right information and some are just really guesses that tell us nothing but what we want to see. Definitely an appropriate read in an election year and a reminder to ground myself in the 'real signal' and distinguish it from, and tune out all 'the noise' that is really just a distraction. The old GIGO (Garbage in, garbage out) for a new data rich age."
203,159420411X,http://goodreads.com/user/show/5333526-chris-bauer,5,"To be succinct, this book rocked my world.Nate Silver does a remarkable job of explaining the challenges and fallacies of the brave new world of prediction and why humans suck so badly at it.Using easy to understand examples which span everything from baseball to stocks to political elections to earthquakes the author explains the common pitfalls of how humans perceive events and try to make educated guesses about the future.The book is dense and written without any ""dumbing down""; I think the reason it took me so long to finish was the dozens of pages of footnotes which I explored while reading. Haven't done that in a LONG time.When you finish this book you should raise an eyebrow everytime you hear an ""expert"" make a prediction.I really enjoyed it and will carry many of the lessons for a while to come."
204,159420411X,http://goodreads.com/user/show/7049262-michael-nash,3,"Mediocre at best. Silver’s essential point, that knowledge production is not objective was old when Foucault was writing about it in the 60s, and could be explicated in a couple of pages. I don’t know what the other several hundred pages are there for. His other point, urging us to view the world through the lens of Bayes’ theorem as a silver bullet solution to problems of prediction, is, to anyone who has taken a rudimentary prob and stat class (which is to say most people) obvious bullshit. Bayes’ theorem is useful is some situations and not useful in others, and its conclusions are not that far removed from common sense. Still, Silver is presenting these ideas with a droll wit, and he has come up with some interesting examples and talked to some interesting people, so I can’t deny that I enjoyed the read."
205,159420411X,http://goodreads.com/user/show/14519372-gary-baughn,4,"Nate Silver's reputation rests upon his accurate predictions in nearly all of the 2008 political races. This book's thesis is that the key to predicting accurately is discerning between the noise of unhelpful or misleading information or data and the signals that can be correctly use to predict outcomes. He also says that the only true measure of any hypothesis is how well it predicts events in the future.He gives examples in many fields of endeavor, from weather to terrorist attacks to baseball. I found it interesting, and relatively easy to follow, considering there is math involved at times, and I am an English major partially because advanced algebra (at least what we called that in the '60s) is about my limit.Silver has a kind of thought process to share here that I think is generally useful, and fresh."
206,159420411X,http://goodreads.com/user/show/13009180-leslie-ann,5,"Using very approachable language, Silver discusses in depth a number of fields where predictions have improved (e.g., weather, baseball) and where they have not (the stock market, earthquakes) in order to describe how we can get better at making predictions. Essentially, Silver argues for a Bayesian approach, which involves the generation of multiple hypothetical outcomes, assignment of probabilities to those outcomes, and revision of the probabilities when new information arrives. I especially liked his argument for context when analyzing data, i.e., big data alone is not sufficient; theory is necessary to help us distinguish signal from the noise. There were times where the prose was repetitive, but the use of ""she"" as the default pronoun was a nice touch."
207,159420411X,http://goodreads.com/user/show/2910056-matt,4,"This is a great book for anyone interested in statistics and their application to the real world. Silver does an amazing job of making the underlying mathematics behind the theories he puts forth very accessible. On top of this, he has some great chapters on the fallibility of political pundits (Chapter 2) as well as one of the major problems with Wall Street and stock market predictions (Chapter 11). Some of the more interesting parts of the book were when Silver talked about weather predictions (Chapter 4) and how statistics might be able to help us better understand intelligence about terrorism (Chapter 12).While it certainly takes a long time to read this book, and it can be pretty dense at times, it is well worth the read and one that I learned enormously from. "
208,159420411X,http://goodreads.com/user/show/39634232-ben-kester,4,"Nate Silver has been a rising star in political analysis. He became known with his predictions during the 2012 election campaign, and his website 538 has taken off.This title tries to be to forecasting what Freakonomics is to economics. It's written for the general public, albeit a nerdy general public. Nate is a fine storyteller. For better or for worse, he throws Bayes theorem at everything. If you've never heard of Bayes, this is an accessible introduction to the topic with real world storylines. If you have joined the Bayesian bandwagon, you'll enjoy the practical examples even if you are annoyed with his sweeping generalizations. If you're a frequentist, there's more convincing methods for you to learn the error of your ways."
209,159420411X,http://goodreads.com/user/show/14465967-iskender-kebab,1,"Once you get beyond Nate Silver's self-aggrandizing this wouldn't be a bad starter on basic Bayesian statistics. Of course, you might think that Silver actually invented Bayesian statistics after reading this book (spoiler: he didn't).Silver ventures out from his usual corner of political forecasting into other realms, I'm guessing in hopes to show that all statistics are applicable in the same fashion? I don't know. You can see it fails disastrously if you even have an armchair scholar knowledge about the subject he is discussing.He has some good points about data reliability, I'll give him that, but still doesn't appear to grasp the basic ideas behind the gambler's fallacy or overfitting (a subject which is INCREDIBLY important in modern data analysis and is glossed over). "
210,159420411X,http://goodreads.com/user/show/3617921-david-schwan,5,"This took awhile to read. Effectively 1/3 of the book is footnotes. The author has no fear in taking on a wide range of predictions--showing us why some are bad and some are good. Bayesian theory is front and center in this book; while I can't say I completely understand it yet I'm closer to an intuitive sense of how it works then I was prior to reading the book.I had read elsewhere about the idea that making money in the financial markets was hard, the author gives very good explanation why. All of the chapters are interesting, especially those on disease, earthquakes, weather and climate change. "
211,159420411X,http://goodreads.com/user/show/4975902-paul,5,Like many others I initially learned of Nate Silver through his NY Times blogs but what really grabbed my attention was his work on the PECOTA system for baseball. As someone with only limited exposure to statistics and the science of forecasting I found this to book to be fascinating. While technical and detailed the book never read as a dry science manual. There is a lot of meat here and this book could easily justify multiple readings. Already I find myself applying the lessons taught in this book and the concept of Bayesian reasoning has tremendous potential for application on both a person and professional level. Highly recommend.
212,159420411X,http://goodreads.com/user/show/13673292-alvaro-berrios,4,"Anyone who does any type of forecasting or predicting as part of your job needs to read this book. A lot of theory mixed with a good amount of math meaning it's well balanced. But the whole purpose of the book is to change the way you think about forecasting and prediction in order to ultimately become more accurate...separating the signal from the noise.This isn't just for professionals though, there is a lot of information in this book that is applicable to day-to-day life situations. This is one of those books that I will be able to read again and again and will be able to gain new insights each time."
213,159420411X,http://goodreads.com/user/show/1859963-suhrob,5,"Yes, 5 stars. Although I was quite familiar with many of these topics (plus I long time follow his blog), one has to simply tip his hat to Silver. His material is very level headed, well researched, down to earth but smart.There is a flurry of superficial non-fiction roughly in this quadrant, riding the coattails of Kahneman/Tetlock/Taleb/etc.. This definitely stands out by its sheer reasonableness.You could nitpick here-and-there and it is definitely not a terribly deep study, but seriously, if you're going to complain here, I really don't know.Recommended for the layman as well as the practitioner. Well done. We need more stuff like this."
214,159420411X,http://goodreads.com/user/show/10831471-kit,5,"I am a big fan of Nate Silver's blog, and this, his first book, did not let me down. His blog focuses on statistics as applied to election polling and sports, but here he expands the topics much more broadly. For example, did you know that when the weather forecasters say there is a 20% chance of rain that it will rain about 5% of the time? Nate assembled the data to compute that 5% number, and he makes it look easy. The book is very readable and very informative. It should leave you more knowledgeable about statistics and many other topics."
215,159420411X,http://goodreads.com/user/show/9056961-adam,4,"Wow. Total data nerd paradise (but not so much that the uninitiated would be bored or turned off). Just like the 538 blog, Nate isn't afraid to toot his own horn, and frequently, but you forgive him because he is a fantastic communicator and can explain some immensely complicated concepts without cutting corners or getting too dry. The one thing that keeps it from being a 5 star read - it felt more like a loosely tied collection of really really good blog posts than it did a cohesive work, and the changes in subject/tempo threw me off a bit. Otherwise, the read is well worth it."
216,159420411X,http://goodreads.com/user/show/1159787-valerie,5,"Even in the title of these book, he lets you know that a prediction is just a prediction. You must evaluate your data going in, so that you can rate your output. His points about data rich fields like baseball and politics and the relationships between earthquakes and terrorism when it comes to analyzing data were amazing.I went to his blog and the latest post (not by Nate Silver) was about the signal and noise in long term unemployment forecasts. It was depressing, but well written.http://fivethirtyeight.com/features/o...I will be a regular reader."
217,159420411X,http://goodreads.com/user/show/10160415-shannon-appelcline,4,"Silver's book offers intriguing and insightful thoughts on the science of prediction. However, it's just as interesting for its glimpses into many fields that *use* prediction, from weather forecasting to economics, from sports betting to terrorist foresight. There was a ton in this book that gave me more insight into the wider world.If I had a complaint, it's that it's a bit too long. Essentially this is 13 case studies put together. If there were a few less I would have been happier with the book and (ironically) more eager to read a sequel in a few years."
218,159420411X,http://goodreads.com/user/show/2945070-thom,5,"Greatly enjoyed this book, and not just because it touches on baseball, weather, games and politics. Very savvy explanations of how too much data makes finding insights harder and not easier, and how a Bayes interpretation (properly applied) can provide the best predictions. An excellent point towards the end about how predictions should be judged on how they perform in the future rather than how they apply to past data.Well footnoted, indexed, and edited. The epub was not the fastest read - will probably buy this when the revised version comes out as a trade paperback. Recommended."
219,159420411X,http://goodreads.com/user/show/1356377-jay,4,"More information doesn't necessarily lead to better thinking. Nate Silver implores us to learn how to think probabilistically and reminds us that models are crude simplifications. It's a pretty good book but I'm having trouble coming up with any specific lessons learned to put in this review. I guess I'll go with; there's a lot of noise out there on the internet and in the media, and there's incentives for noise so take it with a grain of salt. Signal is much harder to come by. Economic forecasting is hard. "
220,159420411X,http://goodreads.com/user/show/15148585-hadi-zaman,5,"""This book is less about what we know than about the difference between what we know and what we think we know. It recommends a strategy so that we might close that gap.The virtue in thinking probabilistically is that you will force yourself to stop and smell the data—slow down, and consider the imperfections in your thinking. Over time, you should find that this makes your decision making better.What isn’t acceptable under Bayes’s theorem is to pretend that you don’t have any prior beliefs. You should work to reduce your biases, but to say you have none is a sign that you have many."""
221,159420411X,http://goodreads.com/user/show/1242652-cari,4,"A solid read on predictions, probability, and statistics, presented in such a way that even an inferior at math (such as myself) can understand. Silver beats the reader over the head with the Bayes's approach, especially in the second half, which gets repetitive and old after a while, but the book stands up well despite that. The Signal and the Noise fully deserves all the attention it has been garnering the past couple months; definitely recommended."
222,159420411X,http://goodreads.com/user/show/5079061-ivo-crnkovic-rubsamen,3,"I enjoyed Silver's book. I don't have strong feelings either way, the book wasn't incredibly educational, nor was it tremendously entertaining, but it was a bit of both. There are some interesting analyses of other people who make a living out of predictions. The best part of the book comes from how it shows all the possible areas where statistics and computer science can be applied to solve non-traditional problems."
223,159420411X,http://goodreads.com/user/show/5914661-jason-yang,5,"I really enjoyed this book; Nate Silver does a great job introducing probabilistic thinking, Bayesian reasoning and top-down vs. bottom-up modeling to a lay audience. I really appreciated his examples drawn from diverse subjects - similar to Freakonomics, but without the arbitrary/convoluted connections.Definitely a book I'd recommend to my friends. Stimulating, entertaining and thought-provoking."
224,159420411X,http://goodreads.com/user/show/11045720-michael,2,"Some good stuff in here but it doesn't hold together well. Have read better books about EMH and financial crisis, Bayes. Treatment of poker and other games is too shallow. The discussion of election prediction ignored whether systematic errors in polling were possible (eg. cell phones, demographic shifts) while simultaneously criticizing Wall Street firms for modeling financial markets with Gaussian distributions over limited data."
225,159420411X,http://goodreads.com/user/show/1862799-eduardo-omine,3,"It's a nice book, though maybe a bit too long. The book is about the importance of identifying signal from noise when making predictions, and about how our own biases / limitations can lead us to mistake noise for signal and vice-versa.I particularly enjoyed chapter 2, where Silver discusses the ideas from Isaiah Berlin's essay ""The Hedgehog and the Fox"". I would also highlight his introduction to Bayes' theorem, and chapter 11 on financial markets and bubbles."
226,159420411X,http://goodreads.com/user/show/3403389-mike,5,"I would give this twenty stars. A wonderful challenge to our use of prediction. I would recommend this book to anyone interested in current events, business, or being right. I love how Silver is unafraid to take on a challenge. He has his personal views but is quite open with them and does not seek to hide them in 'data.' I think everyone should read this especially college students and anyone fooling around w data."
227,159420411X,http://goodreads.com/user/show/3960665-ryan,3,"Enjoyable, but not really very insightful or educational. 4/5 for entertainment, 3/5 for education. A kind of tortured introduction to Bayes vs. Frequentists (without actually describing Frequentists very well). Lots of examples from crap like baseball which I really don't care about. I'd personally recommend a book like Freakonomics and a brief intro to statistics, instead.(Audible audiobook)"
228,159420411X,http://goodreads.com/user/show/5674343-danny,4,"I liked this book quite a lot (Fair warning: I'm a big fan of Silver's work on both baseball and politics). It was meaty and challenging of the way in which I tend to approach thinking about problems - which is frequently useful.It's approachable, but be prepared to think.I'll be following it up with something light so I can let some of these ideas percolate for awhile."
229,159420411X,http://goodreads.com/user/show/2090801-mark,5,"I thoroughly enjoyed this book! Silver does an excellent job looking at different forecasting situations and providing an analysis of why they work or do not work. He covers topics such as earthquakes, baseball, the stock market, weather, and elections. But, he does a great job explaining the concepts to lay people. I highly recommend this book. "
230,159420411X,http://goodreads.com/user/show/1466848-mark,5,"Against All Odds1.5 oz Bushmills Irish Whiskey1.5 oz Channing Daughters Scuttlehole Chardonnay0.5 oz Rothman & Winter Orchard Apricot0.25 oz Rhum Clement Creole ShrubbIlegal Reposado Mezcal, to rinse coupePansy flower (garnish)Pour all ingredients in mixing glass and stir with ice. Strain into a chilled coupe that has been rinsed with the Mezcal. Garnish with a pansy flower"
231,159420411X,http://goodreads.com/user/show/12333619-tom-czerwinski,2,"Poorly organized and rambles. Argumentative structure is all over the place and occasionally contradicts itself. There are some interesting anecdotes, but it goes on too long. Writing is also stodgy. If there is another book that has more instances of the word “moreover”, I’d be shocked. Overall, while there are times it makes the reader think, it was hard to follow and unpersuasive. "
232,159420411X,http://goodreads.com/user/show/7169524-michael-clifford,3,"This is an interesting guy; baseball statistician, poker player, political predictor.The book is a potpourri of prediction scenarios from weather and earthquakes to politics. The techniques he tries to explain are a little hard to follow but interesting none the less.A little bit of a tough read but a very unique read."
233,159420411X,http://goodreads.com/user/show/19005347-eugenio-gomez-acebo,2,"Nate Silver is a good forecaster, but turns out to be an average writer. The book has a lot of noise, and this makes it harder to the reader to capture its ideas effectively. Very little actionable ideas. And lots of details in irrelevant facts but very little details in the actual process of forecasting"
234,159420411X,http://goodreads.com/user/show/2313120-kyle,3,"I liked the variety of prediction topics that Nate presented in the book. It's really incredible to see how many small things can affect predictions, and how the public sphere can influence how people predict (political pundits, weather, etc). An interesting read for anyone mildly interested in statistics or why it's so hard to forecast weather correctly."
235,159420411X,http://goodreads.com/user/show/19859967-valentin,5,"The book is quite engaging offering forecasting insights in different fields like: Baseball, Weather, Chess, Poker, Earthquakes, Climate change etc. It doesn't go into much details about the exact forecasting process other than emphasizing the Bayesian approach with every chance :). There are also some general rules of thumb for doing ""good"" forecasts in each chapter. Overall an enjoyable read. "
236,159420411X,http://goodreads.com/user/show/7443356-leonore,4,Its a very interesting premise on prediction. Pretty well written and interesting.I call this in the category of Malcolm Gladwell. Interestingly one of the things he discussed was earthquakes and mentioned the Cascadia fault and the New Yorker had an in depth article about the fault this summer.Definitely worth reading and discussing.
237,159420411X,http://goodreads.com/user/show/4743119-oleksandr-zholud,5,"This text comes as a gulp of fresh air after reading Taleb. The author attempts to find out in which areas predictions work, in which don’t and why. The wide range of areas is covered: weather, seismology, sport bets, economy and political foresights. He shows both successes and failures and tries to find out what are the root causes of them. "
238,159420411X,http://goodreads.com/user/show/53405385-tee-ponsukcharoen,4,"A good book to give a general idea not to be trapped by noise and bias. I appreciated various examples but some are not resonant to me (baseball). The math about Bayes's theorem can be presented better. Anyway, well written to remind us that we are not perfect and we should not be ashamed to admit our failure and update your prediction (and way of thinking) once the new information is available."
239,159420411X,http://goodreads.com/user/show/61614884-katka-dzubakova,5,"The book focuses on models and predictions in a variety of fields (baseball, earthquakes, diseases, poker, terrorism, trading, climate change). It explains how the models work and what are their limits and possible interpretations. The book is informative and rather detailed, it does not include much humor though :) Highly recommended."
240,159420411X,http://goodreads.com/user/show/1330557-h-james,3,"I would have taken more in-depth chapters on baseball and earthquakes in place of the comparatively sketchy conclusion on terrorism, but this is nevertheless a good statistics primer for the uninitiated."
241,159420411X,http://goodreads.com/user/show/13741326-shaun-abrahamson,4,"Really enjoyed this. While Nate has fantastically deep knowledge, he does a wonderful job reaching out to experts across a variety of domains. Nate is a great storyteller and it is wonderful to understand more about his approach to building predictive models. "
242,159420411X,http://goodreads.com/user/show/1216051-doug-wells,4,"Intriguing book - Silver is brilliant and able to articulate prediction theory in an approachable way. More and more I find myself fascinated and drawn to people like him who help me see the world in different way including Malcolm Gladwell, Levitt and Dubner (Freakonomics), etc. "
243,159420411X,http://goodreads.com/user/show/2530249-phil-simon,5,"This is an outstanding book about our inability to predict the future. I'd have cut about 50 pages, but it's an exciting read rife with real world political, economic, sports, and gambling examples. Silver's an excellent writer and I can't wait for the next one."
244,159420411X,http://goodreads.com/user/show/17441763-jason,4,"Nate's kinda preaching to the choir with me, but it was well worth reading to see how he would explain Bayesian thinking to a lay audience. Very successful. (Well, my posterior probability assignment of success is now ~99%.)"
245,159420411X,http://goodreads.com/user/show/5728431-barry-mann,4,"Excellent book, fits very well with Kanneman's Thinking Fast and Slow, there are no easy ways to predict the future, the more data you collect the more the noise might overwhelm the signal, but that doesnt mean we should give up."
246,159420411X,http://goodreads.com/user/show/21996917-molly,5,"For many years, I've seen references to Bayesian statistics in scientific papers, but this was this first time I've actually seen it explained and broken down in a way that anyone could understand. Thanks, Mr. Silver!"
247,159420411X,http://goodreads.com/user/show/4899502-john-yurcisin,5,"Particularly in the world of unstructured data with buzz words like Big Data, this book is compelling as it helps give really good examples of thinking deep about the problem and removing the noise. Not an easy problem, but a good read and inspiring"
248,159420411X,http://goodreads.com/user/show/3363047-karel-baloun,3,"I love math, and I like many of the topical areas covered. Yet I had so little patience with the organization and verbose style. Needed a second draft, and answers to why I should care. And I actually liked all of his prior work."
249,159420411X,http://goodreads.com/user/show/312880-melissa,4,"Never thought I'd say this about any book but it needed a few more math equations, IMO. (I've always had trouble with word problems and scraped through biostats with pages of equations). Very good examination of the uses and limits of predictions and how we should always look deeper. "
250,159420411X,http://goodreads.com/user/show/26855073-lada,4,"Well written and argued, but perhaps because Nate Silver tells it like it is, it lacks suspense and other storytelling tricks. Then there were the application areas. Moneyball took all the reading time I was eager to devote to sports stats, so that and poker dragged a bit. "
251,159420411X,http://goodreads.com/user/show/48498481-yip-jung-hon,4,"There are many things I'd like to say about this book. There were some parts I loved, and there were some parts that I downright hated.Let's start with the basic premise of the book: that humans, being pitifully fallible creatures, misinterpret data all the time. Accurate analysis of data does not rely on the amount of data you have, but rather your ability to distinguish the 'signal' from the 'noise'. You must ensure that your data is accurate, that you do not extrapolate from outdated models, that you ensure that you adjust for changing social factors when evaluating, that you pay attention to minute changes in the dataset. This is simple enough, we all know that. Nate Silver goes on to cite examples of data being influenced by corporate profits. One example he cites is of weather agencies. To promote consumer confidence, weather agencies often narrow the scope of probabilistic predictions and skew the data towards the prediction of rain.He goes on to talk about how expert gamblers in sports evaluate the chances of a team winning. Of particular interest is the thoroughness expert gamblers invest when evaluating data. Tweets are scrutinized for hints of 'safe' or 'fast' play. In another sports-related chapter, Nate Silver delves into baseball scouts. He reveals algorithms which rank the most promising players and explains why some players are doomed not to be chosen. He meets a player who isn't really doing well but is (apparently) ranked 2nd in the algorithm he developed. And before you know it - BAM! - that player is suddenly one of the more promising ones in the field.Nate Silver also forays into polls, showing how most polls aren't accurate until a few days before the election. Polls can also be manipulated to favor a candidate winning, which then may turn public opinion against a candidate. The Wall Street crash of 2008 is also mentioned, and he puts the blame of the recession on both the greed of the banks and the failure to rate subprime mortgages accurately. There is a TLDR version of the above paragraphs I've written: namely, that there are a hundred and one ways data can be screwed up, and it requires an unusually perceptive eye to navigate the way between the numbers. Now, which data should interest us? What should we focus our attention on? Nate Silver acknowledges that there are no answers for that. He says that we can only approximate truth, and any step in the right direction inevitably requires us to be aware of the shortcomings in the predictions we make with the data. Here's my quip with this book: every single failure in prediction can be explained away with a convenient explanation. For people who read the book, you'll recognize the following:1) Subprime mortgage crisis: data is evaluated based on past US records of housing prices, failure to rate risky CDOs properly.2) Earthquakes: failure to recognize that foreshocks can lead to a bigger earthquake.3) Baseball batting averages: failure of simple algorithms to recognize patterns like arm strength, batting speed and other variables that are more subtle, which are often analyzed by stronger algorithms.There are many others. The point is that providing a simple, easy-to-go explanation for all the failures of data analysis isn't separating the signal from the noise. There a chapter which explores the intricacies of betting in sports. How do you know which team is going to win? By looking at Twitter, by looking at the weather, by looking at morale, they say. So my next question is: how do you know which data is significant? To that, the book simply shrugs its head and says, 'Careful analysis is required.' No, you can't make me slog through the pages and simply say 'careful analysis is required'. You have to give me more than that. In a way, you can't really fault the author. Predictions are inherently difficult to make and are really subject to myriad ways of misinterpreting the data. Yet if that is the case, the author shouldn't have given some vague suggestions for improvement and claim that the suggestions are advancing mankind's predictive ability or clearing away the smoke and mirrors or some other self-aggrandized claim like that. The next 7 chapters are honestly not worth reading. The solutions the author provides are not targeted at the problems he mentioned, nor are they sufficiently explained. (The chapter on chess was especially off-topic.) There was nothing really new about the concepts mentioned. Most readers already know that fitting data points into a line or narrative is bad science, Nate Silver simply gave a name to this phenomenon. Most infuriating was the fact that his 'solutions' were neither sufficiently explained nor convincing enough. I wasn't expecting clean solutions to the problems faced in data analysis, just as I don't expect clean solutions for hunger or poverty. But if the problem is so painfully obvious and the solution is lackluster, fewer words should have been spent explaining the concept. Simply reframing the same concept and putting it in another turn of phrase is not actually learning; it makes the reader 'feel' smarter when actually, they have learnt nothing new."
252,159420411X,http://goodreads.com/user/show/5921709-riccardo,3,"Interesting anecdotes, not enough statistics."
253,159420411X,http://goodreads.com/user/show/2442004-davy-buntinx,3,"Nice explanation of a few predictions that have been made through history (poker, climate change, terrorist attacks...). But I don't really know what I should take away from this."
254,159420411X,http://goodreads.com/user/show/7805608-neale,5,"Great read, Nate Silver hits a home-run with this one. So many areas of the work are applicable elsewhere. A book to come back to in a years time."
255,159420411X,http://goodreads.com/user/show/2450538-louise,0,It got too boring. I cannot go on.
256,159420411X,http://goodreads.com/user/show/2065825-kate,4,"I enjoyed this book a lot. HOWEVER, I was irritated that he used the word ""data"" as though it were singular and not plural throughout the whole book! "
257,159420411X,http://goodreads.com/user/show/7605903-miguel-eduardo,3,I was expecting more from the book. It ends up being a collection of stories that to me felt like a lot of noise with little signal.
258,159420411X,http://goodreads.com/user/show/91127-herbie,5,I read this book again in 2017 and upgraded my rating to five stars.I've become a full-on Nate Silver acolyte and I'm sorry I'm not sorry.
259,159420411X,http://goodreads.com/user/show/33410025-zaizie-rainyday,5,I never thought a book that I pick up vaguely could be this amazing. The kind of book that gives you new ideas after new ideas. 
260,159420411X,http://goodreads.com/user/show/37084383-erik,5,Refreshing take on why forecasts fail. It doesn't too much into the mathematics. Rather it discusses the methodologies behind forecasts / predictions that prove to be more successful than others.
261,159420411X,http://goodreads.com/user/show/2763419-rina,2,"So hard to read—too long, too boring, too self absorbed. Even the font was wrong. I liked the part about Bayes' Rule because STAT430 was a great class at Penn. I'll stick to fivethirtyeight. "
262,159420411X,http://goodreads.com/user/show/9328328-anna-rish,5,"This was really smart and it made me feel smart. Different from his blog in a way that I didn't expect, but that worked well for a book. "
263,159420411X,http://goodreads.com/user/show/51765593-dominic-mugavin,4,Some chapters piqued my interest more than others. Possibly could have been shorter.
264,159420411X,http://goodreads.com/user/show/19643656-andras-baneth,3,Lots of good stories and insights but way too long. A good editor would have come in handy.
265,159420411X,http://goodreads.com/user/show/26588969-alireza-rg,3,"Too many words, and too few insights. I wish Nate Silver has elaborated more on his modeling techniques."
266,159420411X,http://goodreads.com/user/show/3683417-douglas,3,Good book for stimulating your brain. A little too political towards the end.
267,159420411X,http://goodreads.com/user/show/24780071-ems,5,the only 500 pg book about stats i will now rec to everyone i know. seriously good. & miles better than any of the freakonomics/gladwell/popsci etc books i've read in this genre. 
268,159420411X,http://goodreads.com/user/show/2495892-lisa-hammond,0,couldn't finish this. nate silver is a liar and i hate him. 
269,159420411X,http://goodreads.com/user/show/6326292-boris-limpopo,5,"Silver, Nate (2012). The Signal and the Noise: Why So Many Predictions Fail-but Some Don’t. New York: The Penguin Press. 2012. ISBN 9781101595954. Pagine 545. 15,67 €Un libro molto bello, anche se tutt’altro che perfetto. Un libro pieno di eccessi, più che di difetti: Nate Silver sente il bisogno di scrivere di tutto quello che, nel tempo, lo ha interessato e appassionato – il baseball, il poker, la politica – e di tutti i campi in cui, a suo giudizio, la scienza delle previsioni ha accumulato errori e può fare progressi. Tutto questo fa di The Signal and the Noise un libro monstre, ma al tempo stesso uno dei libri più stimolanti dell’anno.Nate Silver è salito alla ribalta della cronaca in queste ultime settimane, prima per le polemiche sulle sue previsioni sull’esito delle elezioni americane, e poi per il suo trionfo. Ne ho scritto su questo blog in più occasioni, sia a proposito proprio delle elezioni americane (Nate Silver, il vincitore morale delle elezioni americane), sia discutendo dell’affidabilità delle previsioni meteorologiche private (Le previsioni dei meteorologi privati sono distorte?).Poiché leggendo il libro mi sono annotato una cinquantina di citazioni, mi asterrei dal fare una vera recensione – i temi del libro e le tesi di Silver emergono con sufficiente chiarezza dalle citazioni stesse. Ma vi regalo un paio di videoclip, dato che Silver è, secondo me, un tipo molto interessante, con una vaga rassomiglianza con Clark Kent, un prototipo del geek e del gay, il che ne ha naturalmente fatto il bersaglio dei sostenitori di Mitt Romney. [In un'intervista a The Observer (Nate Silver: it's the numbers, stupid), a Carole Cadwalladr che gli chiede «What made you more of a misfit, […] being gay or a geek?», risponde: «Probably the numbers stuff since I had that from when I was six.»]Il primo video è la presentazione del libro:Il secondo una lunga conversazione (circa un’ora) tenuta a Google pochi giorni fa:* * *Prima una curiosità: ho imparato leggendo questo libro (posizione Kindle 261) che la locuzione “information overload” è stata coniata da Alvin Toffler in Future Shock nel 1970.* * *Ecco le minacciate citazioni, che operò vi soggerisco di leggere, o almeno di scorrere(riferimento come sempre alle posizioni sul Kindle). The instinctual shortcut that we take when we have “too much information” is to engage with it selectively, picking out the parts we like and ignoring the remainder, making allies with those who have made the same choices and enemies of the rest. [104] A prediction was what the soothsayer told you; a forecast was something more like Cassius’s idea. The term forecast came from English’s Germanic roots, unlike predict, which is from Latin. Forecasting reflected the new Protestant worldliness rather than the otherworldliness of the Holy Roman Empire. [134] The human brain is quite remarkable; it can store perhaps three terabytes of information. And yet that is only about one one-millionth of the information that IBM says is now produced in the world each day. So we have to be terribly selective about the information we choose to remember. [257] The printing press changed the way in which we made mistakes. Routine errors of transcription became less common. But when there was a mistake, it would be reproduced many times over, as in the case of the Wicked Bible. [275] When you can’t state your innocence, proclaim your ignorance. [399] “The major difference between a thing that might go wrong and a thing that cannot possibly go wrong is that when a thing that cannot possibly go wrong goes wrong it usually turns out to be impossible to get at or repair,” wrote Douglas Adams in The Hitchhiker’s Guide to the Galaxy series. [478] […] even if the amount of knowledge in the world is increasing, the gap between what we know and what we think we know may be widening. [828] “When the facts change, I change my mind,” the economist John Maynard Keynes famously said. “What do you do, sir?” [1169] Olympic gymnasts peak in their teens; poets in their twenties; chess players in their thirties; applied economists in their forties, and the average age of a Fortune 500 CEO is 55. [1437] […] statheads can have their biases too. One of the most pernicious ones is to assume that if something cannot easily be quantified, it does not matter. [1626] When we can’t fit a square peg into a round hole, we’ll usually blame the peg — when sometimes it’s the rigidity of our thinking that accounts for our failure to accommodate it. Our first instinct is to place information into categories — usually a relatively small number of categories since they’ll be easier to keep track of. (Think of how the Census Bureau classifies people from hundreds of ethnic groups into just six racial categories or how thousands of artists are placed into a taxonomy of a few musical genres.) [1808] It’s not merely that there is no longer a signal amid the noise, but that the noise is being amplified. [2322] The statistical reality of accuracy isn’t necessarily the governing paradigm when it comes to commercial weather forecasting. It’s more the perception of accuracy that adds value in the eyes of the consumer. [2326] Forecasts “add value” by subtracting accuracy. [2335] With four parameters I can fit an elephant,” the mathematician John von Neumann once said of this problem. “And with five I can make him wiggle his trunk.” [2865] The government produces data on literally 45,000 economic indicators each year. Private data providers track as many as four million statistics. The temptation that some economists succumb to is to put all this data into a blender and claim that the resulting gruel is haute cuisine. There have been only eleven recessions since the end of World War II. If you have a statistical model that seeks to explain eleven outputs but has to choose from among four million inputs to do so, many of the relationships it identifies are going to be spurious. [3127] But in fact real management is mostly about managing coalitions, maintaining support for a project so it doesn’t evaporate. [3421] […] sophisticatedly simple. [3836] This is why our predictions may be more prone to failure in the era of Big Data. […] Most of the data is just noise, as most of the universe is filled with empty space. [4258-4266] We can think of these simplifications as “models,” but heuristics is the preferred term in the study of computer programming and human decision making. It comes from the same Greek root word from which we derive eureka. A heuristic approach to problem solving consists of employing rules of thumb when a deterministic solution to a problem is beyond our practical capacities. [4542] In many ways, we are our greatest technological constraint. The slow and steady march of human evolution has fallen out of step with technological progress: evolution occurs on millennial time scales, whereas processing power doubles roughly every other year. [4947] […] it is not really “artificial” intelligence if a human designed the artifice. [4972. In tema di fibre, si usa distinguere tra fibre artificiali – quelle ottenute modificando fibre naturali, come nel caso della viscosa – e fibre sintetiche – quelle ottenute per sintesi a partire dagli idrocarburi] As Arthur Conan Doyle once said, “Once you eliminate the impossible, whatever remains, no matter how improbable, must be the truth.” This is sound logic, but we have a lot of trouble distinguishing the impossible from the highly improbable and sometimes get in trouble when we try to make too fine a distinction. [5196: un punto di vista nuovo e stimolante su una citazione che è da anni un mio cavallo di battaglia. La frase è pronunciata da Sherlock Holmes] In the United States, we live in a very results-oriented society. If someone is rich or famous or beautiful, we tend to think they deserve to be those things. Often, in fact, these factors are self-reinforcing: making money begets more opportunities to make money; being famous provides someone with more ways to leverage their celebrity; standards of beauty may change with the look of a Hollywood starlet. [5519] Smith’s “invisible hand” might be thought of as a Bayesian process, in which prices are gradually updated in response to changes in supply and demand, eventually reaching some equilibrium. Or, Bayesian reasoning might be thought of as an “invisible hand” wherein we gradually update and improve our beliefs as we debate our ideas, sometimes placing bets on them when we can’t agree. Both are consensus-seeking processes that take advantage of the wisdom of crowds. [5609] “The market can stay irrational longer than you can stay solvent.” [6066: ancora Keynes] […] “the fight between order and disorder,” [6202: è di Didier Sornett] We could try to legislate our way out of the problem, but that can get tricky. If greater regulation might be called for in some cases, constraints on short-selling — which make it harder to pop bubbles — are almost certainly counter-productive. [6218] CO2 quickly circulates around the planet: emissions from a diesel truck in Qingdao will eventually affect the climate in Quito. [6285] Climate refers to the long-term equilibriums that the planet achieves; weather describes short-term deviations from it. [6501] Uncertainty in forecasts is not necessarily a reason not to act — the Yale economist William Nordhaus has argued instead that it is precisely the uncertainty in climate forecasts that compels action […] [6716] When we advance more confident claims and they fail to come to fruition, this constitutes much more powerful evidence against our hypothesis. We can’t really blame anyone for losing faith in our forecasts when this occurs; they are making the correct inference under Bayesian logic. [6855] The fundamental dilemma faced by climatologists is that global warming is a long-term problem that might require a near-term solution. [6864] In science, progress is possible. In fact, if one believes in Bayes’s theorem, scientific progress is inevitable as predictions are made and as beliefs are tested and refined. […] In politics, by contrast, we seem to be growing ever further away from consensus. The amount of polarization between the two parties in the United States House, which had narrowed from the New Deal through the 1970s, had grown by 2011 to be the worst that it had been in at least a century. […] The dysfunctional state of the American political system is the best reason to be pessimistic about our country’s future. Our scientific and technological prowess is the best reason to be optimistic. [6913-6930] To Wohlstetter, a signal is a piece of evidence that tells us something useful about our enemy’s intentions; this book thinks of a signal as an indication of the underlying truth behind a statistical or predictive problem. Wohlstetter’s definition of noise is subtly different too. Whereas I tend to use noise to mean random patterns that might easily be mistaken for signals, Wohlstetter uses it to mean the sound produced by competing signals. [6999] […] signal detection vs. signal analysis [7023] There is a tendency in our planning to confuse the unfamiliar with the improbable. The contingency we have not considered seriously looks strange; what looks strange is thought improbable; what is improbable need not be considered seriously. [7035: è una citazione di Thomas Schelling] [T]here are known knowns; there are things we know we know. We also know there are known unknowns; that is to say we know there are some things we do not know. But there are also unknown unknowns—there are things we do not know we don’t know.—Donald Rumsfeld [7060] […] detecting a terror plot is much more difficult than finding a needle in a haystack, he said, and more analogous to finding one particular needle in a pile full of needle parts. [7177] The more often you are willing to test your ideas, the sooner you can begin to avoid these problems and learn from your mistakes. Staring at the ocean and waiting for a flash of insight is how ideas are generated in the movies. In the real world, they rarely come when you are standing in place. Nor do the “big” ideas necessarily start out that way. It’s more often with small, incremental, and sometimes even accidental steps that we make progress. [7593] Sanford J. Grossman and Joseph E. Stiglitz, “On the Impossibility of Informationally Efficient Markets,” American Economic Review, 70, 3 (June 1980), pp. 393–408. http://www.math.ku.dk/kurser/2003-1/i.... [9526] A conspiracy theory might be thought of as the laziest form of signal analysis. As the Harvard professor H. L. “Skip” Gates says, “Conspiracy theories are an irresistible labor-saving device in the face of complexity.” [11798]"
270,159420411X,http://goodreads.com/user/show/11780268-parke,5,"Who do you trust? I have asked this question many times and I ask it of myself each day. In this continuation of best books of the past year I will attempt to tie together a few of the connections I have attempted to make between those who I have learned to trust about not trusting ourselves and others.I have an apology to make. For quite a while now I have been repeating a mantra on this blog, on many forums and websites, and in person to all who happened to be in range of my voice: “without data, you are just another person with an opinion”(originally attributed to Andreas Schleicher).After reading Nate Silver’s The Signal and The Noise; why so many predictions fail--but some don't, I have learned that I have been giving out bad advice. So let me rephrase my mantra, without data everything is just an opinion; with data everything may or may not be based on the incomplete assumptions of the researchers or the data interpreters.If my update sounds like a legalism, it isn’t. Silver has convinced me, through data and stories, that much of what we assume to be true because of statistics and data, isn’t. Or isn’t useful. Or if useful, it is useful because designed to create an outcome that is desired rather than an objective assessment of a problem or situation.In his conclusion to what I consider to be an invaluable book, Silver gives a list of statements:1.No investor can beat the stock market. 2. No investor can be the stock market over the long term3. No investor can beat the stock market over the long run relative to his level of risk.4. No investor can beat the stock market over the long run relative to his level of risk and accounting or his transaction cot5. No investor can beat the stock market over the long run relative to his level of risk and accounting or his transaction costs, unless he has inside information.6. Few investors beat the stock market over the long run relative to their level of risk and accounting for their transaction costs, unless they have inside information.7. It is hard to tell how many investors beat the stock market over the long run because the data is very noisy but we know that most cannot relative to their level of risk, since trading produces no net excess return but entails transaction costs, so unless you have inside information you are probably better off investing in an index fund.He then summarizes: “The first approximation—the unqualified statement that no investor can beat the stock market—seems to be extremely powerful. But by the time we get to the last one, which is full of expressions of uncertainty, we have nothing that would fit on a bumper sticker. But it is also a more complete description of the objective world.” Silver essentially asks us to distinguish between what a variety of writers have called some form of the distinction between thinking fast and slow (Daniel Kahnemans’ term as well as the title of his wonderful book). Humans are wired to want quick and easy descriptions. It is evolutionarily advantageous to do so in the world of the savannah of tens of thousands years ago. But while the wiring is still in place, the world we live in is far more complex. And a great deal of the complexity comes about due to the overwhelming amount of data that we are confronted with each day. Today, juts today, there will be more data generated than virtually all the data ever collected in the history of humankind until just a few years ago. (Sliver and many others on TED and elsewhere underscore this data on data). But we still look for the simple solution. Or if we do not look for the simplest solution, then we look for a small amount of data that will lead us to a simple solution. It is this cause and effect action on our parts which makes so many predictions seem laughably bad just a few minutes, months or years down the road. And tracing how this might be avoided is the theme of this book.But let me start with a paradox. Silver is a McArthur genius grant winner for his successful work with data. His analysis of baseball stats and presidential and congressional elections is, for those who follow this kind of stuff, legendary. And yet, the man known to be a data genius has written a cautionary tale. Anyone reading it will come away thinking that what he or she knew they knew as true may not be. In fact, it may be harmful to anyone who bets resources on a business, or an idea, or an ideology.I use the word ‘bet’ for a purpose. Silver has some great words on how professional gambles make it big or fail. And he should know. Silver spent some time as a professional gambler and reveals his secrets and his reasons for giving up this pursuit. He learns a cautionary tale and he seems to come out in favor of what another Peter Sims calls 'little bets.' Little bets assume we do not have enough knowledge or accurate data to go all in with any frequency. Instead, we should spread ideas and resources out to discover as many alternatives and solutions we can. The odds are almost always against a single big win; the odds are much better when multiple hands are played. But as Silver would point out, not always. And this uncertainty is what he demonstrates and then what he advises us to carry around with us inside our heads.We need to be suspicious of unsupported information, but we need to be almost equally suspicious of decisions made with incomplete data, and virtually all data is incomplete.Much of what Silver has to say is in part based on the tutelary god of statistics: Thomas Bayes. Silver devotes much time demonstrating how Bayes's theories on data should be the starting point for all who hope to use data to make important decisions of any sort. I am not adequately educated to go into an analysis of the statistical probabilities of Bayes’s work, but I can come to the conclusion, based on my own experiences with data, that what Bayes’ work demonstrates is that the statistics we use are largely guided by incompleteness and bias: As an empirical matter, we all have beliefs and biases, forged from some combination of our experiences, our values, our knowledge, and perhaps our political or professional agenda. One of the nice characteristic of the Bayesian perspective is that, in explicitly acknowledging that we have prior beliefs that affect how we interpret new evidence, it provides for a very good description of how we rec to changes in our world. P 258As an avowed Rortian pragmatist, I think the word ‘description’ is apt and useful. Statistics are just numbers after all. It is the stories, the descriptions, we have before the numbers are in front of us, that affect our reading of the numbers and then affect significantly the stories we then tell to others once we have ingested the numbers in a way that can never be objective or complete. We are, as Jonathan Gottschall says, story telling animals. And as Rorty so often said, it is our ability to persuade people through words that makes us what we are and determines what we do. Truth has nothing to do with it. So Silver, citing Bayes, demonstrates that the experts with data are all too often flat out wrong. Whether it is pundit on TV, or ‘experts' on the Cold War (also cited by in a good book on this topic), or in any other field, the experts are experts because we call them experts and they believe themselves to be experts. All too often, the data behind the title proves otherwise.And now it is time to say why I think this point is essential, at least for me. I have been giving out bad information based on incomplete data. Here is just one issue in which I was swayed by rhetoric rather than data. Others, in which I was flat out wrong, will be the subjects of forthcoming entries.The first is the most personal in several senses of the word. For many yeas I have been giving lectures and appearing in various media to share my supposed ‘expertise’ on the topic of college admission essays. Many of the entries on this site are devoted to questions about essays, most of them focused on essays written for highly selective colleges and universities.While I still think my training as a writer and my experience in the field permits me to say some things about writing great essays I am now not sure I have the data to continue to tell students, at least in any general way, whist will make a good essay for the purposes of gaining admission to a selective school. Why? Starting this summer I have been doing some research on responses to essays. I have posted essay on my blog, on College Confidential and on quora.com. I asked for feedback and got some surprising results. In a number of cases, the responses from ‘experts’ were almost diametrically opposed in whether the words on the page were great or terrible. Some of the experts hated the essays. And yet some of these essays were significant actors in the students be offered admission to Princeton, Yale, Harvard, and Stanford. This taught me a coupe of things. First, I thought that great essays could be interpreted as such by anyone who is supposedly an ‘expert’. Silver taught me that the experts, like political pundits or academics whose expertise are foreign affairs, frequently get things totally wrong.In addition, the responses to the essays led me to do some research on who are at the front line of gatekeeping at many colleges and universities. In the old days, the teaching faculty had a significant role to play in evaluating applicants to schools. No longer. Now the vast majority of readers of applications are recent graduates with degrees in higher ed. or in subjects often having little to do with the written word. On top of this most schools do not give an intensive training course in how to read essays. They do train people how to read applications but the essay portion is not given any intensive scrutiny in terms of testing readers on what they think a great essay might be. On top of this, the kinds of essays I like and promote are risky. In advice I gave for the US News college issue on writing essays I told students to take risks in no uncertain terms. “Take A Risk”. Period. I have been trained and have read enough from CEOs and other people at the top, that innovative risk-takers are the ones they want and the ones who will succeed at the highest level. But for me this often means stylistic risks. Creative non-fiction is the fastest growing field in creative writing departments these days and yet, if my research means anything, most of the people reading admission essays have little or no familiarity with the form. George Saunders or David Foster Wallace, or even Constance Hale write wonderful prose, but in ways that are not exactly in line with typical educational essays. So some risk takers will be hurt by being creative. And finally, now that the need is for STEM people is being foregrounded, an essay that covers an interest or passion in medical research or engineering may not need a Faulknerian touch. To sum this up, there needs to be a great deal more context about essays before any general advice is thrown out to the masses. Like Silver’s movement from a flat out assertion in step 1 to a much more muted step 7, there is no general rubric for writing an admission essay that is actually all that useful. Each essay needs to be contextualized and if not, then following a how to in some book or essay on essays may have negative consequences. I will be addressing this issue in much more detail over the coming months, but I certainly learned that my easy short answer of ‘take a risk’ sounded good in a speech or the beginning of an essay, but it did not actually have enough data behind it to prove it useful for a number of people. Lady Gaga meets Julius CaesarLet me end my review by returning to Silver: “If there is one thing that defines Americans—one thing that makes us exceptional it is our belief in Cassius’s idea that we are in control of our own fates.” The Signal and the Noise is Shakespearean in its reach. I don’t mean the prose is so lofty that plays will be made of it. Instead, Silver is aware, like Shakespeare, that hubris usually leads to a steep fall. Julius Caesar ignores data in Shakespeare’s play and things do not go well for him. Both Shakespeare and Silver underscore there is far too much data and we are far too self-deceiving for us to lead lives in control of our fates. I think he has proven his case. The last words of his book are ones I hope to keep in my head each day: to be “a little more modest about our forecasting abilities, and a little less likely to repeat our mistakes”. It is no accident that Silver ends with the word ‘mistakes’, those things we can learn from if we take the time to learn why we erred. Silver himself is modest about his many talents, but the book is one of the best of many I have read (for example, Dan Garner’s FutureBabble: Why Pundits are hedgehogs and foxes know best), that ask us to look inward at our preconceived ideas and outward at the complexities that comprise the world. "
271,159420411X,http://goodreads.com/user/show/30356-storiwa,4,this took forever to read because i had to stop and ponder after each tasty bite.
272,159420411X,http://goodreads.com/user/show/897874-bruce,4,"Nate Silver has a superpower, and has only now come to realize the fact. Sure, he can write coherently and entertainingly on arcane subject matter, but so can many a blogger. That’s nice, but nothing special. No, what distinguishes Silver from the rest of us normals is both his ability to read, manipulate, and clearly interpret data, as well as call BS on those who have maladroitly done so. Fortunately for the rest of us, he has decided to use his numeracy for good, sharing his insights with the world. We should be profoundly grateful that Silver is writing books like this instead of, say, playing Vegas and the markets like Ed Thorp.In The Signal and the Noise, Silver uses various fields of study as exemplars to distinguish good, reliable, accurate predictions from random stabs in the dark. As he puts it,[T]his book encourages readers to think carefully about the signal and the noise and to seek out forecasts that couch their predictions in percentage or probabilistic terms. They are a more honest representation of our predictive abilities. When a prediction about a complex phenomenon is expressed with a great deal of confidence, it may be a sign that the forecaster has not thought through the problem carefully, has overfit his statistical model, or is more interested in making a name for himself than in getting at the truth. (pp. 404-5) Depending on your bent, you will find fun chapters on using statistics to respectively analyze major league baseball, poker, hurricanes and earthquakes, the weather, climate change, the expected effects of economic policies, the future of the stock market, threats to national security (ranging from Pearl Harbor to 9/11), and elections. Along the way, Silver diagnoses the pitfalls and mental traps for the unwary that await the intrepid forecaster and suggesting ways the hoi polloi might be more discriminating consumers. While there’s sufficient repetition that the book bogs down at the end, some might consider the deja-view as an aide-memoire. Too, while Silver takes care to build his thesis toward inevitability over the course of the whole book, a casual reader might nonetheless reasonably pick and choose chapters around topics of interest and still come away with an improved understanding of why making informed predictions is challenging, and how to avoid GIGO.The author has plenty of explanations for poor predictive performance, including inherent bias, insufficient understanding of root causes, and our inherent inability to distinguish and account separately for meaningful information (the signal) from irrelevancies or inaccurate data (noise). His elucidation of these through examples is the best reason for reading this book. Basically, it all boils down to the fact that the universe is vast and our perceptions but limited.With respect to honing the accuracy of our educated guesses, Silver saves his strongest advocacy for 18th century philosopher-cleric Thomas Bayes’ theorem (as mathematically articulated by the astronomer Pierre Simon LaPlace). Suppose you wonder whether your partner might be cheating on you. You find a foreign pair of underwear in your sock drawer (example at pp. 244-5; see, you’re intrigued by this book already) Is this proof of your suspicions? The formula needed to determine this looks daunting: xy/[xy + z(1-x)]. If Silver unpacked how it works, his explanation didn’t reach me, but in any case here’s Bayes’ (LaPlace’s) formula put to proper use in the bedroom:Make ‘x’ your initial estimate of probability: how likely you thought it was that you were being cuckolded before finding the underwear (Silver borrows from social scientific studies and pegs this at 4%, low probability).Let ‘y’ be the percentage odds of your new discovery being evidence your partner is unfaithful (a 50-50 prospect, so 50%).Let ‘z’ be the percentage odds of a false positive; the exposure of xeno-pants has an innocent explanation (yeah, right… 5%).So Bayesians crunch their numbers and come up with a revised prediction of… 29%, a low probability, but definitely a big rise from the 4% lean toward innocence with which they began. As the author writes at pp. 248-9, “Bayes’ theorem is not any kind of magic formula…. We have to provide it with information, particularly our estimates of the prior probabilities, for it to yield useful results. However, [it] does require us to think probabilistically about the world, even when it comes to issues that we don’t like to think of as being matters of chance. This does not require us to have taken the position that the world is intrinsically, metaphysically uncertain…. Rather, Bayes’ theorem deals with epistemological uncertainty -- the limits of our knowledge.”As indicated by the chapter heading he chose, Silver’s aspiration for us is not so much to be right, as to be “less and less and less wrong.” Bayes’s formula works best when applied recursively on an ongoing basis. Your first estimate may be wildly inaccurate, but over the course of repeated trials (or observations) your expectations should gradually change to better fit the aggregated results. Want to improve your golf game? Build a machine to make the same swat from the same spot to the same hole 100 times. If 60 shots result in a hole-in-one, you might well say that hitting such a drive has a 60% chance of yielding up an ace. However, Silver would prefer even greater transparency, and recommends that forecasts be published to show the predictive range of likely events, so more like this set of smears  than this bunch of points  A foolish constancy is the hobgoblin of overconfident forecasters; while it’s psychologically easier to rationalize away counterintuitive results than to change one’s assessment, we’re more likely to be honest if we reveal the levels of confidence we have in various outcomes (as opposed to just picking our favorite as though it were the only one). The fact is, if you’re going to pretend to unshakeable certitude there’s really no point in considering new data at all.Silver laments that ever since Ronald Aylmer Fisher devised a means of accounting for sampling bias (so-called “significance testing”) about 100 years ago, acknowledgement of uncertainty has gone largely missing from statistical analyses. As I understand Silver to have explained it, the purpose of Fisher’s equation (analysis of variance or ANOVA, which is too complicated for me to reproduce or demonstrate here) is basically to quantify the amount of error that the 100 or so people randomly surveyed might not really account for the attitudes of the million to whom you hope to extrapolate. As the author explains, Fisher’s method overlooks the quality of the data used in favor of their quantity along with the clarity of any patterns that appear to emerge from them and irrespective of whether or not the resultant picture makes any coherent sense. Metaphorically speaking, if your correlation of coefficient describes a grouping of points as a bear, then Fisher's method makes it okay to parlay the result as a constellation, and not just an arbitrary aggregation of unrelated stars. In other words, your conclusions might be statistically significant but still fanciful.Thus it is that Lucy Van Pelt can theorize that bugs make the grass grow (see, e.g., Little Known Facts in You're a Good Man Charlie Brown). Now even if we suppose that Lucy’s hypothesis derives from positive ANOVA testing of a lifetime’s worth of gathered data demonstrating a statistically significant correlation between the presence of insects and lawns in her neighborhood, we know her conclusions are unwarranted. We know this, because we know that Lucy is ignorant of all the evidence that establishes the actual relationships between ants and grass on the one hand, and grass seeds and growth on the other. We see the full picture the way that Lucy, despite all her accumulated data, cannot. Hers is a comically limited experience and she has overfit her predictive model to rationalize marginal information.Bayes’ formula enters the predictive picture when we further suppose that Snoopy’s brother Spike brings Lucy data about desert-dwelling insects on a visit from Needles, and Marcy contributes information about Peppermint Patty’s bug-free hydroponic grass farm across town (oh yeah, I went there). None of this information changes Lucy’s prediction ANOVA. Her original statistical analysis remains highly-correlated as before, but given the humility built into Bayesian analysis, new and succeeding contraindicators will continually reduce her confidence in her original assumptions until she realizes it makes sense to abandon them entirely. (In the real world, this would never happen, since the well-known fact that Lucy is delusional is a key contributor to her charm.)For prognosticators toiling outside the field of entertainment, delusions are credibility-killers. Unlike normal humans, the author would have us gaze into the twisted mirror of noisy data and see past the hubris of our original assumptions. Ultimately, however, Silver’s superhuman aims are not so much for greater humility, but increased knowledge. His mantra, the words of Samuel Beckett: “Ever tried. Ever failed. No matter. Try again. Fail again. Fail better.”"
273,159420411X,http://goodreads.com/user/show/27614432-justin-tapp,5,"Tim Geithner cited this book in his memoir so I know Silver's writing has been influential on at least one policy maker. I read this book partially out of curiousity over how smart Silver really is when it comes to economics and statistics. This book came after Silver's FiveThirtyEight.com blog successfully predicted Presidential and Senate races and publishers wanted to capitalize on nerdy books like Michael Lewis' Moneyball. While Silver's election forecasting has been lauded, I never found it much more than novel-- he explains in the book that he simply took an average of others' forecasts, weighted by their past accuracy. The digging through data was more impressive to me than the results. I strongly encourage this book to anyone interested in forecasting, especially as applied to economics and policy making in areas such as climate change and financial regulation. It's also a good read for those starting a business or for CEOs looking to push back on their internal forecasters. Prerequisites before reading it are Michael Lewis' Moneyball and The Big Short; Bob Schiller's Irrational Exuberance, Daniel Kahneman's Thinking Fast and Slow; and I would also recommend Benoit Mandelbroit's (Mis)Behavior of Markets. I would also recommend the climate change chapters in Dubner & Levitt's SuperFreakonomics.Silver intends the book to be an investigation of various data-driven predictions. He is also proselytizing in the name of Bayesian analysis with the goal of leading the reader think more probabilistically. Silver writes that we can all improve our predictions by adjusting them when new information arises. This may seem like common sense, but I forecast for a budget office that has to project quarterly tax revenue two years in advance and doesn't have the luxury of regularly updating the published forecast when new information comes in (a real problem when the average retail price of gasoline comes in over $1/gallon below what anyone was forecasting even a year ago). it takes both courage and humility to be Bayesian when our media culture often hammers people for ""flip-flopping"" on issues. Bayesian thinking uses prior estimates as a starting point, and changing them as you encounter new information.Perhaps what I like most about this book are the interviews Silver conducts with people ranging from NASA scientists to economists to Donald Rumsfeld. He converses with Justin Wolfers over his critiques of Silver's predictions at FiveThirtyEight.com. He talks with forecasters about their forecasts, theories, problems, etc. even though he already knows a lot about the field. I work as an economic forecaster for state government, and I see the best practices, the most common mistakes, and the heuristic biases that Silver describes in detail.Silver begins with a seemingly odd-fitting hypothesis: as Gutenberg's printing press made books and knowledge more widespread, conflict increased as people felt they had more control over their own destinies. As we have more information/data, we know less of what to do with it. We pick and choose which data we prefer and become more tribal, more hostile to other tribes who focus on a different set of data.The terms forecasting and prediction are currently used interchangeably but had subtly different meanings with theological implications in the Middle Ages. Even today, seismologists say earthquakes cannot be predicted, as ""predict"" means a set time and date. But they can be forecasted, meaning that a forecast is a probability of an event, usually over a range of time. Forecasts are made in uncertainty. The U.S. has a ""prediction addiction"" and a prediction problem. Predictions for seemingly important series--like GDP, inflation, and unemployment-- have been wildly inaccurate. The important economic variables most frequently forecast tend to be consistently wrong. Silver recounts the housing bubble and 2007 financial crisis where CDOs were being AAA rated by ratings agencies who should have known better. Some economists acknowledged the housing bubble but did not accurately predict the consequences of its bursting. In this lengthy section, Silver cites Schiller, Rogoff & Reinhart, Larry Summers, Dean Baker, Paul Krugman, and others. Silver chalks up the ratings agencies' errors to the common forecasting error of not having a large enough sample size, making later observations appear much more improbable than they should be. Many of Wall Street's forecasters' models only went back to the 1980s, and missed the simple fact that real housing prices did not appreciate very much over the long-haul, not to mention several recessions in American history.Silver performed an amusing survey of The McLaughlin Group's weekly forecasts and found them to be no better than flipping a coin. He looks at how experts in various fields tend to be inaccurate in their forecasts. There are ""foxes"" who know something about a lot of things and ""hedgehogs"" who know one big thing. Hedgehogs make good TV guests but are not as good at predicting, studies have shown, as foxes. (The most recent example of this I've seen was a finding that various ivy league experts' predictions on Russia and foreign policy were more inaccurate in their predictions of Russian aggression toward Crimea than less-credentialed experts or experts in other fields.) Silver remarks that good forecasts are not just purely data-driven, more and better data help but sometimes not all that much. In politics, an incumbent running in a district solid for his party might suddenly be trounced at the news of infidelity or corruption, something a purely statistical model wouldn't predict.Silver cut his forecasting teeth on Major League Baseball, designing a system (PECOTA) to forecast draft picks and minor leaguers' potential output. PECOTA did okay against scouts but not fabulous, and Silver sold the system to Baseball Prospectus while he went on to publish books and start FiveThirtyEight.com. It's easy to conclude that a little bit of computer know-how can give you a huge advantage, but Silver states that's not what he's intending to say. Better models may help you at the margin, but like any business, forecasting is competitive and people will adjust and take away your advantages.What is needed is a good harmony between man and machine (Tyler Cowen picked up this theme in his recent book Average is Over). Algorithms cannot replace humans at forecasting completely, at least not anytime soon. Silver gives the example of weather forecasting, stating that humans add about 25% accuracy to computer models simply by using their eyes to identify outliers on the weather map, faster than computers or t-tests can. He evaluates the forecasts of NOAA and The Weather Channel, noting that as the U.S. government nicely provides weather data for free, for-profit forecasters compete in terms of accuracy. But the perception of being accurate is the most important-- the incentive is ratings, not accuracy, after all.The government also publishes free economic data but it is messy, noisy, and constantly subject to revision. Economic forecasters don't publish confidence intervals for their forecasts because they are ""embarrassed."" As an economic forecaster, I've always wondered why we don't publish such intervals but Silver explains the history here. Silver does explain the problem of ""overfitting"" in forecasts-- putting in too many independent variables to fit the curve or too often being ""fooled by randomness"" (an oddly sly allusion to Nassim Taleb, who Silver leaves out of the book... there must be some history between them).Silver writes of a successful gambler on NBA games who has a statistical model but also watches most of the games and makes personal judgements about how the team is communicating with one another, the effort they're visibly putting out, etc. This theme leads to a long exposition of poker and how gamblers have to quickly calculate the odds of opponents' hands given what you hold, what she has done. A computer would be good at this, but not at the aspects of bluffing which anyone who has watched Star Trek:TNG knows.There is a detailed look at Kasparov vs. IBM's Deep Blue. Chances are that your chess game will result in a position that has never before been played or recorded. Machines programmed with millions of pre-played game data run out of history after a few moves, and have to formulate a strategic analysis of the game. Silver learns in an interview that an undiscovered bug in Deep Blue's program threw off Kasparov's estimation of the computer's ability and strategy when he was analyzing the match results afterward. This resulted in Deep Blue ultimately shaking Kasparov. Kasparov thinks more like a poker player at times, trying to determine if Deep Blue has a ""tell"" or is bluffing.Silver writes that the average forecaster is still probably good relative to average guy on the street and he makes this point looking at poker-player data. He played online poker as a slightly above-average player, making money. When later looking at data he realized that the bottom 10% of players were so bad that they were subsidizing the average players. When the bottom 10% dwindled, the previously average players like Silver became the bottom 10% and lost. Apparently, 52% of online players have bachelor's degree, and are smarter than the average citizen who just buys a lottery ticket. This leads to overconfidence and a sense of entitlement, which Silver admitted to while playing. Poker takes more skill than roulette, but is still heavily dependent on luck. This segways into a comparison with stock traders who also suffer from hubris and a belief he/she is ""above average.""Silver gives a good summary of Eugene Fama's efficient markets hypothesis and Richard Thaler's critique-- the ""no free lunch"" aspect versus the ""price is always right"" aspect. Silver channels Kahneman to describe how heuristics and biases affect buyers/sellers' forecasts. We should all be aware of our biases and working against them (Silver recommends Robin Hanson's blog for help) and purporting we have none shows we have many. Never trust a forecaster or scientists who states he has no biases.From here, Silver looks at the enormously controversial yet important forecasting of climate change. While there is wide agreement among climatologists about the underlying theory, the warming trend, and causes, there is wide disagreement about the models used to forecast. This is important because the forecasts are often 30-100 years out and the margin for error quite high. There is contentious disagreement about the use of computer models. Scientists are dismissive of forecasters and models, where climate skeptic forecasters are dismissive of the science. Silver cautions that one should never trust a forecaster who is dismissive or ignorant of the underlying science behind the data he is forecasting, and never trust a scientist who is dismissive or ignorant of statistics and forecasting.One problem with climate change over time, and betting on various models is that you can easily cherry-pick your start/end date to get a different result (are temperatures trending higher or lower?). Silver examines some of the forecasts and finds the IPCC's model (problematic for reasons he describes) as fairly accurate since the 1990s. Nonetheless, Bayesian analysis would suggest that people are correct to increase their skepticism about the warming trend in recent years, since the earth's temperatures have not warmed from 2004-2011. Each new data point should cause an adjustment of your forecast. Silver laments that we could have been having a debate about the uncertainties of the forecasts all these years, rather than a debate about whether the problem really exists.Silver's faith in Bayesian analysis and lack of thinking through its logical conclusions is perhaps a weakness of the book. Bayesians like Silver say that our technological progress suggests further advancement is inevitable, and that we're converging on a point where we will seemingly be correct about everything; that we're evolving and will eventually achieve a progressive utopia. I'm reminded of Chris Hedges arguments against such thinking that quantum mechanics demonstrates some things will always be unknowable, and that world history shows no progress toward a utopia. There will always be randomness, there will always be noise mistaken for signal. Silver admits that the political polarization in America suggests our technological advancement is not inevitable. (He also touches on chaos theory throughout the book.)An example of climatology frustration is that some simple ideas-- like putting sulfur into the atmosphere-- would seem to be something we can at least experiment with. Volcanoes give evidence that putting a small amount of sulfur into the atmosphere would likely reduce the greenhouse effect, but environmentalists clash with climatologists on the issue. Again, if our technological progress suggests further advancement is inevitable the political disputes and cognitive biases suggest otherwise.Silver closes the book with a look at hindsight bias (although I don't think he uses that term). In hindsight, people wonder how the Pearl Harbor attack could have been a surprise. The silence in radio transmissions from Japan's carrier fleet should have been the signal in the noise. One definition of ""noise"" is not randomness, but multiple--too many-- signals, which is the problem with SIGINT. The FBI and NSA are constantly following up on leads they find to be false signals.The conclusion of the book: Think probabilistically. Move from simplifications and approximations to more precise forecasts and statements when more data is collected.Go from ""investors cannot beat the market"" to ""most investors cannot beat the stock market relative to their risk and transaction costs. It is hard to tell if any can due to noise in the data."" Work to reduce your biases: to say that you have none shows that you have many. ""Try and err:"" Make a lot of forecasts and evaluate them. ""Distinguishing the signal from the noise requires both scientific knowledge and self-knowledge: the serenity to accept the things we cannot predict, the courage to predict the things we can, and the wisdom to know the difference."" I give the book 4.5 stars out of 5. "
274,159420411X,http://goodreads.com/user/show/17607257-darwin-wu,5,"The Signal and the Noise - Why So Many Predictions Fail but Some Don’t I just finished reading this book at about 10 in the morning on Sunday, March 9. Coincidentally, this is also the spring daylight savings adjustment day, meaning the night was an hour shorter than usual. I think I sped through the second half of the book rather quickly, so this will be an interesting exercise in how much information I retain.  1. A Catastrophic Failure of Prediction This chapter was about the 2008 financial crisis. It went through the narrative we are all familiar with, including the subprime mortgages, CDOs, and ratings agencies, with special focus on ratings agencies. They are the ones who made the horrible predictions that people relied on--giving mortgage-backed securities AAA ratings. The chapter’s placement at the very beginning of the book sends a loud and clear message about the limitations of prediction. When applying a model to a novelty, you are making gargantuan assumptions about how the novelty works. Silver also does a good job of putting the reader in the mindset of viewing narratives as predictions. He lists “four major failures of prediction that accompanied the financial crisis”: the housing bubble, mistaken ratings for mortgage-backed securities, failing to connect the extreme leverage placed on the housing bubble to a potential financial crisis, and finally a mistaken prediction about the severity of the upcoming recession.  2. Are You Smarter than a Television Pundit? This chapter touched on the incentives that lead political pundits on television to make bold, unlikely-to-be-true predictions about political races. Perhaps most obviously, the craziest voices are also the loudest, and are thus more likely to get airtime. More interestingly, however, there is also evidence of a combination of the Dunning-Kruger effect and confirmation bias at work, where pundits are more prone to making errors in predictive judgment because the extra information available to them reinforces their biases. This is the chapter in which he makes the distinction between “hedgehogs” and “foxes”, the former being brash and quick to jump to conclusions, the latter being clever and able to balance information and consider more nuances. Very interestingly, hedgehogs tend to get worse at predicting as they acquire additional credentials (in contrast to foxes). “The more facts hedgehogs have at their command, the more opportunities they have to permute and manipulate them in ways that confirm their biases.” Foxiness in prediction includes “thinking probabilistically”, changing predictions in response to new data, and looking for consensus (which is more likely to be correct than outliers). FiveThirtyEight was the result of bringing foxiness to political punditry.  3. All I Care about is W’s and L’s This chapter focuses on Nate Silver’s PECOTA baseball forecasting system. In it, he goes through the process of coming up with models that “account for the context of a player’s statistics”, “separate out skill from luck”, and “understand how a player’s performance evolves as he ages”. Pre-Moneyball, the purely statistical approach was much better than the subjective approach of scouts. Post, scouts started using statistics to complement the information they are able to gather out in the field, including statistics like pitching speed that don’t show up on public stat sheets.  4. For Years You've Been Telling Us That Rain Is Green The title of this chapter refers to feedback The Weather Channel received when they changed the color of rain on their maps from green to blue. The substance of the chapter goes through the relatively brief history of weather forecasting. Beating persistence (the assumption that the weather will be the same today as yesterday) and historical weather (long term historical average of the location on that day of the year) is extremely difficult. All the progress that made thus far has been on the back of supercomputer processing power. However, computation is limited by the fact that weather is a dynamic system in which butterfly effects and chaos theory reign supreme. In fact, after about a week, historical weather begins to outpredict the best professional forecasts. Interestingly, as weather data gets closer to the end consumer (weather.gov > weather.com > local tv weather), the more the forecasts overestimate the probability of precipitation. Government precipitation probabilities match with the actual data, while local tv weather dramatically overestimates, leading to only a 66% chance of rain when a Kansas City meteorologist says that there’s a 100% chance.  5. Desperately Seeking Signal Earthquake forecasting has made good progress, but predicting has made almost none. Predicting means statements about specific times in the future, whereas forecasting means statements about long spans of time, such as “an earthquake of magnitude 6.8 or higher will hit San Francisco about once every thirty-five years.” The latter are made seriously, but have much less utility than would the former, if they existed. The only real progress in earthquake prediction has been the knowledge that aftershocks are likely to be dangerous after the main one hits. This is also the chapter in which Silver introduces the concept of overfitting, blaming overfit models for possibly underestimating the probability of the 9.1 earthquake that hit Fukushima.  6. How To Drown In Three Feet Of Water The title for this chapter comes from the fabled statistician who drowns crossing a river he thought was three feet deep on average. Margins of error are insanely important, as evidenced by the 1997 Grand Forks, North Dakota flood level prediction. The National Weather Service said that a 49 feet flood was likely without any error margins. The 54 feet flood overwhelmed the 51 feet levee. Stupidly, economic predictions in the news almost never include error margins. Macroeconomic realities are at least as complex as the weather, but lack the corresponding data and scientific models.  Maybe I’ll continue this later. I’ve worked on this for two hours now--for some reason I’m not sleepy yet, despite being awake since at least 4PM yesterday. Lemme count… That’s 23-4=19 hours awake. I’m probably going to crash pretty soon. Nighty night.  7. Role Models I’m losing my memory of this stuff by now, almost a week later, so I’ll keep this short and sweet (I’m also getting lazy). This chapter was mostly about predicting the spread of infectious diseases. The most interesting example was that of the prevalence of HIV and syphilis among gay men in San Francisco between 1998 and 2004. Syphilis went up, indicating that more sex was being had, while new cases of HIV went down, indicating that “serosorting” was becoming more advanced, i.e., people choosing partners with the same HIV status.  8. Less and Less and Less Wrong This is the chapter that, in the context of sports betting, finally introduces Bayes’ Theorem. Some asshole named Ronald Aylmer Fisher started “frequentism”, which we learn in introductory statistics today. Fuck that shit. It gives us data without context, which is essentially useless. A Bayesian paradigm would assign probabilities to hypotheses that could vary as time goes on. Wait. I just realized what metastudies are. Now I’m not sure what to think.  9. Rage Against the Machines This chapter details the famous Kasparov vs. Deep Blue match to come to the conclusion that man and machine complement each other.  10. The Poker Bubble Nate Silver made a substantial income off online poker for a couple of years. Basically, the World Series of Poker made online poker popular, and tons of suckers flooded in. Sharks were able to eat them up for a few years, but as the popularity of online poker took a nosedive due to regulation, the skill level needed to make a profit (water level) grew along with it.  11. If You Can’t Beat ‘Em… This is a chapter about the hypothesis that nothing beats markets at predicting, also known as the “efficient-market hypothesis”. Group forecasts, on average, should be better than individual forecasts, and markets are just group forecasts, so QED? Significantly, this does not imply that markets are likely to beat the best individual forecast, and there is clear evidence that agents with insider information beat markets. Disturbingly, congresspeople beat the market by 5-10% each year. This chapter also reviewed the argument between proponents of the efficient market hypothesis and economists who believe that irrational exuberance, i.e. bubbles, really happen. In this context, it makes note of the problem that trader incentives are irreparably short-term focused. This issue is compounded by the herding effect.  12. A Climate of Healthy Skepticism This chapter explores some of the specifics behind climate change prediction. The most important thing for me to take from this chapter is that only a quarter of scientists are confident in the specifics about climate change model predictions, (although they have a decent record so far).  13. What You Don't Know Can Hurt You This chapter is about terrorism. Silver draws an analogy to earthquakes when graphing their death toll/frequency (Logarithmic scales are god). The only really important example this chapter presents is the Israeli approach to terrorism prevention. Israel is forced by sheer volume to tolerate small-scale terrorism and focus on preventing large-scale events, which are the ones that actually cause the majority of the terror.  14. Conclusion ""There is no reason to suppose that the affairs of men are becoming more predictable. The opposite may well be true."" ""Think Probabilistically (section title)."" The general takeaway message of this chapter is a call for humility. The amount of information at our hands is increasing exponentially, so the signal to noise ratio is waning.  My general feeling is that the beginning of this book was the most information-dense (or at least I found the information more interesting), with a brief spike with the introduction of Bayesian probability, and then a bunch of fun case studies. The most interesting proposal from this book, which I am sure will not get the true attention it deserves because it is primarily a book about other things, is the debate between Bayesians and Frequentists in statistics. To me, it seems obvious that introductory statistics should focus on Bayesian statistics, and the whole paradigm should eventually shift over to this more realistic lens through which to view uncertainty. But I am not a statistician. Perhaps I should ask one. "
275,159420411X,http://goodreads.com/user/show/68583290-jamie,4,"“Most of the data is just noise, as most of the universe is filled with empty space.” If you want to sum this book up in a single sentence, that would be it. Humans have evolved to be very adept at pattern seeking, but it is easy to fool ourselves into seeing things that are not there. An entire industry exists to exploit the gullibility of lottery players by selling them lists of numbers that have come up frequently in recent drawings and so must be “hot”, while at the same time selling lists of numbers that have not come up recently and must be “due.” As H.L. Mencken famously said, “No one ever went broke underestimating the intelligence of the American public.” The first part of the book looks at bad and unreliable predictions; the second half at how to do better. The reader will become quite familiar with the Bayesian approach to improving results by constantly assessing and updating probabilities.Sometimes the author will toss out a thought that at first seems counterintuitive, but the more you think about it the more logical it sounds, like, “The information overload after the birth of the printing press produced greater sectarianism. Now those different religious ideas could be testified to with more information, more conviction, more ‘proof’ – and less tolerance for dissenting opinion.” This is a good point. Printing a thousand copies of a false statement doesn’t make it true, except sometimes in religion and politics.Much of what passes for informed discussion these days is pure entertainment. You can tune out political pundits since they are just playing to their partisan supporters, and the actual predictive value of their pronouncements is no better than flipping a coin. “About 15 percent of events that they claimed had no chance of occurring in fact happened, while about 25 percent of those that they were absolutely sure things in fact failed to occur.” Just like the talking head political pundits are not worth your time, do not pay much attention to that economist on TV with his PhD and computer models. He is blowing smoke. Economists are terrible at predicting what will happen, and the last thing they want is for you to track the accuracy of their previous predictions. You might as well go to a palm reader.Over the past few decades weather forecasting has gotten much better, and the ability to predict hurricane landfalls saves many lives. Earthquake prediction has not improved at all; every attempt at a system for calling them in advance ends up falling on its face. Not that meteorologists are any smarter that geophysicists, but the weather space can be modeled with a fair degree of precision, while the operative effects that generate quakes happen kilometers below the surface and show no actionable patterns.If you want the best weather forecast, especially for long range, go to weather.gov, not any of the commercial services like AccuWeather or the Weather Channel. The for-profit companies lean toward “wet” forecasts for a simple reason: “People notice one type of mistake – the failure to predict rain – more than another kind, false alarms. If it rains when it isn’t supposed to, they curse the weatherman for ruining their picnic, whereas an unexpectedly sunny day is taken as a serendipitous bonus.” The farther out the forecasts go, to seven or ten days, the commercial providers are significantly worse than the government site. And, keep in mind that the weather forecasts on the local TV channels are strictly for entertainment; do not use them if you have a serious need to know what the weather will be like. The author makes an astute comment about them, “TV weathermen say they aren’t bothering to make accurate forecasts because they figure the public won’t believe them anyway. But the public shouldn’t believe them, because the forecasts aren’t accurate.”Making better predictions is not an impossible task, but it is easy to be led astray by overconfidence, in ourselves or our computer models, by asking the wrong questions, or expecting the future to behave like the past. The chapter on terrorism is especially good; in hindsight the signal of an impending attack is obvious and it seems like anyone could and should have seen it, but in practice there are many signals, and the analysts are subject to their own personal prejudices and pet theories. Considering how many ways there are to fail at this, it is somewhat surprising that there are any successes at all.I wasn’t sure what to expect when I picked up this book, but I thoroughly enjoyed it and will be recommending it to my friends. There is a quote in the Conclusion that nicely sums up the points he has made, “Information becomes knowledge only when it’s placed in context. Without it, we have no way to differentiate the signal from the noise, and our search for the truth might be swamped by false positives.”"
276,159420411X,http://goodreads.com/user/show/12932684-bobby-den,4,"In late 2016, just before the American elections took place, I was closely monitoring the 538 blog. The blog had greatly proven its value during the 2009 election, predicting the outcome of all 50 states correctly. Scared that Donald Trump may take the presidency, I was holding on to a prediction that gave Hillary Clinton a 71.4% of winning the election. While nowhere near the landslide victory that had Obama reelected in 2012, it was enough to slightly ease my worries upon going to bed on the 8th of November. However, reality was different the next morning and I woke up to a map colored in red. Trump, against all odds, had prevailed and was elected the 45th president of the United States of America. How could so many experts and polls had it wrong for such a long time? With this thought in mind, I found myself encouraged to pick up the signal and the noise to learn more about the art, science and pitfalls of prediction.The signal and the noise starts out discussing the failures of prediction surrounding a number of topics. The foremost example of a ‘catastrophic failure of prediction’ is the discussion of the 2008 financial crisis. Although many factors can be pinpointed as playing a role here, a few stand-out. For instance, the approximations and assumptions about the world were off by a large margin. The forecasted default rates of certain financial products were many times lower than the actual default rates that unfolded. This had to do with the fact that many financial products were dependent on each other and their fate was tied together by a common factors (in this case the housing bubble). By discussing the 2008 financial crisis, Silver directly uncovers some of the major problems of prediction such as the difference between accuracy (our forecasts are not biased and they are thus a close depiction of the reality that is about the unfold) and precision (all the different forecasts that we have made are in agreement). In addition, he discusses the phenomenon of ‘out of sample error’ which refers to making a prediction about a new case about which as of yet you possess no data. He later adds to this list other problems such as the failure to quantify uncertainty, not updating your forecasts when the circumstances change and not seeing the world through different viewpoints when discussing political and baseball forecasting. It is here that he treats some of his best material and where he educates the reader that it is not easy to deliver good forecasts. By doing so, he arms us with the knowledge and tools to be critical towards future forecasts.Notwithstanding the good but critical material in the first few chapters, is the world of forecasting just all doom and gloom ? We do get a pretty good answer to this question as the book further unfolds. In chapter 4 – 7, we are informed of the fact that we have made tremendous progress in forecasting dynamic systems and phenomena. For instance, we have become 350% more accurate in the past 25 years alone in forecasting hurricanes. The hurricane Katrina disaster was not so much a forecasting disaster as a disaster produced by policy and hesitation. Had the mayor of New Orleans acted more adequately on the hurricane prognosis, many losses could have been prevented. As most of the book thus far feels like a case-by-case discussion of forecasting scenarios, do we get any sound advice how to improve our forecasts ? Although Silver dedicates an entire chapter to bayes theorem and Bayesian reasoning (his preferred approach to forecasting), the material here feels a bit meagre and unstructured. For instance, the chapter on chess, albeit interesting, seems more like a discussion of what we achieved with forecasting than a discussion of how to improve our forecasts. I think that Silver could have greatly benefited from the problems he discussed earlier in his book and using them as a starting point for discussing solutions. Furthermore, some of the later chapters are very expansive and rather tedious. Was Silver trying to “overfit” discussing too much noise and too little signal ?All in all, the signal and the noise was an enjoyable and insightful book that teaches us the state of forecasting in many different fields. It definitely satisfied my ambition to learn more about the art, science and pitfalls of prediction. "
277,159420411X,http://goodreads.com/user/show/7807167-ayman,5,"The signal and the noise by Nate SilverThe CIA didn’t see 9/11 coming, Japan didn’t expect a scale 9.3 earthquake in Fukushima, very few people anticipate the financial market collapse of 2008 and no one expected the Arab spring of 2011, yet all these events had loud and clear warning signals. The problem is they were masked by a lot of noise. More than ever, humanity is flooded with mountains of data and equipped with a huge computing power to analyze it, yet, with a few exceptions, our ability to predict the future has not really improved that much because we cannot tell the signal from the noise. This book is about fixing that problem. And here are the key ideas:1- The power log rule: The higher the severity of the event, the lower its probability. Scale 6 earthquakes are more frequent than scale 9, Large scale terrorist attacks are much less frequent than smaller ones, and a full popular uprising is less frequent than scattered protests, yet when Japan built the Fukushima nuclear reactor on a seismic fault line , they should have built it for the worst case, the CIA should have anticipated something fishy when they captured a suspicious pilot trainee trying to get access to commercial flight simulators weeks before 9/11, and finally, Mubarak of Egypt and other tyrants of the world should all know that their day is inevitable. Taking the power log scale to its end forecasts that events like 9+ earthquakes, biological or even nuclear terrorist attacks, as well as severely disruptive public revolutions are a question of when not if. The only way to see them is to open our mind to their mathematical possibility and look for the signals in the noise.2- Simulation models: the famous mathematician Norbert Wiener once said “The best model of a cat IS a cat” because models are never a perfect depiction of reality, yet we tend to have an oversized faith in models, the same way debt rating agencies had faith in models that predicted the possibility of 2008 financial crisis at 200 times less than its actual probability. The access, these firms enjoyed, to terabytes of data and petaflops of computing power was useless because their models were built on a fundamental logical flaw that mortgage defaults are caused by, infrequent, individual factors, e.g., a job loss, rather than large scale economic factors, I.e., the real estate market collapse and economic recession. However, sometimes, models work well, for example, weather forecast models are very accurate for up to 3-5 days in advance despite the terrible noise often found in weather data. In fact, the term, butterfly effect, is coined because the predictions of early weather models varied wildly because of insignificant changes in their input data, for example, a 0.001 degree difference in temperature, the rhetorical equivalent of a flip of a butterfly’s wing, could change the model’s prediction from calm weather to a hurricane, hence the famous cliché. Such sensitivity is because of models’ exponential equations where small noise is so amplified that it masks actual signals. 3- Finally, the author offers some good advice to analyst:a- Steer away from pundits who make flashy unrealistic predictions just to attract attention. The media has a very short term memory and they never punish “experts” for abysmal accuracy.b- Open your mind to mathematical possibilities, or as Arthur Conan Doyle said: Once you eliminate the impossible, whatever remains, no matter how improbable, must be the truth.c- You can make money in the prediction business if the field is full of fish. Many mediocre poker players made tons of money, just because majority of players were dumb fish. You don’t have to be in the top 10%. Stay above the bottom 55% and you will make a profit.A great read."
278,159420411X,http://goodreads.com/user/show/85649072-investingbythebooks-com,3,"Nate Silver became famous due to his innovative use of statistics in baseball followed, by correctly predicting the results of 49 out of 50 states in the 2008 election (50 out of 50 in 2012). In 2008 Silver launched his website FiveThirtyEight.com. In 2009 he was named one of The World's 100 Most Influential People by Time. In 2012 FiveThirtyEight.com won an award as the Best Political Blog from the International Academy of Digital Arts. Amazon.com named The Signal and the Noise the best non-fiction book of 2012.The book is divided into four parts, each consisting of 3-4 chapters. The first part is about failures and successes of historic predictions (Silver presents his baseball predictions). The second part treats the problems of making predictions of dynamic systems like the weather. In the final parts Silver presents his statistical tool, the Bayes theorem, which he then uses to improve prediction skills (this theorem is also used by Daniel Kahneman in his book Thinking Fast and Slow, i.e. this is something we all should learn more about...).In the third part of the book Silver applies this theorem to various examples, like sports betting and his own experience as a successful poker player. In the final part he uses Bayes on such difficult problems as terrorism, financial markets and global warming. When it comes to the latter Silver claims that there is no clear evidence of global warming. It’s not possible to predict higher temperatures with certainty. Temperatures have been flat for a decade, quite the opposite of previous predictions. He is worried that neither side in the debate looks at facts and data, but instead selects noise that supports previous views. In the beautifully written introduction the dramatic increase in data, and therefore also noise, is presented as a problem just because it’s easier for followers to find noise to support their views, which could be one reason why political partisanship has been increasing rapidly.Mr Silver has written a very entertaining book. He takes on the numerous problems with statistics and shows how to find the truth (signal) among the data (noise). My main critique of the book is that the author has few solutions. The main conclusion is that we should be less certain about our views and predictions. In his view many experts who are seen in media tend to exaggerate, oversimplify and to be careless with facts. The reason being that in order to get airtime and attention you need to say something newsworthy that can make a headline, not necessarily what is relevant. One of the few exceptions being Hans Rosling who with his long term graphs has managed to explain more things than most experts - and get attention.Regarding investing in stocks, Silver really doesn’t have anything new to add, but he doesn’t view the market as efficient. Rather his tentative conclusion is that the market is efficient 90% of the time. The main problem is that it’s difficult to identify the interesting 10% (surprise...). Unfortunately Silver leaves it at that. The book however is easy to read and if you are interested in baseball, poker and the climate this is a book for you. If you have less of an interest in those subjects and more into investing, then there are better books. I highly recommend books written by people from the financial industry such as Mauboussin, Taleb, and Montier.This is probably not the last we have heard from the very creative Nate Silver. Since he is born 1978 he has plenty of time to find more signals in the noise that we all can learn from. I will definitely read his next book."
279,159420411X,http://goodreads.com/user/show/31808656-samuelthunder,5,"Within the genre of pop-science non-fiction, this would rank as the most readable and probably most relevant to understanding the world around us, that I've read. Essentially this is a book about science: how we expand upon our knowledge and advance societal progress. The ""signal"" here is shorthand for ""truth"": the objective world in which we live and seek to better understand. ""Noise"" is all the other data and patterns that we may pick up which obscures us from isolating and learning from the signal (correlation, not causation, etc.).More precisely this book is about prediction - making informed statements about how the world behaves and then checking and seeing if, indeed, that is or is not the case - and, more precisely still, about using Bayes theorem in this regard. Prior beliefs are tested against reality in the form of a prediction, or bet, examined statistically, and then updated with new evidence to provide better informed beliefs. Repeat. Each chapter represents a case from a wide assortment of sources - baseball, poker, chess, weather and earthquakes, climate change, stock markets and macroeconomics, and terrorism. Sometimes the predictions are made well-enough and forecasts are continually improving (getting closer to that objective truth) as in the case for hurricane forecasts. More often, however, examples show how poorly predictions have turned out to be: macroeconomics and earthquakes for example (too noisy of data trying to hone in on too small of a signal, only few recessions or earthquakes to work with). Baseball and politics (Nate's main areas) are given as fields that are especially fertile for predictions: lots of data to work with and interesting insights to glean.From such cases a few general principles of good prediction are elucidated. Be upfront about uncertainty (eg macroeconomic forecasts are actually plus/minus a good 5%); be careful of out of sample biases (eg macroeconomic performance excluding the great depression); in general it's safer to follow the herd (aggregate predictions almost always outperform individual ones, politics, stocks, etc.); don't overfit the model (matching your model to the noise so that it looks good, while obscuring any actual trends, the signal); relatedly don't rely on overly complicated models or technology (knowing the underlying theory is tantamount); and mostly don't be overconfident in your predictions. Then, trial and error and update your priors in light of new evidence.While all this may sound dry or overly academic, this really is not the case. Nate Silver is a surprisingly good writer, with many very quotable blurbs (especially on the natural of science and objective truth), some amusing anecdotes (particularly his interviews, with a wide assortment of characters: poker players, baseball players, Donald Rumsfeld), and many interesting historical footnotes and factoids. In general the breadth of this book is pretty incredible; so much is covered. Correspondingly, that would be the major weakness of it as well, when he's writing about a topic he doesn't know quite as well - terrorism and climate change, in particular - you're only getting a very superficial overview of the issue (betting and politics are clearly his forte).Still, the general idea and principles hold even in those cases lacking depth. How do people understand the world, what predictions have they made, and how have those predictions held up? Then, how can they be improved upon? From such a wide look at society, a generally optimistic view is held: we can improve our predictions and improve our understanding of society, the world and Truth.Highly recommended."
280,159420411X,http://goodreads.com/user/show/28668057-jasmine,2,"It was okay.Briefly, the parts I found new and interesting were too sparsely marbled in between bits I found boring, familiar, or problematic. Based on my priors, I could have anticipated that 16 hours of Nate Silver was not going to be a 5-star book experience for me. This book would probably appeal more to people who are unfamiliar with concepts like ""overfitting"", ""bias"" and ""uncertainty"" but are eager to learn more and/or are interested in sports statistics.The parts of the book I found most interesting were the section on predicting the weather (originally hard but we're getting pretty good... here's why this system is interesting), predicting earthquakes (still very hard, here's why this system is interesting), and the history of Bayesian/frequentist statistics. The discussion on human insights versus model predictions during the baseball player scouting chapter was also thought-provoking. My biggest frustration was perhaps with the climate change chapter. Nate gives a cursory overview that yes, climate change is for real. But then spends several pages essentially lamenting the fact that ""ugh all those climate change deniers make it really tough to LEGITIMATELY criticize climate change research. Here, let me give a lot of space to people that quibble with the models, and virtually no space to those who don't quibble with the models."" Which, I understand from an academic perspective, but Nate dismissing his responsibility, reach, and ability to shape the way people think is a oversight he continues to make as Editor-in-Chief at FiveThirtyEight. I'm not sure that this chapter added enough to the discussion, or ended on a sufficiently optimistic note in terms of the accuracy of the models, to ensure every single reader of the book walked away feeling more or equally confident that we need to combat climate change compared to when they started. I eye-rolled my way through the whole self-congratulatory Hedgehog-vs-Fox analogy, which seemed uninformative and only tangentially related to the rest of the material of the book. When Nate complains about talking heads and political pundits, did he see his present life in his future? It brought to mind the ""hedgehog""-like errors he made around 2015-2016 regarding Trump's probability of winning the GOP primary.I also had a few quibbles with the chapter on terrorism. While Nate did a good job at presenting how human biases and motivations play in fields like weather forecasting (ex: forecasters prefer to give a 60% chance of rain or a 40% chance of rain instead of 50%) or the stock market (ex: the game theory of sticking your neck out to predict a crash versus staying close to the herd at the risk of being wrong), I think he completely trivialized the weapons of mass destruction intelligence failure of the Bush administration to be purely one of ""signal and noise"" and not ""signal, noise, ulterior motives and intentionally withheld information.""Final gripe: who calls themselves a child prodigy?!Nate leaves us with the observation that the ratio of the word ""predictable"" to ""unpredictable"" in published scientific articles waxes and wanes. He ties this to the cultural zeitgeist at the time with literally no other supporting evidence. I was expecting Nate to pick apart this argument and show all the problems with these assumptions, but instead the book just ends there, with Nate making all the mistakes he previously critiqued completely non-ironically."
281,159420411X,http://goodreads.com/user/show/59340194-hamish-seamus,3,"I don't know that I particularly learned anything from this book. Unlike, say, Superforecasting which has a clear prescription for how to make better predictions, TSATN instead gives a bunch of case studies in prediction: baseball, finance, terrorism, weather, climate, earthquakes. Some of these were kinda interesting, but for the most part I'd heard all the theory before.Notes; * I got the impression from reading Superforecasting that perverse incentives to make bad predictions was a particular pathology of political punditry, along with a few other domains. The impression I got from TSATN is that perverse incentives for bad predictions are the rule, rather than the exception. Consider weather forecasting. Surely if these were systematically wrong then people would find another forecaster? This isn't true for several reasons. For one thing, people get much more annoyed when the forecast says sunshine but the weather says rain, than they do when vice versa occurs. Thus forecasters should have a wetness bias, to keep people happy. Furthermore, if you say ""50% chance of rain"" then you'll sound indesicive and therefore unreliable. Instead you should flip a coin to either go up to 60% or down to 40%. I'm not sure if you can construct a general argument for why most forecasters should have perverse incentives, but it seems like you might be able to.  * White-house forecasting is some of the worst. * Predictions can be self defeating, such as a GPS recommending everyone take route A rather than route B, so that route A gets clogged. Thus predictors should aim for nash equilibria, rather than the ""business as usual"" prediction. * There was an incident under President Ford when the government predicted there would be a big outbreak of flu and encouraged everyone to get vaccinated. But the vacinnations were hastily produced and had some nasty side effects (like paralysis) and the flu never broke out. * ""If you have good analytic skills... it is often possible to make a profit when the competition succumbs to poor incentives, bad habits, or blind adherence to tradition. Or because you have better data or technology than they do. It is much harder to be very good in fields where everyone is getting the basics right."" * In forecasting you need ""the serenity to accept the things we cannot predict, the courage to predict the things we can, and the wisdom to know the difference."" * On bubbles: they seem to be caused by something like herd behaviour. This is a difficult to fix, but perhaps we can detect when we are in one. Then we could either profit from them, or else soften the landings. Some bubbles like the housing bubble were detecting by a lot of people well in advance, and Schiller's PE ratio have been fairly reliable indicators of bubbles. * ""The market can stay irrational longer than you can stay solvent"" - John Maynard Keynes. (well illustrated in The Big Short). * Being an anti-vaxxer can sometimes be a good move: in the 1960s, President Ford predicted a massive outbreak of H1N1, and ordered 200M doses of a vaccine, while indemnified the drug companies from legal liability for defects. 500 patients contracted Guillan-Barre syndrome (an auto-immune disease that causes paralysis) from the vaccinations, likely due to defects from the rushed order. * I need to read up on Schiller's PE ratio. * There was a Japanese property bubble in the 1990s which was very similar to the 2008 US bubble."
282,159420411X,http://goodreads.com/user/show/1587865-ashley,4,"An interesting but dense and thorough (read: long) book about how we often fail in making predictions. Silver borrows the terms ""signal"" (what the data is trying to tell us) and ""noise"" (distractions in the data) to explain how we often get lost in the data and draw the wrong conclusions when making predictions. Silver looks at complicated scenarios like poker, pandemics (apt!), earthquakes, and weather forecasts. There was a lot of data here and lots of familiar names (including Amos Tversky) as well as some good anecdotes, such as:- The Weather Service was initially organized under the Dept of War by President Ulysses S. Grant in 1870- The NWS gets by on just $900MM per year, about $3/US citizen even though weather has direct effects on some 20% of the US economy- For-profit weather forecasters rarely predict exactly a 5o% chance of rain, which might seem indecisive to consumers. Instead, they'll flip a coin and round up to 60% or down to 40%, even though this makes the forecast both less accurate and less honest.- People notice 1 type of mistake - the failure to predict rain - more than another kind, false alarms. People are more annoyed by unexpected rain, but pleasantly surprised with extra sunshine.- Earthquakes kill more ppl than hurricanes despite seeming more rare- Silver founded 538 after the online poker ban, where he won quite a bit of money - John Maynard Keynes - The market can stay irrational longer than you can stay solvent. One way to look at this is that markets are usually VERY right but occasionally VERY wrong. - The efficient market hypothesis is somewhat self-defeating - if you can't make money trading since the market is unbeatable, why would you trade in the first place and how does the market even exist?- Think of the stock market as two processes in one - you have the signal track, the stock market of the 1950s we read about in textbooks that prevails in the long run where investors play a long game and prices are well tied down to fundamentals. It helps investors to plan for their retirement and helps companies capitalize themselves. Then you have the fast track, the noise track, which is full of momentum trading, positive feedbacks, skewed incentives, and herding behavior. It does no real good to the economy, but also perhaps no harm. It's just a bunch of sweaty traders passing money around. However, these tracks run along the same road and sometimes, like during the financial crisis, there's a big accident and regular investors get run over.- Complex systems (earthquakes, spread of illness, terrorism, markets) can at once seem very predictable and very unpredictable. - Winston Churchill - I think democracy is the worst economic system ever invented - except for all the other ones.- Denmark consumes no more energy today than it did in the late 1960s, in part because of its slow population growth and environmental consciousness whereas the US' energy consumption has roughly doubled over the same time period.- Politics - a domain where the truth enjoys no privileged status- A conspiracy theory might be thought of as the laziest form of signal analysis; they're an irresistible labor-saving device in the face of complexity. Patterns are always clear in hindsight!- There is a tendency in our planning to confuse the unfamiliar with the improbable. The contingency we have not considered seriously looks strange; what looks strange is thought improbable; what is improbable need not be considered seriously. - 9/11 vs Pearl Harbor opposite mistakes - PH dismissed an attack from abroad as they suspected sabotage. 9/11 assumed an attack would come from abroad, not domestically. - Terrorist orgs are fundamentally weak and unstable. Like a new restaurant, they have a 90% failure rate in the first year. - Information becomes knowledge only when it's placed in context"
283,159420411X,http://goodreads.com/user/show/4926756-chris-n,4,"Nate Silver has a knack for all things where calculating probability accurately gives you the upper hand. Ask him anything about baseball, elections or poker. He may start out with intriguing nuggets and then drown you in names you may or may not find relevant. He is a statistics geek, but one whose mission is to help get the world closer to the finer nuances of the truth.Here are the ideas I found most captivating while navigating this gem of a book. 2008 RecessionAnalysts failed to predict the gravity of the Great Recession because the instruments that were traded were completely new, and rating agencies used historical data for other kinds of assets to make predictions of chances to default.ElectionsPeople who make forecasts can be foxes or hedgehogs. One knows a big idea the other knows more little ideas.Foxes may seem more modest and wishy-washy, but they know more about what they don't know and are able to change their perspectives more easily. Hedgehogs seem more believable because they look and sound confident in one idea and they stick to it, even when wrong.This chapter contains lots of granularities of the democratic process in the USA. Not very interesting if you don't live there or are otherwise particularly interested in the subject.Baseball:This is the author's main field, he is very familiar with the subject and you can tell that. Not my cup of tea, so a lot of the enumerated names didn't say anything to me.One main takeaway is that too much trust in numbers can lead to errors. Not everything is easily quantifiable and statistics can miss important signals.Weather: Because weather is a dynamic system, a small inaccuracy can lead to exponential errors.EarthquakesIt is an area with high chance of over-fitting (applying a specific solution to a general situation).Bottom line: nobody really knows how to predict dangerous earthquakes but that doesn't stop anyone from making bold claims.EconomyEconomic forecasts may seem very precise because they declare very precise numbers, but they failed to see the 1991, 2001, 2008 recessions while we were in them.DiseaseA fast spread of a disease in a constrained region in a social group does not mean that it will spread over a wide area exponentially. Statistical models are often too simple for human demographic particularities.PokerDoing fast mental math to asses the chances for a good hand is the hallmark of a good player. Anything else is for show.ChessComputers can beat world champions because they can see much further into the future than them. Or so the humans thought.Stock marketInvestors mix up signal with noise when try to predict moves using chartsJust like earthquakes, stock market moves are not entirely random but not entirely predictable either.All in all, this book is a tour de force in the fields where statistics are (or should be) essential on a daily basis. Nate Silver knows his stuff and doesn't just quote other books like a couch intellectual. I gave 4 stars because some areas where the author is proficient in (baseball and elections) tend to be overemphasized and others seem to be treated at a more basic level. Well, he can't be an expert in all of them, but he tries hard to make a convincing case.Definitely a worthy read that will nudge you to seek the finer nuances of predictions."
284,159420411X,http://goodreads.com/user/show/94994764-darcy,1,"Here are the things I learned from this book:1. Nate Silver really likes Bayes' Theorem.2. Weather forecasters are very good at their jobs.3. On-base percentage is a better indication of something something than the percentage of something else in baseball.That's it. You're done. Go home. I wish I had.Silver writes confidently and occasionally eloquently, and for this book he secured top-class interviews with experts in the thirteen (why?) topics covered. But he also makes me never want to learn anything about baseball, ever. That's odd, because he clearly loves baseball. Usually, enthusiasm is contagious; there is nobody better to talk about complex, obscure topics than someone who really loves them. But Silver is an exception to this rule (or, possibly, the model, depending on if, like Silver, you really love to use the word 'model' for things that aren't.)On the topics in which he is an expert, of which there are many, Silver drones. Incomprehensibility can, but shouldn't, be forgiven in nonfiction, since it can be blamed on the reader; comprehensible droning cannot be forgiven. He glosses over interesting statements in moments for fear of confusing the reader and then leaps into minituae of no interest to the lay reader. But at least he knows what he's talking about. One might question why he didn't just write a book of considerable length on political polling, in which he is an expert.Instead, there are thirteen chapters of superficial perspectives on things he doesn't know much about, but talks confidently about. The sections on disease prediction are like a first-year undergraduate biologist criticising the models they learnt in high school; he's right, but that doesn't matter, because nobody in epidemiology is using those models that way. The chapter on climate change continues to insist that a cold winter in New York is evidence against global warming, which is, to re-use a word, superficial at best. The sections on scientific research are insulting.I had to look up those things to find out the inaccuracies, because I wasn't sure he was representing things correctly. He didn't bother.Bayes is clearly Silver's favourite person in the world. Okay. To each their own. I've my own quibbles with the way he presents Bayesian analysis here, which I'll leave out, since they're not of interest to most of the Goodreads readership, but I've got bigger quibbles with the way he presents it as the be-all, end-all of prediction, in every field, without any discussion of how to actually apply it. How should you work out a prior? What weight do we put on new data? When do we change priors? How do we use Bayes' Theorem for applications other than single-event games of chance? Well, this book hasn't made any attempt to tell me.That's representative of the wider problem, other than the sheer dullness. Silver presents things we shouldn't do: oversimplify, overcomplicate, ignore the context, pay too much attention to the context, be biased, fail to be biased in our own interests. But he doesn't actually tell you how. Or how anyone else does it. He talks about 'analysis' like a black box function, something that some people do, and some people do. Criticisms are presented in vague terms without anything that can be done, by him, by national security, by us. "
285,159420411X,http://goodreads.com/user/show/110565494-ixby-wuff,4,"One of Wall Street Journal's Best Ten Works of Nonfiction in 2012 New York Times Bestseller ""Not so different in spirit from the way public intellectuals like John Kenneth Galbraith once shaped discussions of economic policy and public figures like Walter Cronkite helped sway opinion on the Vietnam War…could turn out to be one of the more momentous books of the decade.""-New York Times Book Review ""Nate Silver's The Signal and the Noise is The Soul of a New Machine for the 21st century.""-Rachel Maddow, author of Drift ""A serious treatise about the craft of prediction-without academic mathematics-cheerily aimed at lay readers. Silver's coverage is polymathic, ranging from poker and earthquakes to climate change and terrorism.""-New York Review of Books Nate Silver built an innovative system for predicting baseball performance, predicted the 2008 election within a hair's breadth, and became a national sensation as a blogger-all by the time he was thirty. He solidified his standing as the nation's foremost political forecaster with his near perfect prediction of the 2012 election. Silver is the founder and editor in chief of FiveThirtyEight.com. Drawing on his own groundbreaking work, Silver examines the world of prediction, investigating how we can distinguish a true signal from a universe of noisy data. Most predictions fail, often at great cost to society, because most of us have a poor understanding of probability and uncertainty. Both experts and laypeople mistake more confident predictions for more accurate ones. But overconfidence is often the reason for failure. If our appreciation of uncertainty improves, our predictions can get better too. This is the ""prediction paradox"": The more humility we have about our ability to make predictions, the more successful we can be in planning for the future. In keeping with his own aim to seek truth from data, Silver visits the most successful forecasters in a range of areas, from hurricanes to baseball, from the poker table to the stock market, from Capitol Hill to the NBA. He explains and evaluates how these forecasters think and what bonds they share. What lies behind their success? Are they good-or just lucky? What patterns have they unraveled? And are their forecasts really right? He explores unanticipated commonalities and exposes unexpected juxtapositions. And sometimes, it is not so much how good a prediction is in an absolute sense that matters but how good it is relative to the competition. In other cases, prediction is still a very rudimentary-and dangerous-science. Silver observes that the most accurate forecasters tend to have a superior command of probability, and they tend to be both humble and hardworking. They distinguish the predictable from the unpredictable, and they notice a thousand little details that lead them closer to the truth. Because of their appreciation of probability, they can distinguish the signal from the noise. With everything from the health of the global economy to our ability to fight terrorism dependent on the quality of our predictions, Nate Silver's insights are an essential read."
286,159420411X,http://goodreads.com/user/show/9948882-don,5,"Moody’s and Standard & Poor’s were charged not too long ago with knowingly misrepresenting the credit risk involved in some of the mortgage-backed securities they rated during the run-up to the 2008 financial crisis. The agencies will resist, saying that they simply erred in predicting the future, as anyone could have.But there were many who called the housing bubble correctly, long before the collapse in 2008. Several economists had pointed out that there was in fact an enormous housing bubble being created by overheated home prices, and it became a matter of public concern long before 2008. The term “housing bubble” appeared in just eight news accounts during the year 2000, but during 2005 it appeared nearly 3500 times, with Google searches for the term growing nearly ten-fold from 2004 to 2005 alone.“And yet, the ratings agencies—whose job it is to measure risk in financial markets—say that they missed it. It should tell you something that they seem to think of this as their best line of defense. The problems with their predictions ran very deep.” This is the indictment leveled by Nate Silver, in his fresh and readable book The Signal and the Noise: Why So Many Predictions Fail - but Some Don't (Penguin, 2012). It is the subject of this week’s Friday Book Share.It remains to be seen whether the government will be able to prove its case that the ratings agencies intentionally misled investors with respect to the risks involved in mortgage-backed securities, but what can’t be denied is that the risk of a meltdown was enormous, and it was well known. So why were their predictions so terrible?One important issue, Silver suggests, is that the agencies were either “unable or uninterested in appreciating the distinction between risk and uncertainty.” Risk, he says, is something that can be calculated, so you can put a price on it. You can price the risk of winning or losing at poker or roulette. By contrast, uncertainty is “risk that is hard to measure,” and your own best estimate could be off by a factor of 1000 or more. According to Silver, “Risk greases the wheels of a free-market economy; uncertainty grinds them to a halt. The alchemy that the ratings agencies performed was to spin uncertainty into what looked and felt like risk.”Overall, the quality of predictions made by experts has been abysmal. The predictions of economic experts, political experts, and other commentators generally perform at about the rate of random chance. In fact, the most widely cited experts’ predictions are generally the least accurate. This is partly because you can generate media demand for your prognostications if they entertaining, controversial, or unusual – but these are not qualities usually associated with accuracy.Nate Silver is the statistician and pollster whose “FiveThirtyEight” blog for the New York Times called the 2012 Presidential election flawlessly, correctly predicting the outcomesin all 50 states and the District of Columbia. He says one of the most important problems that all pollsters have – indeed all researchers, prognosticators, and pundits – is that they tend to predict things that confirm their own hopes and biases. Moreover, it’s very difficult to avoid having your predictions contaminated by your own subjective opinions. “Pure objectivity,” he says, “is desirable but unattainable in this world.” But pure objectivity is something he clearly aspires to, and he suggests that you should, too, if you want to be able to predict future events.(Personal aside: I read The Signal and the Noise prior to the election, while I was still eagerly anticipating a Romney win, and my friends found it odd that I suddenly began suggesting glumly that Romney probably wouldn’t win, after all. My revised opinion was based on my purposeful adoption of some of the strategies Silver suggests for maintaining one's objectivity.)Silver’s book reviews the accuracy of forecasting in a wide array of fields, from politics and economics to sports, weather, and terrorist attacks. Economic forecasting is still so bad, he says, that when it comes to forecasting recessions “a majority of economists did not think we were in one when the three most recent recessions, in 1990, 2001, and 2007, were later determined to have begun.” And earthquakes are so difficult to predict that we’re nowhere near a meaningful tool for doing so.By contrast, there has been a radical increase in the accuracy of weather forecasting over just the last couple of decades. Today’s weather forecasters can predict a hurricane’s landfall within 100 miles of accuracy some 72 hours in advance, while as recently as 1985 this wouldn’t have been possible until 24 hours beforehand. Nevertheless, people’s biases are so strong that they will often ignore very good, quantitatively accurate forecasts and predictions. A full five days in advance of the Katrina disaster, the National Hurricane Center projected a direct hit on New Orleans, and 48 hours in advance of its arrival they predicted that a “nightmare scenario” might well arise when the levees were breached. Even so, the political leaders in New Orleans remained reluctant to act, delaying the call for evacuation until the very last minute. The result was that 80,000 people (20% of the city’s population) didn’t get out, and 2% of them (1600 folks) paid for this with their lives.One of the most important tools for improving prediction is feedback. When meteorologists make daily predictions, they get daily feedback, and the result is a dramatic improvement, aided by computer tools. Business leaders, however, rarely get such immediate feedback, so inaccurate predicting skills in business are rarely improved. Biases intrude, subjectivity reigns, and no one goes back later to see what was correctly foreseen and what was not.And while meteorologists’ predictive skills have greatly improved, the same cannot be said of climatologists' effort to predict global warming, because meaningful “feedback” about climate change won’t be available for decades or more. Nevertheless, Silver spends several pages evaluating the statistical predictions of the IPCC (International Panel on Climate Change), and his conclusion is that, while there can be little doubt that the atmosphere is likely to warm gradually with increasing levels of CO2, the IPCC's own forecasts tend to be more alarmist than necessary, and relative to other forecasts ""might deserve a low but not failing grade.""The massive quantities of data now available, coupled with the computer processing power to sift through it and subject it to microsopic analysis, can easily give us a false sense of confidence. As op-ed columnist David Brooks said recently, it is as if there is a new ""religion"" of ""data-ism,"" leading some to think that ""data is a transparent and reliable lens that allows us to filter out emotionalism and ideology; that data will help us do remarkable things — like foretell the future."" But data without common sense and intuitive, human judgment can be dangerously misleading. Just ask the ratings agencies.According to Silver, “our predictions may be more prone to failure in the era of Big Data. As there is an exponential increase in the amount of available information, there is likewise an exponential increase in the number of hypotheses to investigate. For instance, the U.S. government now publishes data on about 45,000 economic statistics. If you want to test for relationships between all combinations of two pairs of these statistics—is there a causal relationship between the bank prime loan rate and the unemployment rate in Alabama?—that gives you literally one billion hypotheses to test.”And with a billion hypotheses to work with, it isn’t at all difficult to find a few million spurious correlations. In fact, I just wrote about one such spurious correlation earlier this week, when I discussed the Super Bowl Stock Market Indicator (and if you want to see a few human biases up close and personal, just read a few of the irate comments by football fans!)."
287,159420411X,http://goodreads.com/user/show/15785230-paulo-glez-ogando,4,"Nate Silver is an American statistician and writer who analyzes baseball (he manages Baseball Prospectus) and elections (founder of FiveThirtyEight). In Signal and Noise he details the art of building mathematical models using probability and statistics. It is an investigation of data-driven predictions in fields ranging from sports (baseball, basketball, chess, poker) to national security (terrorism). And elections, economy, weather, climate...This is a book for a broad audience, for he leaves out the maths, there is not a single formula in it. He explains how to make predictions and forecasts without explicitly describe any mathematical theory.The book is divided roughly into halves. The first seven chapters explore predictions and forecasts while the final six explain and apply Bayes's theorem. I quote the author when he says «I use the terms signal and noise very loosely in this book, but they originally come from electric engineering. There are different types of noise that engineers recognize —all of them are random, but they follow different underlying probability distributions».Prediction is fundamentally a type of information-processing activity, using new data to test hypotheses with the goal of coming to more accurate conceptions about the world. Many of the more problematic areas of prediction in this book come from fields in which useful data is sparse.He takes a big-picture approach to statistical tools, combining sources of unique data with historical data and principles of statistical analysis. Silver rejects the more traditional statistical method, specifically the 'frequentist' approach of Ronald Fischer. In contrast, he believes in a ""Bayesian"" approach, updating probabilities with new data sets.The book explores reasons to explain why error rates are so high, like psychological biases, methodological errors, misaligned incentives or even a flawed type of statistical thinking that researchers apply. So it is common to mistake correlation for causation and noise for a signal.Silver encourages us to think carefulley about the signal and the noise and to seek out forecasts that couch predictions in probabilistic terms, something that most of us are not very good at. Our brains are wired to detect patterns, are always looking for a signal, when instead we should appreciate how noisy the data is. He would like to see us moving away from vague terminology to probability and percentage statements.Among the bunch of examples provided, there are some worthy of mention. How weather forecasting is one of the success stories in this book, anticipating better and better in las decades the complexities of nature. Or Silver's claims according to which it is really difficult for any investor to beat the market over the long-term. And the application of power-law distribution to terrorism or earthquakes. Even his life metaphor of the attitudes of foxes and hedgehogs."
288,159420411X,http://goodreads.com/user/show/41583106-alyssa-grace,4,"The trouble with writing nonfiction books about semi-specialised subjects is that it's easy to fall into pitfalls on either side: On one end, make your book too technical, and nobody will read it because the average layman can't make heads or tails of it. On the other, ""dumb it down"" to a level that's readable by any 12-year-old, and you open yourself to the criticism of peddling oversimplified pop science. The Signal and the Noise skilfully treads the delicate line between the two, presenting ideas in an accessible, easily understood manner while still conveying a fair deal of substance. I don't think I'll ever not be a fan of Nate Silver. Here, his writing's not only clearly and succinct, but has a witty tone without which the subject matter would be an order of magnitude less interesting. It also helps that he illustrates his points with a bunch of real-world examples from all disciplines, not just politics and sport as one might mistakenly be led to believe (although specific questions in both of these fields are indeed covered). Here we have topics explored from plague infection models to artificial intelligence in chess and everybody's favourite, poker probabilities. (You may not know, as I certainly didn't before reading, that Silver was himself a professional poker player at one point, with final profits totalling $400,000.)The book frequently refers to Bayesian thinking in lieu of frequentist methods or, god forbid, gut feeling, as a cornerstone of good forecasting. While the maths of Bayes's Theorem can get quite complicated, Silver sticks to the basics: X, a ""prior probability"" representing one's initial beliefs of how likely a hypothesis is to be true before evidence is introduced; Y, the probability that an event/piece of evidence would occur if the hypothesis were true; and Z, the probability that that event/evidence would show itself if the hypothesis were false. Of course this is basic conditional probability that everyone remembers from school, but Silver turns it into an entire mode of thinking and applies it to terrorist attacks and Texas Hold 'Em, showing the effectiveness of thinking about predictions in terms of probabilities (the weather forecast said there's a 60% chance of rain tomorrow) rather than certainties (the weather forecast said that it will rain tomorrow).One thing that should be noted is that The Signal and the Noise isn't the most academic book out there, you can undoubtedly find other forecasting-related books ten times more dry and in-depth. If that's what you read this book for, you may very well be disappointed. In terms of style, I'd say that it's slightly more weighty than Freakonomics and definitely more so than Malcolm Gladwell. So if you're like me and want something that balances the statistics with the quirky real-life examples, you're more likely than not to enjoy The Signal and the Noise.My Blog"
289,159420411X,http://goodreads.com/user/show/40896347-andy-scott,3,"I read this book as part of my grad school class on predictive analytics, for which I had to write a summary paper. Overall, this book was interesting as the author takes a look at statistical analysis and prediction of popular topics including: the economic recession of 2008, political election results, baseball player success, weather, earthquakes, the economy, disease outbreaks, sports betting, computer chess, poker, stock market trading, climate change, and terrorism. The common theme throughout the book is the idea of Bayesian statistics. The author believes and makes a case that Bayesian statistics is a better method for making predictions or evaluating hypotheses in the real-world than the method generally used during the 20th century. Essentially, Bayesian statistics starts with a prediction about the probability of some event, then gathers information or data to update that probability. This process is repeated several times in order to zero-in on the true probability. He also makes a case for having the courage to make a measurable prediction, evaluating your prediction based on results, and then being willing (having the humility) to change your prediction when you have new information.It was interesting to learn how much more successful weather forecasting has become in recent years. He also had some advice that it is probably better to just match some index to the stock market than try to beat it, because you probably won't unless you put a lot of work into it.With disease outbreaks, it was interesting to learn that the most commonly used models usually overestimate the severity of the outbreak, and basically are fundamentally flawed. It was particularly interesting reading this section since it was written before the coronavirus outbreak, as it referred to previous outbreaks of H1N1 and the Spanish flu. Regarding terrorism, it was interesting to read that the largest threat is really some sort of biological attack, like a contagious disease that would have a devastating effect on a large segment of the population.His take on climate change is basically that everyone agrees on the greenhouse effect - that carbon dioxide increases the temperature of the earth, which leads to an increase in water vapor, which also leads to an increase in temperature. Where the disagreement lies is to what extent the temperature will change and how much effect it will have on the earth. The problem is trying to make long-range predictions with noisy data that is changing so slowly.Overall, I thought the book was somewhat repetitive and not clearly organized within the chapters. It seemed it just took a long time to say what needed to be said, and the applications or principles were not as clear as I would have liked. But they were interesting topics and I learned a few things that I think will help me in the future."
290,159420411X,http://goodreads.com/user/show/18293816-bill-leach,3,"A book on forecasting with some interesting ideas.While Gutenberg's printing press made a huge contribution to the recording and dissemination of knowledge, the bestseller list soon became dominated by religious texts and psuedoscientific ones. To some degree, the web suffers the same problem.The concept of the hedgehogs and foxes (from the Greek poet Archilochus), where the fox knows many things for defense while the hedgehog knows only one: his spiny coat. Silver sees the two as differing in forecasting methods. The hedgehogs are more assured of their forecasts and speak to them more fervently making better TV. The foxes tend to recognize the variety of factors at play, making them less certain and more likely to change their mind. The foxes tend to provide better forecasts as they modify their forecasts as better information becomes available.The author provides detailed information on statistical systems used in baseball to identify the best future players. Techniques for statistical analysis have been adopted by most baseball clubs but scouting is still the more important method of choosing new players.Weather forecasting depends upon models where many runs are done with slight variances in initial conditions. Human forecasters add value in being able to eliminate the outlier runs. Humans improve the accuracy of precipitation forecasts by about 25 percent and temperature forecasts by 10 percent. Reviews of accuracy show the day ahead temperature forecast to have half the error of long term climatology (the historical average), but the forecasts are about as accurate as climatology eight days out. Moreover, the forecasts are actually less accurate than climatology further out.No good methods are available for forecasting earthquakes. A number of people have examined the data, seen apparent correlations and become convinced that they have a method. All have failed, however. Similarly, until 1997 there was the idea that the outcome of the Superbowl would predict the direction of the stock market over the next year. Thirty years of mostly successful results implied a 1-in-4,700,000 possibility of chance alone. However, the indicator failed in many subsequent years. If enough possible correlations are examined, one is likely to appear a strong indicator even without causation.The dangers of extrapolation are demonstrated by disease forecasting failures of the swine flu in 1978 and HIV in San Francisco in the late 1990's.A good chapter on poker describes the importance of probabilities in the play. The expected earnings are calculated for various skills of player.Other chapters seem to muddle around with various observations that don't really add to subject."
291,159420411X,http://goodreads.com/user/show/40064681-caleb,3,"This book might be a bit too long. Split into 13 chapters, all case studies of different situations where understanding probability brings a new view to the topic, the book is an interesting read, all right. The stories it tells are interesting.Most of them - I found, with my lack of knowledge in baseball, the entire chapter on baseball statistics and analysis - a bit... of a haze. I remember little to nothing of it - or at least, I remember not enjoying it as much as other sections. Perhaps it was part of the lack of explaining terminology, or just some disinterest in baseball.Before starting onto the actual content: I have to say; the table of contents is wholly unhelpful, at least in the edition that I've read. Chapter titles give an idea of the story it tells (although, reading them again now, they're not wholly helpful either), but there were no sub-chapters, or whatever it is called - and the story/case study, while interesting, is *not* the main thing to take away from this book. Trying to remember the key concepts and where to find them after your first read is not... the easiest thing to do.The first half of the book is devoted to describing our statistical biases and blind spots, mostly that (from what I can recall) we think in terms of binary options rather than a spectrum of outcomes.The main problem that I see (or at least, remember poorly) is that, a lot of things tend to seem applicable only to this certain case study/chapter. Things that might be key concepts in statistics tend to be touched on only once, briefly, in a chapter, and barely spoken of again.The second half of the book introduces Bayesian modelling, and how it is derived. Which is not to say it is a be-all-end-all solution, just an alternative. The book does not say that it is, but might put too much emphasis/oomph/belief in it.Yet another thing that I tend to recall is that perhaps some chapters require quite a bit more general knowledge:* A chapter on poker that includes some basic Texas Hold'em.* A brief intro to the stock market and P/E ratios (just a smidge of the stock market here).* How mortgages work, especially in the case of the 2008 crisis.The conclusion provides another look-through on the some concepts briefly, but... spread over 8 pages, theres only so much it can describe.All together, it might be a good read for more general knowledge on events, but... I am sad to say that I don't remember most of the statistical concepts that the book tries to introduce to us. At the end, I thought this book could be a longer New Yorker article, or just a series of long New Yorker articles."
292,159420411X,http://goodreads.com/user/show/6112060-c,5,"As someone currently taking compulsory statistics for a degree, it is a delight to find a book about statistics that I actually came home wanting to read after a boatload of statistical coursework!Perhaps the best part of this book is the fact that it stands both as individual chapters and as an entire text. For as diverse as the topics of each chapter are, I had some concerns going in that it was going to act more like a series of vignettes without much of a central argument. Thankfully, this couldn't be further from the truth - as you move through this book, you will find yourself drawing new connections (even for a statistical newbie!) either explicitly stated by Silver or just growing naturally out of the text. Separating the signal from the noise is the central theme of this book - and Silver's signal of a thesis doesn't get lost in the noise of each individual chapter. If you have ever read a FiveThirtyEight article, the voice of this book is going to feel very familiar. Thankfully, for a fan like myself, this played to the book's strengths. Despite being structured as an academic text with a central thesis, the journalistic quality of each chapter makes for a unique read. The more casual tone does two things. First, it makes a book that deep down is quite technical a decent read even for those who aren't necessarily non-fiction fans (sorry not sorry). And, where a reader's familiarity with a topic may vary, the chapters are structured in such a way to allow space for a quick yet fruitful introduction before diving into the argument where a more academic text may assume some base knowledge. For context, I study environmental health, so the chapters on weather and disease felt very familiar. Yet I am a casual fan at best of baseball and haven't played chess in 10 years. This book engaged me on topics I would normally shy away from. In fact, I think Moneyball is my next read!My only caveat with this book (what I would give to be able to do half-stars) is the some of the repetition in the chapters. This could be my own biases speaking (Nate Silver would be so ashamed), but Chapter 1 (A Catastrophic Failure of Prediction"") and Chapter 11 (If You Can't Beat Them...) both talk about the stock market and trading. I understand this is one of the prime examples of forecasting, but the chapters really did get at the same idea - forecasting the economy and stocks is rife with biases and not all that helpful. I found myself kind of slogging through Chapter 11 because of this."
293,159420411X,http://goodreads.com/user/show/42218058-tony-perez,1,"Within the world of political punditry, Nate Silver is known as the 'numbers' guy. He distinguishes himself from the rest of mainstream media news pundits by claiming to base his beliefs and political forecasts on his use of (Bayesian) statistical methods and models. So given this fact, I would have expected to see some degree of mathematical rigor in this book. Sadly, this is not the case.The writing is more akin to a pop-sci book: It is almost entirely free of any equations, and he provides nothing more than hand-wavy descriptions of his of statistical methods, leaving the reader with very little hope of even trying to replicate/verify any of his results. Each chapter consists of a mostly self-contained topic of discussion that lends itself well to some form of statistical analysis (e.g., the 2008 recession, political election forecasting, sports betting, chess, financial markets, climate change, etc.). Unfortunately, despite his interesting selection of topics, he then just goes on to tell a story about someone whose use of (or lack thereof) statistical models to make predictions went horribly wrong; or occasionally about how someone's (implicit) use of Bayesian statistics is marginally better than classical/frequentist statistics. He doesn't build any rigorous case for how or why the predictions fail/succeed other than to repeat his mantra of bashing on frequentists and praising Bayesians. With few exceptions, the chapters might as well be a series of extended blog posts describing things we've all heard about in the news and then assigning blame as he sees fit.The only thread tying together each chapter his use of the descriptors 'signal' and 'noise', where he makes some vague claim about how the prediction failures of others have to do with people mixing the 'noise' for the 'signal', as if he were a superhero using a catchphrase.To make matters worse, Nate appears to have an incessant need to interject his blog post anecdotes with descriptions of his meetings with people who (I think) he presumes for the reader regard as someone important. I couldn't find any apparent reason for the inclusion of such irrelevant details other than for him to try to bolster his own appearance or credibility. It accomplished neither for me, and I would have found the book less annoying without them.I, admittedly, skipped around and only read about half the chapters. I just couldn't continue reading his series of extended blog posts on random topics with virtually nonexistent rigorous statistical justification."
294,159420411X,http://goodreads.com/user/show/59510944-anima,5,"Nate Silver says “Almost all the forecast that I publish, in politics and other fields, are probabilistic”. A book for nerds and not only! I liked Silver’s ability to make statistical analysis a friendly topic by integrating numbers and theoretical knowledge into a variety of examples related to politics, sports, stock market, poker game, weather, earthquakes. He does an excellent job at explaining the difficulty of getting rid of the “noise” that surrounds the correct signal of a good prediction. The chapter about the Bayesian prediction caught my attention -the technique does not limit the probability of an event with only yes or no answers to 50 percent probability . Bayesian reasoning considers that besides black and white options, there might be clear answers to “what if” and assigns probabilities to these events too . The book is great, and although there was nothing to touch my heart, I much enjoyed reading it!“Almost all the forecast that I publish, in politics and other fields, are probabilistic...Instead of spitting out just a number and claiming to know exactly what will happen, I instead articulate a range of possible outcomes. On November 2, 2010, for instance, my forecast for how many seats Republicans might gain in the US House looks like what you see in figure 2-3... ...the most likely range of outcomes... was.. between 45 and 65 seats (their actual gain was 63 seats )But there was also the possibility that Republicans might win 78 or 80 sets- if almost certainly not the 100 that Dick Morris has predicted.” “By late 2007 there were clear signs of trouble: home prices had declined over the year in seventeen of the twenty largest markets....Creditors..were becoming less willingly to make loans.....Governor Charlie Crist of Florida, one of the worst-hit states, proposed a $10,000 credit for new home buyers.....housing prices continued their inexorable decline, falling a further 20 percent during 2008. While quite a few economists identified the housing bubble as it occurred, fewer grasped the consequences of a housing-price collapse for the broader economy... But in contrast to the activity that was taking place on Main street, wall Street was making bets on housing at furious rates..What particularly distinguished Lehman Brothers, however, was its voracious appetite for mortgage-backed securities. The 85% billion it held in mortgage-backed securities in 2007 was about four times more than the underlying value of its capital, meaning that a 25% decline in their value would likely be enough to bankrupt the company.......This is the role that the rating agencies played. They vouched for mortgage-backed securities with lots of AAA ratings.....In a conference call in March 2007, Lehman CFO Christopher O’Meara told investors that the recent “hiccup” in the markets did not concern him ...One year later, .., Lehman was desperately trying to sell its position..Lehman went bankrupt on September 14, 2008”"
295,159420411X,http://goodreads.com/user/show/19077848-len-knighton,4,"I sometimes wonder why I put a book on my TO READ list. The first chapter of this book gave me reason to ponder, but by chapter 3 I had figured it out: I must have heard an interview of the author on NPR in which the focus was on baseball. Indeed, Nate Silver is very involved in baseball and Houston's World Series triumph prior to my starting this book is a sign that signals in baseball are changing. All that aside, this is NOT a book about baseball; it is a book about making predictions or forecasting in a variety of fields: economics, politics, sports, weather, earthquakes, terrorism, and so on. If you can grasp the scientific/mathematical concepts, it may make you better at predicting things that are predictable and knowing those things that are not. One also needs to sift through the noise to discover the signals. All book reviews are subjective; we do not read in a vacuum. We bring a set of interests, information, and ideas into our reading. If the book satisfies some of them we give it a high rating and a good review; if not, we go the other way. As I approached the final pages I was ready to give the book 3 stars. But the last 3 pages brought it up to four. And the reason for the rise is purely subjective and may not come into your conscience at all. More than thirty years ago I took a trip to Israel as part of required work for a Seminary degree. I was one of more than a hundred students engaged in this educational venture. Dr. David Fleming, director of the Jerusalem Center for Biblical Studies was our guide and lecturer. His first lecture had a major impact on me and has influenced my ministry in the three decades since. That lecture came back to me as I read the final pages of the book. Silver, in chart and words, wrote that unpredictability reached its peak during the Great Depression and World War II. Predictability became the norm in the 1950's, '60's and '70's but that unpredictably is increasing in the 21st Century. This corresponds with the patterns of church membership and with Dr. Fleming's lecture, for he stated that Christians praise God and attend worship services more regularly when their lives are ""PREDICTABLE, NOISY, BUSY, and EASY,"" but that ministry should focus on those whose lives are ""UNPREDICTABLE, SILENT, LONELY, and DIFFICULT."" If Silver is right, then clergy and dedicated lay folks will be busy indeed. Four stars waxing. "
296,159420411X,http://goodreads.com/user/show/62035772-yoyo,4,"Which animal would you think defines a good forecaster, fox or hedgehog?The hedgehog knows one big thing, but the fox knows many little things. If an original method is not surely working, the hedgehog is reluctant to change, but the fox is tolerant of complexity and is adaptable to find a new approach. That is why the author suggests being foxy is a right attitude toward a good forecaster.We live in a world in which information is pervasive so that the gap between what we know and what we think we know is widening. As the study has shown, even the experts usually make incorrect predictions. For example, the probability of the skyscraper being crashed into by the terrorists is 0.05%. The possibility would rise to 38% given that the first building is under attack. If we could use one of the principles, “Today’s Forecast is the First Forecast of the Rest of Your Life” in this book, we could make a better forecast possible today—regardless what we said yesterday, last month, or last year-- and prevent the formidable catastrophe from happening. Other suggestions the writer proposes are below:-Think probabilistically: Acknowledging the real-world uncertainty in our forecast.-Look for consensus: It’s not easy to be objective. Other options could help us see the world in different viewpoints to reduce biases.-Weighing qualitative information- accounting for the qualitative information along with quantitative factorThis book is a little long but readable, not a formula-heavy, general science book.It consists of four sections. The first section considers the failures of predictions in finance, baseball, and politics. Then, the author gives the readers some advice about how we can apply our judgment to the data without succumbing to the biases. The second section focuses on dynamical systems (weather, economy, earthquake, and economy) that make forecasting more difficult. Following the third section, it turns toward a solution by an introduction of Bayer’s theorem. Finally, the discussion of applying Bayer’s theorem to more existential types of problems.If you’re interested in general science books or statistically forecasting, please enjoy it.However, if you are the audience who need depth in measuring and making data-driven decisions, you might not appreciate this book as much. I would suggest to read “How To Measure Anything” by Douglas W. Hubbard."
297,159420411X,http://goodreads.com/user/show/14108214-grant-cousineau,5,"Published in 2012, the concepts in this book couldn't be more applicable than they are today. From predicting when the next pandemic might occur, to how investors didn't read the tea leaves well enough before the 2008 recession, to climate change, and especially, how poorly political pundits forecast elections, it's as if most of the topics covered in this book were predicting the next decade. To those who read it, Trump's election victory may not have been as surprising, or a deadly pandemic as shocking. Not that we should have guessed these outcomes before they transpired, but rather, we need to be more wary of what the data tells us sometimes, and what it does not.With other chapters looking at how statisticians predict a baseball player's value, how geologists predict earthquakes, and expert poker players can quick-guess the possible outcomes of winning as the river cards are revealed, Silver consults with the experts to show what the data can tell us, and what it cannot. I wouldn't say that reading this book made be an expert in evaluating predictions, but it definitely could serve as a textbook worth studying on the matter. I intentionally waited to read this until an election year, when I could try to apply some of the theories to the polls (as Silver has a sterling reputation in predicting presidential elections), and while we wait to see the results, already it's much clearer how uncertain things are. As of late August 2020, Biden's held an average of an 8-point lead, which would seem substantial, but when you factor in the disproportionate Electoral College, the weaknesses in survey sciences, the unreliability of polls months in advance of the election, and the ability of outside factors to shift the outcome, no singular poll should be taken too seriously, though weighted averages like the one at FiveThirtyEight.com, and how they apply ""fundamentals"" like the economy's impact and the trend of incumbents to do well, are the clearest signals available we can have.Selfishly, I kind of want to read an updated version of this book, given all that's transpired these past eight years. Though I've read more than enough about the 2016 election, how ""upsets"" like those and massive events like COVID-19 have affected Silver's approach would be immensely insightful. Then again, their website does a stellar job of telling those micro-learnings, which is why I came to this book in the first place."
298,159420411X,http://goodreads.com/user/show/950704-megan,3,"Started well, but I ran into a couple issues.The biggest issue for me was that I am not intimately familiar with baseball—and this was a HUGE issue in this book. In most domain-specific chapters, the author provides definitions and background as he goes. In the poker chapter, he took time out to provide a glossary on top of the general context. This was amusing to me because I don't know any statisticians or data scientists who are unfamiliar with basic poker terminology, but I do understand this book is intended for a wider audience and I approve of providing appropriate context. Yet in the baseball chapter he provided nothing—just assumed that we all know and love baseball. My knowledge is limited to number of players on the field, number of innings in the game, and names of player positions (though I would probably draw some locations incorrectly if asked). I don't know the names of specific plays other than ""home run"". I don't know any of the jargon. I don't remember famous players, or at least I don't remember why they're famous. I had no way of understanding why he was talking about certain things as though they were obviously good or bad. In fact, sometimes I could't tell whether the last few paragraphs were good or bad because I didn't understand enough context. I was completely lost in that chapter.You'd think this problem would only affect the baseball chapter, but think again. From that chapter on, he threw in baseball similes more and more. It is of zero use to me for the author to say ""it's like if this baseball player made this baseball play in the middle of something baseball related"". I have no idea whether that's good or bad, so the simile added nothing to my understanding of the topic at hand. Baseball is a pervasive theme that you are assumed to understand at an enthusiast level. If you don't, don't bother with this book.The other issue I had was that the book is not well organized. It rambles a lot, and the ""conclusions"" section didn't really seem to result from the rest of the book. But I expected the book to be more of a general exploration anyway, so it's not a big deal.I do also wish that more statistics concepts and techniques were introduced/discussed in the book. At this conceptual level, I would give this book to a high schooler or gifted middle schooler (as long as they loved baseball)."
299,159420411X,http://goodreads.com/user/show/51588923-shhhhh-ahhhhh,5,"This is a book that I'll need to revisit. I won't do too much recapping. I'll say that if you're familiar with, and appreciate, Nassim Nicholas Taleb's work or Daniel Kahneman's work, you'll definitely appreciate the theory work done in this book that seeks to broach the meeting point of the pitfalls of cognition with the pitfalls of prediction in a deterministic universe for which you do not know all the rules (or cannot observe all the variables, which makes i appear probabilistic). One of my big takeaways, which the author leaves unstated but which I've synthesized from this book and others, is that the optimal predictions we can make will rely on direct measurements, computer assistance, the wisdom of the crowd and will be ranged narratives. That last part especially almost seems at odds with what Silver states directly in the book but isn't. Silver criticizes the way that demagogues and ""hedgehogs"" in general collapse the probabilistic ranges of what can occur into neat causal narratives that are wildly over-confident and downplay the role of luck/ fundamental unpredictability. He also praised predictive models that cast potential occurrences in probabilities generated by running and rerunning simulations with slightly different starting conditions. This being the ""foxy"" approach. I think that he should have taken this reasoning one step further to come to the conclusion that more nuanced narratives are needed to make foxy thinking more accessible to hedgehog types (and thus make our entire society slightly more antifragile to risks posed by faulty thinking around probability). I also believe that, though he certainly says nothing about it, couching probabilistic predictions/ forecasts in narrative can make them easier to process cognitively and make decisions about. While he explicitly discusses how foxy strategists are able to see more clearly because they do not couch their understanding of the world in any single narrative, I believe that overlooks the possibility of using many, relatively isolated narratives to address individual points of probability or data. Anyway, I'll definitely enjoy rereading. I was mildly disappointed to not have seen anything about genetic algorithms in the book though. "
