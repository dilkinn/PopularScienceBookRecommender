,isbn,user_link,ranking,review
0,0735611319,http://goodreads.com/user/show/4248907-craig,5,"I'll be honest. I only read this book because it was quoted as a must read by Joel Spolsky on a stackexchange answer about how to go about learning programming (and finding out if you want/should be a programmer).I was a little hesitant due to the year of release. Being at least some 11 years old that's a lot of time in the tech world. Ultimately though that doesn't matter. I defy any developer/programmer/system builder to read this book and not blitz through it lapping it up. Yes if you've done some schooling in computing or computer science you may be happy with much of the content but you'll surely find things you've either not thought about before in much depth or just wasn't explained in quite the elegant way that Petzold does. For me, whether it was due to age, experience or just maturity through both I found it filled gaps in my memory and indeed gaps in student course material.Petzold opens up the world of computing through a concise linear storytelling format. Starting with a basis in Morse Code and Braille through the telegraph system, barcodes, boolean logic, circuits with memory, von neumann machines, adding peripherals, I/O devices and GUI interfaces we just about catch up to the modern era with talk of HTTP and the world wide web. Having pretty much built the systems (or simplified versions of) we're discussing in the incremental circuit and systems diagrams on the way.Admittedly there's some rather 'of their time' phrases and facts that raise a smile (low resolutions, high costs for 'small' HD storage sizes, usage of cassette tapes by consumers) but this is all still valid information when taken in the context of the time of writing.If you are a Developer/Programmer you're not going to go into work having had an epiphany of how better to do things, but you may have a new found respect for what you're doing and the many, many ingenious shoulders you are standing upon."
1,0735611319,http://goodreads.com/user/show/6751318-naessens,3,"My opinion on this book is really divided : on the one hand I enjoy some chapters, on the other hand I hardly managed to restrain myself from flipping through other chapters. Basically, this book designs and builds a basic computer by introducing in each chapter a concept or a technology used inside computers. It was written from 1987 to 1999, consequently one shouldn't expect any description of newest technologies.It starts really slowly with the first chapters, but then things get more and more complicated. One of the things that bother me with this book is the difference in complexity between chapters. Some chapters can be easily understood by a junior school or high school student while some of the latest chapters remind me bad memories of electronic circuits from my engineering school years. For example, a whole chapter is dedicated to explain how to communicate with your neighbour using a flashlight, an other chapter tackles the same issue with light bulbs and electrical wires, whereas all the gates or all the flip-flops are dealt with in a single chapter. I admit I have never been either fond of or good at electrokinetics, but I confess I didn't try to understand how all the electronic circuits of these later chapters work. I guess these chapters mostly interest hard code computer enthusiasts, but don't they already know these stuffs ?Besides, few chapters are a little boring : a whole chapter to describe every op-code of Intel 8080, come on ! Does the decimal system really deserve a whole chapter ? In my opinion, decimal and alternative number systems should have been presented in a single chapter instead of two.Moreover, the huge difference in complexity leads to some contradiction. The binary number system is so well described that a high school student can easily understand it, binary addition and subtraction are very detailed, but multiplication is done with a simple inefficient loop ! In my opinion, it would have been opportune to present at least a more efficient version based on the binary representation of the multiplicand as well as introduce exponentiation by squaring (a.k.a. square-and-multiply or binary exponentiation).Additionally, I think that Charles Petzold tries to explain in too many details how each part works so that readers with less technical knowledge can understand, but in the end I guess these readers get lost or confused by so many details anyway, whereas a few technical references are missing. For instance, both Von Neumann and Harvard architectures are described but I don't recall them being mentioned.Nevertheless, I really liked when the author gives historical anecdotes or references. The chapters I enjoyed the most are the ones where Charles Petzold gives readers some background history to introduce a concept or technology (for instance, Morse and Braille's codes, Bell's telegraph, the invention of telegraph relays, the evolution of transistors, chips or programming languages).Eventually, I find it a bit contradictory for this book that most of the interesting chapters are the less technical ones indeed. Moreover, due to the important difference of knowledge required to understand chapters, I don't think someone may understand or find interesting every chapter."
2,0735611319,http://goodreads.com/user/show/7557125-cardinal-biggles,5,"Raise your hand if you think metaphors and analogies should be used sparingly. I'll raise my hand with you. This book is for us.After reading this book, I can see behind the pixels on my computer screen. I know what I'm really looking at. So many layers of abstraction are removed by learning about how logic gates can be arranged as processors and RAM, how code is simply a representation of those microscopic switches being flipped, and how pixels are simply a graphical interpretation of the state of particular switches. Moreover, I also have a little bit of an understanding of the historical evolutions these inventions and conventions went through: not just how computers work, but why they work that way and how they came to be.The book was tougher to grasp than I thought it would be (I do not have an extensive background in electronics or programming). Although it started off easily, it became progressively more complicated except for the last chapter or two. Of course, this was to be expected, as the book began with the basic building blocks of a computer, and built progressively more complicated systems from those initial components. However, the problem wasn't really a result of the subject matter, but of the writing style, which seemed to grow more terse in later chapters. I was left with the impression that the author felt he was running out of space, which I'm sure he was; it must be difficult to keep a book with such a vast scope to a manageable size and prevent it from turning into a reference manual. I would characterize this book as grueling, but that might be because I was obstinate in making sure I fully understood every detail of every page. There were a few pages that I had to pore over repeatedly until I received a eureka moment. A few more explanatory sentences here and there would have alleviated this, but ultimately, drawing my own conclusions was very rewarding. The book seemed to recover from its gradually adopted terseness with an appreciated but sudden reference to the first chapter in the very last sentence. Someone less focused and more inclined to skim might find this book to be a bit lighter reading, but it still only took me a few days to read the whole thing.I was surprised to see that the book did not really cover how transistors work at the electron level, which leaves what I consider to be a major gap in any understanding of how modern computers based on integrated circuits work. The text says that transistors are functionally equivalent to electromechanical relays or vacuum tubes and work similarly, but hardly any more than that. This missing knowledge is something that would have been appreciated and wouldn't have taken up much space. It seems like an especially glaring omission when juxtaposed with the inclusion of a few pages on EBCDIC, an obsolete alternative to ASCII text codes descended from paper punch cards.Despite these minor gripes, this is a really great book, and I highly recommend it to anyone who has the interest and persistence to get through it. It teaches and ties together many mathematical and electrical concepts, and the payoff for the reader is a new perspective on computing. Despite being first published in 1999, it hardly seems dated at all, probably because it's really a history book and most of the computing history it covers happened in the 1980s and earlier. All computing history after that is basically just increasingly complex variations on those simpler foundations. A sequel would be welcome.P.S. I think I've discovered a typo in the assembly code program on page 322. It seems to me that there should be an additional ""AND A,0Fh"" after the four lines of ""RRC"" and before the first ""CALL NibbleToAscii"" line. If I'm wrong, would anyone mind explaining why? And if I'm correct, would anyone mind giving me peace of mind by confirming this? Thanks! :)"
3,0735611319,http://goodreads.com/user/show/172457-mike,5,"Electricity is like nothing else in this universe, and we must confront it on it's own terms. That sentence, casually buried near the beginning of the book, exemplifies the engineer's muse: a striving to become aware of the inhuman, how it operates, and to find means of creating a socket for human enterprise, something to extend the fallible chassis of our flesh.The first two-thirds or so of this book follows a double track. One track covers the ways in which meaning may be encoded into messages, the other weaves repetitions of a relatively simple device ‚Äî the telegraph relay ‚Äî into machines that marshall electricity into the forms of logic and memory. These two tracks eventually coincide at the device we know as a computer. Though it would be impossible to build a computer from telegraph relays, the machines we use today perform the same tricks with electricity that were possible in the 19th century.The last third of the book is more concerned with the makeup and successive improvements in implementation of the devices that embody the marriage of electricity and meaning. For someone like me, accustomed to the elves of the internet bringing me a regular helpings of news, porn, and status updates from the virtual sm√∂rg√•sbord, it was interesting to see how they have been made so much easier to use since the era of assembly code and text terminals.Regarding electricity, that prime mover of the information age, it has struck me that electricity is the stuff minerals dream with, and we may have subjected an alien order to the vagaries of our desire without being prepared to one day pay the price. We live, all of us, in an era of debt, making allowances for even a future of cities submerged and massive conflicts fostered by drought. When it finally comes time to pay off our mineral deficit, will it be our dreams ‚Äî that which makes us human ‚Äî to ultimately be forfeit?"
4,0735611319,http://goodreads.com/user/show/1363537-yevgeniy-brikman,5,"Every single person in tech should read this book. Or if you're just interested in tech. Or if you just want a basic appreciation of one of the most important technologies in human history‚Äîthe computer. This book contains the best, most accessible explanation I've seen of how computers work, from hardware to software. The author manages to cover a huge range of topics‚Äîelectricity, circuits, relays, binary, logic, gates, microprocessors, code, and much more‚Äîwhile doing a remarkable job of gradually building up your mental model using lots of analogies, diagrams, and examples, so just about everyone should be able to understand the majority of the book, and gain a deep appreciation of what's really happening every time you use your laptop or smartphone or read this review online. I wish I had this book back in high school and college. I've been coding for 20 years and I still found a vast array of insights in the book. Some of the topics I knew already, and this book helped me appreciate them more; others, I knew poorly, and now understand with better clarity; still others were totally new. A small sampling of the insights:* Current is the number of electrons flowing past a point per second. Voltage is a measure of potential energy. The resistance is how much the substance through which electricity is flowing resists the passage of those electrons. The water/pipes analogy is great: current is similar to the amount of water flowing through a pipe; voltage is similar to the water pressure; resistance is similar to the width of the pipe. I took an E&M physics course in college and while I learned all the current/voltage/etc equations, I never got this simple, intuitive understanding of what it actually means!* We use base 10 because we have 10 fingers; a ""digit,"" after all, is just a finger (so obvious when you actually take a second to think about it!). Had we been born with 8 fingers, like most cartoon characters, we'd probably use base 8 math. Computers use base 2 because building circuitry based on two states‚Äîthe presence or absence of voltage (on and off, 1 or 0)‚Äîis much easier than circuitry based on ten states. * The notation we use in math is essential. It's not about looking pretty or not, but actually making the math easier or harder. For example, addition and subtraction is easy in Roman numerals but multiplication and division are much harder. Arabic numerals make multiplication and division much easier, especially as they introduce a 0. Sometimes in math, you switch to different coordinate systems or different geometries to make solving a problem easier. So it's no surprise that different programming languages would have the same properties: while any language can, in theory, solve the same problems as any other, in practice, some languages make certain problems much easier than others.* This book does a superb job of showing how logic gates (AND, OR, etc) can be built from simple physical circuits‚Äîe.g., from relays, which are much easier to imagine and think about than, for example, transistors‚Äîand how easy it is to do math with simple logic gates. I remember learning this back in college, but it still amazes me every time I see it, and with the crystal-clear examples in this book, I found myself smiling when I could picture a simple physical circuit of relays that could do arithmetic just by entering numbers with switches and passing some electricity through the system (e.g., to add, you have a sum and a carry, where the sum is an XOR and the carry is an AND).* The explanation of circuits that can ""remember"" (e.g., the memory in your computer) was superb and something I don't remember learning at all in college (how ironic). I love the idea that circuits with memory (e.g., latches) work based on a feedback mechanism: the output of the circuit is fed back into the same circuit, so if it gets into one state (e.g., on, because electricity is flowing through it), that feedback mechanism keeps it in that state (e.g., by continuing to the flow of electricity through it), effectively ""remembering"" the value. And all of this is possible because it takes a finite amount of time for electricity to travel through a circuit and for that circuit to switch state.* The opcodes in a CPU consist of an operation to perform (e.g., load) and an address. You can write assembly code to express the opcodes, but each assembly instruction is just a human-friendly way to represent an exactly equivalent binary string (e.g., 32 or 64 binary digits in modern CPUs). You can enter these opcodes in manually (e.g., via switches on a board that control ""on"" and ""off"") and each instruction becomes a high or low voltage. These high and low voltages pass through the physical circuitry of the CPU, which consist of logic gates. Based purely on the layout of these logic gates, voltage comes out the ""other end,"" triggering new actions: e.g., they may result in low and high voltages in a memory chip that then ""remembers"" the information (store) or returns information that was previously ""remembered"" (load); they may result in low and high voltages being passed to a video adapter that, based on the layout of its own logic gates, results in an image being drawn on a screen; or they may result in low and high voltages being fed back into the CPU itself, resulting in it reading another opcode (e.g., perhaps from ROM or a hard drive, rather than physical switches), and repeating the whole process again. This is my lame attempt at describing, end-to-end, how software affects hardware and results in something happening in the real world, solely based on the ""physical layout"" of a bunch of circuits with electricity passing through them. I think there is something magical about the fact that the ""shape"" of an object is what makes it possible to send emails, watch movies, listen to music, and browse the Internet. But then again, the ""shape"" of DNA molecules, plus the laws of physics, is what makes all of life possible too! And, of course, you can't help but wonder what sort of ""opcodes"" and ""logic gates"" are used in your brain, as your very consciousness consists entirely of electricity passing through the physical ""shape"" of your neurons and the connections between them.There are a few places the book seems to go into a little too much detail‚Äîe.g., going over all the opcodes of a specific Intel CPU‚Äîand a few places where it seems to skip over all the important details‚Äîe.g., the final chapter on modern software and the web‚Äîbut overall, I have not found another book anywhere that provides as complete of a picture of how a computer works. Given the ubiquity of computers today, I'd recommend this book to just about everyone. It'll make you appreciate just how simple computers really are‚Äîand how that simplicity can be used to create something truly magical.As always, I've saved a few of my favorite quotes from the book:A computer processor does moronically simple things‚Äîit moves a byte from memory to register, adds a byte to another byte, moves the result back to memory. The only reason anything substantial gets completed is that these operations occur very quickly. To quote Robert Noyce, ‚ÄúAfter you become reconciled to the nanosecond, computer operations are conceptually fairly simple.‚ÄùThe first person to write the first assembler had to hand-assemble the program, of course. A person who writes a new (perhaps improved) assembler for the same computer can write it in assembly language and then use the first assembler to assemble it. Once the new assembler is assembled, it can assemble itself."
5,0735611319,http://goodreads.com/user/show/8940497-alex-palcuie,5,"If you work with computers and didn't read this book, you are lame."
6,0735611319,http://goodreads.com/user/show/10471943-igor-ljubuncic,5,"This is a great book. Surprisingly interesting.While the subject matter is not a new thing to me - far from it - the way the author goes about telling the story of how modern computers came to life is exciting, engaging and fun. He starts with morse and braille, talks about the principles of mathematics and information, explains the critical concept of switches, and finally moves into the world of circuit boards and binary data, cultimating in ALU. After that, he discusses the idea of analytical and computational engines and machines developed through the late 19th and early 20th century, before we finally start seeing the modern computer around 1940s, with Turing and von Neumann laying down the foundations of what we know and use today.The book is really cool because it's also a nostalgic trip down the memory lane. Charles mentions the famous Bell Labs, the legendary Shannon, Ritchie, Noyce, Moore, UNIX, C language, and other people and concepts without which we would not be sitting here, writing reviews on Goodreads. Or we might, but the fundamentals of the computing devices would be completely different.Computers sound like magic, but the thing is, they are a culmination of 150 years of electric progress, 200 years of data/information progress, and about 350 years of math progress. The first boards, the first programs, the first assembler and the first compiler, they were all written by hand. Control signals are still essentially the same, and if you look at a typical x86 Intel processor, the legacy support for machine instructions goes back to the first microprocessor. The problem is, when you condense the centuries of hard work into a cool, whirring appliance, it does feel like magic.The author wrote the book in the late 80s and then revised it in the late 90s, so some of the stuff may look quaint to us, like the mention of floppy disks, VGA displays and such. But then he also shows uncanny foresight around overall information exchange, because the information principles are universal, and he correctly predicted that Moore's Law would taper out around 2015.He also cheated a little.He described the flip-flop as a perpetuum mobile, which can be sort of excused, and he also skimmed on the concepts of oscillators, transistors (and did not mention capacitors), but then those are fairly complex, and I guess it's not really possible to do that without going deep into the fields of physics and electric engineering. Excusable, because the book is compelling and delightful.Even if you have a PhD in Physics from a top university or have done computer science all your life, you can rap in ASM and name all LoTR characters by heart, this is still a good read. Do not feel like you'd be highschooling yourself with silly analogies. Far from it. This is a splendid combo of history, technology, mathematics, information, and nostalgia.Highly recommended,x49 x67 x6F x72"
7,0735611319,http://goodreads.com/user/show/7970562-lynn,4,"I have been an IT professional for 20 years, but I never knew what the switches on the front panel of the Altar computer were for. I do now.In fact, because of this book, I know many things about how a computer really works that I never did before. I think this book is great for anyone, except Electrical Engineers who would be bored. Having some background in computers probably makes this book easier to get through, but Petzold assumes nothing and starts from scratch. He does a good job of making potentially dry subjects fairly interesting.I think an update to this book would be great, because the discussion of 1999 capacity and pricing makes the book feel dated. Also, the last chapter seemed rushed and not as well focused as the rest of the book.So, if you want to know how any computer really works, read this book."
8,0735611319,http://goodreads.com/user/show/19846169-jan-martinek,5,"What a ride! A book about computers ‚Äúwithout pictures of trains carrying a cargo of zeroes and ones‚Äù ‚Äî the absolute no-nonsense book on the internals of the computer. From circuits with a battery, switch and bulb to logic gates to a thorough description of the Intel 8080. Great way to fill blanks in my computer knowledge.The book takes the approach of constructing the computer ‚Äúon the paper and in our minds‚Äù ‚Äî that's great when you're at least a little familiar with the topic, maybe not so when trying to discover a completely unknown territory (but the author takes great lengths to go through everything step by step ‚Äî e. g. the various gates, binary subtraction, memory handling, etc.).In a way, this is a perfect book on the topic. If you know a better one, I want to read it."
9,0735611319,http://goodreads.com/user/show/104131377-miranda-sikorsky,5,"It is a great book, I demystified some thoughts I had about software architecture."
10,0735611319,http://goodreads.com/user/show/1525909-jule,5,"I LOVE this book. I regard myself an innocent computer illiterate. And Petzold helps me to walk inside an electrical circuit, a telephone, a telegraph, an adding machine, a computer, and to understand the basics behind the design, of what is going on inside. I start getting the math, the logic behind all this technology that has become pretty much the center of my life today. And I should understand the logic behind the center of my life, right? What is so good about this book: it is written in a simple language anyone can understand. It uses examples that are entertaining and amusing. Like explaining an electrical circuit with AND, OR, NOR and NAND gates to pick your favourite kitty from a bunch of neutered, unneutered, black, white, brown, tan, male and female cats in their various combinations. Also, he interlinks the historical evolution to the logic and development of technology as we use it today, so you get pretty much a round picture of the whole thing. Love it!"
11,0735611319,http://goodreads.com/user/show/2298410-damon,4,"This book basicaly tries to take you from the very basics of how to encode information, such as how binary is used to represent complex information, to understanding how a computer uses information like this to perform intricate operations. The route between those two points is the interesting part, and there was some parts that I foudn really illuminating and important. For example, I didn't understand hexadecimal numbers (or indeed what base 4, base 8, etc) numbers meant before I read this book. Similarly I knew a fair amount about how various electrical gates work but not how by pairing multiple gates together you eventually get to RAM, a CPU, etc.It did lose me at times, however, and I zoned out a bit when Petzold was talking about the way in which math calculations are carried out using gates and binary information. I probably should have paid more attention, because this is fundamental to understanding how higher level systems work. I really enjoyed the explanatuon of how certain chipsets were important, especially the 8080 and the 6800, and then the creation of assembly language and compilers. Most striking to me was the realisation that modern computing is essentially a brute force operation. We are using the same switches that were invented 150 years ago or so but now they are gigantically faster, smaller and on a exponentially more massive scale. "
12,0735611319,http://goodreads.com/user/show/29089487-alex-telfar,5,"Very close to my ideal book. Starts from understandable foundations and builds from there. Charles doesnt try to explain through high level metaphors (that do a poor job of capturing the truth -- I am frustrated after picking up another apparently interesting physics book only to find it contains no math), rather, he slowly builds on simple examples. And while it does get pretty complex, Charles doesnt avoid it. !!!For a while I have been frustrated about my understanding of computers. I understood how bits can encode information, what the von Neumann architecture was and some of it flaws, how programming languages are compiled to assembly/machine code, what transistors are and how to make logical circuits. But I could never really link them together. I am still a little hazy, and I think I will have to go over a couple of chapters from no. 17 onward (automation, buses, OS) just to cement and clarify, but understanding feels close.More thoughts to come on my blog. Just drafting atm."
13,0735611319,http://goodreads.com/user/show/4073465-carlos-martinez,5,"Such a fun and interesting book. Petzold goes back to the very basics to explain how to build a computer (of sorts) from the ground up. First he explains binary (via morse code and Braille), then he introduces relays and switches, then gates and Boolean logic, and before you know it you're building an electronic counting machine. He continues with a potted history of transistors, microchips, RAM, ROM, character encoding and all sorts of other fun stuff.I skipped over some pages, because I don't actually need to know the full set of opcodes for a 1970s CPU, no matter how significant they are to computing history.The only obvious 'flaw' is that the book has aged a bit. Written in 2000, it just about manages a mention of the internet/HTTP/TCP-IP and modems, but not wifi, cloud computing, touchscreen devices, and the brave new world of machine learning. Personally I don't think that detracts from the book at all - the really interesting stuff runs from around 1870 to 1970.Definitely recommended for those that didn't study (or don't remember much) computer science."
14,0735611319,http://goodreads.com/user/show/4216776-baq,5,"Wow. I wish I had had this book back when I was taking my first Computer Architecture course in college! It carries you along from the very fundamentals of both codes (like braille) and electric circuits in the telegraph days all the way to the web in a way that even a layperson could understand, with plenty of verbal and diagrammatic explanation. It does at points get pretty deep into the weeds but I really appreciated the author's efforts to provide such an exhaustive dive into how computers work (and I regained much of my awe at these machines we take so for granted nowadays). The final chapter was a rushed overview of the web and felt almost like an afterthought after the thoroughness of the rest of the book, but I didn't ding the author on it--there's plenty of great writing about how the web works that you can read elsewhere. Read this book to gain a deeper understanding and appreciation for the birth of the modern digital age. Thank you Charles Petzold!"
15,0735611319,http://goodreads.com/user/show/47635748-laura-marelic,5,"This book is the perfect depth for novices but also people who are ‚Äúin tech‚Äù and don‚Äôt really understand how it all works (like me). I can now look around at all the electronics in my house and feel like I know what‚Äôs fundamentally going on. Knowledge is empowering! The last chapter of the book felt a bit rushed and ended abruptly, but maybe that‚Äôs just my wanting the book to go on longer/end at present day. Overall, I loved it and will surely be recommending it to anyone who asks how computers work. üë©üèª‚Äçüíªü§ñüëæOh, also I am simultaneously reading The Innovators (Isaacson) on audio and the two books pair very nicely. It was great to read about the tech in Code and then the story of who‚Äôs behind it in The Innovators. I recommend this pairing!"
16,0735611319,http://goodreads.com/user/show/1239910-rik-eberhardt,4,"In brief: be prepared to skim through at least 25% of this book! If I had this book in a seminar freshman year, I might have completed the Computer Science program. In a very fun manner, this book presents 3 years of introductory CS curricula: discrete structures, algorithms, logic gates, ... After reading this during two cross-country flights, I better understand (and remember) classes I took 10 years ago. Almost makes me want to try again (*almost*)."
17,0735611319,http://goodreads.com/user/show/3126915-imi,4,"This book has really taught me a lot, despite the fact that many of the later chapters lost me somewhat; it felt like it became much more complicated and hard to follow after the earlier chapters, which were great, slowly paced and well explained. While Petzold does assume the reader is starting from scratch, I think it would be easier to follow later on if you had some background in computers/technology. As it was, I had to bombard my dad (an electronic engineer) with questions to even make it to the end of some chapters, but then I haven't attended regular maths/science classes since about age 14, so maybe it's not surprising that I'm missing some of the needed background information.It is outdated, having been written in 1999, but I guess the history, which Petzold follows nearly chronologically, hasn't changed, and the early history is necessary to understand what has come since this book was written. Having said that, the last chapter (on the 'graphical revolution') was strangely rushed and an updated edition would do it some good, I think. Even if I couldn't grasp all of the technical detail, the majority of this book was extremely eye-opening and I have definitely come away from it with new found respect for these devices that we now use day-to-day. Even while using this laptop to complete a supposedly ""simple"" task such as writing this review, I am fascinated by how much work has gone on behind the scenes to allow me to do this. It's fairly awe-inspiring, the more you think about it."
18,0735611319,http://goodreads.com/user/show/83327216-alisa-mansurova,5,"Just finished reading my b-day gift, the 'Code' by Charles Petzold - probably the best engineering book I've ever read. By saying 'engineering', I mean it. Unlike other computer science books, the 'Code' teaches how computers work in a nutshell. It leads you from the very basics like morse & braille codes to boolean algebra and various numeric systems, from simple tiny electric circuits which bulb the lamp to primitive adding machine (built from relays, hehe), up to history of development and enhancement of computers in the 20th century. There's not much programming or CS (apart from some machine code and assembly language examples). Still, the purpose of the book, as I mentioned, is rather to explain the nature of computer codes and hardware at the very low-level. Written in 1999, the book yet actual nowadays (well, there are funny moments regarding computers' capacity and performance, and probably some other stuff but those don't matter much).Highly recommended for those (like myself...) who work closely with computers but have a lack of engineering education to feel comfortable with this magic going on around when you write your code"
19,0735611319,http://goodreads.com/user/show/6645592-mark-seemann,4,"Since I loved Charles Petzold's The Annotated Turing: A Guided Tour Through Alan Turing's Historic Paper on Computability and the Turing Machine, I wondered if he'd written other books about the foundations of computer science. Code seemed like an obvious candidate.This book explains, in as much details as you could possibly hope, and then some, how a computer works.Since I've been a professional software developer for about two decades, the title of the book, Code, gave me an impression that it'd mostly be about the logic of software - something that I already know intimately. The first chapters seemed to meet my expectations with their introductions to binary and hexadecimal numbers, Boolean logic, and the like.Soon, though, I was pleasantly surprised that the book was teaching me something I didn't know: how a computer actually works. It starts by explaining how one can construct logic gates from relays, and then builds on those physical creations to explain how logic gates can be used to add numbers, how RAM works, and so on!Like The Annotated Turing, this part of the book met me at exactly the level I needed. So technical, with no shortcuts, and nothing swept under the rug, that I felt that I deeply understand how things work, but still thrilling and engaging. Whenever I found myself with a question like, ...but how about..? or ...but what if..?, I soon learned to read on with confidence because I consistently found answers to my questions two or three paragraphs further on.The final part of the book, again, moves into territory that should be familiar for any programmers, such as ASCII, high-level programming languages, graphical user interfaces, and such, and that, unfortunately, bored me somewhat. Thus, the overall reading experience was uneven, which is why I only give it four stars.Would someone who's not a professional programmer rate it higher? I don't know. I could imagine that for some, the explanation of logical gates, adders, latches, etc. made from relays would be too technical."
20,0735611319,http://goodreads.com/user/show/3335829-geoff-rich,4,"I really enjoyed most of this book. The slow unfolding of how computers are built actually work was extremely fascinating - from simple lightbulb circuits to logic gates to RAM to keyboards and monitors. Unfortunately, parts of this book seem quite dated (most anything discussing ""contemporary"" technology, i.e. 1990s computers) and the final chapter on the graphical revolution goes through way too much, way too fast to be of any use. A few chapters were tempting to skim For example, Petzold includes 25 pages on the machine code instructions of an Intel 8080 microprocessor - did we really need all that detail? The majority of the book, however, is great - I had never really delved into logic gates and circuitry, so it was truly eye-opening even if I couldn't fully understand some parts. I think if I read this when it was released it would manage to eke out 5 stars from me. From a 2017 viewpoint, however, it only manages 4. I'd still recommend it to anyone curious how computers work, down to the nitty-gritty of ones and zeroes. Most of it should be accessible to a layperson, though I may be blinded by my own CS experience. Even if there are parts you don't understand, you'll come out of it with a greater understanding and appreciation of the technology you use daily."
21,0735611319,http://goodreads.com/user/show/14897481-trevan-hetzel,5,"With a desire to learn how the high level code (HTML, CSS, JavaScript, etc.) I write on a daily basis actually makes its way through the magical land that is a computer and returns pleasantries to a human being behind the screen, I sat down with this ""Code"" book. The book is very intriguing from the start, beginning with the earliest forms of code (Morse, Braille, etc.). Petzold spends a long time laying down the basic blocks of electrical engineering before progressing to how bits flow through a circuit board and control things. I'll admit that I got very confused at times as to how a computer works, but Petzold gives you all the information you need. It's just a matter of how much time you're willing to spend re-reading and studying each piece of information he gives (there's a LOT to take in). If you have a background in electrical engineering, this book would probably make a lot more sense to you than it did to me. But, nonetheless, it will sit on my shelf awaiting the time I start playing with Arduinos and hacking on things. At that point, the book will REALLY come in handy!"
22,0735611319,http://goodreads.com/user/show/54320432-ieva-gr,4,"The book reminds me of the courses that students usually have during the first year of the University. It provides a general overview of how computers function. Starting from workings of an electrical circuit and building up to various logical elements with gradually increasing complexity. It also discusses some relevant historical moments as a typical professor in a typical lecture would do and ends with a broad overview of personal computers as they were in 1999. The summary on the back of the book says ‚ÄúNo matter what your level of technical savvy, CODE will charm you ‚Äì and perhaps even awaken the technophile in you‚Äù. That didn‚Äôt happen at all. At some more detailed and complex parts reading the book felt like going through a swamp waste deep. But I think it really helped me to gain some overview on the history of computers and a better understanding of things I sort of kind of heard of but never bothered to read about. "
23,0735611319,http://goodreads.com/user/show/972512-k-c,4,"This was a wonderful non-fiction read, especially the first 15 or so chapters. Chapter 17 (""Automation""), however, was where I began to feel a bit in over my head. While that chapter was fairly thorough, when I got to later chapters and realized I couldn't quite grok what was going on in these chips, it was hard for me to tell whether I was holding myself back by not fully understanding the concepts of Chapter 17, or if Petzold was simply glossing over some of the details that might have clued me in. It was probably a combination of both. While I did enjoy the later chapters as well, much of it felt so rushed compared to the earlier, slower pace of the book. Recommended for anyone who would really like to understand the basic concepts behind computer technology, but doesn't want to go back to graduate school."
24,0735611319,http://goodreads.com/user/show/4644723-eva,5,"This book is quite incredible. You start with braille and simple light switches, make your way to oscillators, flip-flops and multiplexer, and suddenly you understand how computer hardware works. And that's coming from someone who already thought they ""sorta"" understood how it worked. I didn't really. Now I do. Best bottom-up education ever. "
25,0735611319,http://goodreads.com/user/show/2645334-travis-johnson,5,"I really, really truly love this book. The beginning is slightly slow, but after the 1/3 mark or so, I couldn't put it down(literally. hello, 5am.)I probably learned more about architecture from this book than the quarter in my Architecture & OS class at university."
26,0735611319,http://goodreads.com/user/show/3494451-randall-hunt,5,"Definitely one of the greats. If not already, it soon will be, a staple of computer science literature. It's both a narrative history of Computer Science and a brilliant introduction to systems and programming. This book should be a pre-requisite for introductory CS classes."
27,0735611319,http://goodreads.com/user/show/16730283-ondrej-urban,5,"One - in this case one in how the Queen would use this - cannot really talk about this book without comparing it to But How Do It Know? - The Basic Principles of Computers for Everyone, since they cover a lot of the same ground (and one has read the other one first).Code's mission in life is to help the user understand the basic principles behind the computer design and convince them that it's not actually that tricky and that your great-grandparents could have build one themselves. Naturally, there is a lot of overlap with But How Do It Know, since there don't seem to be better ways do design a basic RAM. Starting with looking for ways to transfer information, Code goes from flashlights to telegraph to relays to the logical gates, doing a better job than the other book, which introduces the NAND gate as the basic black box and goes from there. Point Code, reading about that is super enlightening and exciting!The middle part of both books is kind of similar, spend building basic computer parts out of logical gates. I'd maybe lean towards Code doing a bit of a better job reminding the reader of the basics but the other book prevails in its focus and overall a better teaching approach, explaining every single thing done, while computer parts seems to start randomly appearing at some point of Code (buses and registers being two examples).In the final part, Code goes a bit overboard with talking about not-so-basic stuff that at this level needs to necessarily happen at a bit of a high level. Disregarding it having been written in the later 90s and talking about DVDs will at some point take over from CDs as the main software distribution channel, I had a feeling that the author could have stopped a bit sooner, or perhaps expand the chapter on compilers a bit more. In any case, when it comes to the overall impression, the How book wins thanks to its laser-tight focus on building a computer and nothing else, while Code seems more open-ended and talking about particular technologies specific for a given time period makes it feel a bit aged. That said, Code is a great book to read to refresh your knowledge of the basic computer design that can add a lot of basics and fill in the picture. Highly recommended!"
28,0735611319,http://goodreads.com/user/show/6274559-angelos,4,"A very nice introduction into what makes computers tick. It's detailed enough to give you a sense on how things work, yet not overly complicated to intimidate you. I really liked the gradual introduction to concepts of increasing complexity where each builds on the one before it. I feel like I've learned a lot by reading this book, especially since we had no relevant computer architecture courses in college.That said, I have a couple of complaints.One is that I feel the author covers the initial, simple, concepts like Morse code, binary numbers, Braille etc in excruciating detail, yet is quick to cover complex concepts and areas as the book progresses on digital circuits, CPUs etc. Ideally I'd like fewer details on the initial concepts and a better and more detailed explanation of later ones.The second complaint, which is to be expected, is that the book was written in 1999. Although still highly relevant when it comes to computer architecture, it contains a lot of references that feel a bit dated, especially in the later chapters that cover multimedia (CDs, DVDs), GUIs, the WWW, etc.So, I highly recommend this book to anyone interested in how computers are built, from the ground up. I find it's a good fit even for CompSci students/graduates that want to fill-in their knowledge gaps like me. That said, this book is not an easy/quick read. It's pretty technical so prepare to put in some time to grok some concepts if you really want to understand how things work."
29,0735611319,http://goodreads.com/user/show/28989798-andrew,4,"Although there are a few chapters that I probably gained very little from (I‚Äôm looking at you flip flops!) , there were a majority of them that was incredibly useful in taking what seems a complicated subject matter and simplified it in ways that makes it accessible. When I was near the end I sort of circled back to the chapters on binary and hexadecimal and it definitely helped me with the conversions. I also really like how the author tied in the use of a flashlight for on and off, to morse code, to the computer age we are in currently. I would love for this book to be updated now that fiber optics and blu-ray are prevalent and that we are going more towards cloud computing. I‚Äôll be using this for a reference for years to come. The downside is that this type of book really needs some sort of practicals because when there are just random pages of assembly code and hexadecimal reference codes for a chipset that I am sure is no longer in any of my devices, it doesn‚Äôt add any sort of value to the reader."
30,0735611319,http://goodreads.com/user/show/35899146-jakub,5,"I wish I discovered this book earlier. It is a great introduction to computation and I do recommend it to everybody that would like to understand what is going on inside their laptops or phones.I found the ""big picture"" to ""the actual details"" ratio very suitable for a casual read. There are some parts that require more attention though - especially when the author explains the details of a data flow in the computer or the way Intel 8080 chip works. Nevertheless that's an inherent characteristics of any subject related to computation - some thinking is required to grasp the concept.All in all great introduction for non-programmers and even better refresher for people that are into programming but treat all the layers below the OS as a 'back box'. This is highly recommended."
31,0735611319,http://goodreads.com/user/show/11199869-prashant-ghabak,5,The author starts from electrons then electricity and builds right up to a high level programming language to explain fundamentals of a computer science while walking us through the history of the field. Very well written. 
32,0735611319,http://goodreads.com/user/show/37846711-andrew-alkema,5,"This was an excellent book. I love Charles' approach to explaining how a computer works. All of a sudden this collection of electrical circuits was a microprocessor. I really enjoyed the history of computing mixed in as well, that's something we did not learn enough of in CS at university, so I really enjoyed getting more of that here. Even though this book was written in 2000, the information is still just as relevant. To make the concepts simpler, most of the hardware he looks at in detail is from the 1970's anyway, so that part hasn't changed. The rest of the concept haven't gone anywhere. Don't let the age turn you off."
33,0735611319,http://goodreads.com/user/show/74276701-chad-lavimoniere,4,"A good overview of the basics of CS and early history, but at this point hilariously outdated. Could do with a major second edition. "
34,0735611319,http://goodreads.com/user/show/31121633-leonid,5,"Absolutely great book.It starts from the general idea of codes, like Morse or Braille, and then shows how computers designed and built, starting from relays, and finishing with asm (and then moves on to memory, storage, cpu design, text and graphics i/o)."
35,0735611319,http://goodreads.com/user/show/21307031,5,"A fascinating journey into the history and ""nature"" of computers written with an effort to make things easy to understand. A very interesting and important read for anyone related to IT"
36,0735611319,http://goodreads.com/user/show/4130219-moayyed-almizyen,5,Thank you Charles Pertzold ..
37,0735611319,http://goodreads.com/user/show/45773827-rick-sam,5,"An excellent book to learn about bottom-up details of Computers. It starts with two friends trying to communicate with each other, Morse Code, Braille, Flashlights... Gates, ... Assembly Language, Operating System and finally Graphical Revolution. Overall, this would help you to understand from First Principles, you can build from there. What an impressive accomplishment and progress! I would recommend this to anyone interested in Software, Hardware, Computer Science. Deus Vult, Gottfriend"
38,0735611319,http://goodreads.com/user/show/84459920-nick,4,"When I started reading this book I had an inkling that this book is suitable for nerds/technical people but after reading the first few chapters I thought that probably I was wrong. As it turned out in the end, it's at best a mixture of two with a heavy hilt towards first.The book explains in detail the working of a computer system along with the history of its progress over the year. It starts with a very innocuous example of communication between two close by living friend using just a torch. After that, chapter after chapter, it builds upon that by explaining in subsequent chapters the shortcomings and the solutions developed by industry, until it reaches the working of high-level languages and how a computer does calculations!!While the language of the book is lucid and definitely an improvement over dry engineering books that a lot of people here would be so used to, it couldn't help falling in that trap as it progresses. If you don't follow the writing closely and sequentially the preceding chapters may just be undecipherable to you. The book goes deep into circuits and their working and although it starts with a very good example and maintains continuity, it becomes very tech oriented and complex. Even after having studied a lot of the topics from the book during graduation, I myself couldn't help but skim over some parts as the writing became too technical and fully resembles an engineering book. Guess you can't escape that however much you try!!This book is more suitable for nerds/technical-oriented readers. If you want to know how and what of your system, this book can give you a very good idea. I may even say that if that's how the engineering books were written a lot more people would benefit. If someone here, during their course of engineering, is starting with the subject of digital systems; I strongly suggest reading this book cover to cover over (say over a period of two weeks or a month) before they start with their assigned books. You would be thanking me and the author later!! For others, if you pay close attention from the very start, this is the book to gain some understanding system since a very complex topic is explained in a very easy language starting with a very basic and relatable example and improving upon it. If this book doesn't do it for you, nothing would."
39,0735611319,http://goodreads.com/user/show/5268938-lauren,4,Fascinating look at code and computers and the building blocks that make them. Not my usual cup of tea but fascinating nonetheless. From flashlight communication to 2000-era processors the author builds a computer.
40,0735611319,http://goodreads.com/user/show/15643804-gustav-ton-r,5,"Basically everything you need to build a modern computer, both hardware and software, from scratch. Perfect read after the apocalypse. Too few people grasp these relatively easy concepts that power modern society through the internet and the cell phones in our pockets. This book summarizes computer technology in a clear and concise way."
41,0735611319,http://goodreads.com/user/show/8017610-james-millikan-sj,5,"A fascinating account of how electronics work, using stories, clever examples, and clear illustrations. Starting with elementary logic, examples from Morse code, and basic circuits, Petzold explains how to build a computer.This sounds like a rather dry read, but in reality it is an engaging and well written journey that weaves electrical engineering and history together into a compelling synthesis.If you‚Äôve ever wondered how your digital watch works, or the structure of binary, or how erasable digital memory is possible, look no further than Petzold‚Äôs classic. Highly recommended."
42,0735611319,http://goodreads.com/user/show/4606766-scott-johnson,5,"This, finally, was what I was looking for in my quest to understand computers.We start with some really boring, basic things about the most elementary circuits. It gets a pass because it's necessary if you're coming in blind, but I've taken courses in things like the physics of transistors, so I wasn't a fan personally. The same goes for assorted later sections on number systems (already overly familiar with Binary and Hex, thanks).Then we move into how you take very simple components and construct logic gates. I was vaguely familiar with their use, but had not actually seen one built from relays.The theme of building further complexity just by stacking blocks is introduced, and is the key to everything. Once I understand how a NOR gate works, I don't need to constantly trace the paths, we can shortcut now to using the correct symbol in the circuit with two (or more) inputs and an output. Similarly, 3+ input gates that are really cascades of 2+ gates being abbreviated as what without context looked like a new component makes complete sense.Using that, we construct a simple circuit to add binary bits. My only big criticism is that this is where we're building up to how a microprocessor works, but we suddenly veer left to look at memory.This was the most enlightening and involved portion. We gradually build up from a single bit being stored as a consequence of a quirky configuration of logic gates that ""trap"" a bit to adding things like control mechanisms (only write to the bit if this input is on), refreshes, and, most importantly, arrays of these storage bits.This was where I finally started to settle into this paradigm. Trying to think about the big picture is impossible. We're talking about quantities of components that the human mind cannot track, time spans we can't conceive, and frequencies that are impossible to imagine.Faith in the process is the key. I understood the science behind relays and transistors. I understood how these allow us to introduce logic. I understood how combining these gates leads to operations like addition or controlling the routing of a circuit. I understand how a moderate array of these things leads to a useful component like a byte of memory. I understand how each bit can be addressed to be written, read, and cleared.So I just needed to learn to relax and remember that I know these things, and that all we're doing now is that same thing....a few million times. Yes, that seems impossible, but you don't need to deal with all of that. You just have to do what we did with cascading logic gates being combined into 3+ input gates: Simplify it into a known component. Who cares what's happening in each of the 500 black boxes that all do the same thing. We just know we need this input and this is how output behaves.Once you understand how to chain them together into large arrays, why not simplify this into just ONE big black box, the DIMM. I now know how a signal from the processor (or, later, storage device) goes through an unimaginably long branching addressing system.I could not easily trace exactly which circuit a signal will follow to access a certain memory address, but I have faith that, given what I understand, I COULD do it if I really had to. I know what I understand really well on a small scale still works perfectly fine even if there are a few tens of thousands of branches now. It's not even complicated, every single node in the branching decision tree is identical, it's just big.This was a little more fuzzy when we returned to the microprocessor and the idea of instructions and opcodes, but I think I get it. This was unfortunately not covered in as much detail, and it's the one section I do want to do more reading about specifically.My understanding is that instruction sets are hard-wired into the processor. Similar to memory addressing, there seems to be some kind of input gate where the opcode instruction is interpreted to route the data it's carrying to the right part of the chip? Or there could be dedicated inputs for the specific operations, and the routing is done on the motherboard or by a secondary controller chip? Or both?It's a little unclear to me exactly how that control process happens, but I DO understand the important part: That processors have a finite instruction set, and perform very simple operations......but really, really fast. Another revelation was the idea of a clock signal as an input that ratchets the data forward one step through the circuits per iteration. I guess a lemma to my ""it's not complicated, just big"" theorem would be, ""don't think of circuits as continuously evolving, they are discretely iterating"".I was struggling at first to fathom how just adding these two bits together in any way led to these characters being displayed on my screen right now. I kept thinking, ""But you still have to do this, and this, and....."" Ok.....so then we do those things. In 2018 we're doing BILLIONS of things EACH SECOND. Response times are in nanoseconds. Sure, it's a lot....but that's fine. It's not complicated, it's just a tediously long chain of really simple events.Storage was easy. There was a bit of hand waving about how the addressing translates to moving the read head to this part of the disk or whatever, but I know enough about RAM addressing now to assume how it works. I can assume that logic is built into a controller on that disk that's hard-wired for that particular hardware configuration (hence why you don't really need drivers for components like these.....and also makes me realize why drivers are necessary to essentially tell the rest of the computer what a new component's inputs, outputs, and instruction sets are).Displays were REALLY simple thanks to one phrase: ""visual memory"". This wasn't from this book, it was from my last read, Turing's Cathedral, in discussing the use of CRTs as memory devices briefly, so you could see the memory live. They said something offhanded about, ""That's all the display on your monitor is anyway, showing an array of memory as pixel values, "" or something like that. The video card just constantly accesses that visual memory, turns it into pixel instructions, and sends it out the adapter for the monitor (with its own hard-wired controller) to translate to the actual pixels.The last piece was....how does everything know where to send the data? Answer: BUS! I now get how you can have so many things on so few buses and not have every component getting useless data from the rest. I had never heard of three-state logic, but it makes total sense (especially after an explanation that ""0 and 1"" are really more like ""high and low"") that there could be a state that would essentially block a component from the circuit so it has the ""state"" that it's neither high nor low. It then follows quite simply that the address of that component on the bus would trigger that control circuit to leave that non-state and connect to the bus again.Put all of this together, and I get it now. It's kind of like one of those Magic Eye pictures: If I relax and let my eyes go fuzzy, I can see the kitty, but as soon as I try to focus on anything it dissolves into chaos, and at best I can resolve a few of the dots. I just need to stop worrying about the 4 billion dots on the paper, just relax my eyes and have confidence that they're all still there.Aside from the sequencing of memory and processors, I think this was really well structured to flow logically and build up these complicated ideas from just a few simpler ones. A big point in its favor, also, was introducing that idea itself at the very beginning so it doesn't blindside the reader. It lets you read seemingly scary ideas with the context that once you get it you can just file it away as another building block for later use.10/10 would recommend to anyone who, like me, really wants to understand how you get from simple circuits to complex operating systems without resorting to magic."
43,0735611319,http://goodreads.com/user/show/117278-simmoril,4,"One of the biggest difficulties that is unique to Computer Science is this idea of 'layers of abstraction' - interfaces created to help hide the complexity of the underlying layer. While this can be a boon when developing, it becomes a problem when those lower layers start misbehaving, and you don't know why. Or, at a more basic level, these layers of abstraction can make it hard to understand why things are the way that they are (like why computers don't count in base 10, or why I can't run Unix programs in Windows). Petzold's book attempts to resolve this issue by starting out at the absolute most basic, physical level of a computer and working up through the different layers until he gets to what is pretty much a modern day computer. Through the use of good examples and an informal style of writing, Petzold introduces the reader to a large number of fundamental topics in Computer Science/Engineering, such as binary, logic gates, processor design, assembly, operating systems and programming languages. Petzold does a great job of not getting too bogged down in the details, and only giving the reader enough information to move up to the next layer.I enjoyed Code a great deal. Although much of this wasn't new to me, Petzold still surprised me with some historical facts I was not aware of, and the book itself served as a nice refresher course for some of the things that I studied and promptly forgotten in college. Although this book won't turn you into an expert by any means, it is a great starting point for understanding just how computers actually work.The only small issue I had with Code were that some areas were a little tough to get through. Towards the middle of the book, where Petzold starts to assemble memory circuits and building a rudimentary CPU, I found myself glossing over a lot of the details. For a complete understanding, these chapters would probably require a couple of passes. Also, the last couple of chapters seemed a little more hodge-podge than the rest. While they are good tidbits of information to know, it just seemed a little less organized than the rest of the book.Overall, Code is a fantastic book, and I highly recommend it as a starting point for anyone who wants to learn more about computers. In fact, I'd nearly go so far as to say it should be required reading!"
44,0735611319,http://goodreads.com/user/show/53697998-dave-voyles,5,"While looking for an answer on Stack Overflow one day, I saw several people recommend this book, to get a grasp on what was happening under the hood of my computer -- specifically, from beginning to end, how is a computer made?This book broke it down in such a way that I now understand it completely. The author starts of small and slow, and gradually builds up, from explaining how different number systems work as well as why we have so many, and the shortcomings of each (hex, binary, decimal, etc). From there, he goes into the use of relays, a bit about electrical engineering, and next thing you know, you're looking at Assembly code. It all starts with communication, languages (I‚Äôm talking Morse code and brail), and goes from there. Incredibly useful.code-bookI now have a far better understanding of the things my friends were speaking of on technical threads before, largely from the terms I learned in this book:carry bit, high-order bit, lower-order bit, etc,.I wish I had read something like this when I first started programming. If you know anyone who wants to get started in coding, then I'd start here, not because of the code it will teach you (you won't learn much on that here), but understanding what your code is doing to the actual hardware. The computer no longer appears like a black box.It was published by Microsoft Press in 2000, and the author, Charles Petzold, actually works for Microsoft (now Xamarin).Read this if you want to understand how your computer really works."
45,0735611319,http://goodreads.com/user/show/25094561-elijah-oyekunle,5,"Really good read for newbies looking to understand computers, behind the scenes."
46,0735611319,http://goodreads.com/user/show/981470-clarence,4,"Most people nowadays, if they wanted to explain how computers work, would probably ensure that the reader knew binary arithmetic, then talk about processor instructions, and from there work up through the higher levels of programming.Petzold takes an entirely different tack, which is completely centered around hardware. In fact, he starts with electric circuits, describing how a boy might build a circuit to light a lamp in his friend's house. He builds on that, getting into circuits that with multiple lamps, talks about telegraphs, and on and on. He does eventually take a detour in one chapter to talk about number bases, but even goes to the trouble of using not just base 10 or base 2 but unusual others to ensure that the reader understands both that the concepts are independent of the base, and yet also why base 2 is particularly useful in building digital computers.In addition, whether you know the subject matter or not, this books is an enjoyable read. People who recognize the name Petzold will most likely think of his old ""Programming Windows"" and similar books, which were dry textbooks. Not so this volume. I was very pleasantly surprised at what a fun read this is.Highly recommended to anyone, whether they have an interest in computers or not."
47,0735611319,http://goodreads.com/user/show/28438106-michael,5,"This book covers a variety of topics about what is going on under the hood of a computer, without muddling up the explanations with too many technical details. The author favors explaining the big picture and the components that make up that big picture, rather than staying too focused on one topic for too long and providing too many technical and insignificant details.For example, the concepts of logic gates and boolean functions are worthy of having entire books dedicated to them, but this book instead spends a concise two or three chapters on them, explaining the essentials to understand the big picture of what they are, and how they relate to the inside of a CPU. These concepts are then utilized later in the book to help explain other topics such as how a computer's memory works by utilizing those logic gates.Perhaps this books best feature is that, as I was reading and questions popped into my head, the very next paragraph would often open with something like, ""You might be wondering..."", and time and time again, I *was* wondering exactly what the author thought I might be wondering.It's all about the big picture, and this book absolutely nails it."
48,0735611319,http://goodreads.com/user/show/44327766,5,"This is the first book I would recommend to anyone wanting to learn about how computers work. It was written in 1999 and shows its age in some respects, but overall I would consider it a timeless classic.The one thing I was a bit sad to see was the incorrect use of the metric unit prefixes when refering to binary quantities. In the context of the time this book was written, the authors usage of metric units was common, and even today there is much confusion about it. A year before this book was published, the IEC and others published standards recommending binary prefixes. https://en.wikipedia.org/wiki/Binary_...Unicode was also only briefly mentioned. I would LOVE to see a revision of this book to update dated references, go over (and properly use) the differences between metric and binary prefixes, and to expand the content with details about advances that have happened since 1999.I was made aware of this book by Fabian Sanglard's blog: http://fabiensanglard.net/books_recom...He recommends ""Computer Organization and Design"" as a more in-depth follow-up. Expect a review of the 5th edition from me in the future!"
49,0735611319,http://goodreads.com/user/show/4090833-lingliang,5,"Excellent lucid explanation of the legacy of genius that has left us with the incredible abstracted world of computers. The abstraction allows us to accomplish creations of unimaginable complexity. This is a delight to read, as it clearly goes through layers and layers of genius, great minds building upon the remarkable history of computing, leaving us with a much more worthy appreciation of the beautiful creation that is the modern computer. It goes through each step of abstraction, starting with a compute relay and how it works (in terms of electromagnetics), and then goes step by step, all the way to the implementation of modern operating systems and programming languages. It's complexity is sufficient to leave little lingering questions but not so complex to be burdened by unnecessary rigor, leaving this boom a delight to read for both experts and laymen alike. This is an essential read for any computer scientist or even anyone who wants an appreciation of the creation that is a cornerstone of the modern human experience."
50,0735611319,http://goodreads.com/user/show/2478866-sonofabit,5,"Absolutely phenomenal book that's not so much about code but rather about the deep underlying concepts behind how a computer works, how it ""thinks"". If you've ever wanted to know more about bits and bytes and the mechanics behind the ones and zeros that everyone takes for granted as they browse facebook or listen to mp3s, this is the book for you!There were several ""AHA!"" moments that FINALLY cleared up unresolved questions from my Digital Circuits class back in college; I don't know why this wasn't the textbook they used :) However it's more than just a book about logic and circuits; it takes you so deep into the lowest levels of code and logic that it's almost an Anatomy course for computers. You literally get to see what happens at the smallest level, such as a circuit diagram for storing a byte!A truly fascinating read for anyone interested in programming, digital circuitry, or electrical engineering. Pick this one up today!"
51,0735611319,http://goodreads.com/user/show/676492-jean-luc,5,"There's a long, long list of books where my common reaction to them is ""I wish I'd read this in high school, it could've set me straight much earlier!"" Unfortunately, this isn't one of them... because I graduated in 1998 and this was published in 1999.At some point in your computer science career, you will take a courses and labs in digital systems. At Stevens, when I was your age, this was 381 (Switching Theory and Logical Design) and 383 (Computer Organization). This book combines both of those courses, starting from counting numbers all the way to building a programmable computer. The main difference is that the author, Charles Petzold, actually cares about the subject and makes it interesting. I would gladly have payed $4800 to read this book instead of paying that amount to take those 2 courses because I actually learned something from this book.Would make a great, great, great gift for any high school student considering a career in programming."
52,0735611319,http://goodreads.com/user/show/121883-mike,4,"I'd definitely recommend this to anyone who is even remotely curious how computers work, and doesn't mind going all the way back to basic physics to find out. The author builds a very basic but working computer all the way from the ground up, and giving you a unique view of the concepts involved that certainly wasn't taught in any engineering or computer science class I've ever taken (B.S.E.E./Rutgers '01). Although he gets a little lost near the end and doesn't adequately show how everything since the basics came out has just been ""making it better"", showing you the basics is the hard part, and combined with the complexity that comes later its all you really need to understand the emergent behavior of computing. "
53,0735611319,http://goodreads.com/user/show/11831233-jon,5,"Intimidated by digital technology? Think your computer secretly hates you? Can't understand why your device won't do what you tell it to? (Even though it is doing exactly what you told it to do...)Read this book. All complicated technology is made up of layer-upon-layer of less complicated pieces down to some very simple straight forward parts that do only one thing in response to something else that only does one other thing. Learn from the bottom up how digital, and in some cases analog, machines that we have allowed to control our live exist and do more in a second than you would want to do in your lifetime.And... No I will not fix your computer, that's what children are for now-a-days."
54,0735611319,http://goodreads.com/user/show/50247467-matthew-porche,3,"I will not lie, this book was tough to read. It flowed in some chapters, but not others. Petzold likes to talk about how much he hates analogies, but I seem to require them to understand a lot of things. I love computers, and this book helped me understand how they work, but it took forever before he started talking about them. I learned a lot on the small end of the scale, but he didn't discuss a whole lot about computers that I didn't already know. I was planning on citing this book for a research paper on cyber security, but this will not make it in the paper. There were a lot of dry spots in the book where I recall having to force myself to read the book. I cannot say I'd recommend this to anyone else, but I would not deter anyone from reading this book either. "
55,0735611319,http://goodreads.com/user/show/54838935-aaron-manley,5,"This is the book to read if you want to ""get"" computers. Not just how code executes programs, or how your operating system is what you use to run everything else - this goes all the way down to the computer architecture at the microcode level (relatively speaking). The book looks at basic human problems, discovers some ingenious quirks of nature involving switches and eventually electricity that allow us to use code to represent information to help us solve problems. Starting at a on/off relay, and ending in a full-fledged computer processor with memory, this is the book to read to understand how computer hardware and computer software are truly connected. It's also a good primer if you want to give nand2tetris a try and build your own virtual computer online."
56,0735611319,http://goodreads.com/user/show/7962528-michael,5,"What a great book! I asked myself why I didn't read this as a freshman in college, and then realized that it was published in 1999 when I was a sophomore.This book distills the workings of computers down to their most basic elements - voltages moving across circuits in a controlled way - and builds on itself until you get to a 1970s era microprocessor. From there, everything modern computers is simply layered on top one step at a time.What's going on inside your computer (or phone!) is simply amazing, but it's also completely understandable. I'm going to have my kids read this when they're old enough to understand the concepts. "
57,0735611319,http://goodreads.com/user/show/31061871-alex,4,"Good intro to Computer Science. Last few chapters didn't age too well since the book was written, but provide a valuable history lesson for younger programmers."
58,0735611319,http://goodreads.com/user/show/10165915-yogesh,5,"Perfect book if you want to understand how computer works at the very low level. This book answers most of the questions I had during my under graduation, didnt only help me bruis my engg concepts but also placed everything at one place. This is a must read for tech enthusiast. Reading this book does not require expertise abt computers, a person having bare minimum knowledge will also be able to understand though in between sometimes it becomes a little confusing but if you are attentive and is putting all of it in one place you will be able to understand it comfortably."
59,0735611319,http://goodreads.com/user/show/32751589-amy,4,"This book does a good job explaining a lot of the basics of computing along with explaining some of the history of computers. It goes from explaining the basics of circuits, relays, and binary to assembly languages and the breakdown of basic parts of computers. Even though technology today has come so far since this book was written, I still recommend this book if you want to understand many of the basic concepts of computing and where a lot of it started."
60,0735611319,http://goodreads.com/user/show/35899146-jakub,5,"I wish I discovered this book earlier. It is a great introduction to computation and I do recommend it to everybody that would like to understand what is going on inside their laptops or phones.I found the ""big picture"" to ""the actual details"" ratio very suitable for a casual read. There are some parts that require more attention though - especially when the author explains the details of a data flow in the computer or the way Intel 8080 chip works. Nevertheless that's an inherent characteristics of any subject related to computation - some thinking is required to grasp the concept.All in all great introduction for non-programmers and even better refresher for people that are into programming but treat all the layers below the OS as a 'back box'. This is highly recommended."
61,0735611319,http://goodreads.com/user/show/11199869-prashant-ghabak,5,The author starts from electrons then electricity and builds right up to a high level programming language to explain fundamentals of a computer science while walking us through the history of the field. Very well written. 
62,0735611319,http://goodreads.com/user/show/37846711-andrew-alkema,5,"This was an excellent book. I love Charles' approach to explaining how a computer works. All of a sudden this collection of electrical circuits was a microprocessor. I really enjoyed the history of computing mixed in as well, that's something we did not learn enough of in CS at university, so I really enjoyed getting more of that here. Even though this book was written in 2000, the information is still just as relevant. To make the concepts simpler, most of the hardware he looks at in detail is from the 1970's anyway, so that part hasn't changed. The rest of the concept haven't gone anywhere. Don't let the age turn you off."
63,0735611319,http://goodreads.com/user/show/74276701-chad-lavimoniere,4,"A good overview of the basics of CS and early history, but at this point hilariously outdated. Could do with a major second edition. "
64,0735611319,http://goodreads.com/user/show/31121633-leonid,5,"Absolutely great book.It starts from the general idea of codes, like Morse or Braille, and then shows how computers designed and built, starting from relays, and finishing with asm (and then moves on to memory, storage, cpu design, text and graphics i/o)."
65,0735611319,http://goodreads.com/user/show/21307031,5,"A fascinating journey into the history and ""nature"" of computers written with an effort to make things easy to understand. A very interesting and important read for anyone related to IT"
66,0735611319,http://goodreads.com/user/show/4130219-moayyed-almizyen,5,Thank you Charles Pertzold ..
67,0735611319,http://goodreads.com/user/show/45773827-rick-sam,5,"An excellent book to learn about bottom-up details of Computers. It starts with two friends trying to communicate with each other, Morse Code, Braille, Flashlights... Gates, ... Assembly Language, Operating System and finally Graphical Revolution. Overall, this would help you to understand from First Principles, you can build from there. What an impressive accomplishment and progress! I would recommend this to anyone interested in Software, Hardware, Computer Science. Deus Vult, Gottfriend"
68,0735611319,http://goodreads.com/user/show/84459920-nick,4,"When I started reading this book I had an inkling that this book is suitable for nerds/technical people but after reading the first few chapters I thought that probably I was wrong. As it turned out in the end, it's at best a mixture of two with a heavy hilt towards first.The book explains in detail the working of a computer system along with the history of its progress over the year. It starts with a very innocuous example of communication between two close by living friend using just a torch. After that, chapter after chapter, it builds upon that by explaining in subsequent chapters the shortcomings and the solutions developed by industry, until it reaches the working of high-level languages and how a computer does calculations!!While the language of the book is lucid and definitely an improvement over dry engineering books that a lot of people here would be so used to, it couldn't help falling in that trap as it progresses. If you don't follow the writing closely and sequentially the preceding chapters may just be undecipherable to you. The book goes deep into circuits and their working and although it starts with a very good example and maintains continuity, it becomes very tech oriented and complex. Even after having studied a lot of the topics from the book during graduation, I myself couldn't help but skim over some parts as the writing became too technical and fully resembles an engineering book. Guess you can't escape that however much you try!!This book is more suitable for nerds/technical-oriented readers. If you want to know how and what of your system, this book can give you a very good idea. I may even say that if that's how the engineering books were written a lot more people would benefit. If someone here, during their course of engineering, is starting with the subject of digital systems; I strongly suggest reading this book cover to cover over (say over a period of two weeks or a month) before they start with their assigned books. You would be thanking me and the author later!! For others, if you pay close attention from the very start, this is the book to gain some understanding system since a very complex topic is explained in a very easy language starting with a very basic and relatable example and improving upon it. If this book doesn't do it for you, nothing would."
69,0735611319,http://goodreads.com/user/show/5268938-lauren,4,Fascinating look at code and computers and the building blocks that make them. Not my usual cup of tea but fascinating nonetheless. From flashlight communication to 2000-era processors the author builds a computer.
70,0735611319,http://goodreads.com/user/show/15643804-gustav-ton-r,5,"Basically everything you need to build a modern computer, both hardware and software, from scratch. Perfect read after the apocalypse. Too few people grasp these relatively easy concepts that power modern society through the internet and the cell phones in our pockets. This book summarizes computer technology in a clear and concise way."
71,0735611319,http://goodreads.com/user/show/8017610-james-millikan-sj,5,"A fascinating account of how electronics work, using stories, clever examples, and clear illustrations. Starting with elementary logic, examples from Morse code, and basic circuits, Petzold explains how to build a computer.This sounds like a rather dry read, but in reality it is an engaging and well written journey that weaves electrical engineering and history together into a compelling synthesis.If you‚Äôve ever wondered how your digital watch works, or the structure of binary, or how erasable digital memory is possible, look no further than Petzold‚Äôs classic. Highly recommended."
72,0735611319,http://goodreads.com/user/show/4606766-scott-johnson,5,"This, finally, was what I was looking for in my quest to understand computers.We start with some really boring, basic things about the most elementary circuits. It gets a pass because it's necessary if you're coming in blind, but I've taken courses in things like the physics of transistors, so I wasn't a fan personally. The same goes for assorted later sections on number systems (already overly familiar with Binary and Hex, thanks).Then we move into how you take very simple components and construct logic gates. I was vaguely familiar with their use, but had not actually seen one built from relays.The theme of building further complexity just by stacking blocks is introduced, and is the key to everything. Once I understand how a NOR gate works, I don't need to constantly trace the paths, we can shortcut now to using the correct symbol in the circuit with two (or more) inputs and an output. Similarly, 3+ input gates that are really cascades of 2+ gates being abbreviated as what without context looked like a new component makes complete sense.Using that, we construct a simple circuit to add binary bits. My only big criticism is that this is where we're building up to how a microprocessor works, but we suddenly veer left to look at memory.This was the most enlightening and involved portion. We gradually build up from a single bit being stored as a consequence of a quirky configuration of logic gates that ""trap"" a bit to adding things like control mechanisms (only write to the bit if this input is on), refreshes, and, most importantly, arrays of these storage bits.This was where I finally started to settle into this paradigm. Trying to think about the big picture is impossible. We're talking about quantities of components that the human mind cannot track, time spans we can't conceive, and frequencies that are impossible to imagine.Faith in the process is the key. I understood the science behind relays and transistors. I understood how these allow us to introduce logic. I understood how combining these gates leads to operations like addition or controlling the routing of a circuit. I understand how a moderate array of these things leads to a useful component like a byte of memory. I understand how each bit can be addressed to be written, read, and cleared.So I just needed to learn to relax and remember that I know these things, and that all we're doing now is that same thing....a few million times. Yes, that seems impossible, but you don't need to deal with all of that. You just have to do what we did with cascading logic gates being combined into 3+ input gates: Simplify it into a known component. Who cares what's happening in each of the 500 black boxes that all do the same thing. We just know we need this input and this is how output behaves.Once you understand how to chain them together into large arrays, why not simplify this into just ONE big black box, the DIMM. I now know how a signal from the processor (or, later, storage device) goes through an unimaginably long branching addressing system.I could not easily trace exactly which circuit a signal will follow to access a certain memory address, but I have faith that, given what I understand, I COULD do it if I really had to. I know what I understand really well on a small scale still works perfectly fine even if there are a few tens of thousands of branches now. It's not even complicated, every single node in the branching decision tree is identical, it's just big.This was a little more fuzzy when we returned to the microprocessor and the idea of instructions and opcodes, but I think I get it. This was unfortunately not covered in as much detail, and it's the one section I do want to do more reading about specifically.My understanding is that instruction sets are hard-wired into the processor. Similar to memory addressing, there seems to be some kind of input gate where the opcode instruction is interpreted to route the data it's carrying to the right part of the chip? Or there could be dedicated inputs for the specific operations, and the routing is done on the motherboard or by a secondary controller chip? Or both?It's a little unclear to me exactly how that control process happens, but I DO understand the important part: That processors have a finite instruction set, and perform very simple operations......but really, really fast. Another revelation was the idea of a clock signal as an input that ratchets the data forward one step through the circuits per iteration. I guess a lemma to my ""it's not complicated, just big"" theorem would be, ""don't think of circuits as continuously evolving, they are discretely iterating"".I was struggling at first to fathom how just adding these two bits together in any way led to these characters being displayed on my screen right now. I kept thinking, ""But you still have to do this, and this, and....."" Ok.....so then we do those things. In 2018 we're doing BILLIONS of things EACH SECOND. Response times are in nanoseconds. Sure, it's a lot....but that's fine. It's not complicated, it's just a tediously long chain of really simple events.Storage was easy. There was a bit of hand waving about how the addressing translates to moving the read head to this part of the disk or whatever, but I know enough about RAM addressing now to assume how it works. I can assume that logic is built into a controller on that disk that's hard-wired for that particular hardware configuration (hence why you don't really need drivers for components like these.....and also makes me realize why drivers are necessary to essentially tell the rest of the computer what a new component's inputs, outputs, and instruction sets are).Displays were REALLY simple thanks to one phrase: ""visual memory"". This wasn't from this book, it was from my last read, Turing's Cathedral, in discussing the use of CRTs as memory devices briefly, so you could see the memory live. They said something offhanded about, ""That's all the display on your monitor is anyway, showing an array of memory as pixel values, "" or something like that. The video card just constantly accesses that visual memory, turns it into pixel instructions, and sends it out the adapter for the monitor (with its own hard-wired controller) to translate to the actual pixels.The last piece was....how does everything know where to send the data? Answer: BUS! I now get how you can have so many things on so few buses and not have every component getting useless data from the rest. I had never heard of three-state logic, but it makes total sense (especially after an explanation that ""0 and 1"" are really more like ""high and low"") that there could be a state that would essentially block a component from the circuit so it has the ""state"" that it's neither high nor low. It then follows quite simply that the address of that component on the bus would trigger that control circuit to leave that non-state and connect to the bus again.Put all of this together, and I get it now. It's kind of like one of those Magic Eye pictures: If I relax and let my eyes go fuzzy, I can see the kitty, but as soon as I try to focus on anything it dissolves into chaos, and at best I can resolve a few of the dots. I just need to stop worrying about the 4 billion dots on the paper, just relax my eyes and have confidence that they're all still there.Aside from the sequencing of memory and processors, I think this was really well structured to flow logically and build up these complicated ideas from just a few simpler ones. A big point in its favor, also, was introducing that idea itself at the very beginning so it doesn't blindside the reader. It lets you read seemingly scary ideas with the context that once you get it you can just file it away as another building block for later use.10/10 would recommend to anyone who, like me, really wants to understand how you get from simple circuits to complex operating systems without resorting to magic."
73,0735611319,http://goodreads.com/user/show/117278-simmoril,4,"One of the biggest difficulties that is unique to Computer Science is this idea of 'layers of abstraction' - interfaces created to help hide the complexity of the underlying layer. While this can be a boon when developing, it becomes a problem when those lower layers start misbehaving, and you don't know why. Or, at a more basic level, these layers of abstraction can make it hard to understand why things are the way that they are (like why computers don't count in base 10, or why I can't run Unix programs in Windows). Petzold's book attempts to resolve this issue by starting out at the absolute most basic, physical level of a computer and working up through the different layers until he gets to what is pretty much a modern day computer. Through the use of good examples and an informal style of writing, Petzold introduces the reader to a large number of fundamental topics in Computer Science/Engineering, such as binary, logic gates, processor design, assembly, operating systems and programming languages. Petzold does a great job of not getting too bogged down in the details, and only giving the reader enough information to move up to the next layer.I enjoyed Code a great deal. Although much of this wasn't new to me, Petzold still surprised me with some historical facts I was not aware of, and the book itself served as a nice refresher course for some of the things that I studied and promptly forgotten in college. Although this book won't turn you into an expert by any means, it is a great starting point for understanding just how computers actually work.The only small issue I had with Code were that some areas were a little tough to get through. Towards the middle of the book, where Petzold starts to assemble memory circuits and building a rudimentary CPU, I found myself glossing over a lot of the details. For a complete understanding, these chapters would probably require a couple of passes. Also, the last couple of chapters seemed a little more hodge-podge than the rest. While they are good tidbits of information to know, it just seemed a little less organized than the rest of the book.Overall, Code is a fantastic book, and I highly recommend it as a starting point for anyone who wants to learn more about computers. In fact, I'd nearly go so far as to say it should be required reading!"
74,0735611319,http://goodreads.com/user/show/53697998-dave-voyles,5,"While looking for an answer on Stack Overflow one day, I saw several people recommend this book, to get a grasp on what was happening under the hood of my computer -- specifically, from beginning to end, how is a computer made?This book broke it down in such a way that I now understand it completely. The author starts of small and slow, and gradually builds up, from explaining how different number systems work as well as why we have so many, and the shortcomings of each (hex, binary, decimal, etc). From there, he goes into the use of relays, a bit about electrical engineering, and next thing you know, you're looking at Assembly code. It all starts with communication, languages (I‚Äôm talking Morse code and brail), and goes from there. Incredibly useful.code-bookI now have a far better understanding of the things my friends were speaking of on technical threads before, largely from the terms I learned in this book:carry bit, high-order bit, lower-order bit, etc,.I wish I had read something like this when I first started programming. If you know anyone who wants to get started in coding, then I'd start here, not because of the code it will teach you (you won't learn much on that here), but understanding what your code is doing to the actual hardware. The computer no longer appears like a black box.It was published by Microsoft Press in 2000, and the author, Charles Petzold, actually works for Microsoft (now Xamarin).Read this if you want to understand how your computer really works."
75,0735611319,http://goodreads.com/user/show/25094561-elijah-oyekunle,5,"Really good read for newbies looking to understand computers, behind the scenes."
76,0735611319,http://goodreads.com/user/show/981470-clarence,4,"Most people nowadays, if they wanted to explain how computers work, would probably ensure that the reader knew binary arithmetic, then talk about processor instructions, and from there work up through the higher levels of programming.Petzold takes an entirely different tack, which is completely centered around hardware. In fact, he starts with electric circuits, describing how a boy might build a circuit to light a lamp in his friend's house. He builds on that, getting into circuits that with multiple lamps, talks about telegraphs, and on and on. He does eventually take a detour in one chapter to talk about number bases, but even goes to the trouble of using not just base 10 or base 2 but unusual others to ensure that the reader understands both that the concepts are independent of the base, and yet also why base 2 is particularly useful in building digital computers.In addition, whether you know the subject matter or not, this books is an enjoyable read. People who recognize the name Petzold will most likely think of his old ""Programming Windows"" and similar books, which were dry textbooks. Not so this volume. I was very pleasantly surprised at what a fun read this is.Highly recommended to anyone, whether they have an interest in computers or not."
77,0735611319,http://goodreads.com/user/show/28438106-michael,5,"This book covers a variety of topics about what is going on under the hood of a computer, without muddling up the explanations with too many technical details. The author favors explaining the big picture and the components that make up that big picture, rather than staying too focused on one topic for too long and providing too many technical and insignificant details.For example, the concepts of logic gates and boolean functions are worthy of having entire books dedicated to them, but this book instead spends a concise two or three chapters on them, explaining the essentials to understand the big picture of what they are, and how they relate to the inside of a CPU. These concepts are then utilized later in the book to help explain other topics such as how a computer's memory works by utilizing those logic gates.Perhaps this books best feature is that, as I was reading and questions popped into my head, the very next paragraph would often open with something like, ""You might be wondering..."", and time and time again, I *was* wondering exactly what the author thought I might be wondering.It's all about the big picture, and this book absolutely nails it."
78,0735611319,http://goodreads.com/user/show/44327766,5,"This is the first book I would recommend to anyone wanting to learn about how computers work. It was written in 1999 and shows its age in some respects, but overall I would consider it a timeless classic.The one thing I was a bit sad to see was the incorrect use of the metric unit prefixes when refering to binary quantities. In the context of the time this book was written, the authors usage of metric units was common, and even today there is much confusion about it. A year before this book was published, the IEC and others published standards recommending binary prefixes. https://en.wikipedia.org/wiki/Binary_...Unicode was also only briefly mentioned. I would LOVE to see a revision of this book to update dated references, go over (and properly use) the differences between metric and binary prefixes, and to expand the content with details about advances that have happened since 1999.I was made aware of this book by Fabian Sanglard's blog: http://fabiensanglard.net/books_recom...He recommends ""Computer Organization and Design"" as a more in-depth follow-up. Expect a review of the 5th edition from me in the future!"
79,0735611319,http://goodreads.com/user/show/4090833-lingliang,5,"Excellent lucid explanation of the legacy of genius that has left us with the incredible abstracted world of computers. The abstraction allows us to accomplish creations of unimaginable complexity. This is a delight to read, as it clearly goes through layers and layers of genius, great minds building upon the remarkable history of computing, leaving us with a much more worthy appreciation of the beautiful creation that is the modern computer. It goes through each step of abstraction, starting with a compute relay and how it works (in terms of electromagnetics), and then goes step by step, all the way to the implementation of modern operating systems and programming languages. It's complexity is sufficient to leave little lingering questions but not so complex to be burdened by unnecessary rigor, leaving this boom a delight to read for both experts and laymen alike. This is an essential read for any computer scientist or even anyone who wants an appreciation of the creation that is a cornerstone of the modern human experience."
80,0735611319,http://goodreads.com/user/show/2478866-sonofabit,5,"Absolutely phenomenal book that's not so much about code but rather about the deep underlying concepts behind how a computer works, how it ""thinks"". If you've ever wanted to know more about bits and bytes and the mechanics behind the ones and zeros that everyone takes for granted as they browse facebook or listen to mp3s, this is the book for you!There were several ""AHA!"" moments that FINALLY cleared up unresolved questions from my Digital Circuits class back in college; I don't know why this wasn't the textbook they used :) However it's more than just a book about logic and circuits; it takes you so deep into the lowest levels of code and logic that it's almost an Anatomy course for computers. You literally get to see what happens at the smallest level, such as a circuit diagram for storing a byte!A truly fascinating read for anyone interested in programming, digital circuitry, or electrical engineering. Pick this one up today!"
81,0735611319,http://goodreads.com/user/show/676492-jean-luc,5,"There's a long, long list of books where my common reaction to them is ""I wish I'd read this in high school, it could've set me straight much earlier!"" Unfortunately, this isn't one of them... because I graduated in 1998 and this was published in 1999.At some point in your computer science career, you will take a courses and labs in digital systems. At Stevens, when I was your age, this was 381 (Switching Theory and Logical Design) and 383 (Computer Organization). This book combines both of those courses, starting from counting numbers all the way to building a programmable computer. The main difference is that the author, Charles Petzold, actually cares about the subject and makes it interesting. I would gladly have payed $4800 to read this book instead of paying that amount to take those 2 courses because I actually learned something from this book.Would make a great, great, great gift for any high school student considering a career in programming."
82,0735611319,http://goodreads.com/user/show/121883-mike,4,"I'd definitely recommend this to anyone who is even remotely curious how computers work, and doesn't mind going all the way back to basic physics to find out. The author builds a very basic but working computer all the way from the ground up, and giving you a unique view of the concepts involved that certainly wasn't taught in any engineering or computer science class I've ever taken (B.S.E.E./Rutgers '01). Although he gets a little lost near the end and doesn't adequately show how everything since the basics came out has just been ""making it better"", showing you the basics is the hard part, and combined with the complexity that comes later its all you really need to understand the emergent behavior of computing. "
83,0735611319,http://goodreads.com/user/show/11831233-jon,5,"Intimidated by digital technology? Think your computer secretly hates you? Can't understand why your device won't do what you tell it to? (Even though it is doing exactly what you told it to do...)Read this book. All complicated technology is made up of layer-upon-layer of less complicated pieces down to some very simple straight forward parts that do only one thing in response to something else that only does one other thing. Learn from the bottom up how digital, and in some cases analog, machines that we have allowed to control our live exist and do more in a second than you would want to do in your lifetime.And... No I will not fix your computer, that's what children are for now-a-days."
84,0735611319,http://goodreads.com/user/show/50247467-matthew-porche,3,"I will not lie, this book was tough to read. It flowed in some chapters, but not others. Petzold likes to talk about how much he hates analogies, but I seem to require them to understand a lot of things. I love computers, and this book helped me understand how they work, but it took forever before he started talking about them. I learned a lot on the small end of the scale, but he didn't discuss a whole lot about computers that I didn't already know. I was planning on citing this book for a research paper on cyber security, but this will not make it in the paper. There were a lot of dry spots in the book where I recall having to force myself to read the book. I cannot say I'd recommend this to anyone else, but I would not deter anyone from reading this book either. "
85,0735611319,http://goodreads.com/user/show/54838935-aaron-manley,5,"This is the book to read if you want to ""get"" computers. Not just how code executes programs, or how your operating system is what you use to run everything else - this goes all the way down to the computer architecture at the microcode level (relatively speaking). The book looks at basic human problems, discovers some ingenious quirks of nature involving switches and eventually electricity that allow us to use code to represent information to help us solve problems. Starting at a on/off relay, and ending in a full-fledged computer processor with memory, this is the book to read to understand how computer hardware and computer software are truly connected. It's also a good primer if you want to give nand2tetris a try and build your own virtual computer online."
86,0735611319,http://goodreads.com/user/show/7962528-michael,5,"What a great book! I asked myself why I didn't read this as a freshman in college, and then realized that it was published in 1999 when I was a sophomore.This book distills the workings of computers down to their most basic elements - voltages moving across circuits in a controlled way - and builds on itself until you get to a 1970s era microprocessor. From there, everything modern computers is simply layered on top one step at a time.What's going on inside your computer (or phone!) is simply amazing, but it's also completely understandable. I'm going to have my kids read this when they're old enough to understand the concepts. "
87,0735611319,http://goodreads.com/user/show/31061871-alex,4,"Good intro to Computer Science. Last few chapters didn't age too well since the book was written, but provide a valuable history lesson for younger programmers."
88,0735611319,http://goodreads.com/user/show/10165915-yogesh,5,"Perfect book if you want to understand how computer works at the very low level. This book answers most of the questions I had during my under graduation, didnt only help me bruis my engg concepts but also placed everything at one place. This is a must read for tech enthusiast. Reading this book does not require expertise abt computers, a person having bare minimum knowledge will also be able to understand though in between sometimes it becomes a little confusing but if you are attentive and is putting all of it in one place you will be able to understand it comfortably."
89,0735611319,http://goodreads.com/user/show/32751589-amy,4,"This book does a good job explaining a lot of the basics of computing along with explaining some of the history of computers. It goes from explaining the basics of circuits, relays, and binary to assembly languages and the breakdown of basic parts of computers. Even though technology today has come so far since this book was written, I still recommend this book if you want to understand many of the basic concepts of computing and where a lot of it started."
90,0735611319,http://goodreads.com/user/show/35899146-jakub,5,"I wish I discovered this book earlier. It is a great introduction to computation and I do recommend it to everybody that would like to understand what is going on inside their laptops or phones.I found the ""big picture"" to ""the actual details"" ratio very suitable for a casual read. There are some parts that require more attention though - especially when the author explains the details of a data flow in the computer or the way Intel 8080 chip works. Nevertheless that's an inherent characteristics of any subject related to computation - some thinking is required to grasp the concept.All in all great introduction for non-programmers and even better refresher for people that are into programming but treat all the layers below the OS as a 'back box'. This is highly recommended."
91,0735611319,http://goodreads.com/user/show/11199869-prashant-ghabak,5,The author starts from electrons then electricity and builds right up to a high level programming language to explain fundamentals of a computer science while walking us through the history of the field. Very well written. 
92,0735611319,http://goodreads.com/user/show/37846711-andrew-alkema,5,"This was an excellent book. I love Charles' approach to explaining how a computer works. All of a sudden this collection of electrical circuits was a microprocessor. I really enjoyed the history of computing mixed in as well, that's something we did not learn enough of in CS at university, so I really enjoyed getting more of that here. Even though this book was written in 2000, the information is still just as relevant. To make the concepts simpler, most of the hardware he looks at in detail is from the 1970's anyway, so that part hasn't changed. The rest of the concept haven't gone anywhere. Don't let the age turn you off."
93,0735611319,http://goodreads.com/user/show/74276701-chad-lavimoniere,4,"A good overview of the basics of CS and early history, but at this point hilariously outdated. Could do with a major second edition. "
94,0735611319,http://goodreads.com/user/show/31121633-leonid,5,"Absolutely great book.It starts from the general idea of codes, like Morse or Braille, and then shows how computers designed and built, starting from relays, and finishing with asm (and then moves on to memory, storage, cpu design, text and graphics i/o)."
95,0735611319,http://goodreads.com/user/show/21307031,5,"A fascinating journey into the history and ""nature"" of computers written with an effort to make things easy to understand. A very interesting and important read for anyone related to IT"
96,0735611319,http://goodreads.com/user/show/4130219-moayyed-almizyen,5,Thank you Charles Pertzold ..
97,0735611319,http://goodreads.com/user/show/45773827-rick-sam,5,"An excellent book to learn about bottom-up details of Computers. It starts with two friends trying to communicate with each other, Morse Code, Braille, Flashlights... Gates, ... Assembly Language, Operating System and finally Graphical Revolution. Overall, this would help you to understand from First Principles, you can build from there. What an impressive accomplishment and progress! I would recommend this to anyone interested in Software, Hardware, Computer Science. Deus Vult, Gottfriend"
98,0735611319,http://goodreads.com/user/show/84459920-nick,4,"When I started reading this book I had an inkling that this book is suitable for nerds/technical people but after reading the first few chapters I thought that probably I was wrong. As it turned out in the end, it's at best a mixture of two with a heavy hilt towards first.The book explains in detail the working of a computer system along with the history of its progress over the year. It starts with a very innocuous example of communication between two close by living friend using just a torch. After that, chapter after chapter, it builds upon that by explaining in subsequent chapters the shortcomings and the solutions developed by industry, until it reaches the working of high-level languages and how a computer does calculations!!While the language of the book is lucid and definitely an improvement over dry engineering books that a lot of people here would be so used to, it couldn't help falling in that trap as it progresses. If you don't follow the writing closely and sequentially the preceding chapters may just be undecipherable to you. The book goes deep into circuits and their working and although it starts with a very good example and maintains continuity, it becomes very tech oriented and complex. Even after having studied a lot of the topics from the book during graduation, I myself couldn't help but skim over some parts as the writing became too technical and fully resembles an engineering book. Guess you can't escape that however much you try!!This book is more suitable for nerds/technical-oriented readers. If you want to know how and what of your system, this book can give you a very good idea. I may even say that if that's how the engineering books were written a lot more people would benefit. If someone here, during their course of engineering, is starting with the subject of digital systems; I strongly suggest reading this book cover to cover over (say over a period of two weeks or a month) before they start with their assigned books. You would be thanking me and the author later!! For others, if you pay close attention from the very start, this is the book to gain some understanding system since a very complex topic is explained in a very easy language starting with a very basic and relatable example and improving upon it. If this book doesn't do it for you, nothing would."
99,0735611319,http://goodreads.com/user/show/5268938-lauren,4,Fascinating look at code and computers and the building blocks that make them. Not my usual cup of tea but fascinating nonetheless. From flashlight communication to 2000-era processors the author builds a computer.
100,0735611319,http://goodreads.com/user/show/15643804-gustav-ton-r,5,"Basically everything you need to build a modern computer, both hardware and software, from scratch. Perfect read after the apocalypse. Too few people grasp these relatively easy concepts that power modern society through the internet and the cell phones in our pockets. This book summarizes computer technology in a clear and concise way."
101,0735611319,http://goodreads.com/user/show/8017610-james-millikan-sj,5,"A fascinating account of how electronics work, using stories, clever examples, and clear illustrations. Starting with elementary logic, examples from Morse code, and basic circuits, Petzold explains how to build a computer.This sounds like a rather dry read, but in reality it is an engaging and well written journey that weaves electrical engineering and history together into a compelling synthesis.If you‚Äôve ever wondered how your digital watch works, or the structure of binary, or how erasable digital memory is possible, look no further than Petzold‚Äôs classic. Highly recommended."
102,0735611319,http://goodreads.com/user/show/4606766-scott-johnson,5,"This, finally, was what I was looking for in my quest to understand computers.We start with some really boring, basic things about the most elementary circuits. It gets a pass because it's necessary if you're coming in blind, but I've taken courses in things like the physics of transistors, so I wasn't a fan personally. The same goes for assorted later sections on number systems (already overly familiar with Binary and Hex, thanks).Then we move into how you take very simple components and construct logic gates. I was vaguely familiar with their use, but had not actually seen one built from relays.The theme of building further complexity just by stacking blocks is introduced, and is the key to everything. Once I understand how a NOR gate works, I don't need to constantly trace the paths, we can shortcut now to using the correct symbol in the circuit with two (or more) inputs and an output. Similarly, 3+ input gates that are really cascades of 2+ gates being abbreviated as what without context looked like a new component makes complete sense.Using that, we construct a simple circuit to add binary bits. My only big criticism is that this is where we're building up to how a microprocessor works, but we suddenly veer left to look at memory.This was the most enlightening and involved portion. We gradually build up from a single bit being stored as a consequence of a quirky configuration of logic gates that ""trap"" a bit to adding things like control mechanisms (only write to the bit if this input is on), refreshes, and, most importantly, arrays of these storage bits.This was where I finally started to settle into this paradigm. Trying to think about the big picture is impossible. We're talking about quantities of components that the human mind cannot track, time spans we can't conceive, and frequencies that are impossible to imagine.Faith in the process is the key. I understood the science behind relays and transistors. I understood how these allow us to introduce logic. I understood how combining these gates leads to operations like addition or controlling the routing of a circuit. I understand how a moderate array of these things leads to a useful component like a byte of memory. I understand how each bit can be addressed to be written, read, and cleared.So I just needed to learn to relax and remember that I know these things, and that all we're doing now is that same thing....a few million times. Yes, that seems impossible, but you don't need to deal with all of that. You just have to do what we did with cascading logic gates being combined into 3+ input gates: Simplify it into a known component. Who cares what's happening in each of the 500 black boxes that all do the same thing. We just know we need this input and this is how output behaves.Once you understand how to chain them together into large arrays, why not simplify this into just ONE big black box, the DIMM. I now know how a signal from the processor (or, later, storage device) goes through an unimaginably long branching addressing system.I could not easily trace exactly which circuit a signal will follow to access a certain memory address, but I have faith that, given what I understand, I COULD do it if I really had to. I know what I understand really well on a small scale still works perfectly fine even if there are a few tens of thousands of branches now. It's not even complicated, every single node in the branching decision tree is identical, it's just big.This was a little more fuzzy when we returned to the microprocessor and the idea of instructions and opcodes, but I think I get it. This was unfortunately not covered in as much detail, and it's the one section I do want to do more reading about specifically.My understanding is that instruction sets are hard-wired into the processor. Similar to memory addressing, there seems to be some kind of input gate where the opcode instruction is interpreted to route the data it's carrying to the right part of the chip? Or there could be dedicated inputs for the specific operations, and the routing is done on the motherboard or by a secondary controller chip? Or both?It's a little unclear to me exactly how that control process happens, but I DO understand the important part: That processors have a finite instruction set, and perform very simple operations......but really, really fast. Another revelation was the idea of a clock signal as an input that ratchets the data forward one step through the circuits per iteration. I guess a lemma to my ""it's not complicated, just big"" theorem would be, ""don't think of circuits as continuously evolving, they are discretely iterating"".I was struggling at first to fathom how just adding these two bits together in any way led to these characters being displayed on my screen right now. I kept thinking, ""But you still have to do this, and this, and....."" Ok.....so then we do those things. In 2018 we're doing BILLIONS of things EACH SECOND. Response times are in nanoseconds. Sure, it's a lot....but that's fine. It's not complicated, it's just a tediously long chain of really simple events.Storage was easy. There was a bit of hand waving about how the addressing translates to moving the read head to this part of the disk or whatever, but I know enough about RAM addressing now to assume how it works. I can assume that logic is built into a controller on that disk that's hard-wired for that particular hardware configuration (hence why you don't really need drivers for components like these.....and also makes me realize why drivers are necessary to essentially tell the rest of the computer what a new component's inputs, outputs, and instruction sets are).Displays were REALLY simple thanks to one phrase: ""visual memory"". This wasn't from this book, it was from my last read, Turing's Cathedral, in discussing the use of CRTs as memory devices briefly, so you could see the memory live. They said something offhanded about, ""That's all the display on your monitor is anyway, showing an array of memory as pixel values, "" or something like that. The video card just constantly accesses that visual memory, turns it into pixel instructions, and sends it out the adapter for the monitor (with its own hard-wired controller) to translate to the actual pixels.The last piece was....how does everything know where to send the data? Answer: BUS! I now get how you can have so many things on so few buses and not have every component getting useless data from the rest. I had never heard of three-state logic, but it makes total sense (especially after an explanation that ""0 and 1"" are really more like ""high and low"") that there could be a state that would essentially block a component from the circuit so it has the ""state"" that it's neither high nor low. It then follows quite simply that the address of that component on the bus would trigger that control circuit to leave that non-state and connect to the bus again.Put all of this together, and I get it now. It's kind of like one of those Magic Eye pictures: If I relax and let my eyes go fuzzy, I can see the kitty, but as soon as I try to focus on anything it dissolves into chaos, and at best I can resolve a few of the dots. I just need to stop worrying about the 4 billion dots on the paper, just relax my eyes and have confidence that they're all still there.Aside from the sequencing of memory and processors, I think this was really well structured to flow logically and build up these complicated ideas from just a few simpler ones. A big point in its favor, also, was introducing that idea itself at the very beginning so it doesn't blindside the reader. It lets you read seemingly scary ideas with the context that once you get it you can just file it away as another building block for later use.10/10 would recommend to anyone who, like me, really wants to understand how you get from simple circuits to complex operating systems without resorting to magic."
103,0735611319,http://goodreads.com/user/show/117278-simmoril,4,"One of the biggest difficulties that is unique to Computer Science is this idea of 'layers of abstraction' - interfaces created to help hide the complexity of the underlying layer. While this can be a boon when developing, it becomes a problem when those lower layers start misbehaving, and you don't know why. Or, at a more basic level, these layers of abstraction can make it hard to understand why things are the way that they are (like why computers don't count in base 10, or why I can't run Unix programs in Windows). Petzold's book attempts to resolve this issue by starting out at the absolute most basic, physical level of a computer and working up through the different layers until he gets to what is pretty much a modern day computer. Through the use of good examples and an informal style of writing, Petzold introduces the reader to a large number of fundamental topics in Computer Science/Engineering, such as binary, logic gates, processor design, assembly, operating systems and programming languages. Petzold does a great job of not getting too bogged down in the details, and only giving the reader enough information to move up to the next layer.I enjoyed Code a great deal. Although much of this wasn't new to me, Petzold still surprised me with some historical facts I was not aware of, and the book itself served as a nice refresher course for some of the things that I studied and promptly forgotten in college. Although this book won't turn you into an expert by any means, it is a great starting point for understanding just how computers actually work.The only small issue I had with Code were that some areas were a little tough to get through. Towards the middle of the book, where Petzold starts to assemble memory circuits and building a rudimentary CPU, I found myself glossing over a lot of the details. For a complete understanding, these chapters would probably require a couple of passes. Also, the last couple of chapters seemed a little more hodge-podge than the rest. While they are good tidbits of information to know, it just seemed a little less organized than the rest of the book.Overall, Code is a fantastic book, and I highly recommend it as a starting point for anyone who wants to learn more about computers. In fact, I'd nearly go so far as to say it should be required reading!"
104,0735611319,http://goodreads.com/user/show/53697998-dave-voyles,5,"While looking for an answer on Stack Overflow one day, I saw several people recommend this book, to get a grasp on what was happening under the hood of my computer -- specifically, from beginning to end, how is a computer made?This book broke it down in such a way that I now understand it completely. The author starts of small and slow, and gradually builds up, from explaining how different number systems work as well as why we have so many, and the shortcomings of each (hex, binary, decimal, etc). From there, he goes into the use of relays, a bit about electrical engineering, and next thing you know, you're looking at Assembly code. It all starts with communication, languages (I‚Äôm talking Morse code and brail), and goes from there. Incredibly useful.code-bookI now have a far better understanding of the things my friends were speaking of on technical threads before, largely from the terms I learned in this book:carry bit, high-order bit, lower-order bit, etc,.I wish I had read something like this when I first started programming. If you know anyone who wants to get started in coding, then I'd start here, not because of the code it will teach you (you won't learn much on that here), but understanding what your code is doing to the actual hardware. The computer no longer appears like a black box.It was published by Microsoft Press in 2000, and the author, Charles Petzold, actually works for Microsoft (now Xamarin).Read this if you want to understand how your computer really works."
105,0735611319,http://goodreads.com/user/show/25094561-elijah-oyekunle,5,"Really good read for newbies looking to understand computers, behind the scenes."
106,0735611319,http://goodreads.com/user/show/981470-clarence,4,"Most people nowadays, if they wanted to explain how computers work, would probably ensure that the reader knew binary arithmetic, then talk about processor instructions, and from there work up through the higher levels of programming.Petzold takes an entirely different tack, which is completely centered around hardware. In fact, he starts with electric circuits, describing how a boy might build a circuit to light a lamp in his friend's house. He builds on that, getting into circuits that with multiple lamps, talks about telegraphs, and on and on. He does eventually take a detour in one chapter to talk about number bases, but even goes to the trouble of using not just base 10 or base 2 but unusual others to ensure that the reader understands both that the concepts are independent of the base, and yet also why base 2 is particularly useful in building digital computers.In addition, whether you know the subject matter or not, this books is an enjoyable read. People who recognize the name Petzold will most likely think of his old ""Programming Windows"" and similar books, which were dry textbooks. Not so this volume. I was very pleasantly surprised at what a fun read this is.Highly recommended to anyone, whether they have an interest in computers or not."
107,0735611319,http://goodreads.com/user/show/28438106-michael,5,"This book covers a variety of topics about what is going on under the hood of a computer, without muddling up the explanations with too many technical details. The author favors explaining the big picture and the components that make up that big picture, rather than staying too focused on one topic for too long and providing too many technical and insignificant details.For example, the concepts of logic gates and boolean functions are worthy of having entire books dedicated to them, but this book instead spends a concise two or three chapters on them, explaining the essentials to understand the big picture of what they are, and how they relate to the inside of a CPU. These concepts are then utilized later in the book to help explain other topics such as how a computer's memory works by utilizing those logic gates.Perhaps this books best feature is that, as I was reading and questions popped into my head, the very next paragraph would often open with something like, ""You might be wondering..."", and time and time again, I *was* wondering exactly what the author thought I might be wondering.It's all about the big picture, and this book absolutely nails it."
108,0735611319,http://goodreads.com/user/show/44327766,5,"This is the first book I would recommend to anyone wanting to learn about how computers work. It was written in 1999 and shows its age in some respects, but overall I would consider it a timeless classic.The one thing I was a bit sad to see was the incorrect use of the metric unit prefixes when refering to binary quantities. In the context of the time this book was written, the authors usage of metric units was common, and even today there is much confusion about it. A year before this book was published, the IEC and others published standards recommending binary prefixes. https://en.wikipedia.org/wiki/Binary_...Unicode was also only briefly mentioned. I would LOVE to see a revision of this book to update dated references, go over (and properly use) the differences between metric and binary prefixes, and to expand the content with details about advances that have happened since 1999.I was made aware of this book by Fabian Sanglard's blog: http://fabiensanglard.net/books_recom...He recommends ""Computer Organization and Design"" as a more in-depth follow-up. Expect a review of the 5th edition from me in the future!"
109,0735611319,http://goodreads.com/user/show/4090833-lingliang,5,"Excellent lucid explanation of the legacy of genius that has left us with the incredible abstracted world of computers. The abstraction allows us to accomplish creations of unimaginable complexity. This is a delight to read, as it clearly goes through layers and layers of genius, great minds building upon the remarkable history of computing, leaving us with a much more worthy appreciation of the beautiful creation that is the modern computer. It goes through each step of abstraction, starting with a compute relay and how it works (in terms of electromagnetics), and then goes step by step, all the way to the implementation of modern operating systems and programming languages. It's complexity is sufficient to leave little lingering questions but not so complex to be burdened by unnecessary rigor, leaving this boom a delight to read for both experts and laymen alike. This is an essential read for any computer scientist or even anyone who wants an appreciation of the creation that is a cornerstone of the modern human experience."
110,0735611319,http://goodreads.com/user/show/2478866-sonofabit,5,"Absolutely phenomenal book that's not so much about code but rather about the deep underlying concepts behind how a computer works, how it ""thinks"". If you've ever wanted to know more about bits and bytes and the mechanics behind the ones and zeros that everyone takes for granted as they browse facebook or listen to mp3s, this is the book for you!There were several ""AHA!"" moments that FINALLY cleared up unresolved questions from my Digital Circuits class back in college; I don't know why this wasn't the textbook they used :) However it's more than just a book about logic and circuits; it takes you so deep into the lowest levels of code and logic that it's almost an Anatomy course for computers. You literally get to see what happens at the smallest level, such as a circuit diagram for storing a byte!A truly fascinating read for anyone interested in programming, digital circuitry, or electrical engineering. Pick this one up today!"
111,0735611319,http://goodreads.com/user/show/676492-jean-luc,5,"There's a long, long list of books where my common reaction to them is ""I wish I'd read this in high school, it could've set me straight much earlier!"" Unfortunately, this isn't one of them... because I graduated in 1998 and this was published in 1999.At some point in your computer science career, you will take a courses and labs in digital systems. At Stevens, when I was your age, this was 381 (Switching Theory and Logical Design) and 383 (Computer Organization). This book combines both of those courses, starting from counting numbers all the way to building a programmable computer. The main difference is that the author, Charles Petzold, actually cares about the subject and makes it interesting. I would gladly have payed $4800 to read this book instead of paying that amount to take those 2 courses because I actually learned something from this book.Would make a great, great, great gift for any high school student considering a career in programming."
112,0735611319,http://goodreads.com/user/show/121883-mike,4,"I'd definitely recommend this to anyone who is even remotely curious how computers work, and doesn't mind going all the way back to basic physics to find out. The author builds a very basic but working computer all the way from the ground up, and giving you a unique view of the concepts involved that certainly wasn't taught in any engineering or computer science class I've ever taken (B.S.E.E./Rutgers '01). Although he gets a little lost near the end and doesn't adequately show how everything since the basics came out has just been ""making it better"", showing you the basics is the hard part, and combined with the complexity that comes later its all you really need to understand the emergent behavior of computing. "
113,0735611319,http://goodreads.com/user/show/11831233-jon,5,"Intimidated by digital technology? Think your computer secretly hates you? Can't understand why your device won't do what you tell it to? (Even though it is doing exactly what you told it to do...)Read this book. All complicated technology is made up of layer-upon-layer of less complicated pieces down to some very simple straight forward parts that do only one thing in response to something else that only does one other thing. Learn from the bottom up how digital, and in some cases analog, machines that we have allowed to control our live exist and do more in a second than you would want to do in your lifetime.And... No I will not fix your computer, that's what children are for now-a-days."
114,0735611319,http://goodreads.com/user/show/50247467-matthew-porche,3,"I will not lie, this book was tough to read. It flowed in some chapters, but not others. Petzold likes to talk about how much he hates analogies, but I seem to require them to understand a lot of things. I love computers, and this book helped me understand how they work, but it took forever before he started talking about them. I learned a lot on the small end of the scale, but he didn't discuss a whole lot about computers that I didn't already know. I was planning on citing this book for a research paper on cyber security, but this will not make it in the paper. There were a lot of dry spots in the book where I recall having to force myself to read the book. I cannot say I'd recommend this to anyone else, but I would not deter anyone from reading this book either. "
115,0735611319,http://goodreads.com/user/show/54838935-aaron-manley,5,"This is the book to read if you want to ""get"" computers. Not just how code executes programs, or how your operating system is what you use to run everything else - this goes all the way down to the computer architecture at the microcode level (relatively speaking). The book looks at basic human problems, discovers some ingenious quirks of nature involving switches and eventually electricity that allow us to use code to represent information to help us solve problems. Starting at a on/off relay, and ending in a full-fledged computer processor with memory, this is the book to read to understand how computer hardware and computer software are truly connected. It's also a good primer if you want to give nand2tetris a try and build your own virtual computer online."
116,0735611319,http://goodreads.com/user/show/7962528-michael,5,"What a great book! I asked myself why I didn't read this as a freshman in college, and then realized that it was published in 1999 when I was a sophomore.This book distills the workings of computers down to their most basic elements - voltages moving across circuits in a controlled way - and builds on itself until you get to a 1970s era microprocessor. From there, everything modern computers is simply layered on top one step at a time.What's going on inside your computer (or phone!) is simply amazing, but it's also completely understandable. I'm going to have my kids read this when they're old enough to understand the concepts. "
117,0735611319,http://goodreads.com/user/show/31061871-alex,4,"Good intro to Computer Science. Last few chapters didn't age too well since the book was written, but provide a valuable history lesson for younger programmers."
118,0735611319,http://goodreads.com/user/show/10165915-yogesh,5,"Perfect book if you want to understand how computer works at the very low level. This book answers most of the questions I had during my under graduation, didnt only help me bruis my engg concepts but also placed everything at one place. This is a must read for tech enthusiast. Reading this book does not require expertise abt computers, a person having bare minimum knowledge will also be able to understand though in between sometimes it becomes a little confusing but if you are attentive and is putting all of it in one place you will be able to understand it comfortably."
119,0735611319,http://goodreads.com/user/show/32751589-amy,4,"This book does a good job explaining a lot of the basics of computing along with explaining some of the history of computers. It goes from explaining the basics of circuits, relays, and binary to assembly languages and the breakdown of basic parts of computers. Even though technology today has come so far since this book was written, I still recommend this book if you want to understand many of the basic concepts of computing and where a lot of it started."
120,0735611319,http://goodreads.com/user/show/35899146-jakub,5,"I wish I discovered this book earlier. It is a great introduction to computation and I do recommend it to everybody that would like to understand what is going on inside their laptops or phones.I found the ""big picture"" to ""the actual details"" ratio very suitable for a casual read. There are some parts that require more attention though - especially when the author explains the details of a data flow in the computer or the way Intel 8080 chip works. Nevertheless that's an inherent characteristics of any subject related to computation - some thinking is required to grasp the concept.All in all great introduction for non-programmers and even better refresher for people that are into programming but treat all the layers below the OS as a 'back box'. This is highly recommended."
121,0735611319,http://goodreads.com/user/show/11199869-prashant-ghabak,5,The author starts from electrons then electricity and builds right up to a high level programming language to explain fundamentals of a computer science while walking us through the history of the field. Very well written. 
122,0735611319,http://goodreads.com/user/show/37846711-andrew-alkema,5,"This was an excellent book. I love Charles' approach to explaining how a computer works. All of a sudden this collection of electrical circuits was a microprocessor. I really enjoyed the history of computing mixed in as well, that's something we did not learn enough of in CS at university, so I really enjoyed getting more of that here. Even though this book was written in 2000, the information is still just as relevant. To make the concepts simpler, most of the hardware he looks at in detail is from the 1970's anyway, so that part hasn't changed. The rest of the concept haven't gone anywhere. Don't let the age turn you off."
123,0735611319,http://goodreads.com/user/show/74276701-chad-lavimoniere,4,"A good overview of the basics of CS and early history, but at this point hilariously outdated. Could do with a major second edition. "
124,0735611319,http://goodreads.com/user/show/31121633-leonid,5,"Absolutely great book.It starts from the general idea of codes, like Morse or Braille, and then shows how computers designed and built, starting from relays, and finishing with asm (and then moves on to memory, storage, cpu design, text and graphics i/o)."
125,0735611319,http://goodreads.com/user/show/21307031,5,"A fascinating journey into the history and ""nature"" of computers written with an effort to make things easy to understand. A very interesting and important read for anyone related to IT"
126,0735611319,http://goodreads.com/user/show/4130219-moayyed-almizyen,5,Thank you Charles Pertzold ..
127,0735611319,http://goodreads.com/user/show/45773827-rick-sam,5,"An excellent book to learn about bottom-up details of Computers. It starts with two friends trying to communicate with each other, Morse Code, Braille, Flashlights... Gates, ... Assembly Language, Operating System and finally Graphical Revolution. Overall, this would help you to understand from First Principles, you can build from there. What an impressive accomplishment and progress! I would recommend this to anyone interested in Software, Hardware, Computer Science. Deus Vult, Gottfriend"
128,0735611319,http://goodreads.com/user/show/84459920-nick,4,"When I started reading this book I had an inkling that this book is suitable for nerds/technical people but after reading the first few chapters I thought that probably I was wrong. As it turned out in the end, it's at best a mixture of two with a heavy hilt towards first.The book explains in detail the working of a computer system along with the history of its progress over the year. It starts with a very innocuous example of communication between two close by living friend using just a torch. After that, chapter after chapter, it builds upon that by explaining in subsequent chapters the shortcomings and the solutions developed by industry, until it reaches the working of high-level languages and how a computer does calculations!!While the language of the book is lucid and definitely an improvement over dry engineering books that a lot of people here would be so used to, it couldn't help falling in that trap as it progresses. If you don't follow the writing closely and sequentially the preceding chapters may just be undecipherable to you. The book goes deep into circuits and their working and although it starts with a very good example and maintains continuity, it becomes very tech oriented and complex. Even after having studied a lot of the topics from the book during graduation, I myself couldn't help but skim over some parts as the writing became too technical and fully resembles an engineering book. Guess you can't escape that however much you try!!This book is more suitable for nerds/technical-oriented readers. If you want to know how and what of your system, this book can give you a very good idea. I may even say that if that's how the engineering books were written a lot more people would benefit. If someone here, during their course of engineering, is starting with the subject of digital systems; I strongly suggest reading this book cover to cover over (say over a period of two weeks or a month) before they start with their assigned books. You would be thanking me and the author later!! For others, if you pay close attention from the very start, this is the book to gain some understanding system since a very complex topic is explained in a very easy language starting with a very basic and relatable example and improving upon it. If this book doesn't do it for you, nothing would."
129,0735611319,http://goodreads.com/user/show/5268938-lauren,4,Fascinating look at code and computers and the building blocks that make them. Not my usual cup of tea but fascinating nonetheless. From flashlight communication to 2000-era processors the author builds a computer.
130,0735611319,http://goodreads.com/user/show/15643804-gustav-ton-r,5,"Basically everything you need to build a modern computer, both hardware and software, from scratch. Perfect read after the apocalypse. Too few people grasp these relatively easy concepts that power modern society through the internet and the cell phones in our pockets. This book summarizes computer technology in a clear and concise way."
131,0735611319,http://goodreads.com/user/show/8017610-james-millikan-sj,5,"A fascinating account of how electronics work, using stories, clever examples, and clear illustrations. Starting with elementary logic, examples from Morse code, and basic circuits, Petzold explains how to build a computer.This sounds like a rather dry read, but in reality it is an engaging and well written journey that weaves electrical engineering and history together into a compelling synthesis.If you‚Äôve ever wondered how your digital watch works, or the structure of binary, or how erasable digital memory is possible, look no further than Petzold‚Äôs classic. Highly recommended."
132,0735611319,http://goodreads.com/user/show/4606766-scott-johnson,5,"This, finally, was what I was looking for in my quest to understand computers.We start with some really boring, basic things about the most elementary circuits. It gets a pass because it's necessary if you're coming in blind, but I've taken courses in things like the physics of transistors, so I wasn't a fan personally. The same goes for assorted later sections on number systems (already overly familiar with Binary and Hex, thanks).Then we move into how you take very simple components and construct logic gates. I was vaguely familiar with their use, but had not actually seen one built from relays.The theme of building further complexity just by stacking blocks is introduced, and is the key to everything. Once I understand how a NOR gate works, I don't need to constantly trace the paths, we can shortcut now to using the correct symbol in the circuit with two (or more) inputs and an output. Similarly, 3+ input gates that are really cascades of 2+ gates being abbreviated as what without context looked like a new component makes complete sense.Using that, we construct a simple circuit to add binary bits. My only big criticism is that this is where we're building up to how a microprocessor works, but we suddenly veer left to look at memory.This was the most enlightening and involved portion. We gradually build up from a single bit being stored as a consequence of a quirky configuration of logic gates that ""trap"" a bit to adding things like control mechanisms (only write to the bit if this input is on), refreshes, and, most importantly, arrays of these storage bits.This was where I finally started to settle into this paradigm. Trying to think about the big picture is impossible. We're talking about quantities of components that the human mind cannot track, time spans we can't conceive, and frequencies that are impossible to imagine.Faith in the process is the key. I understood the science behind relays and transistors. I understood how these allow us to introduce logic. I understood how combining these gates leads to operations like addition or controlling the routing of a circuit. I understand how a moderate array of these things leads to a useful component like a byte of memory. I understand how each bit can be addressed to be written, read, and cleared.So I just needed to learn to relax and remember that I know these things, and that all we're doing now is that same thing....a few million times. Yes, that seems impossible, but you don't need to deal with all of that. You just have to do what we did with cascading logic gates being combined into 3+ input gates: Simplify it into a known component. Who cares what's happening in each of the 500 black boxes that all do the same thing. We just know we need this input and this is how output behaves.Once you understand how to chain them together into large arrays, why not simplify this into just ONE big black box, the DIMM. I now know how a signal from the processor (or, later, storage device) goes through an unimaginably long branching addressing system.I could not easily trace exactly which circuit a signal will follow to access a certain memory address, but I have faith that, given what I understand, I COULD do it if I really had to. I know what I understand really well on a small scale still works perfectly fine even if there are a few tens of thousands of branches now. It's not even complicated, every single node in the branching decision tree is identical, it's just big.This was a little more fuzzy when we returned to the microprocessor and the idea of instructions and opcodes, but I think I get it. This was unfortunately not covered in as much detail, and it's the one section I do want to do more reading about specifically.My understanding is that instruction sets are hard-wired into the processor. Similar to memory addressing, there seems to be some kind of input gate where the opcode instruction is interpreted to route the data it's carrying to the right part of the chip? Or there could be dedicated inputs for the specific operations, and the routing is done on the motherboard or by a secondary controller chip? Or both?It's a little unclear to me exactly how that control process happens, but I DO understand the important part: That processors have a finite instruction set, and perform very simple operations......but really, really fast. Another revelation was the idea of a clock signal as an input that ratchets the data forward one step through the circuits per iteration. I guess a lemma to my ""it's not complicated, just big"" theorem would be, ""don't think of circuits as continuously evolving, they are discretely iterating"".I was struggling at first to fathom how just adding these two bits together in any way led to these characters being displayed on my screen right now. I kept thinking, ""But you still have to do this, and this, and....."" Ok.....so then we do those things. In 2018 we're doing BILLIONS of things EACH SECOND. Response times are in nanoseconds. Sure, it's a lot....but that's fine. It's not complicated, it's just a tediously long chain of really simple events.Storage was easy. There was a bit of hand waving about how the addressing translates to moving the read head to this part of the disk or whatever, but I know enough about RAM addressing now to assume how it works. I can assume that logic is built into a controller on that disk that's hard-wired for that particular hardware configuration (hence why you don't really need drivers for components like these.....and also makes me realize why drivers are necessary to essentially tell the rest of the computer what a new component's inputs, outputs, and instruction sets are).Displays were REALLY simple thanks to one phrase: ""visual memory"". This wasn't from this book, it was from my last read, Turing's Cathedral, in discussing the use of CRTs as memory devices briefly, so you could see the memory live. They said something offhanded about, ""That's all the display on your monitor is anyway, showing an array of memory as pixel values, "" or something like that. The video card just constantly accesses that visual memory, turns it into pixel instructions, and sends it out the adapter for the monitor (with its own hard-wired controller) to translate to the actual pixels.The last piece was....how does everything know where to send the data? Answer: BUS! I now get how you can have so many things on so few buses and not have every component getting useless data from the rest. I had never heard of three-state logic, but it makes total sense (especially after an explanation that ""0 and 1"" are really more like ""high and low"") that there could be a state that would essentially block a component from the circuit so it has the ""state"" that it's neither high nor low. It then follows quite simply that the address of that component on the bus would trigger that control circuit to leave that non-state and connect to the bus again.Put all of this together, and I get it now. It's kind of like one of those Magic Eye pictures: If I relax and let my eyes go fuzzy, I can see the kitty, but as soon as I try to focus on anything it dissolves into chaos, and at best I can resolve a few of the dots. I just need to stop worrying about the 4 billion dots on the paper, just relax my eyes and have confidence that they're all still there.Aside from the sequencing of memory and processors, I think this was really well structured to flow logically and build up these complicated ideas from just a few simpler ones. A big point in its favor, also, was introducing that idea itself at the very beginning so it doesn't blindside the reader. It lets you read seemingly scary ideas with the context that once you get it you can just file it away as another building block for later use.10/10 would recommend to anyone who, like me, really wants to understand how you get from simple circuits to complex operating systems without resorting to magic."
133,0735611319,http://goodreads.com/user/show/117278-simmoril,4,"One of the biggest difficulties that is unique to Computer Science is this idea of 'layers of abstraction' - interfaces created to help hide the complexity of the underlying layer. While this can be a boon when developing, it becomes a problem when those lower layers start misbehaving, and you don't know why. Or, at a more basic level, these layers of abstraction can make it hard to understand why things are the way that they are (like why computers don't count in base 10, or why I can't run Unix programs in Windows). Petzold's book attempts to resolve this issue by starting out at the absolute most basic, physical level of a computer and working up through the different layers until he gets to what is pretty much a modern day computer. Through the use of good examples and an informal style of writing, Petzold introduces the reader to a large number of fundamental topics in Computer Science/Engineering, such as binary, logic gates, processor design, assembly, operating systems and programming languages. Petzold does a great job of not getting too bogged down in the details, and only giving the reader enough information to move up to the next layer.I enjoyed Code a great deal. Although much of this wasn't new to me, Petzold still surprised me with some historical facts I was not aware of, and the book itself served as a nice refresher course for some of the things that I studied and promptly forgotten in college. Although this book won't turn you into an expert by any means, it is a great starting point for understanding just how computers actually work.The only small issue I had with Code were that some areas were a little tough to get through. Towards the middle of the book, where Petzold starts to assemble memory circuits and building a rudimentary CPU, I found myself glossing over a lot of the details. For a complete understanding, these chapters would probably require a couple of passes. Also, the last couple of chapters seemed a little more hodge-podge than the rest. While they are good tidbits of information to know, it just seemed a little less organized than the rest of the book.Overall, Code is a fantastic book, and I highly recommend it as a starting point for anyone who wants to learn more about computers. In fact, I'd nearly go so far as to say it should be required reading!"
134,0735611319,http://goodreads.com/user/show/53697998-dave-voyles,5,"While looking for an answer on Stack Overflow one day, I saw several people recommend this book, to get a grasp on what was happening under the hood of my computer -- specifically, from beginning to end, how is a computer made?This book broke it down in such a way that I now understand it completely. The author starts of small and slow, and gradually builds up, from explaining how different number systems work as well as why we have so many, and the shortcomings of each (hex, binary, decimal, etc). From there, he goes into the use of relays, a bit about electrical engineering, and next thing you know, you're looking at Assembly code. It all starts with communication, languages (I‚Äôm talking Morse code and brail), and goes from there. Incredibly useful.code-bookI now have a far better understanding of the things my friends were speaking of on technical threads before, largely from the terms I learned in this book:carry bit, high-order bit, lower-order bit, etc,.I wish I had read something like this when I first started programming. If you know anyone who wants to get started in coding, then I'd start here, not because of the code it will teach you (you won't learn much on that here), but understanding what your code is doing to the actual hardware. The computer no longer appears like a black box.It was published by Microsoft Press in 2000, and the author, Charles Petzold, actually works for Microsoft (now Xamarin).Read this if you want to understand how your computer really works."
135,0735611319,http://goodreads.com/user/show/25094561-elijah-oyekunle,5,"Really good read for newbies looking to understand computers, behind the scenes."
136,0735611319,http://goodreads.com/user/show/981470-clarence,4,"Most people nowadays, if they wanted to explain how computers work, would probably ensure that the reader knew binary arithmetic, then talk about processor instructions, and from there work up through the higher levels of programming.Petzold takes an entirely different tack, which is completely centered around hardware. In fact, he starts with electric circuits, describing how a boy might build a circuit to light a lamp in his friend's house. He builds on that, getting into circuits that with multiple lamps, talks about telegraphs, and on and on. He does eventually take a detour in one chapter to talk about number bases, but even goes to the trouble of using not just base 10 or base 2 but unusual others to ensure that the reader understands both that the concepts are independent of the base, and yet also why base 2 is particularly useful in building digital computers.In addition, whether you know the subject matter or not, this books is an enjoyable read. People who recognize the name Petzold will most likely think of his old ""Programming Windows"" and similar books, which were dry textbooks. Not so this volume. I was very pleasantly surprised at what a fun read this is.Highly recommended to anyone, whether they have an interest in computers or not."
137,0735611319,http://goodreads.com/user/show/28438106-michael,5,"This book covers a variety of topics about what is going on under the hood of a computer, without muddling up the explanations with too many technical details. The author favors explaining the big picture and the components that make up that big picture, rather than staying too focused on one topic for too long and providing too many technical and insignificant details.For example, the concepts of logic gates and boolean functions are worthy of having entire books dedicated to them, but this book instead spends a concise two or three chapters on them, explaining the essentials to understand the big picture of what they are, and how they relate to the inside of a CPU. These concepts are then utilized later in the book to help explain other topics such as how a computer's memory works by utilizing those logic gates.Perhaps this books best feature is that, as I was reading and questions popped into my head, the very next paragraph would often open with something like, ""You might be wondering..."", and time and time again, I *was* wondering exactly what the author thought I might be wondering.It's all about the big picture, and this book absolutely nails it."
138,0735611319,http://goodreads.com/user/show/44327766,5,"This is the first book I would recommend to anyone wanting to learn about how computers work. It was written in 1999 and shows its age in some respects, but overall I would consider it a timeless classic.The one thing I was a bit sad to see was the incorrect use of the metric unit prefixes when refering to binary quantities. In the context of the time this book was written, the authors usage of metric units was common, and even today there is much confusion about it. A year before this book was published, the IEC and others published standards recommending binary prefixes. https://en.wikipedia.org/wiki/Binary_...Unicode was also only briefly mentioned. I would LOVE to see a revision of this book to update dated references, go over (and properly use) the differences between metric and binary prefixes, and to expand the content with details about advances that have happened since 1999.I was made aware of this book by Fabian Sanglard's blog: http://fabiensanglard.net/books_recom...He recommends ""Computer Organization and Design"" as a more in-depth follow-up. Expect a review of the 5th edition from me in the future!"
139,0735611319,http://goodreads.com/user/show/4090833-lingliang,5,"Excellent lucid explanation of the legacy of genius that has left us with the incredible abstracted world of computers. The abstraction allows us to accomplish creations of unimaginable complexity. This is a delight to read, as it clearly goes through layers and layers of genius, great minds building upon the remarkable history of computing, leaving us with a much more worthy appreciation of the beautiful creation that is the modern computer. It goes through each step of abstraction, starting with a compute relay and how it works (in terms of electromagnetics), and then goes step by step, all the way to the implementation of modern operating systems and programming languages. It's complexity is sufficient to leave little lingering questions but not so complex to be burdened by unnecessary rigor, leaving this boom a delight to read for both experts and laymen alike. This is an essential read for any computer scientist or even anyone who wants an appreciation of the creation that is a cornerstone of the modern human experience."
140,0735611319,http://goodreads.com/user/show/2478866-sonofabit,5,"Absolutely phenomenal book that's not so much about code but rather about the deep underlying concepts behind how a computer works, how it ""thinks"". If you've ever wanted to know more about bits and bytes and the mechanics behind the ones and zeros that everyone takes for granted as they browse facebook or listen to mp3s, this is the book for you!There were several ""AHA!"" moments that FINALLY cleared up unresolved questions from my Digital Circuits class back in college; I don't know why this wasn't the textbook they used :) However it's more than just a book about logic and circuits; it takes you so deep into the lowest levels of code and logic that it's almost an Anatomy course for computers. You literally get to see what happens at the smallest level, such as a circuit diagram for storing a byte!A truly fascinating read for anyone interested in programming, digital circuitry, or electrical engineering. Pick this one up today!"
141,0735611319,http://goodreads.com/user/show/676492-jean-luc,5,"There's a long, long list of books where my common reaction to them is ""I wish I'd read this in high school, it could've set me straight much earlier!"" Unfortunately, this isn't one of them... because I graduated in 1998 and this was published in 1999.At some point in your computer science career, you will take a courses and labs in digital systems. At Stevens, when I was your age, this was 381 (Switching Theory and Logical Design) and 383 (Computer Organization). This book combines both of those courses, starting from counting numbers all the way to building a programmable computer. The main difference is that the author, Charles Petzold, actually cares about the subject and makes it interesting. I would gladly have payed $4800 to read this book instead of paying that amount to take those 2 courses because I actually learned something from this book.Would make a great, great, great gift for any high school student considering a career in programming."
142,0735611319,http://goodreads.com/user/show/121883-mike,4,"I'd definitely recommend this to anyone who is even remotely curious how computers work, and doesn't mind going all the way back to basic physics to find out. The author builds a very basic but working computer all the way from the ground up, and giving you a unique view of the concepts involved that certainly wasn't taught in any engineering or computer science class I've ever taken (B.S.E.E./Rutgers '01). Although he gets a little lost near the end and doesn't adequately show how everything since the basics came out has just been ""making it better"", showing you the basics is the hard part, and combined with the complexity that comes later its all you really need to understand the emergent behavior of computing. "
143,0735611319,http://goodreads.com/user/show/11831233-jon,5,"Intimidated by digital technology? Think your computer secretly hates you? Can't understand why your device won't do what you tell it to? (Even though it is doing exactly what you told it to do...)Read this book. All complicated technology is made up of layer-upon-layer of less complicated pieces down to some very simple straight forward parts that do only one thing in response to something else that only does one other thing. Learn from the bottom up how digital, and in some cases analog, machines that we have allowed to control our live exist and do more in a second than you would want to do in your lifetime.And... No I will not fix your computer, that's what children are for now-a-days."
144,0735611319,http://goodreads.com/user/show/50247467-matthew-porche,3,"I will not lie, this book was tough to read. It flowed in some chapters, but not others. Petzold likes to talk about how much he hates analogies, but I seem to require them to understand a lot of things. I love computers, and this book helped me understand how they work, but it took forever before he started talking about them. I learned a lot on the small end of the scale, but he didn't discuss a whole lot about computers that I didn't already know. I was planning on citing this book for a research paper on cyber security, but this will not make it in the paper. There were a lot of dry spots in the book where I recall having to force myself to read the book. I cannot say I'd recommend this to anyone else, but I would not deter anyone from reading this book either. "
145,0735611319,http://goodreads.com/user/show/54838935-aaron-manley,5,"This is the book to read if you want to ""get"" computers. Not just how code executes programs, or how your operating system is what you use to run everything else - this goes all the way down to the computer architecture at the microcode level (relatively speaking). The book looks at basic human problems, discovers some ingenious quirks of nature involving switches and eventually electricity that allow us to use code to represent information to help us solve problems. Starting at a on/off relay, and ending in a full-fledged computer processor with memory, this is the book to read to understand how computer hardware and computer software are truly connected. It's also a good primer if you want to give nand2tetris a try and build your own virtual computer online."
146,0735611319,http://goodreads.com/user/show/7962528-michael,5,"What a great book! I asked myself why I didn't read this as a freshman in college, and then realized that it was published in 1999 when I was a sophomore.This book distills the workings of computers down to their most basic elements - voltages moving across circuits in a controlled way - and builds on itself until you get to a 1970s era microprocessor. From there, everything modern computers is simply layered on top one step at a time.What's going on inside your computer (or phone!) is simply amazing, but it's also completely understandable. I'm going to have my kids read this when they're old enough to understand the concepts. "
147,0735611319,http://goodreads.com/user/show/31061871-alex,4,"Good intro to Computer Science. Last few chapters didn't age too well since the book was written, but provide a valuable history lesson for younger programmers."
148,0735611319,http://goodreads.com/user/show/10165915-yogesh,5,"Perfect book if you want to understand how computer works at the very low level. This book answers most of the questions I had during my under graduation, didnt only help me bruis my engg concepts but also placed everything at one place. This is a must read for tech enthusiast. Reading this book does not require expertise abt computers, a person having bare minimum knowledge will also be able to understand though in between sometimes it becomes a little confusing but if you are attentive and is putting all of it in one place you will be able to understand it comfortably."
149,0735611319,http://goodreads.com/user/show/32751589-amy,4,"This book does a good job explaining a lot of the basics of computing along with explaining some of the history of computers. It goes from explaining the basics of circuits, relays, and binary to assembly languages and the breakdown of basic parts of computers. Even though technology today has come so far since this book was written, I still recommend this book if you want to understand many of the basic concepts of computing and where a lot of it started."
150,0735611319,http://goodreads.com/user/show/35899146-jakub,5,"I wish I discovered this book earlier. It is a great introduction to computation and I do recommend it to everybody that would like to understand what is going on inside their laptops or phones.I found the ""big picture"" to ""the actual details"" ratio very suitable for a casual read. There are some parts that require more attention though - especially when the author explains the details of a data flow in the computer or the way Intel 8080 chip works. Nevertheless that's an inherent characteristics of any subject related to computation - some thinking is required to grasp the concept.All in all great introduction for non-programmers and even better refresher for people that are into programming but treat all the layers below the OS as a 'back box'. This is highly recommended."
151,0735611319,http://goodreads.com/user/show/11199869-prashant-ghabak,5,The author starts from electrons then electricity and builds right up to a high level programming language to explain fundamentals of a computer science while walking us through the history of the field. Very well written. 
152,0735611319,http://goodreads.com/user/show/37846711-andrew-alkema,5,"This was an excellent book. I love Charles' approach to explaining how a computer works. All of a sudden this collection of electrical circuits was a microprocessor. I really enjoyed the history of computing mixed in as well, that's something we did not learn enough of in CS at university, so I really enjoyed getting more of that here. Even though this book was written in 2000, the information is still just as relevant. To make the concepts simpler, most of the hardware he looks at in detail is from the 1970's anyway, so that part hasn't changed. The rest of the concept haven't gone anywhere. Don't let the age turn you off."
153,0735611319,http://goodreads.com/user/show/74276701-chad-lavimoniere,4,"A good overview of the basics of CS and early history, but at this point hilariously outdated. Could do with a major second edition. "
154,0735611319,http://goodreads.com/user/show/31121633-leonid,5,"Absolutely great book.It starts from the general idea of codes, like Morse or Braille, and then shows how computers designed and built, starting from relays, and finishing with asm (and then moves on to memory, storage, cpu design, text and graphics i/o)."
155,0735611319,http://goodreads.com/user/show/21307031,5,"A fascinating journey into the history and ""nature"" of computers written with an effort to make things easy to understand. A very interesting and important read for anyone related to IT"
156,0735611319,http://goodreads.com/user/show/4130219-moayyed-almizyen,5,Thank you Charles Pertzold ..
157,0735611319,http://goodreads.com/user/show/45773827-rick-sam,5,"An excellent book to learn about bottom-up details of Computers. It starts with two friends trying to communicate with each other, Morse Code, Braille, Flashlights... Gates, ... Assembly Language, Operating System and finally Graphical Revolution. Overall, this would help you to understand from First Principles, you can build from there. What an impressive accomplishment and progress! I would recommend this to anyone interested in Software, Hardware, Computer Science. Deus Vult, Gottfriend"
158,0735611319,http://goodreads.com/user/show/84459920-nick,4,"When I started reading this book I had an inkling that this book is suitable for nerds/technical people but after reading the first few chapters I thought that probably I was wrong. As it turned out in the end, it's at best a mixture of two with a heavy hilt towards first.The book explains in detail the working of a computer system along with the history of its progress over the year. It starts with a very innocuous example of communication between two close by living friend using just a torch. After that, chapter after chapter, it builds upon that by explaining in subsequent chapters the shortcomings and the solutions developed by industry, until it reaches the working of high-level languages and how a computer does calculations!!While the language of the book is lucid and definitely an improvement over dry engineering books that a lot of people here would be so used to, it couldn't help falling in that trap as it progresses. If you don't follow the writing closely and sequentially the preceding chapters may just be undecipherable to you. The book goes deep into circuits and their working and although it starts with a very good example and maintains continuity, it becomes very tech oriented and complex. Even after having studied a lot of the topics from the book during graduation, I myself couldn't help but skim over some parts as the writing became too technical and fully resembles an engineering book. Guess you can't escape that however much you try!!This book is more suitable for nerds/technical-oriented readers. If you want to know how and what of your system, this book can give you a very good idea. I may even say that if that's how the engineering books were written a lot more people would benefit. If someone here, during their course of engineering, is starting with the subject of digital systems; I strongly suggest reading this book cover to cover over (say over a period of two weeks or a month) before they start with their assigned books. You would be thanking me and the author later!! For others, if you pay close attention from the very start, this is the book to gain some understanding system since a very complex topic is explained in a very easy language starting with a very basic and relatable example and improving upon it. If this book doesn't do it for you, nothing would."
159,0735611319,http://goodreads.com/user/show/5268938-lauren,4,Fascinating look at code and computers and the building blocks that make them. Not my usual cup of tea but fascinating nonetheless. From flashlight communication to 2000-era processors the author builds a computer.
160,0735611319,http://goodreads.com/user/show/15643804-gustav-ton-r,5,"Basically everything you need to build a modern computer, both hardware and software, from scratch. Perfect read after the apocalypse. Too few people grasp these relatively easy concepts that power modern society through the internet and the cell phones in our pockets. This book summarizes computer technology in a clear and concise way."
161,0735611319,http://goodreads.com/user/show/8017610-james-millikan-sj,5,"A fascinating account of how electronics work, using stories, clever examples, and clear illustrations. Starting with elementary logic, examples from Morse code, and basic circuits, Petzold explains how to build a computer.This sounds like a rather dry read, but in reality it is an engaging and well written journey that weaves electrical engineering and history together into a compelling synthesis.If you‚Äôve ever wondered how your digital watch works, or the structure of binary, or how erasable digital memory is possible, look no further than Petzold‚Äôs classic. Highly recommended."
162,0735611319,http://goodreads.com/user/show/4606766-scott-johnson,5,"This, finally, was what I was looking for in my quest to understand computers.We start with some really boring, basic things about the most elementary circuits. It gets a pass because it's necessary if you're coming in blind, but I've taken courses in things like the physics of transistors, so I wasn't a fan personally. The same goes for assorted later sections on number systems (already overly familiar with Binary and Hex, thanks).Then we move into how you take very simple components and construct logic gates. I was vaguely familiar with their use, but had not actually seen one built from relays.The theme of building further complexity just by stacking blocks is introduced, and is the key to everything. Once I understand how a NOR gate works, I don't need to constantly trace the paths, we can shortcut now to using the correct symbol in the circuit with two (or more) inputs and an output. Similarly, 3+ input gates that are really cascades of 2+ gates being abbreviated as what without context looked like a new component makes complete sense.Using that, we construct a simple circuit to add binary bits. My only big criticism is that this is where we're building up to how a microprocessor works, but we suddenly veer left to look at memory.This was the most enlightening and involved portion. We gradually build up from a single bit being stored as a consequence of a quirky configuration of logic gates that ""trap"" a bit to adding things like control mechanisms (only write to the bit if this input is on), refreshes, and, most importantly, arrays of these storage bits.This was where I finally started to settle into this paradigm. Trying to think about the big picture is impossible. We're talking about quantities of components that the human mind cannot track, time spans we can't conceive, and frequencies that are impossible to imagine.Faith in the process is the key. I understood the science behind relays and transistors. I understood how these allow us to introduce logic. I understood how combining these gates leads to operations like addition or controlling the routing of a circuit. I understand how a moderate array of these things leads to a useful component like a byte of memory. I understand how each bit can be addressed to be written, read, and cleared.So I just needed to learn to relax and remember that I know these things, and that all we're doing now is that same thing....a few million times. Yes, that seems impossible, but you don't need to deal with all of that. You just have to do what we did with cascading logic gates being combined into 3+ input gates: Simplify it into a known component. Who cares what's happening in each of the 500 black boxes that all do the same thing. We just know we need this input and this is how output behaves.Once you understand how to chain them together into large arrays, why not simplify this into just ONE big black box, the DIMM. I now know how a signal from the processor (or, later, storage device) goes through an unimaginably long branching addressing system.I could not easily trace exactly which circuit a signal will follow to access a certain memory address, but I have faith that, given what I understand, I COULD do it if I really had to. I know what I understand really well on a small scale still works perfectly fine even if there are a few tens of thousands of branches now. It's not even complicated, every single node in the branching decision tree is identical, it's just big.This was a little more fuzzy when we returned to the microprocessor and the idea of instructions and opcodes, but I think I get it. This was unfortunately not covered in as much detail, and it's the one section I do want to do more reading about specifically.My understanding is that instruction sets are hard-wired into the processor. Similar to memory addressing, there seems to be some kind of input gate where the opcode instruction is interpreted to route the data it's carrying to the right part of the chip? Or there could be dedicated inputs for the specific operations, and the routing is done on the motherboard or by a secondary controller chip? Or both?It's a little unclear to me exactly how that control process happens, but I DO understand the important part: That processors have a finite instruction set, and perform very simple operations......but really, really fast. Another revelation was the idea of a clock signal as an input that ratchets the data forward one step through the circuits per iteration. I guess a lemma to my ""it's not complicated, just big"" theorem would be, ""don't think of circuits as continuously evolving, they are discretely iterating"".I was struggling at first to fathom how just adding these two bits together in any way led to these characters being displayed on my screen right now. I kept thinking, ""But you still have to do this, and this, and....."" Ok.....so then we do those things. In 2018 we're doing BILLIONS of things EACH SECOND. Response times are in nanoseconds. Sure, it's a lot....but that's fine. It's not complicated, it's just a tediously long chain of really simple events.Storage was easy. There was a bit of hand waving about how the addressing translates to moving the read head to this part of the disk or whatever, but I know enough about RAM addressing now to assume how it works. I can assume that logic is built into a controller on that disk that's hard-wired for that particular hardware configuration (hence why you don't really need drivers for components like these.....and also makes me realize why drivers are necessary to essentially tell the rest of the computer what a new component's inputs, outputs, and instruction sets are).Displays were REALLY simple thanks to one phrase: ""visual memory"". This wasn't from this book, it was from my last read, Turing's Cathedral, in discussing the use of CRTs as memory devices briefly, so you could see the memory live. They said something offhanded about, ""That's all the display on your monitor is anyway, showing an array of memory as pixel values, "" or something like that. The video card just constantly accesses that visual memory, turns it into pixel instructions, and sends it out the adapter for the monitor (with its own hard-wired controller) to translate to the actual pixels.The last piece was....how does everything know where to send the data? Answer: BUS! I now get how you can have so many things on so few buses and not have every component getting useless data from the rest. I had never heard of three-state logic, but it makes total sense (especially after an explanation that ""0 and 1"" are really more like ""high and low"") that there could be a state that would essentially block a component from the circuit so it has the ""state"" that it's neither high nor low. It then follows quite simply that the address of that component on the bus would trigger that control circuit to leave that non-state and connect to the bus again.Put all of this together, and I get it now. It's kind of like one of those Magic Eye pictures: If I relax and let my eyes go fuzzy, I can see the kitty, but as soon as I try to focus on anything it dissolves into chaos, and at best I can resolve a few of the dots. I just need to stop worrying about the 4 billion dots on the paper, just relax my eyes and have confidence that they're all still there.Aside from the sequencing of memory and processors, I think this was really well structured to flow logically and build up these complicated ideas from just a few simpler ones. A big point in its favor, also, was introducing that idea itself at the very beginning so it doesn't blindside the reader. It lets you read seemingly scary ideas with the context that once you get it you can just file it away as another building block for later use.10/10 would recommend to anyone who, like me, really wants to understand how you get from simple circuits to complex operating systems without resorting to magic."
163,0735611319,http://goodreads.com/user/show/117278-simmoril,4,"One of the biggest difficulties that is unique to Computer Science is this idea of 'layers of abstraction' - interfaces created to help hide the complexity of the underlying layer. While this can be a boon when developing, it becomes a problem when those lower layers start misbehaving, and you don't know why. Or, at a more basic level, these layers of abstraction can make it hard to understand why things are the way that they are (like why computers don't count in base 10, or why I can't run Unix programs in Windows). Petzold's book attempts to resolve this issue by starting out at the absolute most basic, physical level of a computer and working up through the different layers until he gets to what is pretty much a modern day computer. Through the use of good examples and an informal style of writing, Petzold introduces the reader to a large number of fundamental topics in Computer Science/Engineering, such as binary, logic gates, processor design, assembly, operating systems and programming languages. Petzold does a great job of not getting too bogged down in the details, and only giving the reader enough information to move up to the next layer.I enjoyed Code a great deal. Although much of this wasn't new to me, Petzold still surprised me with some historical facts I was not aware of, and the book itself served as a nice refresher course for some of the things that I studied and promptly forgotten in college. Although this book won't turn you into an expert by any means, it is a great starting point for understanding just how computers actually work.The only small issue I had with Code were that some areas were a little tough to get through. Towards the middle of the book, where Petzold starts to assemble memory circuits and building a rudimentary CPU, I found myself glossing over a lot of the details. For a complete understanding, these chapters would probably require a couple of passes. Also, the last couple of chapters seemed a little more hodge-podge than the rest. While they are good tidbits of information to know, it just seemed a little less organized than the rest of the book.Overall, Code is a fantastic book, and I highly recommend it as a starting point for anyone who wants to learn more about computers. In fact, I'd nearly go so far as to say it should be required reading!"
164,0735611319,http://goodreads.com/user/show/53697998-dave-voyles,5,"While looking for an answer on Stack Overflow one day, I saw several people recommend this book, to get a grasp on what was happening under the hood of my computer -- specifically, from beginning to end, how is a computer made?This book broke it down in such a way that I now understand it completely. The author starts of small and slow, and gradually builds up, from explaining how different number systems work as well as why we have so many, and the shortcomings of each (hex, binary, decimal, etc). From there, he goes into the use of relays, a bit about electrical engineering, and next thing you know, you're looking at Assembly code. It all starts with communication, languages (I‚Äôm talking Morse code and brail), and goes from there. Incredibly useful.code-bookI now have a far better understanding of the things my friends were speaking of on technical threads before, largely from the terms I learned in this book:carry bit, high-order bit, lower-order bit, etc,.I wish I had read something like this when I first started programming. If you know anyone who wants to get started in coding, then I'd start here, not because of the code it will teach you (you won't learn much on that here), but understanding what your code is doing to the actual hardware. The computer no longer appears like a black box.It was published by Microsoft Press in 2000, and the author, Charles Petzold, actually works for Microsoft (now Xamarin).Read this if you want to understand how your computer really works."
165,0735611319,http://goodreads.com/user/show/25094561-elijah-oyekunle,5,"Really good read for newbies looking to understand computers, behind the scenes."
166,0735611319,http://goodreads.com/user/show/981470-clarence,4,"Most people nowadays, if they wanted to explain how computers work, would probably ensure that the reader knew binary arithmetic, then talk about processor instructions, and from there work up through the higher levels of programming.Petzold takes an entirely different tack, which is completely centered around hardware. In fact, he starts with electric circuits, describing how a boy might build a circuit to light a lamp in his friend's house. He builds on that, getting into circuits that with multiple lamps, talks about telegraphs, and on and on. He does eventually take a detour in one chapter to talk about number bases, but even goes to the trouble of using not just base 10 or base 2 but unusual others to ensure that the reader understands both that the concepts are independent of the base, and yet also why base 2 is particularly useful in building digital computers.In addition, whether you know the subject matter or not, this books is an enjoyable read. People who recognize the name Petzold will most likely think of his old ""Programming Windows"" and similar books, which were dry textbooks. Not so this volume. I was very pleasantly surprised at what a fun read this is.Highly recommended to anyone, whether they have an interest in computers or not."
167,0735611319,http://goodreads.com/user/show/28438106-michael,5,"This book covers a variety of topics about what is going on under the hood of a computer, without muddling up the explanations with too many technical details. The author favors explaining the big picture and the components that make up that big picture, rather than staying too focused on one topic for too long and providing too many technical and insignificant details.For example, the concepts of logic gates and boolean functions are worthy of having entire books dedicated to them, but this book instead spends a concise two or three chapters on them, explaining the essentials to understand the big picture of what they are, and how they relate to the inside of a CPU. These concepts are then utilized later in the book to help explain other topics such as how a computer's memory works by utilizing those logic gates.Perhaps this books best feature is that, as I was reading and questions popped into my head, the very next paragraph would often open with something like, ""You might be wondering..."", and time and time again, I *was* wondering exactly what the author thought I might be wondering.It's all about the big picture, and this book absolutely nails it."
168,0735611319,http://goodreads.com/user/show/44327766,5,"This is the first book I would recommend to anyone wanting to learn about how computers work. It was written in 1999 and shows its age in some respects, but overall I would consider it a timeless classic.The one thing I was a bit sad to see was the incorrect use of the metric unit prefixes when refering to binary quantities. In the context of the time this book was written, the authors usage of metric units was common, and even today there is much confusion about it. A year before this book was published, the IEC and others published standards recommending binary prefixes. https://en.wikipedia.org/wiki/Binary_...Unicode was also only briefly mentioned. I would LOVE to see a revision of this book to update dated references, go over (and properly use) the differences between metric and binary prefixes, and to expand the content with details about advances that have happened since 1999.I was made aware of this book by Fabian Sanglard's blog: http://fabiensanglard.net/books_recom...He recommends ""Computer Organization and Design"" as a more in-depth follow-up. Expect a review of the 5th edition from me in the future!"
169,0735611319,http://goodreads.com/user/show/4090833-lingliang,5,"Excellent lucid explanation of the legacy of genius that has left us with the incredible abstracted world of computers. The abstraction allows us to accomplish creations of unimaginable complexity. This is a delight to read, as it clearly goes through layers and layers of genius, great minds building upon the remarkable history of computing, leaving us with a much more worthy appreciation of the beautiful creation that is the modern computer. It goes through each step of abstraction, starting with a compute relay and how it works (in terms of electromagnetics), and then goes step by step, all the way to the implementation of modern operating systems and programming languages. It's complexity is sufficient to leave little lingering questions but not so complex to be burdened by unnecessary rigor, leaving this boom a delight to read for both experts and laymen alike. This is an essential read for any computer scientist or even anyone who wants an appreciation of the creation that is a cornerstone of the modern human experience."
170,0735611319,http://goodreads.com/user/show/2478866-sonofabit,5,"Absolutely phenomenal book that's not so much about code but rather about the deep underlying concepts behind how a computer works, how it ""thinks"". If you've ever wanted to know more about bits and bytes and the mechanics behind the ones and zeros that everyone takes for granted as they browse facebook or listen to mp3s, this is the book for you!There were several ""AHA!"" moments that FINALLY cleared up unresolved questions from my Digital Circuits class back in college; I don't know why this wasn't the textbook they used :) However it's more than just a book about logic and circuits; it takes you so deep into the lowest levels of code and logic that it's almost an Anatomy course for computers. You literally get to see what happens at the smallest level, such as a circuit diagram for storing a byte!A truly fascinating read for anyone interested in programming, digital circuitry, or electrical engineering. Pick this one up today!"
171,0735611319,http://goodreads.com/user/show/676492-jean-luc,5,"There's a long, long list of books where my common reaction to them is ""I wish I'd read this in high school, it could've set me straight much earlier!"" Unfortunately, this isn't one of them... because I graduated in 1998 and this was published in 1999.At some point in your computer science career, you will take a courses and labs in digital systems. At Stevens, when I was your age, this was 381 (Switching Theory and Logical Design) and 383 (Computer Organization). This book combines both of those courses, starting from counting numbers all the way to building a programmable computer. The main difference is that the author, Charles Petzold, actually cares about the subject and makes it interesting. I would gladly have payed $4800 to read this book instead of paying that amount to take those 2 courses because I actually learned something from this book.Would make a great, great, great gift for any high school student considering a career in programming."
172,0735611319,http://goodreads.com/user/show/121883-mike,4,"I'd definitely recommend this to anyone who is even remotely curious how computers work, and doesn't mind going all the way back to basic physics to find out. The author builds a very basic but working computer all the way from the ground up, and giving you a unique view of the concepts involved that certainly wasn't taught in any engineering or computer science class I've ever taken (B.S.E.E./Rutgers '01). Although he gets a little lost near the end and doesn't adequately show how everything since the basics came out has just been ""making it better"", showing you the basics is the hard part, and combined with the complexity that comes later its all you really need to understand the emergent behavior of computing. "
173,0735611319,http://goodreads.com/user/show/11831233-jon,5,"Intimidated by digital technology? Think your computer secretly hates you? Can't understand why your device won't do what you tell it to? (Even though it is doing exactly what you told it to do...)Read this book. All complicated technology is made up of layer-upon-layer of less complicated pieces down to some very simple straight forward parts that do only one thing in response to something else that only does one other thing. Learn from the bottom up how digital, and in some cases analog, machines that we have allowed to control our live exist and do more in a second than you would want to do in your lifetime.And... No I will not fix your computer, that's what children are for now-a-days."
174,0735611319,http://goodreads.com/user/show/50247467-matthew-porche,3,"I will not lie, this book was tough to read. It flowed in some chapters, but not others. Petzold likes to talk about how much he hates analogies, but I seem to require them to understand a lot of things. I love computers, and this book helped me understand how they work, but it took forever before he started talking about them. I learned a lot on the small end of the scale, but he didn't discuss a whole lot about computers that I didn't already know. I was planning on citing this book for a research paper on cyber security, but this will not make it in the paper. There were a lot of dry spots in the book where I recall having to force myself to read the book. I cannot say I'd recommend this to anyone else, but I would not deter anyone from reading this book either. "
175,0735611319,http://goodreads.com/user/show/54838935-aaron-manley,5,"This is the book to read if you want to ""get"" computers. Not just how code executes programs, or how your operating system is what you use to run everything else - this goes all the way down to the computer architecture at the microcode level (relatively speaking). The book looks at basic human problems, discovers some ingenious quirks of nature involving switches and eventually electricity that allow us to use code to represent information to help us solve problems. Starting at a on/off relay, and ending in a full-fledged computer processor with memory, this is the book to read to understand how computer hardware and computer software are truly connected. It's also a good primer if you want to give nand2tetris a try and build your own virtual computer online."
176,0735611319,http://goodreads.com/user/show/7962528-michael,5,"What a great book! I asked myself why I didn't read this as a freshman in college, and then realized that it was published in 1999 when I was a sophomore.This book distills the workings of computers down to their most basic elements - voltages moving across circuits in a controlled way - and builds on itself until you get to a 1970s era microprocessor. From there, everything modern computers is simply layered on top one step at a time.What's going on inside your computer (or phone!) is simply amazing, but it's also completely understandable. I'm going to have my kids read this when they're old enough to understand the concepts. "
177,0735611319,http://goodreads.com/user/show/31061871-alex,4,"Good intro to Computer Science. Last few chapters didn't age too well since the book was written, but provide a valuable history lesson for younger programmers."
178,0735611319,http://goodreads.com/user/show/10165915-yogesh,5,"Perfect book if you want to understand how computer works at the very low level. This book answers most of the questions I had during my under graduation, didnt only help me bruis my engg concepts but also placed everything at one place. This is a must read for tech enthusiast. Reading this book does not require expertise abt computers, a person having bare minimum knowledge will also be able to understand though in between sometimes it becomes a little confusing but if you are attentive and is putting all of it in one place you will be able to understand it comfortably."
179,0735611319,http://goodreads.com/user/show/32751589-amy,4,"This book does a good job explaining a lot of the basics of computing along with explaining some of the history of computers. It goes from explaining the basics of circuits, relays, and binary to assembly languages and the breakdown of basic parts of computers. Even though technology today has come so far since this book was written, I still recommend this book if you want to understand many of the basic concepts of computing and where a lot of it started."
180,0735611319,http://goodreads.com/user/show/35899146-jakub,5,"I wish I discovered this book earlier. It is a great introduction to computation and I do recommend it to everybody that would like to understand what is going on inside their laptops or phones.I found the ""big picture"" to ""the actual details"" ratio very suitable for a casual read. There are some parts that require more attention though - especially when the author explains the details of a data flow in the computer or the way Intel 8080 chip works. Nevertheless that's an inherent characteristics of any subject related to computation - some thinking is required to grasp the concept.All in all great introduction for non-programmers and even better refresher for people that are into programming but treat all the layers below the OS as a 'back box'. This is highly recommended."
181,0735611319,http://goodreads.com/user/show/11199869-prashant-ghabak,5,The author starts from electrons then electricity and builds right up to a high level programming language to explain fundamentals of a computer science while walking us through the history of the field. Very well written. 
182,0735611319,http://goodreads.com/user/show/37846711-andrew-alkema,5,"This was an excellent book. I love Charles' approach to explaining how a computer works. All of a sudden this collection of electrical circuits was a microprocessor. I really enjoyed the history of computing mixed in as well, that's something we did not learn enough of in CS at university, so I really enjoyed getting more of that here. Even though this book was written in 2000, the information is still just as relevant. To make the concepts simpler, most of the hardware he looks at in detail is from the 1970's anyway, so that part hasn't changed. The rest of the concept haven't gone anywhere. Don't let the age turn you off."
183,0735611319,http://goodreads.com/user/show/74276701-chad-lavimoniere,4,"A good overview of the basics of CS and early history, but at this point hilariously outdated. Could do with a major second edition. "
184,0735611319,http://goodreads.com/user/show/31121633-leonid,5,"Absolutely great book.It starts from the general idea of codes, like Morse or Braille, and then shows how computers designed and built, starting from relays, and finishing with asm (and then moves on to memory, storage, cpu design, text and graphics i/o)."
185,0735611319,http://goodreads.com/user/show/21307031,5,"A fascinating journey into the history and ""nature"" of computers written with an effort to make things easy to understand. A very interesting and important read for anyone related to IT"
186,0735611319,http://goodreads.com/user/show/4130219-moayyed-almizyen,5,Thank you Charles Pertzold ..
187,0735611319,http://goodreads.com/user/show/45773827-rick-sam,5,"An excellent book to learn about bottom-up details of Computers. It starts with two friends trying to communicate with each other, Morse Code, Braille, Flashlights... Gates, ... Assembly Language, Operating System and finally Graphical Revolution. Overall, this would help you to understand from First Principles, you can build from there. What an impressive accomplishment and progress! I would recommend this to anyone interested in Software, Hardware, Computer Science. Deus Vult, Gottfriend"
188,0735611319,http://goodreads.com/user/show/84459920-nick,4,"When I started reading this book I had an inkling that this book is suitable for nerds/technical people but after reading the first few chapters I thought that probably I was wrong. As it turned out in the end, it's at best a mixture of two with a heavy hilt towards first.The book explains in detail the working of a computer system along with the history of its progress over the year. It starts with a very innocuous example of communication between two close by living friend using just a torch. After that, chapter after chapter, it builds upon that by explaining in subsequent chapters the shortcomings and the solutions developed by industry, until it reaches the working of high-level languages and how a computer does calculations!!While the language of the book is lucid and definitely an improvement over dry engineering books that a lot of people here would be so used to, it couldn't help falling in that trap as it progresses. If you don't follow the writing closely and sequentially the preceding chapters may just be undecipherable to you. The book goes deep into circuits and their working and although it starts with a very good example and maintains continuity, it becomes very tech oriented and complex. Even after having studied a lot of the topics from the book during graduation, I myself couldn't help but skim over some parts as the writing became too technical and fully resembles an engineering book. Guess you can't escape that however much you try!!This book is more suitable for nerds/technical-oriented readers. If you want to know how and what of your system, this book can give you a very good idea. I may even say that if that's how the engineering books were written a lot more people would benefit. If someone here, during their course of engineering, is starting with the subject of digital systems; I strongly suggest reading this book cover to cover over (say over a period of two weeks or a month) before they start with their assigned books. You would be thanking me and the author later!! For others, if you pay close attention from the very start, this is the book to gain some understanding system since a very complex topic is explained in a very easy language starting with a very basic and relatable example and improving upon it. If this book doesn't do it for you, nothing would."
189,0735611319,http://goodreads.com/user/show/5268938-lauren,4,Fascinating look at code and computers and the building blocks that make them. Not my usual cup of tea but fascinating nonetheless. From flashlight communication to 2000-era processors the author builds a computer.
190,0735611319,http://goodreads.com/user/show/15643804-gustav-ton-r,5,"Basically everything you need to build a modern computer, both hardware and software, from scratch. Perfect read after the apocalypse. Too few people grasp these relatively easy concepts that power modern society through the internet and the cell phones in our pockets. This book summarizes computer technology in a clear and concise way."
191,0735611319,http://goodreads.com/user/show/8017610-james-millikan-sj,5,"A fascinating account of how electronics work, using stories, clever examples, and clear illustrations. Starting with elementary logic, examples from Morse code, and basic circuits, Petzold explains how to build a computer.This sounds like a rather dry read, but in reality it is an engaging and well written journey that weaves electrical engineering and history together into a compelling synthesis.If you‚Äôve ever wondered how your digital watch works, or the structure of binary, or how erasable digital memory is possible, look no further than Petzold‚Äôs classic. Highly recommended."
192,0735611319,http://goodreads.com/user/show/4606766-scott-johnson,5,"This, finally, was what I was looking for in my quest to understand computers.We start with some really boring, basic things about the most elementary circuits. It gets a pass because it's necessary if you're coming in blind, but I've taken courses in things like the physics of transistors, so I wasn't a fan personally. The same goes for assorted later sections on number systems (already overly familiar with Binary and Hex, thanks).Then we move into how you take very simple components and construct logic gates. I was vaguely familiar with their use, but had not actually seen one built from relays.The theme of building further complexity just by stacking blocks is introduced, and is the key to everything. Once I understand how a NOR gate works, I don't need to constantly trace the paths, we can shortcut now to using the correct symbol in the circuit with two (or more) inputs and an output. Similarly, 3+ input gates that are really cascades of 2+ gates being abbreviated as what without context looked like a new component makes complete sense.Using that, we construct a simple circuit to add binary bits. My only big criticism is that this is where we're building up to how a microprocessor works, but we suddenly veer left to look at memory.This was the most enlightening and involved portion. We gradually build up from a single bit being stored as a consequence of a quirky configuration of logic gates that ""trap"" a bit to adding things like control mechanisms (only write to the bit if this input is on), refreshes, and, most importantly, arrays of these storage bits.This was where I finally started to settle into this paradigm. Trying to think about the big picture is impossible. We're talking about quantities of components that the human mind cannot track, time spans we can't conceive, and frequencies that are impossible to imagine.Faith in the process is the key. I understood the science behind relays and transistors. I understood how these allow us to introduce logic. I understood how combining these gates leads to operations like addition or controlling the routing of a circuit. I understand how a moderate array of these things leads to a useful component like a byte of memory. I understand how each bit can be addressed to be written, read, and cleared.So I just needed to learn to relax and remember that I know these things, and that all we're doing now is that same thing....a few million times. Yes, that seems impossible, but you don't need to deal with all of that. You just have to do what we did with cascading logic gates being combined into 3+ input gates: Simplify it into a known component. Who cares what's happening in each of the 500 black boxes that all do the same thing. We just know we need this input and this is how output behaves.Once you understand how to chain them together into large arrays, why not simplify this into just ONE big black box, the DIMM. I now know how a signal from the processor (or, later, storage device) goes through an unimaginably long branching addressing system.I could not easily trace exactly which circuit a signal will follow to access a certain memory address, but I have faith that, given what I understand, I COULD do it if I really had to. I know what I understand really well on a small scale still works perfectly fine even if there are a few tens of thousands of branches now. It's not even complicated, every single node in the branching decision tree is identical, it's just big.This was a little more fuzzy when we returned to the microprocessor and the idea of instructions and opcodes, but I think I get it. This was unfortunately not covered in as much detail, and it's the one section I do want to do more reading about specifically.My understanding is that instruction sets are hard-wired into the processor. Similar to memory addressing, there seems to be some kind of input gate where the opcode instruction is interpreted to route the data it's carrying to the right part of the chip? Or there could be dedicated inputs for the specific operations, and the routing is done on the motherboard or by a secondary controller chip? Or both?It's a little unclear to me exactly how that control process happens, but I DO understand the important part: That processors have a finite instruction set, and perform very simple operations......but really, really fast. Another revelation was the idea of a clock signal as an input that ratchets the data forward one step through the circuits per iteration. I guess a lemma to my ""it's not complicated, just big"" theorem would be, ""don't think of circuits as continuously evolving, they are discretely iterating"".I was struggling at first to fathom how just adding these two bits together in any way led to these characters being displayed on my screen right now. I kept thinking, ""But you still have to do this, and this, and....."" Ok.....so then we do those things. In 2018 we're doing BILLIONS of things EACH SECOND. Response times are in nanoseconds. Sure, it's a lot....but that's fine. It's not complicated, it's just a tediously long chain of really simple events.Storage was easy. There was a bit of hand waving about how the addressing translates to moving the read head to this part of the disk or whatever, but I know enough about RAM addressing now to assume how it works. I can assume that logic is built into a controller on that disk that's hard-wired for that particular hardware configuration (hence why you don't really need drivers for components like these.....and also makes me realize why drivers are necessary to essentially tell the rest of the computer what a new component's inputs, outputs, and instruction sets are).Displays were REALLY simple thanks to one phrase: ""visual memory"". This wasn't from this book, it was from my last read, Turing's Cathedral, in discussing the use of CRTs as memory devices briefly, so you could see the memory live. They said something offhanded about, ""That's all the display on your monitor is anyway, showing an array of memory as pixel values, "" or something like that. The video card just constantly accesses that visual memory, turns it into pixel instructions, and sends it out the adapter for the monitor (with its own hard-wired controller) to translate to the actual pixels.The last piece was....how does everything know where to send the data? Answer: BUS! I now get how you can have so many things on so few buses and not have every component getting useless data from the rest. I had never heard of three-state logic, but it makes total sense (especially after an explanation that ""0 and 1"" are really more like ""high and low"") that there could be a state that would essentially block a component from the circuit so it has the ""state"" that it's neither high nor low. It then follows quite simply that the address of that component on the bus would trigger that control circuit to leave that non-state and connect to the bus again.Put all of this together, and I get it now. It's kind of like one of those Magic Eye pictures: If I relax and let my eyes go fuzzy, I can see the kitty, but as soon as I try to focus on anything it dissolves into chaos, and at best I can resolve a few of the dots. I just need to stop worrying about the 4 billion dots on the paper, just relax my eyes and have confidence that they're all still there.Aside from the sequencing of memory and processors, I think this was really well structured to flow logically and build up these complicated ideas from just a few simpler ones. A big point in its favor, also, was introducing that idea itself at the very beginning so it doesn't blindside the reader. It lets you read seemingly scary ideas with the context that once you get it you can just file it away as another building block for later use.10/10 would recommend to anyone who, like me, really wants to understand how you get from simple circuits to complex operating systems without resorting to magic."
193,0735611319,http://goodreads.com/user/show/117278-simmoril,4,"One of the biggest difficulties that is unique to Computer Science is this idea of 'layers of abstraction' - interfaces created to help hide the complexity of the underlying layer. While this can be a boon when developing, it becomes a problem when those lower layers start misbehaving, and you don't know why. Or, at a more basic level, these layers of abstraction can make it hard to understand why things are the way that they are (like why computers don't count in base 10, or why I can't run Unix programs in Windows). Petzold's book attempts to resolve this issue by starting out at the absolute most basic, physical level of a computer and working up through the different layers until he gets to what is pretty much a modern day computer. Through the use of good examples and an informal style of writing, Petzold introduces the reader to a large number of fundamental topics in Computer Science/Engineering, such as binary, logic gates, processor design, assembly, operating systems and programming languages. Petzold does a great job of not getting too bogged down in the details, and only giving the reader enough information to move up to the next layer.I enjoyed Code a great deal. Although much of this wasn't new to me, Petzold still surprised me with some historical facts I was not aware of, and the book itself served as a nice refresher course for some of the things that I studied and promptly forgotten in college. Although this book won't turn you into an expert by any means, it is a great starting point for understanding just how computers actually work.The only small issue I had with Code were that some areas were a little tough to get through. Towards the middle of the book, where Petzold starts to assemble memory circuits and building a rudimentary CPU, I found myself glossing over a lot of the details. For a complete understanding, these chapters would probably require a couple of passes. Also, the last couple of chapters seemed a little more hodge-podge than the rest. While they are good tidbits of information to know, it just seemed a little less organized than the rest of the book.Overall, Code is a fantastic book, and I highly recommend it as a starting point for anyone who wants to learn more about computers. In fact, I'd nearly go so far as to say it should be required reading!"
194,0735611319,http://goodreads.com/user/show/53697998-dave-voyles,5,"While looking for an answer on Stack Overflow one day, I saw several people recommend this book, to get a grasp on what was happening under the hood of my computer -- specifically, from beginning to end, how is a computer made?This book broke it down in such a way that I now understand it completely. The author starts of small and slow, and gradually builds up, from explaining how different number systems work as well as why we have so many, and the shortcomings of each (hex, binary, decimal, etc). From there, he goes into the use of relays, a bit about electrical engineering, and next thing you know, you're looking at Assembly code. It all starts with communication, languages (I‚Äôm talking Morse code and brail), and goes from there. Incredibly useful.code-bookI now have a far better understanding of the things my friends were speaking of on technical threads before, largely from the terms I learned in this book:carry bit, high-order bit, lower-order bit, etc,.I wish I had read something like this when I first started programming. If you know anyone who wants to get started in coding, then I'd start here, not because of the code it will teach you (you won't learn much on that here), but understanding what your code is doing to the actual hardware. The computer no longer appears like a black box.It was published by Microsoft Press in 2000, and the author, Charles Petzold, actually works for Microsoft (now Xamarin).Read this if you want to understand how your computer really works."
195,0735611319,http://goodreads.com/user/show/25094561-elijah-oyekunle,5,"Really good read for newbies looking to understand computers, behind the scenes."
196,0735611319,http://goodreads.com/user/show/981470-clarence,4,"Most people nowadays, if they wanted to explain how computers work, would probably ensure that the reader knew binary arithmetic, then talk about processor instructions, and from there work up through the higher levels of programming.Petzold takes an entirely different tack, which is completely centered around hardware. In fact, he starts with electric circuits, describing how a boy might build a circuit to light a lamp in his friend's house. He builds on that, getting into circuits that with multiple lamps, talks about telegraphs, and on and on. He does eventually take a detour in one chapter to talk about number bases, but even goes to the trouble of using not just base 10 or base 2 but unusual others to ensure that the reader understands both that the concepts are independent of the base, and yet also why base 2 is particularly useful in building digital computers.In addition, whether you know the subject matter or not, this books is an enjoyable read. People who recognize the name Petzold will most likely think of his old ""Programming Windows"" and similar books, which were dry textbooks. Not so this volume. I was very pleasantly surprised at what a fun read this is.Highly recommended to anyone, whether they have an interest in computers or not."
197,0735611319,http://goodreads.com/user/show/28438106-michael,5,"This book covers a variety of topics about what is going on under the hood of a computer, without muddling up the explanations with too many technical details. The author favors explaining the big picture and the components that make up that big picture, rather than staying too focused on one topic for too long and providing too many technical and insignificant details.For example, the concepts of logic gates and boolean functions are worthy of having entire books dedicated to them, but this book instead spends a concise two or three chapters on them, explaining the essentials to understand the big picture of what they are, and how they relate to the inside of a CPU. These concepts are then utilized later in the book to help explain other topics such as how a computer's memory works by utilizing those logic gates.Perhaps this books best feature is that, as I was reading and questions popped into my head, the very next paragraph would often open with something like, ""You might be wondering..."", and time and time again, I *was* wondering exactly what the author thought I might be wondering.It's all about the big picture, and this book absolutely nails it."
198,0735611319,http://goodreads.com/user/show/44327766,5,"This is the first book I would recommend to anyone wanting to learn about how computers work. It was written in 1999 and shows its age in some respects, but overall I would consider it a timeless classic.The one thing I was a bit sad to see was the incorrect use of the metric unit prefixes when refering to binary quantities. In the context of the time this book was written, the authors usage of metric units was common, and even today there is much confusion about it. A year before this book was published, the IEC and others published standards recommending binary prefixes. https://en.wikipedia.org/wiki/Binary_...Unicode was also only briefly mentioned. I would LOVE to see a revision of this book to update dated references, go over (and properly use) the differences between metric and binary prefixes, and to expand the content with details about advances that have happened since 1999.I was made aware of this book by Fabian Sanglard's blog: http://fabiensanglard.net/books_recom...He recommends ""Computer Organization and Design"" as a more in-depth follow-up. Expect a review of the 5th edition from me in the future!"
199,0735611319,http://goodreads.com/user/show/4090833-lingliang,5,"Excellent lucid explanation of the legacy of genius that has left us with the incredible abstracted world of computers. The abstraction allows us to accomplish creations of unimaginable complexity. This is a delight to read, as it clearly goes through layers and layers of genius, great minds building upon the remarkable history of computing, leaving us with a much more worthy appreciation of the beautiful creation that is the modern computer. It goes through each step of abstraction, starting with a compute relay and how it works (in terms of electromagnetics), and then goes step by step, all the way to the implementation of modern operating systems and programming languages. It's complexity is sufficient to leave little lingering questions but not so complex to be burdened by unnecessary rigor, leaving this boom a delight to read for both experts and laymen alike. This is an essential read for any computer scientist or even anyone who wants an appreciation of the creation that is a cornerstone of the modern human experience."
200,0735611319,http://goodreads.com/user/show/2478866-sonofabit,5,"Absolutely phenomenal book that's not so much about code but rather about the deep underlying concepts behind how a computer works, how it ""thinks"". If you've ever wanted to know more about bits and bytes and the mechanics behind the ones and zeros that everyone takes for granted as they browse facebook or listen to mp3s, this is the book for you!There were several ""AHA!"" moments that FINALLY cleared up unresolved questions from my Digital Circuits class back in college; I don't know why this wasn't the textbook they used :) However it's more than just a book about logic and circuits; it takes you so deep into the lowest levels of code and logic that it's almost an Anatomy course for computers. You literally get to see what happens at the smallest level, such as a circuit diagram for storing a byte!A truly fascinating read for anyone interested in programming, digital circuitry, or electrical engineering. Pick this one up today!"
201,0735611319,http://goodreads.com/user/show/676492-jean-luc,5,"There's a long, long list of books where my common reaction to them is ""I wish I'd read this in high school, it could've set me straight much earlier!"" Unfortunately, this isn't one of them... because I graduated in 1998 and this was published in 1999.At some point in your computer science career, you will take a courses and labs in digital systems. At Stevens, when I was your age, this was 381 (Switching Theory and Logical Design) and 383 (Computer Organization). This book combines both of those courses, starting from counting numbers all the way to building a programmable computer. The main difference is that the author, Charles Petzold, actually cares about the subject and makes it interesting. I would gladly have payed $4800 to read this book instead of paying that amount to take those 2 courses because I actually learned something from this book.Would make a great, great, great gift for any high school student considering a career in programming."
202,0735611319,http://goodreads.com/user/show/121883-mike,4,"I'd definitely recommend this to anyone who is even remotely curious how computers work, and doesn't mind going all the way back to basic physics to find out. The author builds a very basic but working computer all the way from the ground up, and giving you a unique view of the concepts involved that certainly wasn't taught in any engineering or computer science class I've ever taken (B.S.E.E./Rutgers '01). Although he gets a little lost near the end and doesn't adequately show how everything since the basics came out has just been ""making it better"", showing you the basics is the hard part, and combined with the complexity that comes later its all you really need to understand the emergent behavior of computing. "
203,0735611319,http://goodreads.com/user/show/11831233-jon,5,"Intimidated by digital technology? Think your computer secretly hates you? Can't understand why your device won't do what you tell it to? (Even though it is doing exactly what you told it to do...)Read this book. All complicated technology is made up of layer-upon-layer of less complicated pieces down to some very simple straight forward parts that do only one thing in response to something else that only does one other thing. Learn from the bottom up how digital, and in some cases analog, machines that we have allowed to control our live exist and do more in a second than you would want to do in your lifetime.And... No I will not fix your computer, that's what children are for now-a-days."
204,0735611319,http://goodreads.com/user/show/50247467-matthew-porche,3,"I will not lie, this book was tough to read. It flowed in some chapters, but not others. Petzold likes to talk about how much he hates analogies, but I seem to require them to understand a lot of things. I love computers, and this book helped me understand how they work, but it took forever before he started talking about them. I learned a lot on the small end of the scale, but he didn't discuss a whole lot about computers that I didn't already know. I was planning on citing this book for a research paper on cyber security, but this will not make it in the paper. There were a lot of dry spots in the book where I recall having to force myself to read the book. I cannot say I'd recommend this to anyone else, but I would not deter anyone from reading this book either. "
205,0735611319,http://goodreads.com/user/show/54838935-aaron-manley,5,"This is the book to read if you want to ""get"" computers. Not just how code executes programs, or how your operating system is what you use to run everything else - this goes all the way down to the computer architecture at the microcode level (relatively speaking). The book looks at basic human problems, discovers some ingenious quirks of nature involving switches and eventually electricity that allow us to use code to represent information to help us solve problems. Starting at a on/off relay, and ending in a full-fledged computer processor with memory, this is the book to read to understand how computer hardware and computer software are truly connected. It's also a good primer if you want to give nand2tetris a try and build your own virtual computer online."
206,0735611319,http://goodreads.com/user/show/7962528-michael,5,"What a great book! I asked myself why I didn't read this as a freshman in college, and then realized that it was published in 1999 when I was a sophomore.This book distills the workings of computers down to their most basic elements - voltages moving across circuits in a controlled way - and builds on itself until you get to a 1970s era microprocessor. From there, everything modern computers is simply layered on top one step at a time.What's going on inside your computer (or phone!) is simply amazing, but it's also completely understandable. I'm going to have my kids read this when they're old enough to understand the concepts. "
207,0735611319,http://goodreads.com/user/show/31061871-alex,4,"Good intro to Computer Science. Last few chapters didn't age too well since the book was written, but provide a valuable history lesson for younger programmers."
208,0735611319,http://goodreads.com/user/show/10165915-yogesh,5,"Perfect book if you want to understand how computer works at the very low level. This book answers most of the questions I had during my under graduation, didnt only help me bruis my engg concepts but also placed everything at one place. This is a must read for tech enthusiast. Reading this book does not require expertise abt computers, a person having bare minimum knowledge will also be able to understand though in between sometimes it becomes a little confusing but if you are attentive and is putting all of it in one place you will be able to understand it comfortably."
209,0735611319,http://goodreads.com/user/show/32751589-amy,4,"This book does a good job explaining a lot of the basics of computing along with explaining some of the history of computers. It goes from explaining the basics of circuits, relays, and binary to assembly languages and the breakdown of basic parts of computers. Even though technology today has come so far since this book was written, I still recommend this book if you want to understand many of the basic concepts of computing and where a lot of it started."
210,0735611319,http://goodreads.com/user/show/35899146-jakub,5,"I wish I discovered this book earlier. It is a great introduction to computation and I do recommend it to everybody that would like to understand what is going on inside their laptops or phones.I found the ""big picture"" to ""the actual details"" ratio very suitable for a casual read. There are some parts that require more attention though - especially when the author explains the details of a data flow in the computer or the way Intel 8080 chip works. Nevertheless that's an inherent characteristics of any subject related to computation - some thinking is required to grasp the concept.All in all great introduction for non-programmers and even better refresher for people that are into programming but treat all the layers below the OS as a 'back box'. This is highly recommended."
211,0735611319,http://goodreads.com/user/show/11199869-prashant-ghabak,5,The author starts from electrons then electricity and builds right up to a high level programming language to explain fundamentals of a computer science while walking us through the history of the field. Very well written. 
212,0735611319,http://goodreads.com/user/show/37846711-andrew-alkema,5,"This was an excellent book. I love Charles' approach to explaining how a computer works. All of a sudden this collection of electrical circuits was a microprocessor. I really enjoyed the history of computing mixed in as well, that's something we did not learn enough of in CS at university, so I really enjoyed getting more of that here. Even though this book was written in 2000, the information is still just as relevant. To make the concepts simpler, most of the hardware he looks at in detail is from the 1970's anyway, so that part hasn't changed. The rest of the concept haven't gone anywhere. Don't let the age turn you off."
213,0735611319,http://goodreads.com/user/show/74276701-chad-lavimoniere,4,"A good overview of the basics of CS and early history, but at this point hilariously outdated. Could do with a major second edition. "
214,0735611319,http://goodreads.com/user/show/31121633-leonid,5,"Absolutely great book.It starts from the general idea of codes, like Morse or Braille, and then shows how computers designed and built, starting from relays, and finishing with asm (and then moves on to memory, storage, cpu design, text and graphics i/o)."
215,0735611319,http://goodreads.com/user/show/21307031,5,"A fascinating journey into the history and ""nature"" of computers written with an effort to make things easy to understand. A very interesting and important read for anyone related to IT"
216,0735611319,http://goodreads.com/user/show/4130219-moayyed-almizyen,5,Thank you Charles Pertzold ..
217,0735611319,http://goodreads.com/user/show/45773827-rick-sam,5,"An excellent book to learn about bottom-up details of Computers. It starts with two friends trying to communicate with each other, Morse Code, Braille, Flashlights... Gates, ... Assembly Language, Operating System and finally Graphical Revolution. Overall, this would help you to understand from First Principles, you can build from there. What an impressive accomplishment and progress! I would recommend this to anyone interested in Software, Hardware, Computer Science. Deus Vult, Gottfriend"
218,0735611319,http://goodreads.com/user/show/84459920-nick,4,"When I started reading this book I had an inkling that this book is suitable for nerds/technical people but after reading the first few chapters I thought that probably I was wrong. As it turned out in the end, it's at best a mixture of two with a heavy hilt towards first.The book explains in detail the working of a computer system along with the history of its progress over the year. It starts with a very innocuous example of communication between two close by living friend using just a torch. After that, chapter after chapter, it builds upon that by explaining in subsequent chapters the shortcomings and the solutions developed by industry, until it reaches the working of high-level languages and how a computer does calculations!!While the language of the book is lucid and definitely an improvement over dry engineering books that a lot of people here would be so used to, it couldn't help falling in that trap as it progresses. If you don't follow the writing closely and sequentially the preceding chapters may just be undecipherable to you. The book goes deep into circuits and their working and although it starts with a very good example and maintains continuity, it becomes very tech oriented and complex. Even after having studied a lot of the topics from the book during graduation, I myself couldn't help but skim over some parts as the writing became too technical and fully resembles an engineering book. Guess you can't escape that however much you try!!This book is more suitable for nerds/technical-oriented readers. If you want to know how and what of your system, this book can give you a very good idea. I may even say that if that's how the engineering books were written a lot more people would benefit. If someone here, during their course of engineering, is starting with the subject of digital systems; I strongly suggest reading this book cover to cover over (say over a period of two weeks or a month) before they start with their assigned books. You would be thanking me and the author later!! For others, if you pay close attention from the very start, this is the book to gain some understanding system since a very complex topic is explained in a very easy language starting with a very basic and relatable example and improving upon it. If this book doesn't do it for you, nothing would."
219,0735611319,http://goodreads.com/user/show/5268938-lauren,4,Fascinating look at code and computers and the building blocks that make them. Not my usual cup of tea but fascinating nonetheless. From flashlight communication to 2000-era processors the author builds a computer.
220,0735611319,http://goodreads.com/user/show/15643804-gustav-ton-r,5,"Basically everything you need to build a modern computer, both hardware and software, from scratch. Perfect read after the apocalypse. Too few people grasp these relatively easy concepts that power modern society through the internet and the cell phones in our pockets. This book summarizes computer technology in a clear and concise way."
221,0735611319,http://goodreads.com/user/show/8017610-james-millikan-sj,5,"A fascinating account of how electronics work, using stories, clever examples, and clear illustrations. Starting with elementary logic, examples from Morse code, and basic circuits, Petzold explains how to build a computer.This sounds like a rather dry read, but in reality it is an engaging and well written journey that weaves electrical engineering and history together into a compelling synthesis.If you‚Äôve ever wondered how your digital watch works, or the structure of binary, or how erasable digital memory is possible, look no further than Petzold‚Äôs classic. Highly recommended."
222,0735611319,http://goodreads.com/user/show/4606766-scott-johnson,5,"This, finally, was what I was looking for in my quest to understand computers.We start with some really boring, basic things about the most elementary circuits. It gets a pass because it's necessary if you're coming in blind, but I've taken courses in things like the physics of transistors, so I wasn't a fan personally. The same goes for assorted later sections on number systems (already overly familiar with Binary and Hex, thanks).Then we move into how you take very simple components and construct logic gates. I was vaguely familiar with their use, but had not actually seen one built from relays.The theme of building further complexity just by stacking blocks is introduced, and is the key to everything. Once I understand how a NOR gate works, I don't need to constantly trace the paths, we can shortcut now to using the correct symbol in the circuit with two (or more) inputs and an output. Similarly, 3+ input gates that are really cascades of 2+ gates being abbreviated as what without context looked like a new component makes complete sense.Using that, we construct a simple circuit to add binary bits. My only big criticism is that this is where we're building up to how a microprocessor works, but we suddenly veer left to look at memory.This was the most enlightening and involved portion. We gradually build up from a single bit being stored as a consequence of a quirky configuration of logic gates that ""trap"" a bit to adding things like control mechanisms (only write to the bit if this input is on), refreshes, and, most importantly, arrays of these storage bits.This was where I finally started to settle into this paradigm. Trying to think about the big picture is impossible. We're talking about quantities of components that the human mind cannot track, time spans we can't conceive, and frequencies that are impossible to imagine.Faith in the process is the key. I understood the science behind relays and transistors. I understood how these allow us to introduce logic. I understood how combining these gates leads to operations like addition or controlling the routing of a circuit. I understand how a moderate array of these things leads to a useful component like a byte of memory. I understand how each bit can be addressed to be written, read, and cleared.So I just needed to learn to relax and remember that I know these things, and that all we're doing now is that same thing....a few million times. Yes, that seems impossible, but you don't need to deal with all of that. You just have to do what we did with cascading logic gates being combined into 3+ input gates: Simplify it into a known component. Who cares what's happening in each of the 500 black boxes that all do the same thing. We just know we need this input and this is how output behaves.Once you understand how to chain them together into large arrays, why not simplify this into just ONE big black box, the DIMM. I now know how a signal from the processor (or, later, storage device) goes through an unimaginably long branching addressing system.I could not easily trace exactly which circuit a signal will follow to access a certain memory address, but I have faith that, given what I understand, I COULD do it if I really had to. I know what I understand really well on a small scale still works perfectly fine even if there are a few tens of thousands of branches now. It's not even complicated, every single node in the branching decision tree is identical, it's just big.This was a little more fuzzy when we returned to the microprocessor and the idea of instructions and opcodes, but I think I get it. This was unfortunately not covered in as much detail, and it's the one section I do want to do more reading about specifically.My understanding is that instruction sets are hard-wired into the processor. Similar to memory addressing, there seems to be some kind of input gate where the opcode instruction is interpreted to route the data it's carrying to the right part of the chip? Or there could be dedicated inputs for the specific operations, and the routing is done on the motherboard or by a secondary controller chip? Or both?It's a little unclear to me exactly how that control process happens, but I DO understand the important part: That processors have a finite instruction set, and perform very simple operations......but really, really fast. Another revelation was the idea of a clock signal as an input that ratchets the data forward one step through the circuits per iteration. I guess a lemma to my ""it's not complicated, just big"" theorem would be, ""don't think of circuits as continuously evolving, they are discretely iterating"".I was struggling at first to fathom how just adding these two bits together in any way led to these characters being displayed on my screen right now. I kept thinking, ""But you still have to do this, and this, and....."" Ok.....so then we do those things. In 2018 we're doing BILLIONS of things EACH SECOND. Response times are in nanoseconds. Sure, it's a lot....but that's fine. It's not complicated, it's just a tediously long chain of really simple events.Storage was easy. There was a bit of hand waving about how the addressing translates to moving the read head to this part of the disk or whatever, but I know enough about RAM addressing now to assume how it works. I can assume that logic is built into a controller on that disk that's hard-wired for that particular hardware configuration (hence why you don't really need drivers for components like these.....and also makes me realize why drivers are necessary to essentially tell the rest of the computer what a new component's inputs, outputs, and instruction sets are).Displays were REALLY simple thanks to one phrase: ""visual memory"". This wasn't from this book, it was from my last read, Turing's Cathedral, in discussing the use of CRTs as memory devices briefly, so you could see the memory live. They said something offhanded about, ""That's all the display on your monitor is anyway, showing an array of memory as pixel values, "" or something like that. The video card just constantly accesses that visual memory, turns it into pixel instructions, and sends it out the adapter for the monitor (with its own hard-wired controller) to translate to the actual pixels.The last piece was....how does everything know where to send the data? Answer: BUS! I now get how you can have so many things on so few buses and not have every component getting useless data from the rest. I had never heard of three-state logic, but it makes total sense (especially after an explanation that ""0 and 1"" are really more like ""high and low"") that there could be a state that would essentially block a component from the circuit so it has the ""state"" that it's neither high nor low. It then follows quite simply that the address of that component on the bus would trigger that control circuit to leave that non-state and connect to the bus again.Put all of this together, and I get it now. It's kind of like one of those Magic Eye pictures: If I relax and let my eyes go fuzzy, I can see the kitty, but as soon as I try to focus on anything it dissolves into chaos, and at best I can resolve a few of the dots. I just need to stop worrying about the 4 billion dots on the paper, just relax my eyes and have confidence that they're all still there.Aside from the sequencing of memory and processors, I think this was really well structured to flow logically and build up these complicated ideas from just a few simpler ones. A big point in its favor, also, was introducing that idea itself at the very beginning so it doesn't blindside the reader. It lets you read seemingly scary ideas with the context that once you get it you can just file it away as another building block for later use.10/10 would recommend to anyone who, like me, really wants to understand how you get from simple circuits to complex operating systems without resorting to magic."
223,0735611319,http://goodreads.com/user/show/117278-simmoril,4,"One of the biggest difficulties that is unique to Computer Science is this idea of 'layers of abstraction' - interfaces created to help hide the complexity of the underlying layer. While this can be a boon when developing, it becomes a problem when those lower layers start misbehaving, and you don't know why. Or, at a more basic level, these layers of abstraction can make it hard to understand why things are the way that they are (like why computers don't count in base 10, or why I can't run Unix programs in Windows). Petzold's book attempts to resolve this issue by starting out at the absolute most basic, physical level of a computer and working up through the different layers until he gets to what is pretty much a modern day computer. Through the use of good examples and an informal style of writing, Petzold introduces the reader to a large number of fundamental topics in Computer Science/Engineering, such as binary, logic gates, processor design, assembly, operating systems and programming languages. Petzold does a great job of not getting too bogged down in the details, and only giving the reader enough information to move up to the next layer.I enjoyed Code a great deal. Although much of this wasn't new to me, Petzold still surprised me with some historical facts I was not aware of, and the book itself served as a nice refresher course for some of the things that I studied and promptly forgotten in college. Although this book won't turn you into an expert by any means, it is a great starting point for understanding just how computers actually work.The only small issue I had with Code were that some areas were a little tough to get through. Towards the middle of the book, where Petzold starts to assemble memory circuits and building a rudimentary CPU, I found myself glossing over a lot of the details. For a complete understanding, these chapters would probably require a couple of passes. Also, the last couple of chapters seemed a little more hodge-podge than the rest. While they are good tidbits of information to know, it just seemed a little less organized than the rest of the book.Overall, Code is a fantastic book, and I highly recommend it as a starting point for anyone who wants to learn more about computers. In fact, I'd nearly go so far as to say it should be required reading!"
224,0735611319,http://goodreads.com/user/show/53697998-dave-voyles,5,"While looking for an answer on Stack Overflow one day, I saw several people recommend this book, to get a grasp on what was happening under the hood of my computer -- specifically, from beginning to end, how is a computer made?This book broke it down in such a way that I now understand it completely. The author starts of small and slow, and gradually builds up, from explaining how different number systems work as well as why we have so many, and the shortcomings of each (hex, binary, decimal, etc). From there, he goes into the use of relays, a bit about electrical engineering, and next thing you know, you're looking at Assembly code. It all starts with communication, languages (I‚Äôm talking Morse code and brail), and goes from there. Incredibly useful.code-bookI now have a far better understanding of the things my friends were speaking of on technical threads before, largely from the terms I learned in this book:carry bit, high-order bit, lower-order bit, etc,.I wish I had read something like this when I first started programming. If you know anyone who wants to get started in coding, then I'd start here, not because of the code it will teach you (you won't learn much on that here), but understanding what your code is doing to the actual hardware. The computer no longer appears like a black box.It was published by Microsoft Press in 2000, and the author, Charles Petzold, actually works for Microsoft (now Xamarin).Read this if you want to understand how your computer really works."
225,0735611319,http://goodreads.com/user/show/25094561-elijah-oyekunle,5,"Really good read for newbies looking to understand computers, behind the scenes."
226,0735611319,http://goodreads.com/user/show/981470-clarence,4,"Most people nowadays, if they wanted to explain how computers work, would probably ensure that the reader knew binary arithmetic, then talk about processor instructions, and from there work up through the higher levels of programming.Petzold takes an entirely different tack, which is completely centered around hardware. In fact, he starts with electric circuits, describing how a boy might build a circuit to light a lamp in his friend's house. He builds on that, getting into circuits that with multiple lamps, talks about telegraphs, and on and on. He does eventually take a detour in one chapter to talk about number bases, but even goes to the trouble of using not just base 10 or base 2 but unusual others to ensure that the reader understands both that the concepts are independent of the base, and yet also why base 2 is particularly useful in building digital computers.In addition, whether you know the subject matter or not, this books is an enjoyable read. People who recognize the name Petzold will most likely think of his old ""Programming Windows"" and similar books, which were dry textbooks. Not so this volume. I was very pleasantly surprised at what a fun read this is.Highly recommended to anyone, whether they have an interest in computers or not."
227,0735611319,http://goodreads.com/user/show/28438106-michael,5,"This book covers a variety of topics about what is going on under the hood of a computer, without muddling up the explanations with too many technical details. The author favors explaining the big picture and the components that make up that big picture, rather than staying too focused on one topic for too long and providing too many technical and insignificant details.For example, the concepts of logic gates and boolean functions are worthy of having entire books dedicated to them, but this book instead spends a concise two or three chapters on them, explaining the essentials to understand the big picture of what they are, and how they relate to the inside of a CPU. These concepts are then utilized later in the book to help explain other topics such as how a computer's memory works by utilizing those logic gates.Perhaps this books best feature is that, as I was reading and questions popped into my head, the very next paragraph would often open with something like, ""You might be wondering..."", and time and time again, I *was* wondering exactly what the author thought I might be wondering.It's all about the big picture, and this book absolutely nails it."
228,0735611319,http://goodreads.com/user/show/44327766,5,"This is the first book I would recommend to anyone wanting to learn about how computers work. It was written in 1999 and shows its age in some respects, but overall I would consider it a timeless classic.The one thing I was a bit sad to see was the incorrect use of the metric unit prefixes when refering to binary quantities. In the context of the time this book was written, the authors usage of metric units was common, and even today there is much confusion about it. A year before this book was published, the IEC and others published standards recommending binary prefixes. https://en.wikipedia.org/wiki/Binary_...Unicode was also only briefly mentioned. I would LOVE to see a revision of this book to update dated references, go over (and properly use) the differences between metric and binary prefixes, and to expand the content with details about advances that have happened since 1999.I was made aware of this book by Fabian Sanglard's blog: http://fabiensanglard.net/books_recom...He recommends ""Computer Organization and Design"" as a more in-depth follow-up. Expect a review of the 5th edition from me in the future!"
229,0735611319,http://goodreads.com/user/show/4090833-lingliang,5,"Excellent lucid explanation of the legacy of genius that has left us with the incredible abstracted world of computers. The abstraction allows us to accomplish creations of unimaginable complexity. This is a delight to read, as it clearly goes through layers and layers of genius, great minds building upon the remarkable history of computing, leaving us with a much more worthy appreciation of the beautiful creation that is the modern computer. It goes through each step of abstraction, starting with a compute relay and how it works (in terms of electromagnetics), and then goes step by step, all the way to the implementation of modern operating systems and programming languages. It's complexity is sufficient to leave little lingering questions but not so complex to be burdened by unnecessary rigor, leaving this boom a delight to read for both experts and laymen alike. This is an essential read for any computer scientist or even anyone who wants an appreciation of the creation that is a cornerstone of the modern human experience."
230,0735611319,http://goodreads.com/user/show/2478866-sonofabit,5,"Absolutely phenomenal book that's not so much about code but rather about the deep underlying concepts behind how a computer works, how it ""thinks"". If you've ever wanted to know more about bits and bytes and the mechanics behind the ones and zeros that everyone takes for granted as they browse facebook or listen to mp3s, this is the book for you!There were several ""AHA!"" moments that FINALLY cleared up unresolved questions from my Digital Circuits class back in college; I don't know why this wasn't the textbook they used :) However it's more than just a book about logic and circuits; it takes you so deep into the lowest levels of code and logic that it's almost an Anatomy course for computers. You literally get to see what happens at the smallest level, such as a circuit diagram for storing a byte!A truly fascinating read for anyone interested in programming, digital circuitry, or electrical engineering. Pick this one up today!"
231,0735611319,http://goodreads.com/user/show/676492-jean-luc,5,"There's a long, long list of books where my common reaction to them is ""I wish I'd read this in high school, it could've set me straight much earlier!"" Unfortunately, this isn't one of them... because I graduated in 1998 and this was published in 1999.At some point in your computer science career, you will take a courses and labs in digital systems. At Stevens, when I was your age, this was 381 (Switching Theory and Logical Design) and 383 (Computer Organization). This book combines both of those courses, starting from counting numbers all the way to building a programmable computer. The main difference is that the author, Charles Petzold, actually cares about the subject and makes it interesting. I would gladly have payed $4800 to read this book instead of paying that amount to take those 2 courses because I actually learned something from this book.Would make a great, great, great gift for any high school student considering a career in programming."
232,0735611319,http://goodreads.com/user/show/121883-mike,4,"I'd definitely recommend this to anyone who is even remotely curious how computers work, and doesn't mind going all the way back to basic physics to find out. The author builds a very basic but working computer all the way from the ground up, and giving you a unique view of the concepts involved that certainly wasn't taught in any engineering or computer science class I've ever taken (B.S.E.E./Rutgers '01). Although he gets a little lost near the end and doesn't adequately show how everything since the basics came out has just been ""making it better"", showing you the basics is the hard part, and combined with the complexity that comes later its all you really need to understand the emergent behavior of computing. "
233,0735611319,http://goodreads.com/user/show/11831233-jon,5,"Intimidated by digital technology? Think your computer secretly hates you? Can't understand why your device won't do what you tell it to? (Even though it is doing exactly what you told it to do...)Read this book. All complicated technology is made up of layer-upon-layer of less complicated pieces down to some very simple straight forward parts that do only one thing in response to something else that only does one other thing. Learn from the bottom up how digital, and in some cases analog, machines that we have allowed to control our live exist and do more in a second than you would want to do in your lifetime.And... No I will not fix your computer, that's what children are for now-a-days."
234,0735611319,http://goodreads.com/user/show/50247467-matthew-porche,3,"I will not lie, this book was tough to read. It flowed in some chapters, but not others. Petzold likes to talk about how much he hates analogies, but I seem to require them to understand a lot of things. I love computers, and this book helped me understand how they work, but it took forever before he started talking about them. I learned a lot on the small end of the scale, but he didn't discuss a whole lot about computers that I didn't already know. I was planning on citing this book for a research paper on cyber security, but this will not make it in the paper. There were a lot of dry spots in the book where I recall having to force myself to read the book. I cannot say I'd recommend this to anyone else, but I would not deter anyone from reading this book either. "
235,0735611319,http://goodreads.com/user/show/54838935-aaron-manley,5,"This is the book to read if you want to ""get"" computers. Not just how code executes programs, or how your operating system is what you use to run everything else - this goes all the way down to the computer architecture at the microcode level (relatively speaking). The book looks at basic human problems, discovers some ingenious quirks of nature involving switches and eventually electricity that allow us to use code to represent information to help us solve problems. Starting at a on/off relay, and ending in a full-fledged computer processor with memory, this is the book to read to understand how computer hardware and computer software are truly connected. It's also a good primer if you want to give nand2tetris a try and build your own virtual computer online."
236,0735611319,http://goodreads.com/user/show/7962528-michael,5,"What a great book! I asked myself why I didn't read this as a freshman in college, and then realized that it was published in 1999 when I was a sophomore.This book distills the workings of computers down to their most basic elements - voltages moving across circuits in a controlled way - and builds on itself until you get to a 1970s era microprocessor. From there, everything modern computers is simply layered on top one step at a time.What's going on inside your computer (or phone!) is simply amazing, but it's also completely understandable. I'm going to have my kids read this when they're old enough to understand the concepts. "
237,0735611319,http://goodreads.com/user/show/31061871-alex,4,"Good intro to Computer Science. Last few chapters didn't age too well since the book was written, but provide a valuable history lesson for younger programmers."
238,0735611319,http://goodreads.com/user/show/10165915-yogesh,5,"Perfect book if you want to understand how computer works at the very low level. This book answers most of the questions I had during my under graduation, didnt only help me bruis my engg concepts but also placed everything at one place. This is a must read for tech enthusiast. Reading this book does not require expertise abt computers, a person having bare minimum knowledge will also be able to understand though in between sometimes it becomes a little confusing but if you are attentive and is putting all of it in one place you will be able to understand it comfortably."
239,0735611319,http://goodreads.com/user/show/32751589-amy,4,"This book does a good job explaining a lot of the basics of computing along with explaining some of the history of computers. It goes from explaining the basics of circuits, relays, and binary to assembly languages and the breakdown of basic parts of computers. Even though technology today has come so far since this book was written, I still recommend this book if you want to understand many of the basic concepts of computing and where a lot of it started."
240,0735611319,http://goodreads.com/user/show/35899146-jakub,5,"I wish I discovered this book earlier. It is a great introduction to computation and I do recommend it to everybody that would like to understand what is going on inside their laptops or phones.I found the ""big picture"" to ""the actual details"" ratio very suitable for a casual read. There are some parts that require more attention though - especially when the author explains the details of a data flow in the computer or the way Intel 8080 chip works. Nevertheless that's an inherent characteristics of any subject related to computation - some thinking is required to grasp the concept.All in all great introduction for non-programmers and even better refresher for people that are into programming but treat all the layers below the OS as a 'back box'. This is highly recommended."
241,0735611319,http://goodreads.com/user/show/11199869-prashant-ghabak,5,The author starts from electrons then electricity and builds right up to a high level programming language to explain fundamentals of a computer science while walking us through the history of the field. Very well written. 
242,0735611319,http://goodreads.com/user/show/37846711-andrew-alkema,5,"This was an excellent book. I love Charles' approach to explaining how a computer works. All of a sudden this collection of electrical circuits was a microprocessor. I really enjoyed the history of computing mixed in as well, that's something we did not learn enough of in CS at university, so I really enjoyed getting more of that here. Even though this book was written in 2000, the information is still just as relevant. To make the concepts simpler, most of the hardware he looks at in detail is from the 1970's anyway, so that part hasn't changed. The rest of the concept haven't gone anywhere. Don't let the age turn you off."
243,0735611319,http://goodreads.com/user/show/74276701-chad-lavimoniere,4,"A good overview of the basics of CS and early history, but at this point hilariously outdated. Could do with a major second edition. "
244,0735611319,http://goodreads.com/user/show/31121633-leonid,5,"Absolutely great book.It starts from the general idea of codes, like Morse or Braille, and then shows how computers designed and built, starting from relays, and finishing with asm (and then moves on to memory, storage, cpu design, text and graphics i/o)."
245,0735611319,http://goodreads.com/user/show/21307031,5,"A fascinating journey into the history and ""nature"" of computers written with an effort to make things easy to understand. A very interesting and important read for anyone related to IT"
246,0735611319,http://goodreads.com/user/show/4130219-moayyed-almizyen,5,Thank you Charles Pertzold ..
247,0735611319,http://goodreads.com/user/show/45773827-rick-sam,5,"An excellent book to learn about bottom-up details of Computers. It starts with two friends trying to communicate with each other, Morse Code, Braille, Flashlights... Gates, ... Assembly Language, Operating System and finally Graphical Revolution. Overall, this would help you to understand from First Principles, you can build from there. What an impressive accomplishment and progress! I would recommend this to anyone interested in Software, Hardware, Computer Science. Deus Vult, Gottfriend"
248,0735611319,http://goodreads.com/user/show/84459920-nick,4,"When I started reading this book I had an inkling that this book is suitable for nerds/technical people but after reading the first few chapters I thought that probably I was wrong. As it turned out in the end, it's at best a mixture of two with a heavy hilt towards first.The book explains in detail the working of a computer system along with the history of its progress over the year. It starts with a very innocuous example of communication between two close by living friend using just a torch. After that, chapter after chapter, it builds upon that by explaining in subsequent chapters the shortcomings and the solutions developed by industry, until it reaches the working of high-level languages and how a computer does calculations!!While the language of the book is lucid and definitely an improvement over dry engineering books that a lot of people here would be so used to, it couldn't help falling in that trap as it progresses. If you don't follow the writing closely and sequentially the preceding chapters may just be undecipherable to you. The book goes deep into circuits and their working and although it starts with a very good example and maintains continuity, it becomes very tech oriented and complex. Even after having studied a lot of the topics from the book during graduation, I myself couldn't help but skim over some parts as the writing became too technical and fully resembles an engineering book. Guess you can't escape that however much you try!!This book is more suitable for nerds/technical-oriented readers. If you want to know how and what of your system, this book can give you a very good idea. I may even say that if that's how the engineering books were written a lot more people would benefit. If someone here, during their course of engineering, is starting with the subject of digital systems; I strongly suggest reading this book cover to cover over (say over a period of two weeks or a month) before they start with their assigned books. You would be thanking me and the author later!! For others, if you pay close attention from the very start, this is the book to gain some understanding system since a very complex topic is explained in a very easy language starting with a very basic and relatable example and improving upon it. If this book doesn't do it for you, nothing would."
249,0735611319,http://goodreads.com/user/show/5268938-lauren,4,Fascinating look at code and computers and the building blocks that make them. Not my usual cup of tea but fascinating nonetheless. From flashlight communication to 2000-era processors the author builds a computer.
250,0735611319,http://goodreads.com/user/show/15643804-gustav-ton-r,5,"Basically everything you need to build a modern computer, both hardware and software, from scratch. Perfect read after the apocalypse. Too few people grasp these relatively easy concepts that power modern society through the internet and the cell phones in our pockets. This book summarizes computer technology in a clear and concise way."
251,0735611319,http://goodreads.com/user/show/8017610-james-millikan-sj,5,"A fascinating account of how electronics work, using stories, clever examples, and clear illustrations. Starting with elementary logic, examples from Morse code, and basic circuits, Petzold explains how to build a computer.This sounds like a rather dry read, but in reality it is an engaging and well written journey that weaves electrical engineering and history together into a compelling synthesis.If you‚Äôve ever wondered how your digital watch works, or the structure of binary, or how erasable digital memory is possible, look no further than Petzold‚Äôs classic. Highly recommended."
252,0735611319,http://goodreads.com/user/show/4606766-scott-johnson,5,"This, finally, was what I was looking for in my quest to understand computers.We start with some really boring, basic things about the most elementary circuits. It gets a pass because it's necessary if you're coming in blind, but I've taken courses in things like the physics of transistors, so I wasn't a fan personally. The same goes for assorted later sections on number systems (already overly familiar with Binary and Hex, thanks).Then we move into how you take very simple components and construct logic gates. I was vaguely familiar with their use, but had not actually seen one built from relays.The theme of building further complexity just by stacking blocks is introduced, and is the key to everything. Once I understand how a NOR gate works, I don't need to constantly trace the paths, we can shortcut now to using the correct symbol in the circuit with two (or more) inputs and an output. Similarly, 3+ input gates that are really cascades of 2+ gates being abbreviated as what without context looked like a new component makes complete sense.Using that, we construct a simple circuit to add binary bits. My only big criticism is that this is where we're building up to how a microprocessor works, but we suddenly veer left to look at memory.This was the most enlightening and involved portion. We gradually build up from a single bit being stored as a consequence of a quirky configuration of logic gates that ""trap"" a bit to adding things like control mechanisms (only write to the bit if this input is on), refreshes, and, most importantly, arrays of these storage bits.This was where I finally started to settle into this paradigm. Trying to think about the big picture is impossible. We're talking about quantities of components that the human mind cannot track, time spans we can't conceive, and frequencies that are impossible to imagine.Faith in the process is the key. I understood the science behind relays and transistors. I understood how these allow us to introduce logic. I understood how combining these gates leads to operations like addition or controlling the routing of a circuit. I understand how a moderate array of these things leads to a useful component like a byte of memory. I understand how each bit can be addressed to be written, read, and cleared.So I just needed to learn to relax and remember that I know these things, and that all we're doing now is that same thing....a few million times. Yes, that seems impossible, but you don't need to deal with all of that. You just have to do what we did with cascading logic gates being combined into 3+ input gates: Simplify it into a known component. Who cares what's happening in each of the 500 black boxes that all do the same thing. We just know we need this input and this is how output behaves.Once you understand how to chain them together into large arrays, why not simplify this into just ONE big black box, the DIMM. I now know how a signal from the processor (or, later, storage device) goes through an unimaginably long branching addressing system.I could not easily trace exactly which circuit a signal will follow to access a certain memory address, but I have faith that, given what I understand, I COULD do it if I really had to. I know what I understand really well on a small scale still works perfectly fine even if there are a few tens of thousands of branches now. It's not even complicated, every single node in the branching decision tree is identical, it's just big.This was a little more fuzzy when we returned to the microprocessor and the idea of instructions and opcodes, but I think I get it. This was unfortunately not covered in as much detail, and it's the one section I do want to do more reading about specifically.My understanding is that instruction sets are hard-wired into the processor. Similar to memory addressing, there seems to be some kind of input gate where the opcode instruction is interpreted to route the data it's carrying to the right part of the chip? Or there could be dedicated inputs for the specific operations, and the routing is done on the motherboard or by a secondary controller chip? Or both?It's a little unclear to me exactly how that control process happens, but I DO understand the important part: That processors have a finite instruction set, and perform very simple operations......but really, really fast. Another revelation was the idea of a clock signal as an input that ratchets the data forward one step through the circuits per iteration. I guess a lemma to my ""it's not complicated, just big"" theorem would be, ""don't think of circuits as continuously evolving, they are discretely iterating"".I was struggling at first to fathom how just adding these two bits together in any way led to these characters being displayed on my screen right now. I kept thinking, ""But you still have to do this, and this, and....."" Ok.....so then we do those things. In 2018 we're doing BILLIONS of things EACH SECOND. Response times are in nanoseconds. Sure, it's a lot....but that's fine. It's not complicated, it's just a tediously long chain of really simple events.Storage was easy. There was a bit of hand waving about how the addressing translates to moving the read head to this part of the disk or whatever, but I know enough about RAM addressing now to assume how it works. I can assume that logic is built into a controller on that disk that's hard-wired for that particular hardware configuration (hence why you don't really need drivers for components like these.....and also makes me realize why drivers are necessary to essentially tell the rest of the computer what a new component's inputs, outputs, and instruction sets are).Displays were REALLY simple thanks to one phrase: ""visual memory"". This wasn't from this book, it was from my last read, Turing's Cathedral, in discussing the use of CRTs as memory devices briefly, so you could see the memory live. They said something offhanded about, ""That's all the display on your monitor is anyway, showing an array of memory as pixel values, "" or something like that. The video card just constantly accesses that visual memory, turns it into pixel instructions, and sends it out the adapter for the monitor (with its own hard-wired controller) to translate to the actual pixels.The last piece was....how does everything know where to send the data? Answer: BUS! I now get how you can have so many things on so few buses and not have every component getting useless data from the rest. I had never heard of three-state logic, but it makes total sense (especially after an explanation that ""0 and 1"" are really more like ""high and low"") that there could be a state that would essentially block a component from the circuit so it has the ""state"" that it's neither high nor low. It then follows quite simply that the address of that component on the bus would trigger that control circuit to leave that non-state and connect to the bus again.Put all of this together, and I get it now. It's kind of like one of those Magic Eye pictures: If I relax and let my eyes go fuzzy, I can see the kitty, but as soon as I try to focus on anything it dissolves into chaos, and at best I can resolve a few of the dots. I just need to stop worrying about the 4 billion dots on the paper, just relax my eyes and have confidence that they're all still there.Aside from the sequencing of memory and processors, I think this was really well structured to flow logically and build up these complicated ideas from just a few simpler ones. A big point in its favor, also, was introducing that idea itself at the very beginning so it doesn't blindside the reader. It lets you read seemingly scary ideas with the context that once you get it you can just file it away as another building block for later use.10/10 would recommend to anyone who, like me, really wants to understand how you get from simple circuits to complex operating systems without resorting to magic."
253,0735611319,http://goodreads.com/user/show/117278-simmoril,4,"One of the biggest difficulties that is unique to Computer Science is this idea of 'layers of abstraction' - interfaces created to help hide the complexity of the underlying layer. While this can be a boon when developing, it becomes a problem when those lower layers start misbehaving, and you don't know why. Or, at a more basic level, these layers of abstraction can make it hard to understand why things are the way that they are (like why computers don't count in base 10, or why I can't run Unix programs in Windows). Petzold's book attempts to resolve this issue by starting out at the absolute most basic, physical level of a computer and working up through the different layers until he gets to what is pretty much a modern day computer. Through the use of good examples and an informal style of writing, Petzold introduces the reader to a large number of fundamental topics in Computer Science/Engineering, such as binary, logic gates, processor design, assembly, operating systems and programming languages. Petzold does a great job of not getting too bogged down in the details, and only giving the reader enough information to move up to the next layer.I enjoyed Code a great deal. Although much of this wasn't new to me, Petzold still surprised me with some historical facts I was not aware of, and the book itself served as a nice refresher course for some of the things that I studied and promptly forgotten in college. Although this book won't turn you into an expert by any means, it is a great starting point for understanding just how computers actually work.The only small issue I had with Code were that some areas were a little tough to get through. Towards the middle of the book, where Petzold starts to assemble memory circuits and building a rudimentary CPU, I found myself glossing over a lot of the details. For a complete understanding, these chapters would probably require a couple of passes. Also, the last couple of chapters seemed a little more hodge-podge than the rest. While they are good tidbits of information to know, it just seemed a little less organized than the rest of the book.Overall, Code is a fantastic book, and I highly recommend it as a starting point for anyone who wants to learn more about computers. In fact, I'd nearly go so far as to say it should be required reading!"
254,0735611319,http://goodreads.com/user/show/53697998-dave-voyles,5,"While looking for an answer on Stack Overflow one day, I saw several people recommend this book, to get a grasp on what was happening under the hood of my computer -- specifically, from beginning to end, how is a computer made?This book broke it down in such a way that I now understand it completely. The author starts of small and slow, and gradually builds up, from explaining how different number systems work as well as why we have so many, and the shortcomings of each (hex, binary, decimal, etc). From there, he goes into the use of relays, a bit about electrical engineering, and next thing you know, you're looking at Assembly code. It all starts with communication, languages (I‚Äôm talking Morse code and brail), and goes from there. Incredibly useful.code-bookI now have a far better understanding of the things my friends were speaking of on technical threads before, largely from the terms I learned in this book:carry bit, high-order bit, lower-order bit, etc,.I wish I had read something like this when I first started programming. If you know anyone who wants to get started in coding, then I'd start here, not because of the code it will teach you (you won't learn much on that here), but understanding what your code is doing to the actual hardware. The computer no longer appears like a black box.It was published by Microsoft Press in 2000, and the author, Charles Petzold, actually works for Microsoft (now Xamarin).Read this if you want to understand how your computer really works."
255,0735611319,http://goodreads.com/user/show/25094561-elijah-oyekunle,5,"Really good read for newbies looking to understand computers, behind the scenes."
256,0735611319,http://goodreads.com/user/show/981470-clarence,4,"Most people nowadays, if they wanted to explain how computers work, would probably ensure that the reader knew binary arithmetic, then talk about processor instructions, and from there work up through the higher levels of programming.Petzold takes an entirely different tack, which is completely centered around hardware. In fact, he starts with electric circuits, describing how a boy might build a circuit to light a lamp in his friend's house. He builds on that, getting into circuits that with multiple lamps, talks about telegraphs, and on and on. He does eventually take a detour in one chapter to talk about number bases, but even goes to the trouble of using not just base 10 or base 2 but unusual others to ensure that the reader understands both that the concepts are independent of the base, and yet also why base 2 is particularly useful in building digital computers.In addition, whether you know the subject matter or not, this books is an enjoyable read. People who recognize the name Petzold will most likely think of his old ""Programming Windows"" and similar books, which were dry textbooks. Not so this volume. I was very pleasantly surprised at what a fun read this is.Highly recommended to anyone, whether they have an interest in computers or not."
257,0735611319,http://goodreads.com/user/show/28438106-michael,5,"This book covers a variety of topics about what is going on under the hood of a computer, without muddling up the explanations with too many technical details. The author favors explaining the big picture and the components that make up that big picture, rather than staying too focused on one topic for too long and providing too many technical and insignificant details.For example, the concepts of logic gates and boolean functions are worthy of having entire books dedicated to them, but this book instead spends a concise two or three chapters on them, explaining the essentials to understand the big picture of what they are, and how they relate to the inside of a CPU. These concepts are then utilized later in the book to help explain other topics such as how a computer's memory works by utilizing those logic gates.Perhaps this books best feature is that, as I was reading and questions popped into my head, the very next paragraph would often open with something like, ""You might be wondering..."", and time and time again, I *was* wondering exactly what the author thought I might be wondering.It's all about the big picture, and this book absolutely nails it."
258,0735611319,http://goodreads.com/user/show/44327766,5,"This is the first book I would recommend to anyone wanting to learn about how computers work. It was written in 1999 and shows its age in some respects, but overall I would consider it a timeless classic.The one thing I was a bit sad to see was the incorrect use of the metric unit prefixes when refering to binary quantities. In the context of the time this book was written, the authors usage of metric units was common, and even today there is much confusion about it. A year before this book was published, the IEC and others published standards recommending binary prefixes. https://en.wikipedia.org/wiki/Binary_...Unicode was also only briefly mentioned. I would LOVE to see a revision of this book to update dated references, go over (and properly use) the differences between metric and binary prefixes, and to expand the content with details about advances that have happened since 1999.I was made aware of this book by Fabian Sanglard's blog: http://fabiensanglard.net/books_recom...He recommends ""Computer Organization and Design"" as a more in-depth follow-up. Expect a review of the 5th edition from me in the future!"
259,0735611319,http://goodreads.com/user/show/4090833-lingliang,5,"Excellent lucid explanation of the legacy of genius that has left us with the incredible abstracted world of computers. The abstraction allows us to accomplish creations of unimaginable complexity. This is a delight to read, as it clearly goes through layers and layers of genius, great minds building upon the remarkable history of computing, leaving us with a much more worthy appreciation of the beautiful creation that is the modern computer. It goes through each step of abstraction, starting with a compute relay and how it works (in terms of electromagnetics), and then goes step by step, all the way to the implementation of modern operating systems and programming languages. It's complexity is sufficient to leave little lingering questions but not so complex to be burdened by unnecessary rigor, leaving this boom a delight to read for both experts and laymen alike. This is an essential read for any computer scientist or even anyone who wants an appreciation of the creation that is a cornerstone of the modern human experience."
260,0735611319,http://goodreads.com/user/show/2478866-sonofabit,5,"Absolutely phenomenal book that's not so much about code but rather about the deep underlying concepts behind how a computer works, how it ""thinks"". If you've ever wanted to know more about bits and bytes and the mechanics behind the ones and zeros that everyone takes for granted as they browse facebook or listen to mp3s, this is the book for you!There were several ""AHA!"" moments that FINALLY cleared up unresolved questions from my Digital Circuits class back in college; I don't know why this wasn't the textbook they used :) However it's more than just a book about logic and circuits; it takes you so deep into the lowest levels of code and logic that it's almost an Anatomy course for computers. You literally get to see what happens at the smallest level, such as a circuit diagram for storing a byte!A truly fascinating read for anyone interested in programming, digital circuitry, or electrical engineering. Pick this one up today!"
261,0735611319,http://goodreads.com/user/show/676492-jean-luc,5,"There's a long, long list of books where my common reaction to them is ""I wish I'd read this in high school, it could've set me straight much earlier!"" Unfortunately, this isn't one of them... because I graduated in 1998 and this was published in 1999.At some point in your computer science career, you will take a courses and labs in digital systems. At Stevens, when I was your age, this was 381 (Switching Theory and Logical Design) and 383 (Computer Organization). This book combines both of those courses, starting from counting numbers all the way to building a programmable computer. The main difference is that the author, Charles Petzold, actually cares about the subject and makes it interesting. I would gladly have payed $4800 to read this book instead of paying that amount to take those 2 courses because I actually learned something from this book.Would make a great, great, great gift for any high school student considering a career in programming."
262,0735611319,http://goodreads.com/user/show/121883-mike,4,"I'd definitely recommend this to anyone who is even remotely curious how computers work, and doesn't mind going all the way back to basic physics to find out. The author builds a very basic but working computer all the way from the ground up, and giving you a unique view of the concepts involved that certainly wasn't taught in any engineering or computer science class I've ever taken (B.S.E.E./Rutgers '01). Although he gets a little lost near the end and doesn't adequately show how everything since the basics came out has just been ""making it better"", showing you the basics is the hard part, and combined with the complexity that comes later its all you really need to understand the emergent behavior of computing. "
263,0735611319,http://goodreads.com/user/show/11831233-jon,5,"Intimidated by digital technology? Think your computer secretly hates you? Can't understand why your device won't do what you tell it to? (Even though it is doing exactly what you told it to do...)Read this book. All complicated technology is made up of layer-upon-layer of less complicated pieces down to some very simple straight forward parts that do only one thing in response to something else that only does one other thing. Learn from the bottom up how digital, and in some cases analog, machines that we have allowed to control our live exist and do more in a second than you would want to do in your lifetime.And... No I will not fix your computer, that's what children are for now-a-days."
264,0735611319,http://goodreads.com/user/show/50247467-matthew-porche,3,"I will not lie, this book was tough to read. It flowed in some chapters, but not others. Petzold likes to talk about how much he hates analogies, but I seem to require them to understand a lot of things. I love computers, and this book helped me understand how they work, but it took forever before he started talking about them. I learned a lot on the small end of the scale, but he didn't discuss a whole lot about computers that I didn't already know. I was planning on citing this book for a research paper on cyber security, but this will not make it in the paper. There were a lot of dry spots in the book where I recall having to force myself to read the book. I cannot say I'd recommend this to anyone else, but I would not deter anyone from reading this book either. "
265,0735611319,http://goodreads.com/user/show/54838935-aaron-manley,5,"This is the book to read if you want to ""get"" computers. Not just how code executes programs, or how your operating system is what you use to run everything else - this goes all the way down to the computer architecture at the microcode level (relatively speaking). The book looks at basic human problems, discovers some ingenious quirks of nature involving switches and eventually electricity that allow us to use code to represent information to help us solve problems. Starting at a on/off relay, and ending in a full-fledged computer processor with memory, this is the book to read to understand how computer hardware and computer software are truly connected. It's also a good primer if you want to give nand2tetris a try and build your own virtual computer online."
266,0735611319,http://goodreads.com/user/show/7962528-michael,5,"What a great book! I asked myself why I didn't read this as a freshman in college, and then realized that it was published in 1999 when I was a sophomore.This book distills the workings of computers down to their most basic elements - voltages moving across circuits in a controlled way - and builds on itself until you get to a 1970s era microprocessor. From there, everything modern computers is simply layered on top one step at a time.What's going on inside your computer (or phone!) is simply amazing, but it's also completely understandable. I'm going to have my kids read this when they're old enough to understand the concepts. "
267,0735611319,http://goodreads.com/user/show/31061871-alex,4,"Good intro to Computer Science. Last few chapters didn't age too well since the book was written, but provide a valuable history lesson for younger programmers."
268,0735611319,http://goodreads.com/user/show/10165915-yogesh,5,"Perfect book if you want to understand how computer works at the very low level. This book answers most of the questions I had during my under graduation, didnt only help me bruis my engg concepts but also placed everything at one place. This is a must read for tech enthusiast. Reading this book does not require expertise abt computers, a person having bare minimum knowledge will also be able to understand though in between sometimes it becomes a little confusing but if you are attentive and is putting all of it in one place you will be able to understand it comfortably."
269,0735611319,http://goodreads.com/user/show/32751589-amy,4,"This book does a good job explaining a lot of the basics of computing along with explaining some of the history of computers. It goes from explaining the basics of circuits, relays, and binary to assembly languages and the breakdown of basic parts of computers. Even though technology today has come so far since this book was written, I still recommend this book if you want to understand many of the basic concepts of computing and where a lot of it started."
270,0735611319,http://goodreads.com/user/show/35899146-jakub,5,"I wish I discovered this book earlier. It is a great introduction to computation and I do recommend it to everybody that would like to understand what is going on inside their laptops or phones.I found the ""big picture"" to ""the actual details"" ratio very suitable for a casual read. There are some parts that require more attention though - especially when the author explains the details of a data flow in the computer or the way Intel 8080 chip works. Nevertheless that's an inherent characteristics of any subject related to computation - some thinking is required to grasp the concept.All in all great introduction for non-programmers and even better refresher for people that are into programming but treat all the layers below the OS as a 'back box'. This is highly recommended."
271,0735611319,http://goodreads.com/user/show/11199869-prashant-ghabak,5,The author starts from electrons then electricity and builds right up to a high level programming language to explain fundamentals of a computer science while walking us through the history of the field. Very well written. 
272,0735611319,http://goodreads.com/user/show/37846711-andrew-alkema,5,"This was an excellent book. I love Charles' approach to explaining how a computer works. All of a sudden this collection of electrical circuits was a microprocessor. I really enjoyed the history of computing mixed in as well, that's something we did not learn enough of in CS at university, so I really enjoyed getting more of that here. Even though this book was written in 2000, the information is still just as relevant. To make the concepts simpler, most of the hardware he looks at in detail is from the 1970's anyway, so that part hasn't changed. The rest of the concept haven't gone anywhere. Don't let the age turn you off."
273,0735611319,http://goodreads.com/user/show/74276701-chad-lavimoniere,4,"A good overview of the basics of CS and early history, but at this point hilariously outdated. Could do with a major second edition. "
274,0735611319,http://goodreads.com/user/show/31121633-leonid,5,"Absolutely great book.It starts from the general idea of codes, like Morse or Braille, and then shows how computers designed and built, starting from relays, and finishing with asm (and then moves on to memory, storage, cpu design, text and graphics i/o)."
275,0735611319,http://goodreads.com/user/show/21307031,5,"A fascinating journey into the history and ""nature"" of computers written with an effort to make things easy to understand. A very interesting and important read for anyone related to IT"
276,0735611319,http://goodreads.com/user/show/4130219-moayyed-almizyen,5,Thank you Charles Pertzold ..
277,0735611319,http://goodreads.com/user/show/45773827-rick-sam,5,"An excellent book to learn about bottom-up details of Computers. It starts with two friends trying to communicate with each other, Morse Code, Braille, Flashlights... Gates, ... Assembly Language, Operating System and finally Graphical Revolution. Overall, this would help you to understand from First Principles, you can build from there. What an impressive accomplishment and progress! I would recommend this to anyone interested in Software, Hardware, Computer Science. Deus Vult, Gottfriend"
278,0735611319,http://goodreads.com/user/show/84459920-nick,4,"When I started reading this book I had an inkling that this book is suitable for nerds/technical people but after reading the first few chapters I thought that probably I was wrong. As it turned out in the end, it's at best a mixture of two with a heavy hilt towards first.The book explains in detail the working of a computer system along with the history of its progress over the year. It starts with a very innocuous example of communication between two close by living friend using just a torch. After that, chapter after chapter, it builds upon that by explaining in subsequent chapters the shortcomings and the solutions developed by industry, until it reaches the working of high-level languages and how a computer does calculations!!While the language of the book is lucid and definitely an improvement over dry engineering books that a lot of people here would be so used to, it couldn't help falling in that trap as it progresses. If you don't follow the writing closely and sequentially the preceding chapters may just be undecipherable to you. The book goes deep into circuits and their working and although it starts with a very good example and maintains continuity, it becomes very tech oriented and complex. Even after having studied a lot of the topics from the book during graduation, I myself couldn't help but skim over some parts as the writing became too technical and fully resembles an engineering book. Guess you can't escape that however much you try!!This book is more suitable for nerds/technical-oriented readers. If you want to know how and what of your system, this book can give you a very good idea. I may even say that if that's how the engineering books were written a lot more people would benefit. If someone here, during their course of engineering, is starting with the subject of digital systems; I strongly suggest reading this book cover to cover over (say over a period of two weeks or a month) before they start with their assigned books. You would be thanking me and the author later!! For others, if you pay close attention from the very start, this is the book to gain some understanding system since a very complex topic is explained in a very easy language starting with a very basic and relatable example and improving upon it. If this book doesn't do it for you, nothing would."
279,0735611319,http://goodreads.com/user/show/5268938-lauren,4,Fascinating look at code and computers and the building blocks that make them. Not my usual cup of tea but fascinating nonetheless. From flashlight communication to 2000-era processors the author builds a computer.
280,0735611319,http://goodreads.com/user/show/15643804-gustav-ton-r,5,"Basically everything you need to build a modern computer, both hardware and software, from scratch. Perfect read after the apocalypse. Too few people grasp these relatively easy concepts that power modern society through the internet and the cell phones in our pockets. This book summarizes computer technology in a clear and concise way."
281,0735611319,http://goodreads.com/user/show/8017610-james-millikan-sj,5,"A fascinating account of how electronics work, using stories, clever examples, and clear illustrations. Starting with elementary logic, examples from Morse code, and basic circuits, Petzold explains how to build a computer.This sounds like a rather dry read, but in reality it is an engaging and well written journey that weaves electrical engineering and history together into a compelling synthesis.If you‚Äôve ever wondered how your digital watch works, or the structure of binary, or how erasable digital memory is possible, look no further than Petzold‚Äôs classic. Highly recommended."
282,0735611319,http://goodreads.com/user/show/4606766-scott-johnson,5,"This, finally, was what I was looking for in my quest to understand computers.We start with some really boring, basic things about the most elementary circuits. It gets a pass because it's necessary if you're coming in blind, but I've taken courses in things like the physics of transistors, so I wasn't a fan personally. The same goes for assorted later sections on number systems (already overly familiar with Binary and Hex, thanks).Then we move into how you take very simple components and construct logic gates. I was vaguely familiar with their use, but had not actually seen one built from relays.The theme of building further complexity just by stacking blocks is introduced, and is the key to everything. Once I understand how a NOR gate works, I don't need to constantly trace the paths, we can shortcut now to using the correct symbol in the circuit with two (or more) inputs and an output. Similarly, 3+ input gates that are really cascades of 2+ gates being abbreviated as what without context looked like a new component makes complete sense.Using that, we construct a simple circuit to add binary bits. My only big criticism is that this is where we're building up to how a microprocessor works, but we suddenly veer left to look at memory.This was the most enlightening and involved portion. We gradually build up from a single bit being stored as a consequence of a quirky configuration of logic gates that ""trap"" a bit to adding things like control mechanisms (only write to the bit if this input is on), refreshes, and, most importantly, arrays of these storage bits.This was where I finally started to settle into this paradigm. Trying to think about the big picture is impossible. We're talking about quantities of components that the human mind cannot track, time spans we can't conceive, and frequencies that are impossible to imagine.Faith in the process is the key. I understood the science behind relays and transistors. I understood how these allow us to introduce logic. I understood how combining these gates leads to operations like addition or controlling the routing of a circuit. I understand how a moderate array of these things leads to a useful component like a byte of memory. I understand how each bit can be addressed to be written, read, and cleared.So I just needed to learn to relax and remember that I know these things, and that all we're doing now is that same thing....a few million times. Yes, that seems impossible, but you don't need to deal with all of that. You just have to do what we did with cascading logic gates being combined into 3+ input gates: Simplify it into a known component. Who cares what's happening in each of the 500 black boxes that all do the same thing. We just know we need this input and this is how output behaves.Once you understand how to chain them together into large arrays, why not simplify this into just ONE big black box, the DIMM. I now know how a signal from the processor (or, later, storage device) goes through an unimaginably long branching addressing system.I could not easily trace exactly which circuit a signal will follow to access a certain memory address, but I have faith that, given what I understand, I COULD do it if I really had to. I know what I understand really well on a small scale still works perfectly fine even if there are a few tens of thousands of branches now. It's not even complicated, every single node in the branching decision tree is identical, it's just big.This was a little more fuzzy when we returned to the microprocessor and the idea of instructions and opcodes, but I think I get it. This was unfortunately not covered in as much detail, and it's the one section I do want to do more reading about specifically.My understanding is that instruction sets are hard-wired into the processor. Similar to memory addressing, there seems to be some kind of input gate where the opcode instruction is interpreted to route the data it's carrying to the right part of the chip? Or there could be dedicated inputs for the specific operations, and the routing is done on the motherboard or by a secondary controller chip? Or both?It's a little unclear to me exactly how that control process happens, but I DO understand the important part: That processors have a finite instruction set, and perform very simple operations......but really, really fast. Another revelation was the idea of a clock signal as an input that ratchets the data forward one step through the circuits per iteration. I guess a lemma to my ""it's not complicated, just big"" theorem would be, ""don't think of circuits as continuously evolving, they are discretely iterating"".I was struggling at first to fathom how just adding these two bits together in any way led to these characters being displayed on my screen right now. I kept thinking, ""But you still have to do this, and this, and....."" Ok.....so then we do those things. In 2018 we're doing BILLIONS of things EACH SECOND. Response times are in nanoseconds. Sure, it's a lot....but that's fine. It's not complicated, it's just a tediously long chain of really simple events.Storage was easy. There was a bit of hand waving about how the addressing translates to moving the read head to this part of the disk or whatever, but I know enough about RAM addressing now to assume how it works. I can assume that logic is built into a controller on that disk that's hard-wired for that particular hardware configuration (hence why you don't really need drivers for components like these.....and also makes me realize why drivers are necessary to essentially tell the rest of the computer what a new component's inputs, outputs, and instruction sets are).Displays were REALLY simple thanks to one phrase: ""visual memory"". This wasn't from this book, it was from my last read, Turing's Cathedral, in discussing the use of CRTs as memory devices briefly, so you could see the memory live. They said something offhanded about, ""That's all the display on your monitor is anyway, showing an array of memory as pixel values, "" or something like that. The video card just constantly accesses that visual memory, turns it into pixel instructions, and sends it out the adapter for the monitor (with its own hard-wired controller) to translate to the actual pixels.The last piece was....how does everything know where to send the data? Answer: BUS! I now get how you can have so many things on so few buses and not have every component getting useless data from the rest. I had never heard of three-state logic, but it makes total sense (especially after an explanation that ""0 and 1"" are really more like ""high and low"") that there could be a state that would essentially block a component from the circuit so it has the ""state"" that it's neither high nor low. It then follows quite simply that the address of that component on the bus would trigger that control circuit to leave that non-state and connect to the bus again.Put all of this together, and I get it now. It's kind of like one of those Magic Eye pictures: If I relax and let my eyes go fuzzy, I can see the kitty, but as soon as I try to focus on anything it dissolves into chaos, and at best I can resolve a few of the dots. I just need to stop worrying about the 4 billion dots on the paper, just relax my eyes and have confidence that they're all still there.Aside from the sequencing of memory and processors, I think this was really well structured to flow logically and build up these complicated ideas from just a few simpler ones. A big point in its favor, also, was introducing that idea itself at the very beginning so it doesn't blindside the reader. It lets you read seemingly scary ideas with the context that once you get it you can just file it away as another building block for later use.10/10 would recommend to anyone who, like me, really wants to understand how you get from simple circuits to complex operating systems without resorting to magic."
283,0735611319,http://goodreads.com/user/show/117278-simmoril,4,"One of the biggest difficulties that is unique to Computer Science is this idea of 'layers of abstraction' - interfaces created to help hide the complexity of the underlying layer. While this can be a boon when developing, it becomes a problem when those lower layers start misbehaving, and you don't know why. Or, at a more basic level, these layers of abstraction can make it hard to understand why things are the way that they are (like why computers don't count in base 10, or why I can't run Unix programs in Windows). Petzold's book attempts to resolve this issue by starting out at the absolute most basic, physical level of a computer and working up through the different layers until he gets to what is pretty much a modern day computer. Through the use of good examples and an informal style of writing, Petzold introduces the reader to a large number of fundamental topics in Computer Science/Engineering, such as binary, logic gates, processor design, assembly, operating systems and programming languages. Petzold does a great job of not getting too bogged down in the details, and only giving the reader enough information to move up to the next layer.I enjoyed Code a great deal. Although much of this wasn't new to me, Petzold still surprised me with some historical facts I was not aware of, and the book itself served as a nice refresher course for some of the things that I studied and promptly forgotten in college. Although this book won't turn you into an expert by any means, it is a great starting point for understanding just how computers actually work.The only small issue I had with Code were that some areas were a little tough to get through. Towards the middle of the book, where Petzold starts to assemble memory circuits and building a rudimentary CPU, I found myself glossing over a lot of the details. For a complete understanding, these chapters would probably require a couple of passes. Also, the last couple of chapters seemed a little more hodge-podge than the rest. While they are good tidbits of information to know, it just seemed a little less organized than the rest of the book.Overall, Code is a fantastic book, and I highly recommend it as a starting point for anyone who wants to learn more about computers. In fact, I'd nearly go so far as to say it should be required reading!"
284,0735611319,http://goodreads.com/user/show/53697998-dave-voyles,5,"While looking for an answer on Stack Overflow one day, I saw several people recommend this book, to get a grasp on what was happening under the hood of my computer -- specifically, from beginning to end, how is a computer made?This book broke it down in such a way that I now understand it completely. The author starts of small and slow, and gradually builds up, from explaining how different number systems work as well as why we have so many, and the shortcomings of each (hex, binary, decimal, etc). From there, he goes into the use of relays, a bit about electrical engineering, and next thing you know, you're looking at Assembly code. It all starts with communication, languages (I‚Äôm talking Morse code and brail), and goes from there. Incredibly useful.code-bookI now have a far better understanding of the things my friends were speaking of on technical threads before, largely from the terms I learned in this book:carry bit, high-order bit, lower-order bit, etc,.I wish I had read something like this when I first started programming. If you know anyone who wants to get started in coding, then I'd start here, not because of the code it will teach you (you won't learn much on that here), but understanding what your code is doing to the actual hardware. The computer no longer appears like a black box.It was published by Microsoft Press in 2000, and the author, Charles Petzold, actually works for Microsoft (now Xamarin).Read this if you want to understand how your computer really works."
285,0735611319,http://goodreads.com/user/show/25094561-elijah-oyekunle,5,"Really good read for newbies looking to understand computers, behind the scenes."
286,0735611319,http://goodreads.com/user/show/981470-clarence,4,"Most people nowadays, if they wanted to explain how computers work, would probably ensure that the reader knew binary arithmetic, then talk about processor instructions, and from there work up through the higher levels of programming.Petzold takes an entirely different tack, which is completely centered around hardware. In fact, he starts with electric circuits, describing how a boy might build a circuit to light a lamp in his friend's house. He builds on that, getting into circuits that with multiple lamps, talks about telegraphs, and on and on. He does eventually take a detour in one chapter to talk about number bases, but even goes to the trouble of using not just base 10 or base 2 but unusual others to ensure that the reader understands both that the concepts are independent of the base, and yet also why base 2 is particularly useful in building digital computers.In addition, whether you know the subject matter or not, this books is an enjoyable read. People who recognize the name Petzold will most likely think of his old ""Programming Windows"" and similar books, which were dry textbooks. Not so this volume. I was very pleasantly surprised at what a fun read this is.Highly recommended to anyone, whether they have an interest in computers or not."
287,0735611319,http://goodreads.com/user/show/28438106-michael,5,"This book covers a variety of topics about what is going on under the hood of a computer, without muddling up the explanations with too many technical details. The author favors explaining the big picture and the components that make up that big picture, rather than staying too focused on one topic for too long and providing too many technical and insignificant details.For example, the concepts of logic gates and boolean functions are worthy of having entire books dedicated to them, but this book instead spends a concise two or three chapters on them, explaining the essentials to understand the big picture of what they are, and how they relate to the inside of a CPU. These concepts are then utilized later in the book to help explain other topics such as how a computer's memory works by utilizing those logic gates.Perhaps this books best feature is that, as I was reading and questions popped into my head, the very next paragraph would often open with something like, ""You might be wondering..."", and time and time again, I *was* wondering exactly what the author thought I might be wondering.It's all about the big picture, and this book absolutely nails it."
288,0735611319,http://goodreads.com/user/show/44327766,5,"This is the first book I would recommend to anyone wanting to learn about how computers work. It was written in 1999 and shows its age in some respects, but overall I would consider it a timeless classic.The one thing I was a bit sad to see was the incorrect use of the metric unit prefixes when refering to binary quantities. In the context of the time this book was written, the authors usage of metric units was common, and even today there is much confusion about it. A year before this book was published, the IEC and others published standards recommending binary prefixes. https://en.wikipedia.org/wiki/Binary_...Unicode was also only briefly mentioned. I would LOVE to see a revision of this book to update dated references, go over (and properly use) the differences between metric and binary prefixes, and to expand the content with details about advances that have happened since 1999.I was made aware of this book by Fabian Sanglard's blog: http://fabiensanglard.net/books_recom...He recommends ""Computer Organization and Design"" as a more in-depth follow-up. Expect a review of the 5th edition from me in the future!"
289,0735611319,http://goodreads.com/user/show/4090833-lingliang,5,"Excellent lucid explanation of the legacy of genius that has left us with the incredible abstracted world of computers. The abstraction allows us to accomplish creations of unimaginable complexity. This is a delight to read, as it clearly goes through layers and layers of genius, great minds building upon the remarkable history of computing, leaving us with a much more worthy appreciation of the beautiful creation that is the modern computer. It goes through each step of abstraction, starting with a compute relay and how it works (in terms of electromagnetics), and then goes step by step, all the way to the implementation of modern operating systems and programming languages. It's complexity is sufficient to leave little lingering questions but not so complex to be burdened by unnecessary rigor, leaving this boom a delight to read for both experts and laymen alike. This is an essential read for any computer scientist or even anyone who wants an appreciation of the creation that is a cornerstone of the modern human experience."
290,0735611319,http://goodreads.com/user/show/2478866-sonofabit,5,"Absolutely phenomenal book that's not so much about code but rather about the deep underlying concepts behind how a computer works, how it ""thinks"". If you've ever wanted to know more about bits and bytes and the mechanics behind the ones and zeros that everyone takes for granted as they browse facebook or listen to mp3s, this is the book for you!There were several ""AHA!"" moments that FINALLY cleared up unresolved questions from my Digital Circuits class back in college; I don't know why this wasn't the textbook they used :) However it's more than just a book about logic and circuits; it takes you so deep into the lowest levels of code and logic that it's almost an Anatomy course for computers. You literally get to see what happens at the smallest level, such as a circuit diagram for storing a byte!A truly fascinating read for anyone interested in programming, digital circuitry, or electrical engineering. Pick this one up today!"
291,0735611319,http://goodreads.com/user/show/676492-jean-luc,5,"There's a long, long list of books where my common reaction to them is ""I wish I'd read this in high school, it could've set me straight much earlier!"" Unfortunately, this isn't one of them... because I graduated in 1998 and this was published in 1999.At some point in your computer science career, you will take a courses and labs in digital systems. At Stevens, when I was your age, this was 381 (Switching Theory and Logical Design) and 383 (Computer Organization). This book combines both of those courses, starting from counting numbers all the way to building a programmable computer. The main difference is that the author, Charles Petzold, actually cares about the subject and makes it interesting. I would gladly have payed $4800 to read this book instead of paying that amount to take those 2 courses because I actually learned something from this book.Would make a great, great, great gift for any high school student considering a career in programming."
292,0735611319,http://goodreads.com/user/show/121883-mike,4,"I'd definitely recommend this to anyone who is even remotely curious how computers work, and doesn't mind going all the way back to basic physics to find out. The author builds a very basic but working computer all the way from the ground up, and giving you a unique view of the concepts involved that certainly wasn't taught in any engineering or computer science class I've ever taken (B.S.E.E./Rutgers '01). Although he gets a little lost near the end and doesn't adequately show how everything since the basics came out has just been ""making it better"", showing you the basics is the hard part, and combined with the complexity that comes later its all you really need to understand the emergent behavior of computing. "
293,0735611319,http://goodreads.com/user/show/11831233-jon,5,"Intimidated by digital technology? Think your computer secretly hates you? Can't understand why your device won't do what you tell it to? (Even though it is doing exactly what you told it to do...)Read this book. All complicated technology is made up of layer-upon-layer of less complicated pieces down to some very simple straight forward parts that do only one thing in response to something else that only does one other thing. Learn from the bottom up how digital, and in some cases analog, machines that we have allowed to control our live exist and do more in a second than you would want to do in your lifetime.And... No I will not fix your computer, that's what children are for now-a-days."
294,0735611319,http://goodreads.com/user/show/50247467-matthew-porche,3,"I will not lie, this book was tough to read. It flowed in some chapters, but not others. Petzold likes to talk about how much he hates analogies, but I seem to require them to understand a lot of things. I love computers, and this book helped me understand how they work, but it took forever before he started talking about them. I learned a lot on the small end of the scale, but he didn't discuss a whole lot about computers that I didn't already know. I was planning on citing this book for a research paper on cyber security, but this will not make it in the paper. There were a lot of dry spots in the book where I recall having to force myself to read the book. I cannot say I'd recommend this to anyone else, but I would not deter anyone from reading this book either. "
295,0735611319,http://goodreads.com/user/show/54838935-aaron-manley,5,"This is the book to read if you want to ""get"" computers. Not just how code executes programs, or how your operating system is what you use to run everything else - this goes all the way down to the computer architecture at the microcode level (relatively speaking). The book looks at basic human problems, discovers some ingenious quirks of nature involving switches and eventually electricity that allow us to use code to represent information to help us solve problems. Starting at a on/off relay, and ending in a full-fledged computer processor with memory, this is the book to read to understand how computer hardware and computer software are truly connected. It's also a good primer if you want to give nand2tetris a try and build your own virtual computer online."
296,0735611319,http://goodreads.com/user/show/7962528-michael,5,"What a great book! I asked myself why I didn't read this as a freshman in college, and then realized that it was published in 1999 when I was a sophomore.This book distills the workings of computers down to their most basic elements - voltages moving across circuits in a controlled way - and builds on itself until you get to a 1970s era microprocessor. From there, everything modern computers is simply layered on top one step at a time.What's going on inside your computer (or phone!) is simply amazing, but it's also completely understandable. I'm going to have my kids read this when they're old enough to understand the concepts. "
297,0735611319,http://goodreads.com/user/show/31061871-alex,4,"Good intro to Computer Science. Last few chapters didn't age too well since the book was written, but provide a valuable history lesson for younger programmers."
298,0735611319,http://goodreads.com/user/show/10165915-yogesh,5,"Perfect book if you want to understand how computer works at the very low level. This book answers most of the questions I had during my under graduation, didnt only help me bruis my engg concepts but also placed everything at one place. This is a must read for tech enthusiast. Reading this book does not require expertise abt computers, a person having bare minimum knowledge will also be able to understand though in between sometimes it becomes a little confusing but if you are attentive and is putting all of it in one place you will be able to understand it comfortably."
299,0735611319,http://goodreads.com/user/show/32751589-amy,4,"This book does a good job explaining a lot of the basics of computing along with explaining some of the history of computers. It goes from explaining the basics of circuits, relays, and binary to assembly languages and the breakdown of basic parts of computers. Even though technology today has come so far since this book was written, I still recommend this book if you want to understand many of the basic concepts of computing and where a lot of it started."
