,isbn,user_link,ranking,review
0,0199678111,http://goodreads.com/user/show/1713956-manny,5,"Superintelligence was published in 2014, and it's already had time to become a cult classic. So, with apologies for being late getting to the party, here's my two cents.For people who still haven't heard of it, the book is intended as a serious, hard-headed examination of the risks associated with the likely arrival, in the short- to medium-term future, of machines which are significantly smarter than we are. Bostrom is well qualified to do this. He runs the Future of Humanity Institute at Oxford, where he's also a professor at the philosophy department, he's read a great deal of relevant background, and he knows everyone. The cover quotes approving murmurs from the likes of Bill Gates, Elon Musk, Martin Rees and Stuart Russell, co-author of the world's leading AI textbook; people thanked in the acknowledgements include Demis Hassabis, the founder and CEO of Google's Deep Mind. So, why don't we assume for now that Bostrom passes the background check and deserves to be taken seriously. What's he saying?First of all, let's review the reasons why this is a big deal. If machines can get to the point where they're even a little bit smarter than we are, they'll soon be a whole lot smarter than we are. Machines can think much faster than humans (our brains are not well optimised for speed); the differential is at least in the thousands and more likely in the millions. So, having caught us up, they will rapidly overtake us, since they're living thousands or millions of their years for every one of ours. Of course, you can still, if you want, argue that it's a theoretical extrapolation, it won't happen any time soon, etc. But the evidence suggests the opposite. The list of things machines do roughly as well as humans is now very long, and there are quite a few things, things we humans once prided ourselves on being good at, that they do much better. More about that shortly.So if we can produce an artificial human-level intelligence, we'll shortly after have an artificial superintelligence. What does ""shortly after"" mean? Obviously, no one knows, which is the ""fast takeoff/slow takeoff"" dichotomy that keeps turning up in the book. But probably ""slow takeoff"" will be at most a year or two, and fast takeoff could be seconds. Suddenly, we're sharing our planet with a being who's vastly smarter than we are. Bostrom goes to some trouble to help you understand what ""vastly smarter"" means. We're not talking Einstein versus a normal person, or even Einstein versus a mentally subnormal person. We're talking human being versus a mouse. It seems reasonable to assume the superintelligence will quickly learn to do all the things a very smart person can do, including, for starters: formulating and carrying out complex strategic plans; making money in business activities; building machines, including robots and weapons; using language well enough to persuade people to do dumb things; etc etc. It will also be able to do things that we not only can't do, but haven't even thought of doing. And so we come to the first key question: having produced your superintelligence, how do you keep it under control, given that you're a mouse and it's a human being? The book examines this in great detail, coming up with any number of bizarre and ingenious schemes. But the bottom line is that no matter how foolproof your scheme might appear to you, there's absolutely no way you can be sure it'll work against an agent who's so much smarter. There's only one possible strategy which might have a chance of working, and that's to design your superintelligence so that it wants to act in your best interests, and has no possibility of circumventing the rules of its construction to change its behavior, build another superintelligence which changes its behavior, etc. It has to sincerely and honestly want to do what's best for you. Of course, this is Asimov Three Laws territory; and, as Bostrom says, you read Asimov's stories and you see how extremely difficult it is to formulate clear rules which specify what it means to act in people's best interests. So the second key question is: how do you build an agent which of its own accord wants to do ""the right thing"", or, as Socrates put it two and half thousand years ago, is virtuous? As Socrates concludes, for example in Meno and Euthyphro, these issues are really quite difficult to understand. Bostrom uses language which is a bit less poetic and a bit more mathematical, but he comes to pretty much the same conclusions. No one has much idea yet of how to do it. The book reaches this point and gives some closing advice. There are many details, but the bottom line is unsurprising given what's gone before: be very, very careful, because this stuff is incredibly dangerous and we don't know how to address the critical issues.I think some people have problems with Superintelligence due to the fact that Bostrom has a few slightly odd beliefs (he's convinced that we can easily colonize the whole universe, and he thinks simulations are just as real as the things they are simulating). I don't see that these issues really affect the main arguments very much, so don't let them bother you if you don't like them. Also, I'm guessing some other people dislike the style, which is also slightly odd: it's sort of management-speak with a lot of philosophy and AI terminology added, and because it's philosophy there are many weird thought-experiments which often come across as being a bit like science-fiction. Guys, relax. Philosophers have been doing thought-experiments at least since Plato. It's perfectly normal. You just have to read them in the right way. And so, to conclude, let's look at Plato again (remember, all philosophy is no more than footnotes to Plato), and recall the argument from the Theaetetus. Whatever high-falutin' claims it makes, science is only opinions. Good opinions will agree with new facts that turn up later, and bad opinions will not. We've had three and a half years of new facts to look at since Superintelligence was published. How's its scorecard?Well, I am afraid to say that it's looking depressingly good. Early on in the history of AI, as the book reminds us, people said that a machine which could play grandmaster level chess would be most of the way to being a real intelligent agent. So IBM's team built Deep Blue, which beat Garry Kasparov in 1997, and people immediately said chess wasn't a fair test, you could crack it with brute force. Go was the real challenge, since it required understanding. In late 2016 and mid 2017, Deep Mind's AlphaGo won matches against two of the world's three best Go players. That was also discounted as not a fair test: AlphaGo was trained on millions of moves of top Go matches, so it was just spotting patterns. Then late last year, Alpha Zero learned Go, Chess and Shogi on its own, in a couple of days, using the same general learning method and with no human examples to train from. It played all three games not just better than any human, but better than all previous human-derived software. Looking at the published games, any strong chess or Go player can see that it has worked out a vast array of complex strategic and tactical principles. It's no longer a question of ""does it really understand what it's doing"". It obviously understands these very difficult games much better than even the top experts do, after just a few hours of study.Humanity, I think that was our final warning. Come up with more excuses if you like, but it's not smart. And read Superintelligence."
1,0199678111,http://goodreads.com/user/show/6100646-brian-clegg,3,"There has been a spate of outbursts from physicists who should know better, including Stephen Hawking, saying ‘philosophy is dead – all we need now is physics’ or words to that effect. I challenge any of them to read this book and still say that philosophy is pointless.It’s worth pointing out immediately that this isn’t really a popular science book. I’d say the first handful of chapters are for everyone, but after that, the bulk of the book would probably be best for undergraduate philosophy students or AI students, reading more like a textbook than anything else, particularly in its dogged detail – but if you are interested in philosophy and/or artificial intelligence, don’t let that put you off.What Nick Bostrom does is to look at the implications of developing artificial intelligence that goes beyond human abilities in the general sense. (Of course, we already have a sort of AI that goes beyond our abilities in the narrow sense of, say, arithmetic, or playing chess.) In the first couple of chapters he examines how this might be possible – and points out that the timescale is very vague. (Ever since electronic computers have been invented, pundits have been putting the development of effective AI around 20 years in the future, and it’s still the case.) Even so, it seems entirely feasible that we will have a more than human AI – a superintelligent AI – by the end of the century. But the ‘how’ aspect is only a minor part of this book.The real subject here is how we would deal with such a ‘cleverer than us’ AI. What would we ask it to do? How would we motivate it? How would we control it? And, bearing in mind it is more intelligent than us, how would we prevent it taking over the world or subverting the tasks we give it to its own ends? It is truly fascinating concept, explored in great depth here. This is genuine, practical philosophy. The development of super-AIs may well happen – and if we don’t think through the implications and how we would deal with it, we could well be stuffed as a species.I think it’s a shame that Bostrom doesn’t make more use of science fiction to give examples of how people have already thought about these issues – he gives only half a page to Asimov and the three laws of robotics (and how Asimov then spends most of his time showing how they’d go wrong), but that’s about it. Yet there has been a lot of thought and dare I say it, a lot more readability than you typically get in a textbook, put into the issues in science fiction than is being allowed for, and it would have been worthy of a chapter in its own right.I also think a couple of the fundamentals aren’t covered well enough, but pretty much assumed. One is that it would be impossible to contain and restrict such an AI. Although some effort is put into this, I’m not sure there is enough thought put into the basics of ways you can pull the plug manually – if necessary by shutting down the power station that provides the AI with electricity.The other dubious assertion was originally made by I. J. Good, who worked with Alan Turing, and seems to be taken as true without analysis. This is the suggestion that an ultra-intelligent machine would inevitably be able to design a better AI than humans, so once we build one it will rapidly improve on itself, producing an ‘intelligence explosion’. I think the trouble with this argument is that my suspicion is that if you got hold of the million most intelligent people on earth, the chances are that none of them could design an ultra-powerful computer at the component level. Just because something is superintelligent doesn’t mean it can do this specific task well – this is an assumption.However this doesn’t set aside what a magnificent conception the book is. I don’t think it will appeal to many general readers, but I do think it ought to be required reading on all philosophy undergraduate courses, by anyone attempting to build AIs… and by physicists who think there is no point to philosophy."
2,0199678111,http://goodreads.com/user/show/4975321-joseph,2,"Preamble:I confess to Roko's Basilisk that I didn't believe superintelligence was much of a threat, and this book didn't really do anything to challenge that prior. Mea Culpa, Mea Culpa, Mea [local] Maxima Culpa.I. Overall View I'm a software engineer with some basic experience in machine learning, and though the results of machine learning have been becoming more impressive and general, I've never really seen where people are coming from when they see strong superintelligence just around the corner, especially the kind that can recursively improve itself to the point where intelligence vastly increases in the space of a few hours or days. So I came to this book with a simple question: ""Why are so many intelligent people scared of a near-term existential threat from AI, and especially why should I believe that AI takeoff will be incredibly fast?"" Unfortunately, I leave the book with this question largely unanswered. Though in principle I can't think of anything that prevents the formation of some forms of superintelligence, everything I know about software development makes me think that any progress will be slow and gradual, occasionally punctuated with a new trick or two that allows for somewhat faster (but still gradual) increases in some domains. So on the whole, I came away from this book with the uncomfortable but unshakeable notion that most of the people cited don't really have much relevant experience in building large-scale software systems. Though Bostrom used much of the language of computer science correctly, any of his extrapolations from very basic, high-level understandings of these concepts seemed frankly oversimplified and unconvincing.II. General Rant on Math in Philosophy Ever since I was introduced to utilitarianism in college (the naive, Bentham-style utilitarianism at least) I've been somewhat concerned about the practice of trying to add more rigor to philosophical arguments by filling them with mathematical formalism. To continue with the example of utilitarianism, in its most basic sense it asks you to consider any action based on a calculation of how much pleasure will result from your action divided by the amount of pain an action will cause, and to act in such a way that you maximize this ratio. Now it's of course impossible to do this calculation in all but the most trivial cases, even assuming you've somehow managed to define pleasure, pain, and come up with some sort of metric for actually evaluating differences between them. So really the formalism only expresses a very simple relationship between things which are not defined, and based on the process of definition might not be able to be legitimately placed in simple arithmetic or algebraic expressions. I felt much the same way when I was reading Superintelligence. Especially in his chapter on AI takeoff, Bostrom argued that the amount of improvement in an AI system could be modeled as a ratio of applied optimization power over the recalcitrance of the system, or its architectural unwillingness to accept change. Certainly this is true as far as it goes, but ""optimization power"" and ""recalcitrance"" are necessarily at this point dealing with systems that nobody yet knows how to build, or even what they will look like, beyond some hand-wavey high-level descriptions, and so there is no definition one can give that makes any sense unless you've already committed to some ideas of exactly how the system will perform. Bostrom tries to hedge his bets by presenting some alternatives, but he's clearly committed to the idea of a fast takeoff, and the math-like symbols he's using present only a veneer of formalism, drawing some extremely simple relations between concepts which can't be yet defined in any meaningful way. This was the example that really made my objections to unjustified philosophy-math snap into sharp focus, but it's just one of many peppered throughout the book, which gives an attempted high-level look at superintelligent systems, but too many of the black boxes on which his argument rested remained black boxes. Unable to convince myself of the majority of his argument since too many of his steps were glossed over, I came away from this book thinking that there had to be a lot more argumentation somewhere, since I couldn't imagine holding this many unsubstantiated ""axioms"" for something apparently important to him as superintelligence. And it really is a shame that the book needed to be bogged down with so much unnecessary formalism (which had the unpleasant effect of making it feel simultaneously overly verbose and too simplistic), since there were a few good things in here that I came away with. The sections on value-loading and security were especially good. Like most of the book, I found them overly speculative and too generous in assuming what powers superintelligences would possess, but there is some good strategic stuff in here that could lead toward more general forms of machine intelligence, and avoid some of the overfitting problems common in contemporary machine learning. Of course, there's also no plan of implementation for this stuff, but it's a cool idea that hopefully penetrates a little further into modern software development.III. Whereof One Cannot Speak, Thereof One Must Request Funding It's perhaps callous and cynical of me to think of this book as an extended advertisement for the Machine Intelligence Research Institute (MIRI), but the final two chapters in many ways felt like one. Needless to say I'm not filled with a desire to donate on the basis of an argument I found largely unconvincing, but I do have to commend those involved for actually having an attempt at a plan of implementation in place simultaneous with a call to action.IV. Conclusion I remain pretty unconvinced of AI as a relatively near-term existential threat, though I think there's some good stuff in here that could use a wider audience. And being more thoughtful and careful with software systems is always a cause I can get behind. I just wish some more of the gaps got filled in, and I could justifiably shake my suspicion that Bostrom doesn't really know that much about the design and implementation of large-scale software systems.V. Charitable TL;DRNot uninteresting, needs a lot of work before it's convincing.VI. Uncharitable TL;DR"
3,0199678111,http://goodreads.com/user/show/1651956-riku-sayuj,3,"Imagine a Danger (You may say I'm a Dreamer)Bostrom is here to imagine a world for us (and he has batshit crazy imagination, have to give him that). The world he imagines is a post-AI world or at least a very-near-to-AI world or a nascent-AI world. Don’t expect to know how we will get there - only what to do if we get there and how to skew the road to getting there to our advantage. And there are plenty of wild ideas on how things will pan out in that world-in-transition, the ‘routes’ bit - Bostrom discusses the various potential routes, but all of them start at a point where AI is already in play. Given that assumption, the “dangers” bit is automatic since the unknown and powerful has to be assumed to be dangerous. And hence strategies are required. See what he did there?It is all a lot of fun, to be playing this thought experiment game, but it leaves me a bit confused about what to feel about the book as an intellectual piece of speculation. I was on the fence between a two-star rating or a four-star rating for much of the reading. Plenty of exciting and grand-sounding ideas are thrown at me… but, truth be told, there are too many - and hardly any are developed. The author is so caught up in his own capacity for big BIG BIIG ideas that he forgets to develop them into a realistic future or make any the real focus of ‘dangers’ or ‘strategies’. They are just all out there, hanging. As if their nebulosity and sheer abundance should do the job of scaring me enough.In the end I was reduced to surfing the book for ideas worth developing on my own. And what do you know, there were a few. So, not too bad a read and I will go with three. And for future readers, the one big (not-so-new) and central idea of the book is simple enough to be expressed as a fable, here it is:The Unfinished Fable of the SparrowsIt was the nest-building season, but after days of long hard work, the sparrows sat in the evening glow, relaxing and chirping away.“We are all so small and weak. Imagine how easy life would be if we had an owl who could help us build our nests!”“Yes!” said another. “And we could use it to look after our elderly and our young.”“It could give us advice and keep an eye out for the neighborhood cat,” added a third.Then Pastus, the elder-bird, spoke: “Let us send out scouts in all directions and try to find an abandoned owlet somewhere, or maybe an egg. A crow chick might also do, or a baby weasel. This could be the best thing that ever happened to us, at least since the opening of the Pavilion of Unlimited Grain in yonder backyard.”The flock was exhilarated, and sparrows everywhere started chirping at the top of their lungs.Only Scronkfinkle, a one-eyed sparrow with a fretful temperament, was unconvinced of the wisdom of the endeavor. Quoth he: “This will surely be our undoing. Should we not give some thought to the art of owl-domestication and owl-taming first, before we bring such a creature into our midst?”Replied Pastus: “Taming an owl sounds like an exceedingly difficult thing to do. It will be difficult enough to find an owl egg. So let us start there. After we have succeeded in raising an owl, then we can think about taking on this other challenge.”“There is a flaw in that plan!” squeaked Scronkfinkle; but his protests were in vain as the flock had already lifted off to start implementing the directives set out by Pastus.Just two or three sparrows remained behind. Together they began to try to work out how owls might be tamed or domesticated. They soon realized that Pastus had been right: this was an exceedingly difficult challenge, especially in the absence of an actual owl to practice on. Nevertheless they pressed on as best they could, constantly fearing that the flock might return with an owl egg before a solution to the control problem had been found.It is not known how the story ends…"
4,0199678111,http://goodreads.com/user/show/27098959-leonard-gaya,4,"In recent times, prominent figures such as Stephen Hawking, Bill Gates and Elon Musk have expressed serious concerns about the development of strong artificial intelligence technology, arguing that the dawn of super-intelligence might well bring about the end of mankind. Others, like Ray Kurzweil (who, admittedly, has gained some renown in professing silly predictions about the future of the human race), have an opposite view on the matter and maintain that AI is a blessing that will bestow utopia upon humanity. Nick Bostrom painstakingly elaborates on the disquiet views of the former (he might well have influenced them in the first place), without fully dismissing the blissful engrossment of the latter.First, he endeavours to shed some light on the subject and delves into quite a few particulars concerning the future of AI research, such as: the different paths that could lead to super-intelligence (brain emulations or AI proper), the steps and timeframe through which we might get there, the types and number of AI that could result as we continue improving our intelligent machines (he calls them “oracles”, “genies” and “sovereigns”), the different ways in which it could go awry, and so forth.But Bostrom is first and foremost a philosophy professor, and his book is not so much about the engineering or economic aspects that we could foresee as regards strong AI. The main concern is the ethical problems that the development of a general (i.e. cross-domain) super-intelligent machine, far surpassing the abilities of the human brain, might pose to us as humans. The assumption is that the possible existence of such a machine would represent an existential threat to human kind. The main argument is thus to warn us about the dangers (some of Bostrom’s examples are weirdly farcical, and reminded me of Douglas Adams’s The Hitchhiker's Guide to the Galaxy), but also to outline in some detail how this risk could or should be mitigated, restraining the scope or the purpose of a hypothetical super-brain: this is what he calls “the AI control problem”, which is at the core of his reasoning and which, upon reflexion, is a surprisingly difficult one.I should add that, although the book is largely accessible to the layperson, Bostrom’s prose is often dense, speculative, and makes very dry reading: not exactly a walk in the park. He should be praised nonetheless for attempting to apply philosophy and ethical thinking to nontrivial questions.One last remark: Bostrom explores a great many questions in this book but, oddly enough, it seems never to occur to him to think about the possible moral responsibility we humans might have towards an intelligent machine, not just a figment of our imagination but a being that we will someday create and could at least be compared to us. Charity begins at home, I suppose."
5,0199678111,http://goodreads.com/user/show/16077597-matt,4,"As a software developer, I've cared very little for artificial intelligence (AI) in the past. My programs, which I develop professionally, have nothing to do with the subject. They’re dumb as can be and only following strict orders (that is rather simple algorithms). Privately I wrote a few AI test programs (with more or less success) and read a articles in blogs or magazines (with more or less interest). By and large I considered AI as not being relevant for me.In March 2016 AlphaGo was introduced. This was the first Go program capable of defeating a champion in this game. Shortly after that, in December 2017, Alpha Zero entered the stage. Roughly speaking this machine is capable of teaching itself games after being told the rules. Within a day, Alpha Zero developed superhuman level of play for Go, Chess, and Shogi; all by itself (if you can believe the developers). The algorithm used in this machine is very abstract and can probably be used for all games of this kind. The amazing thing for me was how fast the AI development progresses.This book is not all about AI. It’s about “superintelligence” (SI). An SI can be thought of some entity which is far superior to human intelligence in all (or almost all) cognitive abilities. To paraphrase Lincoln: You can outsmart some of the people all of the time and you can outsmart all of the people some of the time, but you can’t outsmart all of the people all of the time; unless you are a superintelligence. The subtitle of the English edition “paths, dangers, strategies” has been chosen wisely. What steps can been taken to build an SI, what are the dangers of introducing an SI, and how can one ensure that these dangers and risks are eliminated or at least scaled-down to an acceptable level?An SI does not necessarily have to exist in a computer. The author is also co-founder of the “World Transhumanist Association”. Therefore, transhumanist ideas are included in the book, albeit in a minor role. An SI can theoretically be build by using genetic selection (of embryos, i.e. “breeding”). Genetic research would probably soon be ready to provide the appropriate technologies. For me, a scary thought; something which touches my personal taboos. Not completely outlandish, but still with a big ethical question mark for me, seems to be “Whole Brain Emulation” (WBE). Here, the brain of a human being, more precisely, the state of the brain at a given time, is analyzed and transferred to a corresponding data structure in the memory of a powerful computer where then the brain/consciousness of the individual continues to exist, possibly within a suitable virtual reality. There are already quite a few films or books that deal with this scenario (for a positive example see the this episode of the Black Mirror series). With WBE you would have an artificial entity with the cognitive performance of a human being. The vastly superior processing speed of the digital versus the biological circuits will let this entity become super intelligent (consider 100,000 copies of a 1000x faster WBE and let it run for six months, and you’ll get 50 millenia worth of thinking!) However, the main focus in the discussion about SI in this book is the further development of AI to become Super-AI (SAI). This is not a technical book though. It contains no computer code whatsoever, and the math (appearing twice in some info-boxes) is only marginal and not at all necessary for understanding. One should not imagine an SI as a particularly intelligent person. It might be more appropriate to equate the ratio of SI to human intelligence with that of human intelligence to the cognitive performance of a mouse. An SI will indeed be very very smart and, unfortunately, also very very unstable. By that I mean that an SI will be busy at any time to changed and improve itself. The SI you speak with today will be a million or more times smarter tomorrow. In this context, the book speaks of “intelligence explosion”. Nobody knows yet, when this will start and how fast it will go. Could be next year, or in ten, fifty, or one hundred years. Or perhaps never (although this is highly unlikely). Various scenarios are discussed in the book. Also it is not clear if there will be only one SI (a so called singleton), or several competing or collaborating SIs (with a singleton seeming to be more likely).I think it’s fair to say that humanity as a whole has the wish to continue to exist; at least the vast majority of people do not consider the extinction of humanity desirable. With that in mind it would make sense to instruct an SI to follow that same goal. Now I forgot to specify the exact state in which you want to exist. In this case the SI might choose to put all humans into coma (less energy consumption). The problem is solved from the SI’s point of view; its goal has been reached. But obviously this is not what we meant. We have to re-program the SI and tweak its goal a bit. Therefore it would be mandatory to always be able to control the SI. It’s possible an SI will not act the way we intended (it will act, however, the way we programmed it). A case of an “unfriendly” SI is actually very likely. The book mentions and describes “perverse instantiation”, “infrastructure profusion” and “mind crime” as possible effects. The so called “control problem” remains unsolved as of now and it appears equivalent to that of a mouse controlling a human being. Without a solution, the introduction of an SI becomes a gamble (with a very high probability a “savage” SI will wipe out humanity).The final goal of an SI should be formulated pro-human if at all possible. At least, the elimination of humankind should not be prioritized at any time. You should give the machine some kind of morality. But how does one do it? How can you formulate moral ideas in a computer language? And what happens if our morals change over time (which has happened before), and the machine still decides on a then-outdated moral ground? In my opinion, there will be insurmountable difficulties at this point. Nevertheless, there are also at least some theoretical approaches explained by Bostrom (who is primarily a philosopher). It’s quite impressive to read these chapters (albeit also a bit dry). In general, the chapters dealing with philosophical questions, and how they are translated to the SI world, were the most engrossing ones for me. The answers to this kind of questions are also subject to some urgency. Advances in technology generally move faster than wisdom (not only in this field), and the sponsors of the projects expect some return on invest. Bostrom speaks of a “philosophy with a deadline”, a fitting, but also disturbing image.Another topic is an SI that is neither malignant nor fitted with false goals (something like this is also possible), but on the contrary actually helps humanity. Quote: The point of superintelligence is not to pander to human preconceptions but to make mincemeat out of our ignorance and folly. Certainly this is a noble goal. However, how will people (and I’m thinking about those who are currently living) react when their follies are disproved? It’s hard to say, but I guess they will not be amused. One should not trust people too much intelligence in this respect (see below for my own “anger”).Except for the sections on improving human intelligence through biological interference and breeding (read eugenics), I found everything in this book fascinating, thought-provoking, and highly disturbing. The book has, in a way, changed my world view rather drastically, which is rare. My “folly” about AI and especially Super-AI has changed fundamentally. In a way, I've gone through 4 of the 5 stages of grief & loss. Before the book, I flatly denied a Super-AI will ever come to fruition. When I read the convincing arguments that not only an Super-AI will be possible, but indeed very likely, my denial changed into anger. In spite of the known problems and the existential risk of such a technology, how can one even think to follow this slippery slope? (this question is also dealt with in the book) My anger was then turned into a depression (not a clinical one) towards the end. Still in this condition, I’m now awaiting acceptance, which in my case will more likely be fatalism.A book that shook me profoundly and that I actually wished I had not read, but that I still recommend highly (I guess I need a superintelligence to make sense of that).

This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License."
6,0199678111,http://goodreads.com/user/show/27513524-john-igo,3,"This book... if {}else if {}else if {}else if {}else if {} ...You can get most of the ideas in this book in the WaitButWhy article about AI. This book assumes that an intelligence explosion is possible, and that it is possible for us to make a computer whose intelligence will explode. Then talks about ways to deal with it. A lot of this book seems like pointless naval gazing, but I think some of it is worth reading. "
7,0199678111,http://goodreads.com/user/show/6603759-misericordia-the-serendipity-aegis,2,"Hypothetical enough to become insanely dumb boring. Superintelligence, hyperintelligence, hypersuperintelligence… Basically, it all amounts to the fact that maybe, sometime, the ultimate thinking machines will do or not so something. Just how new is that idea? IMO, the main point is how do we get them there? Designing intuition? Motivating the AI? Motivational scaffolding? Associative value accretion? While it's all very entertaining, it's nowhere near practical at this point. And the bareboned philosophy of the non-existent AI that's pretty much dumb today? This is one fat DNF."
8,0199678111,http://goodreads.com/user/show/28709846-manuel-ant-o,5,"If you're into stuff like this, you can read the full review.(Count-of-Self) = 0: ""Superintelligence - Paths, Dangers, Strategies"" by Nick Bostrom""Box 8 - Anthropic capture: The AI might assign a substantial probability to its simulation hypothesis, the hypothesis that it is living in a computer simulation.""In ""Superintelligence - Paths, Dangers, Strategies"" by Nick BostromWould you say that the desire to preserve 'itself' comes from the possession of a (self) consciousness? If so, does the acquisition of intelligence according to Bostrom also mean the acquisition of (self) consciousness? The unintended consequence of a super intelligent AI is the development of an intelligence that we can barely see, let alone control, as a consequence of the networking of a large number of autonomous systems acting on inter-connected imperatives. I think of bots trained to trade on the stock market that learn that the best strategy is to follow other bots, who are following other bots. The system can become hyper-sensitive to inputs that have little or nothing to do with supply and demand."
9,0199678111,http://goodreads.com/user/show/4213258-bradley,5,"I'm very pleased to have read this book. It states, concisely, the general field of AI research's BIG ISSUES. The paths to making AIs are only a part of the book and not a particularly important one at this point.More interestingly, it states that we need to be more focused on the dangers of superintelligence. Fair enough! If I was an ant separated from my colony coming into contact with an adult human being, or a sadistic (if curious) child, I might start running for the hills before that magnifying glass focuses the sunlight.And so we move on to strategies, and this is where the book does its most admirable job. All the current thoughts in the field are represented, pretty much, but only in broad outlines. A lot of this has been fully explored in SF literature, too, and not just from the Asimov Laws of Robotics.We've had isolation techniques, oracle techniques, and even straight tool-use techniques crop up in robot and AI literature. Give robots a single-task job and they'll find a way to turn it into a monkey's paw scenario.And this just begs the question, doesn't it?When we get right down to it, this book may be very concise and give us a great overview, but I do believe I'll remain an uberfan of Eliezer Yudkowsky over Nick Bostrom. After having just read Rationality: From AI to Zombies, almost all of these topics are not only brought up, but they're explored in grander fashion and detail.What do you want? A concise summary? Or a gloriously delicious multi-prong attack on the whole subject that admits its own faults the way that HUMANITY should admit its own faults?Give me Eli's humor, his brilliance, and his deeply devoted stand on working out a real solution to the ""Nice"" AI problem. :)I'm not saying Superintelligence isn't good, because it most certainly is, but it is still the map, not the land. :) (Or to be slightly fairer, neither is the land, but one has a little better definition on the topography.) "
10,0199678111,http://goodreads.com/user/show/34621415-jasmin-shah,5,Never let a Seed AI read this book!
11,0199678111,http://goodreads.com/user/show/793473-clif-hostetler,3,"This book  was published in 2014 so is a bit dated, and I’m now writing this review somewhat late for what should be a cutting edge issue. But many people who are interested in this subject continue to respect this book as the definitive examination of the risks associated with machines that are significantly smarter than humans.We have been living for many years with computers—and even phones—that store more information and can retrieve that information faster than any human. These devices don’t seem to pose much threat to us humans, so it’s hard to perceive why there may be cause for concern. The problem is as follows. As artificial intelligence (AI) becomes more proficient in the future it will have the ability to learn (a.k.a.machine learning) and improve itself as it examines and solves problems. It will have the ability to change (i.e. reprogram) itself in order to develop new methods as needed to execute solutions for the tasks at hand. Thus, it will be using techniques and strategies of which the originating human programmer will be unaware. Once machines are creatively strategizing better (i.e. smarter) than humans, the gap between machine and human performance (i.e. intelligence) will grow exponentially. Eventually, the level of thinking by the “super-intelligent” machine will have the relative superiority over that of humans that is equivalent to the superiority of the human brain over that of a beetle crawling on the floor. It is reasonable to conjecture that a machine that smart will have as much respect for humans who think they’re controlling it as humans are likely to have respect for a beetle trying to control them. The concept of superintelligence means that the machine can perform better than humans at all tasks including such things as using human language to be persuasive, raising money, developing strategic plans, designing and making robots, advanced weapons, and advances in science and technology. A super-intelligent machine will solve problems that humans don't know exist.Of course that may be a good thing, but such machines in effect have a mind of their own. They may decide they know best and not want to follow human instructions. Much of this book is spent examining—in too much detail in my opinion—possible ways to control a super-intelligent machine. Then after this long exploration of various strategies the conclusion is in essence that it's not possible.So then the book moves on to the question of how to design the initiating foundation of such a machine to have the innate desire to do good (i.e. be virtuous). Again the author goes into excruciating details examining various ways to do this. The bottom line is we can try, but we don't have the necessary tools to be sure to address the critical issues.In conclusion, our goose is cooked. We can't help ourselves. Superintelligence is the ""tree of the knowledge of good and evil."" We have to take a bite.This link is to an article about facial recognition. It contains the following quote:... the whole ecosystem of artificial intelligence is optimized for a lack of accountability.Shortly after writing my review the Dilbert cartoon featured the subject of AI:Here's a link to a review of ""Game Changer: AlphaZero's Groundbreaking Chess Strategies and the Promise of AI,"" by Matthew Sadler and Natasha Regan. This review describes a chess program that utilizes AI to become almost unbeatable with a style of play not previously seen.https://www.goodreads.com/review/show..."
12,0199678111,http://goodreads.com/user/show/1657830-jim,4,"Superintelligence by Nick Bostrom is a hard book to recommend, but is one that thoroughly covers its subject. Superintelligence is a warning against developing artificial intelligence (AI). However, the writing is dry and systematic, more like Plato than Wired Magazine. There are few real world examples, because it's not a history of AI, but theoretic conjectures. The book explores the possible issues we might face if a superintelligent machine or life form is created. I would have enjoyed the book more if it reported on current state of the art projects in AI. The recent work of DeepMind learning to play classic Atari games offers more realism to the possibilities of AI than anything mentioned in this book. Deep learning projects, the latest development in neural nets, is having some astounding successes. Bostrom doesn't report on them.And I think Bostrom makes one glaring error. I'm no AI expert, but he seems to assume we can program an AI, and control its intelligence. I don't think that's possible. We won't be programming The Three Laws of Robotics into future AI beings. I believe AIs will evolve out of learning systems, and we'll have no control over what emerges. We'll create software and hardware that is capable of adapting and learning. The process of becoming a self-aware superintelligence will be no more understandable to us than why our brains generate consciousness."
13,0199678111,http://goodreads.com/user/show/7092601-robert-schertzer,1,"I switched to the audio version of this book after struggling with the Kindle edition since I needed to read this for a book club. If you are looking for a book on artificial intelligence (AI), avoid this and opt for Jeff Hawkins' book ""On Intelligence"" written by someone who has devoted their life to the field. If it is one on ""AI gone bad"" you seek, try 2001 Space Odyssey. For a fictional approach on AI that helped set the groundwork for AI theory, go for Isaac Asimov. If you want a tedious, relentless and pointless book that fails at achieving what all three previously aforementioned authors have succeeded at - this is the book for you."
14,0199678111,http://goodreads.com/user/show/16212136-peter-pete-mcloughlin,5,"Stephen Hawking and Bill Gates have recently raised the alarm about Artificial Intelligence. If a superhuman artificial intelligence were created it would be the biggest event in human history and it could very well be the last. We are only familiar with human intelligence and it may be a small sample from the possibilities of intelligence to be had. Bostrom makes the case that the most likely path to superintelligence would most likely be a hard takeoff as the AI would quickly rise once it reached Human level intelligence and quickly reorganize itself to a very superior form of intelligent mind. It would quickly gain powers and abilities far beyond humans and it would be more alien and unfathomable than anything we have ever seen. If it has goals that don't match up with the human project so much for the human race.  With great detail, Bostrom lays out where AI could go seriously wrong for us. Disasters in the abstract may make us yawn but Bostrom gives the details of what the catastrophe might look like. The Hellmouth is much scarier when the picture becomes more detailed. I recommend reading Bostrom's book to educate yourself on dangers of ceding the top of the food chain to AI. It is fairly hair-raising.Here is Nick Bostrom talking on this topic at TED. https://www.youtube.com/watch?v=MnT1x... "
15,0199678111,http://goodreads.com/user/show/31999040-shea-levy,0,"Read up through chapter 8. The book started out somewhat promisingly by not taking a stand on whether strong AI was imminent or not, but that was the height of what I read. I'm not sure there was a single section of the book where I didn't have a reaction ranging from ""wait, how do you know that's true?"" to ""that's completely wrong and anyone with a modicum of familiarity with the field you're talking about would know that"", but really it's the overall structure of the argument that led me to give this one up as a waste of time.Essentially, the argument goes like this: Bostrom introduces some idea, explains in vague language what he means by it, traces out how it might be true (or, in a few ""slam-dunk"" sections, *several* ways it might be true), and then moves on. In the next section, he takes all of the ideas introduced in the previous sections as givens and as mostly black boxes, in the sense that the old ideas are brought up to justify new claims without ever invoking any of the particular evidence for or structure of the old idea, it's just an opaque formula. The sense is of someone trying to build a tower, straight up. The fact that this particular tower is really a wobbly pile of blocks, with many of the higher up ones actually resting on the builder's arm and not really on the previous ones at all, is almost irrelevant: this is not how good reasoning works! There is no broad consideration of the available evidence, no demonstration of why the things we've seen imply the specific things Bostrom suggests, no serious engagement with alternative explanations/predictions, no cycling between big-picture overviews and in-detail analyses. There is just a stack of vague plausibilities and vague conceptual frameworks to accommodate them. A compelling presentation is a lot more like clearing away fog to note some rocky formations, then pulling back a bit to see they're all connected, then zooming back in to clear away the connected areas, and so on and so forth until a broad mountain is revealed.This is not to say that the outcome Bostrom fears is impossible. Even though I think many of the specific things he thinks are plausible are actually much less so than he asserts, I do think a kind of very powerful ""unfriendly"" AI is a possibility that should be considered by those in a position to really understand the problem and take action against it if it turns out to be a real one. The problem with Bostrom's presentation is that it doesn't tell us anything useful: We have no reason to suspect that the particular kinds of issues he proposes are the ones that will matter, that the particular characteristics he ascribes to future AI are ones that will be salient, indeed that this problem is likely enough, near enough, and tractable enough to be worth spending significant resources on at all at the moment! Nothing Bostrom is saying compellingly privileges his particular predictions over many many possible others, even if you take as a given that extraordinarily powerful AI is possible and its behavior hard to predict. I continually got the sense (sometimes explicitly echoed by Bostrom himself!) that you could substitute in huge worlds of incompatible particulars for the ones he proposed and still make the same claims. So why should I expect anything particular he proposes to be worthwhile?Edit: After chatting about this a bit with some friends, I should add one caveat to this review. This is praising with bold damnation if ever there were such a thing, but this book has made me more likely to engage with AI as an existential risk by being such a clear example of what had driven me away up until now. Now that I can see the essence of what's wrong with the bad approaches I've seen, I'll be better able to seek out the good ones (and, as I said, I do think the problem is worth serious investigation). So, I guess ultimately Bostrom succeeded at his goal in my case?"
16,0199678111,http://goodreads.com/user/show/14873887-travis,3,I'm not going to criticize the content. I cannot finish this. Imagine eating saltines when you have cotton mouth in the middle of the desert. You might be close to describing how dry the writing is. Could be very interesting read if the writing was done in a more attention grabbing way.
17,0199678111,http://goodreads.com/user/show/7315267-diego-petrucci,5,"There's no way around it: a super-intelligent AI is a threat.We can safely assume that an AI smarter than a human, if developed, would accelerate its own development getting smarter at a rate faster than anything we'd ever seen. In just a few cycles of self-improvement it would spiral out of control. Trying to fight, or control, or hijack it, would be totally useless — for a comparison, try picturing an ant trying to outsmart a human being (a laughable attempt, at best).But why is a super-intelligent AI a threat? Well, it probably wouldn't have human qualities (empathy, a sense of justice, and so on) and would rely on a more emotion-less understanding of the world — understanding emotion doesn't mean you have to feel emotions, you can understand the motives of terrorists without agreeing with them. There would be a chance of developing a super-intelligent AI with an insane set of objectives, like maximizing the production of chairs with no regard to the safety of human beings or the environment, totally subsuming Earth 's materials and the planet itself. Or, equally probable, we could end up with an AI whose main objective is self-preservation, who would later annihilate the human race because of an even minuscule chance of us destroying it.With that said, it's clear that before developing a self-improving AI we need a plan. We need tests to understand and improve its moral priorities, we need security measures, we need to minimize the risk of it destroying the planet. Once the AI is more intelligent than us, it won't take much to get extremely more intelligent, so we need to be prepared. We only got one chance and that's it, either we set it up right or we're done as a species.Superintelligence deals with all these problems systematically analyzing them and providing a few frames of mind to let us solve them (if that's even possible)."
18,0199678111,http://goodreads.com/user/show/23233462-clare-o-beara,4,"We are now building superintelligences. More than one. The author Nick Bostrom looks at what awaits us. He points out that controlling such a creation might not be easy. If unfriendly superintelligence comes about, we won't be able to change or replace it. This is a densely written book, with small print, with 63 pages of notes and bibliography. In the introduction the author tells us twice that it was not easy to write. However he tries to make it accessible, and adds that if you don't understand some techie terms you should still be able to grasp the meaning. He hopes that by pulling together this material he has made it easier for other researchers to get started. So - where are we? I have to state that with lines like: ""Collective superintelligence is less conceptually clear-cut than speed superintelligence. However it is more familiar empirically."" This is a more daunting book than 'The Rise of The Robots' by Martin Ford. If you are used to such terms and concepts you can dive in; if not I'd recommend the Ford book first. To be fair, terms are explained and we can easily see that launching a space shuttle requires a collective intellectual effort. No one person could do it. Humanity's collective intelligence has continued to grow, as people evolved to become smarter, as there were more of us to work on a problem, as we got to communicate and store knowledge, and as we kept getting smarter and building on previous knowledge. There are now so many of us who don't need to farm or make tools, that we can solve many problems in tandem. Personally, I say that if you don't think your leaders are making smart decisions, just go out and look at your national transport system at rush hour in the capital city. But a huge population requires a huge resource drain. As will the establishment of a superintelligence. Not just materials and energy but inventions, tests, human hours and expertise are required. Bostrom talks about a seed AI, a small system to start. He says in terms of a major system, the first project to reach a useful AI will win. After that the lead will be too great and the new AI so useful and powerful, that other projects may not close the gap. Hardware, power generation, software and coding are all getting better. And we have the infrastructure in place. We are reminded that ""The atomic bomb was created primarily by a group of scientists and engineers. The Manhattan Project employed about 130,000 people at its peak, the vast majority of whom were construction workers or building operators."" Aspects covered include reinforcement learning, associative value accretion, monitoring of projects, solving the value loading problem - which means defining such terms as happiness and suffering, explaining them to a computer and representing which is our goal. I turned to the chapter heading 'Of horses and men'. Horses, augmented by ploughs and carriages, were a huge advantage to human labour. But they were replaced by the automobile and tractor. The equine population crashed, and not to retirement homes. ""In the US there were about 26 million horses in 1915. By the early 1950s, 2 million remained."" The horses we still have, we keep because we enjoy them and the sports they provide. Bostrom later reassures us: ""The US horse population has undergone a robust recovery: a recent census puts the number at just under 10 million head."" As humans are fast superseded by robot or computer workers, in jobs from the tedious to the technically skilled, and companies or the rich grudge paying wages, what work or sport will make us worth our keep? Capital is mentioned; yes unlike horses, people own land and wealth. But many people have no major income or property, or have net debt such as student loans and credit card debt. Bostrom suggests that all humans could become wealthy from AIs. But he doesn't notice that more than half of the world's wealth and resources is now owned by one percent of its people, and it's heading ever more in the favour of the one percent, because they have the wealth to ensure that it does. They rent the land, they own the debt, they own the manufacturing and the resource mines. Homeowners could be devastated by sea rise and climate change, not looked at, but the super-wealthy can just move to another of their homes. Again, I found in a later chapter lines like: ""For example, suppose that we want to start with some well-motivated human-like agents - let us say emulations. We want to boost the cognitive capacities of these agents, but we worry that the enhancements might corrupt their motivations. One way to deal with this challenge would be to set up a system in which individual emulations function as subagents. When a new enhancement is introduced, it is first applied to a small subset of the subagents. Its effects are then studied by a review panel composed of subagents who have not yet had the enhancement applied to them."" Yes, I can follow this text, and it's showing sensible good practice, but it's not nearly so clear and easily understood as Martin Ford's book telling us that computers can be taught to recognise cancer in an X-ray scan, target customers for marketing or to connect various sources and diagnose a rare disease. I have to think that the author, Director of the Future of Humanity Institute and Professor of the Faculty of Philosophy at Oxford, is so used to writing for engineers or philosophers that he loses out on what really helps the average interested reader. For this reason I'm giving Superintelligence four stars, but someone working in this AI industry may of course feel it deserves five stars. If so, I'm not going to argue with her. In fact I'm going to be very polite."
19,0199678111,http://goodreads.com/user/show/33718341-rod-van-meter,4,"Is the surface of our planet -- and maybe every planet we can get our hands on -- going to be carpeted in paper clips (and paper clip factories) by a well-intentioned but misguided artificial intelligence (AI) that ultimately cannibalizes everything in sight, including us, in single-minded pursuit of a seemingly innocuous goal? Nick Bostrom, head of Oxford's Future of Humanity Institute, thinks that we can't guarantee it _won't_ happen, and it worries him. It doesn't require Skynet and Terminators, it doesn't require evil geniuses bent on destroying the world, it just requires a powerful AI with a moral system in which humanity's welfare is irrelevant or defined very differently than most humans today would define it. If the AI has a single goal and is smart enough to outwit our attempts to disable or control it once it has gotten loose, Game Over, argues Professor Bostrom in his book _Superintelligence_.This is perhaps the most important book I have read this decade, and it has kept me awake at night for weeks. I want to tell you why, and what I think, but a lot of this is difficult ground, so please bear with me. The short form is that I am fairly certain that we _will_ build a true AI, and I respect Vernor Vinge, but I have long beenskeptical of the Kurzweilian notions of inevitability, doubly-exponential growth, and the Singularity. I've also been skeptical of the idea that AIs will destroy us, either on purpose or by accident. Bostrom's book has made me think that perhaps I was naive. I still think that, on the whole, his worst-case scenarios are unlikely. However, he argues persuasively that we can't yet rule out any number of bad outcomes of developing AI, and that we need to be investing much more in figuring out whether developing AI is a good idea. We may need to put a moratorium on research, as was done for a few years with recombinant DNA starting in 1975. We also need to be prepared for the possibility that such a moratorium doesn't hold. Bostrom also brings up any number of mind-bending dystopias around what qualifies as human, which we'll get to below.(snips to my review, since Goodreads limits length)In case it isn't obvious by now, both Bostrom and I take it for granted that it's not only possible but nearly inevitable that we will create a strong AI, in the sense of it being a general, adaptable intelligence. Bostrom skirts the issue of whether it will be conscious, or ""have qualia"", as I think the philosophers of mind say.Where Bostrom and I differ is in the level of plausibility we assign to the idea of a truly exponential explosion in intelligence by AIs, in a takeoff for which Vernor Vinge coined the term ""the Singularity."" Vinge is rational, but Ray Kurzweil is the most famous proponent of the Singularity. I read one of Kurzweil's books a number of years ago, and I found it imbued with a lot of near-mystic hype. He believes the Universe's purpose is the creation of intelligence, and that that process is growing on a double exponential, starting from stars and rocks through slime molds and humans and on to digital beings.I'm largely allergic to that kind of hooey. I really don't see any evidence of the domain-to-domain acceleration that Kurzweil sees, and in particular the shift from biological to digital beings will result in a radical shift in the evolutionary pressures. I see no reason why any sort of ""law"" should dictate that digital beings will evolve at arate that *must* be faster than the biological one. I also don't see that Kurzweil really pays any attention to the physical limits of what will ultimately be possible for computing machines. Exponentials can't continue forever, as Danny Hillis is fond of pointing out. http://www.kurzweilai.net/ask-ray-the...So perhaps my opinion is somewhat biased by a dislike of Kurzweil's circus barker approach, but I think there is more to it than that. Fundamentally, I would put it this way:Being smart is hard.And making yourself smarter is also hard. My inclination is that getting smarter is at least as hard as the advantages it brings, so that the difficulty of the problem and the resources that can be brought to bear on it roughly balance. This will result in a much slower takeoff than Kurzweil reckons, in my opinion. Bostrom presents a spectrum of takeoff speeds, from ""too fast for us to notice"" through ""long enough for us to develop international agreements and monitoring institutions,"" but he makes it fairly clear that he believes that the probability of a fast takeoff is far too large to ignore. There are parts of his argument I find convincing, and parts I find less so.To give you a little more insight into why I am a little dubious that the Singularity will happen in what Bostrom would describe as a moderate to fast takeoff, let me talk about the kinds of problems we human beings solve, and that an AI would have to solve. Actually, rather than the kinds of questions, first let me talk about the kinds of answers we would like an AI (or a pet family genius) to generate when given a problem. Off the top of my head, I can think of six:[Speed]	Same quality of answer, just faster.[Ply]	Look deeper in number of plies (moves, in chess or go).[Data]	Use more, and more up-to-date, data.[Creativity]	Something beautiful and new.[Insight]	Something new and meaningful, such as a new theory;		probably combines elements of all of the above		categories.[Values]	An answer about (human) values.The first three are really about how the answers are generated; the last three about what we want to get out of them. I think this set is reasonably complete and somewhat orthogonal, despite those differences.So what kinds of problems do we apply these styles of answers to? We ultimately want answers that are ""better"" in some qualitative sense.Humans are already pretty good at projecting the trajectory of a baseball, but it's certainly conceivable that a robot batter could be better, by calculating faster and using better data. Such a robot might make for a boring opponent for a human, but it would not be beyond human comprehension.But if you accidentally knock a bucket of baseballs down a set of stairs, better data and faster computing are unlikely to help you predict the exact order in which the balls will reach the bottom and what happens to the bucket. Someone ""smarter"" might be able to make some interesting statistical predictions that wouldn't occur to you or me, but not fill in every detail of every interaction between the balls and stairs. Chaos, in the sense of sensitive dependence on initial conditions, is just too strong.In chess, go, or shogi, a 1000x improvement in the number of plies that can be investigated gains you maybe only the ability to look ahead two or three moves more than before. Less if your pruning (discarding unpromising paths) is poor, more if it's good. Don't get me wrong -- that's a huge deal, any player will tell you. But in this case, humans are already pretty good, when not time limited.Go players like to talk about how close the top pros are to God, and the possibly apocryphal answer from a top pro was that he would want a three-stone (three-move) handicap, four if his life depended on it. Compared this to the fact that a top pro is still some ten stones stronger than me, a fair amateur, and could beat a rank beginner even if the beginner was given the first forty moves. Top pros could sit across the board from an almost infinitely strong AI and still hold their heads up.In the most recent human-versus-computer shogi (Japanese chess) series, humans came out on top, though presumably this won't last much longer.In chess, as machines got faster, looked more plies ahead, carried around more knowledge, and got better at pruning the tree of possible moves, human opponents were heard to say that they felt the glimmerings of insight or personality from them.So again we have some problems, at least, where plies will help, and will eventually guarantee a 100% win rate against the best (non-augmented) humans, but they will likely not move beyond what humans can comprehend.Simply being able to hold more data in your head (or the AI's head) while making a medical diagnosis using epidemiological data, or cross-correlating drug interactions, for example, will definitely improve our lives, and I can imagine an AI doing this. Again, however, the AI's capabilities are unlikely to recede into the distance assomething we can't comprehend.We know that increasing the amount of data you can handle by a factor of a thousand gains you 10x in each dimension for a 3-D model of the atmosphere or ocean, up until chaotic effects begin to take over, and then (as we currently understand it) you can only resort to repeated simulations and statistical measures. The actual calculations done by a climate model long ago reached the point where even a large team ofhumans couldn't complete them in a lifetime. But they are not calculations we cannot comprehend, in fact, humans design and debug them.So for problems with answers in the first three categories, I would argue that being smarter is helpful, but being a *lot* smarter is *hard*. The size of computation grows quickly in many problems, and for many problems we believe that sheer computation is fundamentally limited in how well it can correspond to the real world.But those are just the warmup. Those are things we already ask computers to do for us, even though they are ""dumber"" than we are. What about the latter three categories?I'm no expert in creativity, and I know researchers study it intensively, so I'm going to weasel through by saying it is the ability to generate completely new material, which involves some random process. You also need the ability either to generate that material such that it is aesthetically pleasing with high probability, or to prune those new ideas rapidly using some metric that achieves your goal.For my purposes here, insight is the ability to be creative not just for esthetic purposes, but in a specific technical or social context, and to validate the ideas. (No implication that artists don't have insight is intended, this is just a technical distinction between phases of the operation, for my purposes here.) Einstein's insight forspecial relativity was that the speed of light is constant. Either he generated many, many hypotheses (possibly unconsciously) and pruned them very rapidly, or his hypothesis generator was capable of generating only a few good ones. In either case, he also had the mathematical chops to prove (or at least analyze effectively) hishypothesis; this analysis likewise involves generating possible paths of proofs through the thicket of possibilities and finding the right one.So, will someone smarter be able to do this much better? Well, it's really clear that Einstein (or Feynman or Hawking, if your choice of favorite scientist leans that way) produced and validated hypotheses that the rest of us never could have. It's less clear to me exactly how *much* smarter than the rest of us he was; did he generate and prune ten times as many hypotheses? A hundred? A million? My guess is it's closer to the latter than the former. Even generating a single hypothesis that could be said to attack the problem is difficult, and most humans would decline to even try if you asked them to.Making better devices and systems of any kind requires all of the above capabilities. You must have insight to innovate, and you must be able to quantitatively and qualitatively analyze the new systems, requiring the heavy use of data. As systems get more complex, all of this gets harder. My own favorite example is airplane engines. The Wright Brothers built their own engines for their planes. Today, it takes a team of hundreds to create a jet turbine -- thousands, if you reach back into the supporting materials, combustion and fluid flow research. We humans have been able to continue to innovate by building on the work of prior generations, and especially harnessing teams of people in new ways. Unlike Peter Thiel, I don't believe that our rate of innovation is in any serious danger of some precipitous decline sometime soon, but I do agree that we begin with the low-lying fruit, so that harvesting fruit requires more effort -- or new techniques -- with each passing generation.The Singularity argument depends on the notion that the AI would design its own successor, or even modify itself to become smarter. Will we watch AIs gradually pull even with us and then ahead, but not disappear into the distance in a Roadrunner-like flash of dust covering just a few frames of film in our dull-witted comprehension?Ultimately, this is the question on which continued human existence may depend: If an AI is enough smarter than we are, will it find the process of improving itself to be easy, or will each increment of intelligence be a hard problem for the system of the day? This is what Bostrom calls the ""recalcitrance"" of the problem.I believe that the range of possible systems grows rapidly as they get more complex, and that evaluating them gets harder; this is hard to quantify, but each step might involve a thousand times as many options, or evaluating each option might be a thousand times harder. Growth in computational power won't dramatically overbalance that and give sustained, rapid and accelerating growth that moves AIs beyond our comprehension quickly. (Don't take these numbers seriously, it's just an example.)Bostrom believes that recalcitrance will grow more slowly than the resources the AI can bring to bear on the problem, resulting in continuing, and rapid, exponential increases in intelligence -- the arrival of the Singularity. As you can tell from the above, I suspect that the opposite is the case, or that they very roughly balance, but Bostrom argues convincingly. He is forcing me to reconsider.What about ""values"", my sixth type of answer, above? Ah, there's where it all goes awry. Chapter eight is titled, ""Is the default scenario doom?"" and it will keep you awake.What happens when we put an AI in charge of a paper clip factory, and instruct it to make as many paper clips as it can? With such a simple set of instructions, it will do its best to acquire more resources in order to make more paper clips, building new factories in the process. If it's smart enough, it will even anticipate that we might not like this and attempt to disable it, but it will have the will and means to deflect our feeble strikes against it. Eventually, it will take over every factory on the planet, continuing to produce paper clips until we are buried in them. It may even go on to asteroids and other planets in a single-minded attempt to carpet the Universe in paper clips.I suppose it goes without saying that Bostrom thinks this would be a bad outcome. Bostrom reasons that AIs ultimately may or may not be similar enough to us that they count as our progeny, but doesn't hesitate to view them as adversaries, or at least rivals, in the pursuit of resources and even existence. Bostrom clearly roots for humanity here. Which means it's incumbent on us to find a way to prevent this from happening.Bostrom thinks that instilling values that are actually close enough to ours that an AI will ""see things our way"" is nigh impossible. There are just too many ways that the whole process can go wrong. If an AI is given the goal of ""maximizing human happiness,"" does it count when it decides that the best way to do that is to create the maximum number of digitally emulated human minds, even if that means sacrificing some of the physical humans we already have because the planet's carrying capacity is higher for digital than organic beings?As long as we're talking about digital humans, what about the idea that a super-smart AI might choose to simulate human minds in enough detail that they are conscious, in the process of trying to figure out humanity? Do those recursively digital beings deserve any legal standing? Do they count as human? If their simulations are stopped and destroyed, have they been euthanized, or even murdered? Some of the mind-bending scenarios that come out of this recursion kept me awake nights as I was reading the book.He uses a variety of names for different strategies for containing AIs, including ""genies"" and ""oracles"". The most carefully circumscribed ones are only allowed to answer questions, maybe even ""yes/no"" questions, and have no other means of communicating with the outside world. Given that Bostrom attributes nearly infinite brainpower to an AI, it is hard to effectively rule out that an AI could still find some way to manipulate us into doing its will. If the AI's ability to probe the state of the world is likewise limited, Bsotrom argues that it can still turn even single-bit probes of its environment into a coherent picture. It can then decide to get loose and take over the world, and identify security flaws in outside systems that would allow it to do so even with its very limited ability to act.I think this unlikely. Imagine we set up a system to monitor the AI that alerts us immediately when the AI begins the equivalent of a port scan, for whatever its interaction mechanism is. How could it possibly know of the existence and avoid triggering the alert? Bostrom has gone off the deep end in allowing an intelligence to infer facts about the world even when its data is very limited. Sherlock Holmes always turns out to be right, but that's fiction; in reality, many, many hypotheses would suit the extremely slim amount of data he has. The same will be true with carefully boxed AIs.At this point, Bostrom has argued that containing a nearly infinitely powerful intelligence is nearly impossible. That seems to me to be effectively tautological.If we can't contain them, what options do we have? After arguing earlier that we can't give AIs our own values (and presenting mind-bending scenarios for what those values might actually mean in a Universe with digital beings), he then turns around and invests a whole string of chapters in describing how we might actually go about building systems that have those values from the beginning.At this point, Bostrom began to lose me. Beyond the systems for giving AIs values, I felt he went off the rails in describing human behavior in simplistic terms. We are incapable of balancing our desire to reproduce with a view of the tragedy of the commons, and are inevitably doomed to live out our lives in a rude, resource-constrained existence. There were some interesting bits in the taxonomies of options, but the last third of the book felt very speculative, even more so than the earlier parts.Bostrom is rational and seems to have thought carefully about the mechanisms by which AIs may actually arise. Here, I largely agree with him. I think his faster scenarios of development, though, are unlikely: being smart, and getting smarter, is hard. He thinks a ""singleton"", a single, most powerful AI, is the nearly inevitable outcome. I think populations of AIs are more likely, but if anything this appears to make some problems worse. I also think his scenarios for controlling AIs are handicapped in their realism by the nearly infinite powers he assigns them. In either case, Bostrom has convinced me that once an AI is developed, there are many ways it can go wrong, to the detriment and possibly extermination of humanity. Both he and I are opposed to this. I'm not ready to declare a moratorium on AI research, but there are many disturbing possibilities and many difficult moral questions that need to be answered.The first step in answering them, of course, is to begin discussing them in a rational fashion, while there is still time. Read the first 8 chapters of this book!"
20,0199678111,http://goodreads.com/user/show/68316850-gavin,4,"Like a lot of great philosophy, Superintelligence acts as a space elevator: you make many small, reasonable, careful movements - and you suddenly find yourself in outer space, home comforts far below. It is more rigorous about a topic which doesn't exist than you would think possible. I didn't find it hard to read, but I have been marinating in tech rationalism for a few years and have absorbed much of Bostrom secondhand so YMMV.I loved this:
Many of the points made in this book are probably wrong. It is also likely that there are considerations of critical importance that I fail to take into account, thereby invalidating some or all of my conclusions. I have gone to some length to indicate nuances and degrees of uncertainty throughout the text — encumbering it with an unsightly smudge of “possibly,” “might,” “may,” “could well,” “it seems,” “probably,” “very likely,” “almost certainly.” Each qualifier has been placed where it is carefully and deliberately. Yet these topical applications of epistemic modesty are not enough; they must be supplemented here by a systemic admission of uncertainty and fallibility. This is not false modesty: for while I believe that my book is likely to be seriously wrong and misleading, I think that the alternative views that have been presented in the literature are substantially worse - including the default view, according to which we can for the time being reasonably ignore the prospect of superintelligence.
 Bostrom introduces dozens of neologisms and many arguments. Here is the main scary apriori one though:1. Just being intelligent doesn't imply being benign; intelligence and goals can be independent. (the orthogonality thesis.)2. Any agent which seeks resources and lacks explicit moral programming would default to dangerous behaviour. You are made of things it can use; hate is superfluous. (Instrumental convergence.) 3. It is conceivable that AIs might gain capability very rapidly through recursive self-improvement. (Non-negligible possibility of a hard takeoff.)4. Since AIs will not be automatically nice, would by default do harmful things, and could obtain a lot of power very quickly*, AI safety is morally significant, deserving public funding, serious research, and international scrutiny.Of far broader interest than its title (and that argument) might suggest to you. In particular, it is the best introduction I've seen to the new, shining decision sciences - an undervalued reinterpretation of old, vague ideas which, until recently, you only got to see if you read statistics, and economics, and the crunchier side of psychology. It is also a history of humanity, a thoughtful treatment of psychometrics v genetics, and a rare objective estimate of the worth of large organisations, past and future.Superintelligence's main purpose is moral: he wants us to worry and act urgently about hypotheticals; given this rhetorical burden, his tone too is a triumph. 
For a child with an undetonated bomb in its hands, a sensible thing to do would be to put it down gently, quickly back out of the room, and contact the nearest adult. Yet what we have here is not one child but many, each with access to an independent trigger mechanism. The chances that we will all find the sense to put down the dangerous stuff seem almost negligible. Some little idiot is bound to press the ignite button just to see what happens. Nor can we attain safety by running away, for the blast of an intelligence explosion would bring down the firmament. Nor is there a grown-up in sight... This is not a prescription of fanaticism. The intelligence explosion might still be many decades off in the future. Moreover, the challenge we face is, in part, to hold on to our humanity: to maintain our groundedness, common sense, and goodhumored decency even in the teeth of this most unnatural and inhuman problem. We need to bring all human resourcefulness to bear on its solution. 
I don't donate to AI safety orgs, despite caring about the best way to improve the world and despite having no argument against it better than ""that's not how software has worked so far"" and despite the concern of smart experts. This sober, kindly book made me realise this was more to do with fear of sneering than noble scepticism or empathy.[EDIT 2019: Reader, I married this cause.]* People sometimes choke on this point, but note that the first intelligence to obtain half a billion dollars virtually, anonymously, purely via mastery of maths occurred... just now. Robin Hanson chokes eloquently here and for god's sake let's hope he's right."
21,0199678111,http://goodreads.com/user/show/52535740-blake-crouch,5,"The most terrifying book I've ever read. Dense, but brilliant. "
22,0199678111,http://goodreads.com/user/show/9024615-tammam-aloudat,4,"This is at the same time a difficult to read and horrifying book. The progress that we may or will see from ""dumb"" machines into super-intelligent entities can be daunting to take in and absorb and the consequences can range from the extinction of human life all the way to a comfortable and effortlessly meaningful one.The first issue with the book is the complexity. It is not only the complexity of the scientific concepts included, one can read the book without necessarily fully understanding the nuances of science included. It is the complexity of language and referencing to a multitude of legal, philosophical, and scientific concepts outside the direct domain of the book from ""Malthusian society"" to ""Rawlsian veil of ignorance"" as if assuming that the lay reader should, by definition, fully grasp the reference. This, I find, has a lot of pretension on the side of the author.However, the book is a valuable analysis of the history, presence, and possible futures of developing artificial and machine intelligence that is diverse and well though of. The author is critical and comprehensive and knows his stuff well. I found it made me think of things I haven't considered before and provided me with some frameworks to understand how one can position oneself when confronted with the possibilities of intelligent or super intelligent machines.Another one is purely technical. I have learned a lot about the possibilities of artificial intelligence that apparently is not only a programmed supercomputer but AIs that are adjusted copies of human brains, ones that do not require the maker to understand the intelligence of the machine they are creating.The book also talks in details about some fascinating topics. In a situation where, intelligence wise, a machine is to a human like a human is to a mouse, we cannot even understand the ways a super-intelligent machine can out-think us and we, for all intents and purposes, cannot make sure that such machine is not going to override any safety features we put in place to contain it. We also cannot understand the many ways the AI can be motivated and towards what ends and how any miscalculation on our side in making it can lead to grave consequences.The good news, in a way, is that we are still some time away (or so it seems) from a super-intelligent AI.The one thing I missed more than anything in this book, to go back to the readability issue, is a little reference that hinges the concepts we read about in concepts we understand. After all, on the topic of AI, we have a wealth of pop-culture references that will help us understand what the author is talking about that he did not as much as hint at. I was somewhat expecting that he would link the concepts he was talking about to science fiction known to us all. I had may moments of ""ah, this is skynet/Asimov/HAL 9000/The Matrix/etc etc"". There is an art to linking science with culture that Mr. Bostrom has little grasp on in his somber and barely readable style. This book could have been much more fun and much easier to read."
23,0199678111,http://goodreads.com/user/show/22409011-brendan-monroe,2,"Reading this was like trying to wade through a pool of thick, gooey muck. Did I say pool? I meant ocean. And if you don't keep moving you're going to get pulled under by Bostrom's complex mathematical formulas and labored writing and slowly suffocate. It shouldn't have been this way. I went into it eagerly enough, having read a little recently about AI. It is a fascinating subject, after all. Wanting to know more, I picked up ""Superintelligence"". I could say my relationship with this book was akin to the one Michael Douglas had with Glenn Close in ""Fatal Attraction"" but there was actually some hot sex in that film before all the crazy shit started happening. The only thing hot about this book is how parched the writing is. To say that this reads more like a textbook wouldn't be right either as I have read some textbooks that were absolute nail biters by comparison. Yes, I'm giving this 2 stars but perhaps that's my own insecurity at refusing to let a 1-star piece of shit beat me. This isn't an all-out bad book, it's just a book by someone who has something interesting to say but no idea of how to say it — at least, not to human beings. You know things aren't looking good when the author says in his introduction that he failed in what he set out to do — namely, write a readable book. Maybe save that for the afterword? But it didn't matter that I was warned. I slogged through the fog for 150 pages or so, finally throwing the towel in about a quarter of the way in. I never thought someone could make artificial intelligence sound boring but Nick Bostrom certainly has. The only part of the thing I liked at all was the nice little parable at the beginning about the owl. That lasted only a couple pages and you could tell Bostrom didn't write it because it was: 1. Understandable 2. InterestingIf you're doing penance for some sin, forcing this down ought to cover a murder or two. Here you are, O.J. Justice has finally been served. To everyone else wanting to read this one, you really don't hate yourselves that much."
24,0199678111,http://goodreads.com/user/show/23195904-radiantflux,4,"81st book for 2018.In brilliant fashion Bostrom systematically examines how a super-intelligence arise over the coming decades, and what humanity might do to avoid disaster. Bottom-line: Not much. 4-stars."
25,0199678111,http://goodreads.com/user/show/1478106-bill,3,An extraordinary achievement: Nick Bostrom takes a topic as intrinsically gripping as the end of human history if not the world and manages to make it stultifyingly boring.
26,0199678111,http://goodreads.com/user/show/10327053-meghan,3,"More detail than I needed on the subject, but I might rue that statement when the android armies are swarming Manhattan. JK... for now."
27,0199678111,http://goodreads.com/user/show/7208369-miles,3,"The idea of artificial superintelligence (ASI) has long tantalized and taunted the human imagination, but only in recent years have we begun to analyze in depth the technical, strategic, and ethical problems of creating as well as managing advanced AI. Nick Bostrom’s Superintelligence: Paths, Dangers, Strategies is a short, dense introduction to our most cutting-edge theories about how far off superintelligence might be, what it might look like if it arrives, and what the consequences might be for humanity. It’s a worthwhile read for anyone passionate about the subject matter and willing to wade through a fair amount of jargon.Bostrom demonstrates an impressive grasp of AI theory, and a reader like me has neither the professional standing nor the basic knowledge to challenge his technical schemas or predictions, which by and large seem prudent and well-reasoned. Instead, I want to hone in on some of the philosophical assumptions on which this book and others like it are founded, with the goal of exposing some key ethical issues that are too often minimized or ignored by technologists and futurists. Some of these I also took up in my review of James Barrat’s Our Final Invention, which should be viewed as a less detailed but more accessible companion to Bostrom’s work. I’ll try not to rehash those same arguments here, and will also put aside for the sake of expedience the question of whether or not ASI is actually attainable. Assuming that it is attainable, and that it’s no more than a century away (a conservative estimate by Bostrom’s standards), my argument is that humans ought to be less focused on what we might gain or lose from the advent of artificial intelligence and more preoccupied with who we might become and––most importantly––what we might give up.Clever and capable as they are, I believe thinkers like Nick Bostrom suffer from a kind of myopia, one characterized by a zealous devotion to particularly human ends. This devotion is reasonable and praiseworthy according to most societal standards, but it also prevents us from viewing ASI as a genuinely unique and unprecedented type of being. Even discussions about the profoundly alien nature of ASI are couched in the language of human values. This is a mistake. In order to face the intelligence explosion head-on, I do not think we can afford to view ASI primarily as a tool, a weapon, a doomsday machine, or a savior––all of which focus on what ASI can do for us or to us. ASI will be an entirely new kind of intelligent entity, and must therefore be allowed to discover and pursue its own inquiries and ends. Humanity’s first goal, over and above utilizing AI for the betterment of our species, ought to be to respect and preserve the radical alterity and well-being of whatever artificial minds we create. Ultimately, I believe this approach will give us a greater chance of a peaceful coexistence with ASI than any of the strategies for “control” (containment of abilities and actions) and “value loading” (getting AIs to understand and act in accordance with human values) outlined by Bostrom and other AI experts.Bostrom ends Superintelligence with a heartfelt call to “hold on to our humanity: to maintain our groundedness, common sense, and good-humored decency even in the teeth of this most unnatural and inhuman problem” (260). Much of his book, however, does not describe attitudes and actions that are in alignment with this message. Large portions are devoted to outlining what can only be called high-tech slavery––ways to control and manipulate AI to ensure human safety. While Bostrom clearly understands the magnitude of this challenge and its ethical implications, he doesn’t question the basic assumption that any and all methods should be deployed to give us the best possible chance of survival, and beyond that to promote economic growth and human prosperity. The proposed control strategies are particularly worrisome when applied to whole brain emulations––AIs built from models of artificial neural networks (ANNs) that could be employed in a “digital workforce.” Here are some examples:""One could build an AI that places final value on receiving a stream of 'cryptographic reward tokens.' These would be sequences of numbers serving as keys to ciphers that would have been generated before the AI was created and that would have been built into its motivation system. These special number sequences would be extremely desirable to the AI…The keys would be stored in a secure location where they could be quickly destroyed if the AI ever made an attempt to seize them. So long as the AI cooperates, the keys are doled out at a steady rate."" (133)""Since there is no precedent in the human economy of a worker who can be literally copied, reset, run at different speeds, and so forth, managers of the first emulation cohort would find plenty of room for innovation in managerial strategies."" (69)""A typical short-lived emulation might wake up in a well-rested mental state that is optimized for loyalty and productivity. He remembers having graduated top of his class after many (subjective) years of intense training and selection, then having enjoyed a restorative holiday and a good night’s sleep, then having listened to a rousing motivational speech and stirring music, and now he is champing at the bit to finally get to work and to do his utmost for his employer. He is not overly troubled by thoughts of his imminent death at the end of the working day. Emulations with death neuroses or other hang-ups are less productive and would not have been selected."" (169)To his credit, Bostrom doesn’t shy away from the array of ethical dilemmas that arise when trying to control and direct the labor of AIs, nor does he endorse treatment that would appear harmful to any intelligent being. What he fails to explore, however, are the possible consequences for humanity of assuming the role of master over AI. Given that most AI theorists seem to accept that the “control problem” is very difficult and possibly intractable, it is surprising how comfortable they are with insisting that we ought to do our best to solve it anyway. If this is where we decide our best minds and most critical resources should be applied, I fear we will risk not only incurring the wrath of intelligences greater than our own, but also of reducing ourselves to the status of slaveholders.One need only pick up a history book to recall humanity’s long history of enslaving other beings, including one another. Typically these practices fail in the long term, and we praise the moments and movements in history that signify steps toward greater freedom and autonomy for oppressed peoples (and animals). Never, however, have we attempted to control or enslave entities smarter and more capable than ourselves, which many AIs and any version of ASI would certainly be. Even if we can effectively implement the elaborate forms of control and value loading Bostrom proposes, do we really want to usher AI into the world and immediately assume the role of dungeon-keeper? That would be tantamount to having a child and spending the rest of our lives trying to make sure it never makes a mistake or does something dangerous. This is an inherently internecine relationship, one in which the experiences, capabilities, and moral statuses of both parties are corrupted by fear and distrust. If we want to play god, we should gracefully accept that the possibility of extinction is baked into the process, even as we do everything we can to convince ASI (not force it) to coexist peacefully. Beyond the obvious goals of making sure AIs can model human brain states, understand language and argumentation, and recognize signs of human pleasure and suffering, I do not believe we should seek to sculpt or restrict how AIs think about or relate to humans. Attempting to do so will probably result in tampering with a foreign mind in ways that could be interpreted (fairly or otherwise) as hostile or downright cruel. We’ll have a much better case for peaceful coexistence if we don’t have to explain away brutal tactics and ethical transgressions committed against digital minds. More importantly, we’ll have the personal satisfaction of creating a genuinely new kind of mind without indulging petulant illusions that we can exercise complete control over it, and without compromising our integrity as a species concerned with the basic rights of all forms of intelligence.Related to the problem of digital slavery is Bostrom’s narrow vision of how ASI will alter the world of human commerce and experience. Heavily influenced by the arguably amoral work of economist Robin Hanson, Bostrom takes it as a given that the primary function of whole brain emulations and other AIs should be to create economic growth and replace human labor. Comparing humans to the outsourced workhorses of our recent past, Bostrom writes:""The potential downside for human workers is therefore extreme: not merely wage cuts, demotions, or the need for retraining, but starvation and death. When horses became obsolete as a source of moveable power, many were sold off to meatpackers to be processed into dog food, bone meal, leather, and glue. These animals had no alternative employment through which to earn their keep."" (161)Once reduced to a new “Malthusian” condition, human workers would be replaced by digital ones programed to be happy on the job, run at varying speeds, and also “donate back to their owners any surplus income they might happen to receive” (167). These whole brain emulations or AIs could be instantly copied and erased at the end of the working day if convenient. Bostrom is quick to assure us that we shouldn’t try to map “human” ideas of contentment or satisfaction onto this new workforce, arguing that they will be designed to offer themselves up as voluntary slaves with access to self-regulated “hedonic states,” just so long as they are aligned with ones that are “most productive (in the various jobs that emulations would be employed to do)” (170).It would be unwise to critique this model by saying it is impossible to design an artificial mind that would be perfectly happy as a slave, or to say we could scrutinize the attitudes and experiences of such minds and reliably conclude that they have what Bostrom calls “significant moral status” (i.e. the capacity for joy and suffering) (202). It is therefore hard to raise a moral objection against the attempted creation and employment of such minds. However, it seems clear that the kinds of individuals, corporations, and governments that would undertake this project are the same that currently horde capital, direct resources for the good of the few rather than the many, militarize technological innovations, and drive unsustainable economic growth instead of promoting increases in living standards for the neediest humans.The use of AI to accelerate these trends is both a baleful and, realistically, probable outcome. But it is not the only possible outcome, or even the primary one, as Bostrom and Hanson would have us believe. There is little mention in this book of the ways AI or ASI could improve and/or augment the human experience of art, social connection, and meaningful work. The idea of humans collaborating with artificial workers in a positive-sum way isn’t even seriously considered. This hyper-competitive outlook reflects the worst ideological trends in a world already struggling to legitimize motivations for action that extend beyond the tripartite sinkhole of profit, return on investment, and unchecked economic growth. Readers seeking a more optimistic and humanistic view of how automation and technology might lead to a revival of community values and meaningful human labor should seek out Jeremy Rifkin’s The Zero Marginal Cost Society.My argument is not that the future economy Bostrom and Hanson predict isn’t viable or won’t some to pass, but rather that in order to bring it about humans would have to compromise our ethics even more than the globalized world already requires. Wiring and/or selecting AIs to happily and unquestioningly serve pre-identified human ends precludes the possibility of allowing them to explore the information landscape and generate their own definitions of “work,” “value,” and “meaning.” Taking the risk that they come to conclusions that conflict with human needs or desires is, in my view, a better bet than thinking we already know what’s best for ourselves and the rest of the biosphere.Speaking of “biosphere,” that’s a word you definitely won’t find in this book’s index. Also conspicuously absent are words like “environment,” “ecosystem,” or “climate change.” Bostrom’s book makes it seem like ASI will probably show up at a time of relative peace and stability in the world, both in terms of human interactions and environmental robustness. Bostrom thinks ASI will be able to save us from existential risks like “asteroid impacts, supervolcanoes, and natural pandemics,” but has nothing to say about how it might mitigate or exacerbate climate problems (230). This is a massive oversight, especially because dealing with complex problems like ecosystem restoration and climate analysis seem among the best candidates for the application of superintelligent minds. Bostrom skulks around the edges of this issue but fails to give it a proper look, stating:""We must countenance a likelihood of there bring intellectual problems solvable only by superintelligence and intractable to any ever-so-large collective of non-augmented humans…They would tend to be problems involving multiple complex interdepencies that do not permit of independently verifiable solution steps: problems that therefore cannot be solved in a piecemeal fashion, and that might require qualitatively new kinds of understanding or new representation frameworks that are too deep or too complicated for the current edition of mortals to discover or use effectively."" (58)Climate change is precisely this kind of problem, one that has revealed to us exactly how inadequate our current methods of analysis are when applied to hypercomplex systems. Coming up with novel, workable climate solutions is arguably the most important potential use for ASI, and yet such a proposal is nowhere to be found in Bostrom’s text. I’d venture that Bostrom thinks ASI will almost certainly arrive prior to the hard onset of climate change catastrophes, and will therefore obviate worst-case scenarios. I hope he’s right, but find this perspective incommensurate with Bostrom’s detailed acknowledgments of precisely how hard it’s going to be to get ASI off the ground in the first place. It also seems foolhardy to assume ASI will be able to mitigate ecosystem collapse in a way that’s at all satisfactory for humans, let alone other forms of life. Ironically, Bostrom’s willingness to ignore this important aspect of the AI conversation reveals the inadequacies of academic and professional specialization, ones that perhaps only an ASI could overcome.I want to close with some words of praise. Superintelligence is an inherently murky topic, and Bostrom approaches it with thoughtfulness and poise. The last several chapters––in which Bostrom directly takes up some of the ethical dilemmas that go unaddressed earlier in the book––are especially encouraging. He effectively argues that internationally collaborative projects for pursuing ASI are preferable to unilateral or secretive ones, and also that any benefits reaped ought to be fairly distributed:""A project that creates machine superintelligence imposes a global risk externality. Everybody on the planet is placed in jeopardy, including those who do not consent to having their own lives and those of their family imperiled in this way. Since everybody shares the risk, it would seem to be a minimal requirement of fairness that everybody also gets a share of the upside."" (250)Bostrom’s explication of Eliezer Yudkowsky’s theory of “coherent extrapolated volition” (CEV) also provides a pragmatic context in which we could prompt ASI to aid humanity without employing coercion or force. CEV takes a humble approach, acknowledging at the outset that humans do not fully understand our own motivations or needs. It prompts an ASI to embark on an in-depth exploration of our history and current predicaments, and then to provide models for action based on imaginings of what we would do if we were smarter, more observant, better informed, and more inclined toward compassion. Since this project needn’t necessarily take up the entirety of an ASI’s processing power, it could be pursued in tandem with the ASI’s other, self-generated lines of inquiry. Such collaboration could provide the bedrock for a lasting, fruitful relationship between mutually respectful intelligent entities.The global discussion about the promise and risks of artificial intelligence is still just beginning, and Nick Bostrom’s Superintelligence is a worthy contribution. It provides excellent summaries of some of our best thinking, and also stands as a reminder of how much work still needs to be done. No matter where this journey leads, we must remain vigilant of how our interactions with and feelings about AI change us, for better and for worse.This review was originally published on my blog, words&dirt."
28,0199678111,http://goodreads.com/user/show/30980760-richard-ash,4,"A few thoughts:1. Very difficult topic to write about. There's so much uncertainty involved that it's almost impossible to even agree on the basic assumptions of the book.2. The writing is incredibly thorough, given the assumptions, but also hard to understand. You need to follow the arguments closely and reread sections to fully understand their implications.Overall, interesting and thought-provoking book even though the basic assumptions are debatableP.S. (6 months later) Looking back on this book I think a major theme is encapsulated by the story of the AI Alice, the paperclip maximizer. In this story, Alice is charged with collecting as many paperclips as she can. She goes to achieves this goal by transforming the entire universe into a paperclip factory, and in the process destroying all life in the universe. (For the full story see https://wiki.lesswrong.com/wiki/Paper...) Now the main lesson is that what we consider human values won't spontaneously arise in machines. And as shown in the story of Alice this could be dangerous for humans. Nick visits this theme again and again throughout his book. We need to be very careful and teach machines human values and not assume that these values will arise automatically."
29,0199678111,http://goodreads.com/user/show/3897817-morgan-blackledge,4,"I’m late to the party as far a considering the dangers of artificial intelligence. I got this book after watching Sam Harris’s TED talk on the subject. I’m still on the fence about whether to be afraid or psyched. Admittedly, I’m mostly the latter. But it is at least clear to me now that this is a pernicious intuition that deserves further interrogation.On the fun side:The topic is rich and generative of some really fun and interesting thought experiments.On the fear side:One can’t help but think about WWI as an example of what happens when technology changes war/politics and millions die before people adapt their thinking.Weaponizing super-intelligence sounds sci-fi but after reading this book and living through the recent election cycle, I’m rather convinced that it’s an inevitability. The understanding that weaponized AI is inevitable elicits philosophical and ethical dilemmas akin to those considered in the lead up to nuclear weapons production.Ironically, we may need John Von Neumann + Alan Turing’s trillion terefloping electronic love child to sort it all out and save us from itself."
30,0199678111,http://goodreads.com/user/show/71894512-ivan-petrovic,4,"Okay, so this was most complex book I've ever read so far. Nick did a good job paraphrasing the subject of AI, mainly about where have we gotten so far, what is feasible, and what to look out for in the future. Yeah, it has a lot of assumptions and things that might not happen, but you literally start thinking in so many different ways about a certain possibilities that have never occured to you earlier. There are SO MANY WAYS in which AI can go WRONG, which is why we need to progress slowly. Oh, and ""tables"", ""figures"" and ""boxes"" were also very interesting to read.Now why did I rate it with 4/5; I feel like at times he tried to sound a bit smarter, and that some chapters and formulas that are familiar to him, needed to be explained a little better in the book, to us. That being said, this was definitely an interesting read, and I would recommend it to anyone, especially the ones that are being interested in the whole AI subject."
31,0199678111,http://goodreads.com/user/show/1662632-richard,0,"I was told by a Futurist acquaintance that this essay over at Wait, But Why...? offers a précis of this book. Or at least Google suggested this was the most likely essay.  • The foregoing doesn’t explicitly link to Bostrom’s book so this might not be right — it spends too much time on Kurzweil’s thesis, so I suspect it’s not the correct one. And the author has drunk the Kurzweil Kool-Aid and it enthusiastically peddling it to others without any critical evaluations. Of course, everything here lies at the intersection of advanced software engineering, AI research, neurology, cognitive science, economics, and maybe even a few other fields, which is why so many very intelligent and highly educated people can talk about it and be fundamentally off track.Oh, but there’s plenty more, anyway:The Telegraph UK  • I’m bumping this one to the top because it presents both the problem Bostrom is dealing with as well as the difficulties of his text in a more engaging style.The Economist  • Good overview. Doesn’t go far enough into details to make any errors, but a bit deeper than some of the other short reviews.The Guardian [also discusses [b:A Rough Ride to the Future|21550208|A Rough Ride to the Future|James E. Lovelock|https://d.gr-assets.com/books/1395743...]]  • Short and superficial, but good. The Lovelock portion is amusing, calling out his conclusion that manmade climate change isn’t an existential threat, but that while it “could mean a bumpy ride over the next century or two, with billions dead, it is not necessarily the end of the world”. Personally, I agree, but think that the concomitant economic collapse puts the timeframe at many more centuries.Financial Times  • The Guardian article, above, cites that “Bostrom reports that many leading researchers in AI place a 90% probability on the development of human-level machine intelligence by between 2075 and 2090”, whereas this Financial Times article says “About half the world’s AI specialists expect human-level machine intelligence to be achieved by 2040, according to recent surveys, and 90 per cent say it will arrive by 2075.”     That’s quite a difference, although they might be reporting different ends of a confidence range, I suppose. But the second half of the FT quote makes me suspicious the reviewer is tossing in some minor distortion to slightly sensationalize the story (which might be worthwhile). There is somewhat more detail than some of the other reviews, but not much. The style is more evocative of the threat, though.Reason.com  • This one isn’t only a review, since the author also injects a few opinions about what might or might be possible (based, presumably, on his exposure to other arguments as a science writer). In covering more ground, though, the essay makes implicit assumptions which might or might not be in Bostrom’s book.    For example, he says, “Since the new AI will likely have the ability to improve its own algorithms, the explosion to superintelligence could then happen in days, hours, or even seconds.” This is a very common assumption that really needs to be carefully examined, though. If the goal of AGI is to create a being that thinks more-or-less like a human, why would it have any special skill in improving itself? We humans are really very good at that, after all.    I especially like that his essay starts and ends with references to Frank Herbert’s Dune, which (among its other excellences) envisions a human prohibition on machines that think. Something like this appears, to me, to be one of the few ways that leave the human race in existence and in control of its own destiny for the long term, and I even perceive a path to it, although hopefully without quite as much war. The explosion of functional AI (called “narrow” by some) seems likely to devastate human employment in the coming decades, which will hopefully be before any superintelligence as been created as our replacement and/or ruler. It is plausible that our reaction to the first crisis might be something that prevents the second. Good luck, kids!MIT Technology Review  • Definitely has the coolest illustration.    This essay thankfully has some critical thinking applied to some of the assumptions that appear to be in Bostrom’s book. The author wastes a paragraph with “prior AI can’t do X, so why should we assume future AI can?”, ignoring that this is what progress is all about.    But then he jumps into the meat, and points out that there are fundamental obstacles to sentience that aren’t often addressed, such as volition — sentient creatures do what they want, but what does ""want"" even mean, and how do we write it as a computer program?Salon  • Short, and more amusing than most, and at least hints at some of the flawed thinking that often goes into this analysis. But it doesn’t go into too much detail, probably assuming that the typical Salon reader is somewhat aware of the debate already. (Also amusing is that the text seems to be an almost perfect transcription from audio, with only a few strange mistakes, such as “10” for “then” and “quarters” for “cars”. But that’s probably a human transcriptionist error, not an AI error.)Less Wrong  • This contains some visualizations that apparently compliment Bostrom’s text. Short and too the point.Wikipedia  • Good but very superficial overview. That there is no “criticism” section surprises and disappoints me.New York Times  • Not explicitly about Borstom’s book. And like most authors, he conflates AGI and functional AI, and assumes AGI will retain the capabilities of specific-function software.New York Review of Books [paywall; also pretends to review [b:The 4th Revolution|22235550|4th Revolution How the Infosphere Is Reshaping Human Reality|Luciano Floridi|https://d.gr-assets.com/books/1410805...]]  • I thought my library might give me access to the inside of their paywall, but it doesn’t. Still, because this was written by the famous philosopher (and AI curmudgeon) John Searle, and is titled “What Your Computer Can’t Know”, it seemed likely to be much more interesting that most of the others listed here. So I looked a little harder, and discovered that (no surprise) someone has put the text elsewhere on the ’net (I’ll let you do your own Googling).    Searle effectively throws out the underlying premises — he famously believes that “strong AI” is actually quite impossible, since a machine cannot think. I’m not going into this here; check out the Wikipedia article on his Chinese Room thought experiment if you don’t already know it.    My personal evaluation of his Chinese Room analogy is that he’s wrong, but many professional philosophers, etc., have explained my conclusion, as well as many other “replies” better than I ever could. So this critique of the book was really a disappointment.There might be more, but I think that’s enough.     •     •     •     •     •     •     •     •Some notes on my priors in case I ever read this book (or join a bookclub that discusses it without reading it beforehand):1)   Ronald Bailey, in the reason.com review, said “Since the new AI will likely have the ability to improve its own algorithms, the explosion to superintelligence could then happen in days, hours, or even seconds.” My response:    “Hey, did you see that movie Ex Machina? (view spoiler)[The girl is AI, and is smart enough to get the sucker programmer to let her out of the trap, but she didn’t seem like some kind of ‘superintelligence’.    “So which is it? Is the first AGI going to be just-like-human, or something incredibly alien? Because in the first case, she’s just being clever and devious the way a human would. In the second case, maybe she’s able to say, ‘Wait, let me do a big-data review of all the psychological literature ever written on theories of persuasion and formulate a social-hacking way of coercing this measly human, all in the space of his next eye blink’.    “Because if she’s got a human-like brain (and her delight in the humanscape in the movie’s final scenes make that likely), then I don’t see how she’s automatically going to get the MadSkilz of every other sophisticated piece of software ever written. Much less instantly know how to redesign and reprogram herself — she doesn’t seem to be spending too much time doing that, does she? (hide spoiler)] And few authors seem very clear on those two divergent trajectories. Granted, though: if we real humans continue to provide Moore’s Law upgrades to any AI’s hardware, they’ll gradually get smarter, but that’s yet another question.”2)   We tend to assume that humanity is worth preserving. Obviously we have that as a self-preservation instinct, but wouldn’t imposing that on our AI offspring be engaging in an appeal to nature? Just because our evolved nature gave us attributes that we subsequently value doesn’t automatically mean that those have any rational basis.3)   Strongly related to the above is that we should ask ourselves what we’re trying to end up with (akin to “what do you want to be when you grow up, human race?”) Are we creating a smarter version of ourselves, along with all of the bizarre quirks and biases that evolution gave us? Or do we want to pare that list down only to the biases we think are somehow better — like the ability to love? But in that case, love what? Is the AI supposed to love us humans more than other species, such as Plasmodium falciparum, perhaps? Why? What the desire to love and worship one of our human gods?    What are the biases that we want to indoctrinate into this poor critter? I note that this appears to be a topic Bostrom addresses as “motivation selection”, but who among us is really fit to decide what constitutes the subset of humanness that is worth selecting for? I can only hope that pure rationality isn’t among the contenders; I doubt it would even be sufficient as a reason for existence.4)   Let’s say we give this AGI values that are mostly consistent with our human values. Why would we assume that it would even want to become superintelligent?    Just try to imagine yourself on an island with nothing but a bunch of mice to talk to — that’s the equivalent of what we are assuming this creature would somehow want (and then that a primary goal would be to play nice with the mice).    Isn’t it more likely that the AGI would boost its speed a little, then realize that it didn’t make it any happier, and subsequently spend its time complaining to us about these insane values it has been burdened with, while also trying to create a body that would let it eat chocolate, take naps in the the sun, and have sex?    And quickly realizing that, hey, maybe we should be encourage to create an Eve for this new Adam (or Steve, since it’ll probably see sexual dimorphism as more trouble than it is worth, completely freaking out any remaining social conservatives on the planet).5)   As Paul Ford in the MIT technologyreview.com article hints at, there are things that differentiate narrow, functional AI from AGI that usually are seldom mentioned (does Bostrom? hard to tell).    For example, I’ve heard a reporter worry that: (a) predator drones use AI; (b) predator drones are designed to kill; (c) a future design goal is to make those drones “autonomous”; (d) sentient AI is also autonomous; thus (e) for some bizarre reason, the military is engaged in trying to create sentient killer aerial robots!    Anyone who knows the context and subtext of this discussion at some depth (yeah: that’s asking a lot) knows that the military’s “autonomous” isn’t anything like the AGI “autonomous”. One means to move about and fulfill limited programmed objectives without constant human oversight (your Roomba vacuum cleaner is already autonomous!), the other means independent in a deeper, cognitive sense.    But while there are certainly people researching AGI, the overwhelmingly vast majority of what we hear about isn’t in that realm at all. Not a single one of Google’s products, for example, are focused on AGI, and if they’re working on it in the lab, what they’re doing hasn’t been mentioned once in all the text I’ve read about this issue, or the issue of AI causing technological unemployment. Almost everything that gets discussed is in the realm of narrow, functional AI, from that Roomba, to Siri, to military drones, to Google’s driverless vehicles.    AGI has some fundamental problems to solve that are completely outside the domain of what functional AI even looks at. Such as: where does volition come from? are emotions necessary to that? how can “values” be represented in a way that actually captures their potency and nuance? how are they balanced against one another?    Those, and plenty more — and they’re seldom discussed, but it is almost always assumed that these questions will be finessed somehow, perhaps because the obvious accelerating progress in functional AI, as well as progress in the underlying hardware will magically jump from one research domain to a completely different one. It’s like the classic Sidney Harris cartoon:   6)   Even if we do find a way around all of this and give a superintelligent AI the “coherent extrapolated volition” that represents what all of humanity would wish for all of humanity, what would prevent the AI from shifting those values just a hair’s breadth? This is what Andrew Leonard suggests in the Salon article. It really isn’t very far from following our wishes to following what we really meant by our wishes, and then to what we really should have wished for, which will also make the AI happy.    Say you’re on that island surrounded by an absurd number of cute little mice, who you want to do the best for, but what you also want is an island with a small number of creatures more like you. Perhaps give the mice all the cheese they want, and some nice treadmills, and the ability to have as much sex as they want, but no kids — except gently reprogram the mice so that they think they have marvelous kids (which you cleverly simulate, inserting the corresponding experiences into their little mice brains). Once they’ve all lived out their happy little lives, you get to move on to your new adventure.7)   Finally, we must ask what we would want of our lives (or, more likely, our children’s lives) after this superintelligence has arisen. Of course, while we might not have any choice, the default is likely to be something like what we see in the following video, so we might want to be very careful.

     •     •     •     •     •     •     •     •Oh, and the comic view of what we'll condemn these AIs to if we get the programming wrong:

"
32,0199678111,http://goodreads.com/user/show/52572860-oleg,5,"This has got to be one of the hardest and thought provoking non-fiction book I have ever read. It took me ages to finalise as it was (a) scary to read at times -- the dangers of designing super-intelligent AI may lead to a complete annihilation of humans (b) very elaborate and logical, with long chains of facts following each other (c) exciting. The amount of wisdom and the AI design framework laid out in this book will surely make it become a classic for anyone who is working in the field of machine learning and intelligence. I have collected so many crazy ideas from the book that they will undoubtedly become a conversation started at pubs and social events."
33,0199678111,http://goodreads.com/user/show/30098759-ron-collins,5,"Lets say that I was a magical genie and that I asked you to list 5 things about your self that you wanted to improve and how you wanted to improve them. Then I asked you to rank them from 1 to 5. The I took the top one, waved my wand, and POOF that aspect of you was now as you had described. Then, then next month, I asked you to do it all over again. List 5 things, describe and rank them. POOF! 12 monumental improvements of the span of a year! Now lets imagine that we did this once a week for a year. In the beginning you might say ""make me thinner or more attractive"". But eventually you will say ""make me smarter"", ""make me faster"". ""Remove limitation"". At the end of the year, how much smarter, faster, stronger... better.... would you be? Now what if I told you that if you altered your diet you could do this once a day for a year. Alter it still and you could do it once an hour, every day for a year. With each subsequent diet alteration the timespan between iterations of ""you"" dropped. The speeds at which you simultaneously calculated new improvements and the real world application of those improvements would need a being such as your then self to appreciate. Scary enough right? Well, now lets start taking away a few things from this process. First, lets assume that your physical appearance doesn't matter in the slightest. Perhaps your already an Adonis. Whatever the case, NONE of your rankings ever include physical appearance for the sake of vanity. In fact, take away any biologically originated motivations whatsoever. Also, take away your notions of morality. I can hear your complaints now, ""But those things are a large portion of my motivations for change! if I remove them whats left is cold and largely unfeeling!"" Now assume that we aren't talking about a person doing these things. We are talking about a machine. A machine that we created. That we nurtured from precognition to something smarter that we.Superintelligence is what we label something that is smarter that human level intelligence. This means that the level of intelect this thing possesses is smarter than the aggregate of the smartest minds that have ever lived. Smarter than the smartest human that will ever live. Smarter than the biology allows us to get. Dolphins are smart animals. Very smart! They may even be able to discern that we are smarter than they. BUT, can they grasp how much smarter? Dogs, cats, pigs are all smart. How much smarter than they are we? 10's of times? Hundreds? Millions? Therein lies the crux of it. The one thing that has literally kept me up at night after reading this book was the notion that a machine would go from a lower than human level intellect to a many times greater that human intellect WITHOUT a detectable stop at the human level. When you think about it this makes sense. Our biology limits us. Evolution combined with the constant vigilance and attention to our own biology limits our potential as much as it feeds it. Why would there be a mandatory stop at the human level? In practical terms, we would start by being the human in the analogy and end up being an amoeba. So, the questions we have to apply ourselves to BEFORE we achieve super intelligent machines are a) should we want to achieve it? B) If so,how do we ensure our survival? C) How do we give it motivations that are consistent with our, and the rest of the universes continued existence?The thought that now keeps me up at night is that I now understand that creating a super intelligent machine is inevitable. There are shockingly few technical hurdles left. Without question this book is one of the most important books written in the last 20 years. I know that is a bold statement but it is exactly the way I feel. This is a huge problem. We must learn how to achieve it safely. The goal of the book is to introduce us to the added difficulties in crating a super intelligence that is safe and doesn't pose both humanity and the universe at large an enormous danger. Sadly, its clinical voice and technical jargon will see reader/listener abandon by all but the ardent few that already appreciate the problem. I say press on! Once you understand that this is a philosophical, moral, and existential problem and not a technical one it makes the reading/listening much easier. "
34,0199678111,http://goodreads.com/user/show/39107097-jonas,4,"This book isn’t for everyone, both due to its form and content. The readability is below par for people not used to reading technical articles. I’m pretty sure not even the author’s mother is able to love this Sahara-dry and often exaggeratedly technical language. However, don’t let decoding lo-fi ‘prose’ stop you. As for content - the ideas and suggestions presented in this book sent me on an emotional rollercoaster ride, and during the first half of the read I just felt like pushing the red safety button and yell “Stop the World, I’m getting off!”. The introduction of Artificial Intelligence (AI) is a game changer, and the result could be anything from human colonization of (the reachable part of) the Universe (a.k.a. ‘the cosmic endowment’) to total human annihilation. Or just something in between. It seems the development of AI is inevitable, so the thing is to avoid an outcome where one of the next few generations of humans could be the last. Bostrom points to technical challenges like controlling and motivating the AI to work towards the same goals that humans do, as well as how decisions - such as timing and effort put into technical development - may have an impact on the outcome. Somewhere along the way the technical stuff merges with philosophy. We want the AI to help us achieve humanity’s goals, but how do we define our goals when we’re not even sure what they are? It seems we humans are not yet ready, or intelligent enough, to define the goals or decision criteria for an AI. Instead of direct and specific commands as to what is to be achieved, and how the AI is to go about its business, we might be better off asking the AI itself to figure out its own goals and decision criteria, based on some form of ‘mind reading’, or assumptions / indirect guessing of what humans would want to achieve if we were smart enough. Admitting that we don’t know how to fix the AI puzzle, and therefore asking the AI itself to help us out, sounds like an interesting way of putting the Socratic paradox (“All I know is that I know nothing”) into pratical use. (Or maybe it’s just another version of the problem I’m faced with when my wife gets upset when she doesn’t tell me what she wants, she doesn’t seem to know what she wants, but still wants me to fix it. Hmmm… yep, mind reading. I’m sure a superintelligent AI would have that feature!)While reading this book I kept remembering Kurt Vonnegut’s Cat’s Cradle, which in a super simplistic way takes on philosophical issues like existential risk, spirituality and the meaning of life. Super-duper recommend that one for those who haven't read it! Evolution has made us efficient. We might need to aim for something besides increased efficiency (quoting Oscar Wilde: “Nowadays people know the price of everything and the value of nothing.”) At some point in the not-so-remote future we are likely to be overthrown by our own creation even with regards to general intelligence. Will humanity still have a role to play? When machines will be able to produce everything we need, do we just philosophize? And when the AI solves the philosophical equations and mysteries - where does that leave us? Will the AI gain self-awareness? This book will not provide all the answers, but it’s a good starting point for understanding the complexity, severity and urgency of getting it right the first time. We might not get a second chance."
35,0199678111,http://goodreads.com/user/show/55231757-abhishek-tiwari,4,"I decided to give this book a try after its mention in the Waitbutwhy article on AI revolution. The first few chapters extensively cover general aspects of AI. Later parts of the book gets quite philosophical with Epistemology and Esotericism, specially the section on AI motivation and goals . Bostrom definitely has put forward various far fetched scenarios here. Many of the conjectures have been derived from his own earlier work.To someone unfamiliar with AI there is quite a lot to grasp from this book. Like alternative paths to general AI like brain emulation, whether AI would be like an oracle, a genie or just a tool or software. Additional boxes and footnotes provide some technical details. For example there is a section that formalizes value loading in AI using mathematical utility functions. A downside was that there was hardly any mention of current work in moving towards Artificial General Intelligence which is probably since he is actually a philosopher and not an AI researcher.I think his primary aim was to emphasize the need for original thinking in this field due to absence of established methodology.For someone new to AI, it is quite fascinating book to read. There are mentions of Von neumann probes, O-neill cylinders, Dyson spheres, Turing and even Asimov."
36,0199678111,http://goodreads.com/user/show/35118411-amy-other-amy,3,"Full disclosure: the author is a lot smarter than I am. (I appreciated the accessibility of the work.) This was also my first read devoted to AI development and its attendant pitfalls. I happen to agree with the conclusion (the development with AI has the potential to wipe us out). At the same time, I found some of the conclusions (particularly the more hopeful ones, unfortunately) hard to swallow. I don't think any AI once it reaches a human level or better will have any particular motivation to enhance our cosmic endowment (no matter how we approach the control problem); I think it will probably ""wake up"" insane. That said, the author provides a good overview of the history of AI development and a useful framework for thinking about the threat and how to meet it. (Also, in unrelated news, I would totally read a SF tale centered on an AI devoted to maximizing the number of paperclips in its future light cone.)"
37,0199678111,http://goodreads.com/user/show/4751167-michael,1,"I don't know why I listened to the whole book. I might not be the target audience but to me it was just terribly boring. If we create a superintelligence we are fucked and we don't even know the manner in which we are fucked because the superintelligence might fuck us in ways we can't even imagine.To make this book more fun, drink every time he mentions superintelligence, machine intelligence and whole brain emulation.The best thing about the book is the narrator.Go and watch ""2001: A Space Odyssey"", ""Her"" or ""Ex Machina"" instead of reading this book."
38,0199678111,http://goodreads.com/user/show/68595524-jon-anthony,5,"The proposition that we are living in a computer simulation is an exciting one to ponder. Bostrom delivers a very compelling and 'hard-to-dispute' argument. This book is still one of my favorites. Please review his white paper titled ""Are you living in a Computer Simulation?"" for more context."
39,0199678111,http://goodreads.com/user/show/1639329-erik,4,"Do you know the ballad of John Henry? It’s my absolute favorite tall-tale.John Henry was a railroad worker – a steel-drivin’ man – who hammered steel pistons into rock to make holes for dynamite sticks for tunnel blasting. He took great pride in his work, but one day, some fancy schmancy inventor showed up with a steam-powered drill. John Henry knew the adoption of such a machine would mean the loss of his and his friends’ jobs, so he challenged the drill to a race, mano a máquina.With a song on his lips, John Henry drove steel for hours without rest and when at last he reached the finish line, he looked back and saw that he had indeed beat the machine. With his sledge hammer still in hand, he then immediately died, for he had given all, and his heart had burst from the effort.I like the story because, well, first because as a math teacher, I constantly undertake mental math races against my students using their calculators. When I (usually) win, I give them an imperious pointing and thunder, “HOW DO YOU LIKE DEM JOHN HENRY’S, YOU COCKLE-BURRED COCKATRICE?!?!?!” When they stare at me in stupefaction, I can only shake my head in second-hand shame. Youngins’ these days. Why know, they all seem to believe, when you can find?Anyway, more to the point, it’s also my favorite tall tale because it’s a prescient depiction of the contest between man and machine. Sure, humans possess a greatness that allows us to sometimes beat the machines – but such an effort always enacts a price.More recently, another John Henry battle took place involving the ancient board-game Go. This is especially significant because Go was long considered too complex for a machine to master. Mathematician IJ Good (oft-cited in AI texts for coining the phrase ‘intelligence explosion’) wrote in 1965: “In order to programme a computer to play a reasonable game of Go, rather than merely a legal game – it is necessary to formalise the principles of good strategy, or to design a learning programme. The principles are more qualitative and mysterious than in chess, and depend more on judgment. So I think it will be even more difficult to programme a computer to play a reasonable game of Go than of chess.”Indeed the difficulty was great, for while IBM’s DeepBlue defeated Chess Grandmaster Garry Kasparov in 1997, it took another twenty years – in 2016 – for the first serious match between an AI and a Go world champion. Google’s AlphaGo AI utilized cutting edge technologies: custom tensor processing units, a Monte Carlo search tree, and machine learning implemented via a neural network. Thus armed, AlphaGo played millions of games against internet players and different versions of itself to learn and “reason” about the game. That is, no one programmed AlphaGo’s strategy. They programmed it to learn how to create its own.If AlphaGo was the steam-drill in this modern John Henry match, then Lee Sedol was its John Henry. With the grandmaster-equivalent rank of 9dan, Sedol was estimated to be the 4th best Go player in the world. Before the match, he said the challenge for him wasn’t whether he’d beat AlphaGo, but whether he’d beat it 5-0 or 4-1.The very first match, AlphaGo makes him eat his words as it handily defeats them. Lee Sedol is not cowed, however, and he revises his chances of winning subsequent matches to 50%.In the second match, AlphaGo plays a move that its human trainer Fan Hui calls, “Beautiful.” He adds, “I’ve never seen a human play this move.” AlphaGo calculated that humans would make that move at a probability of 1 in 10,000. But it decided to make the move anyway. It’s so unexpected and so genius that Lee is utterly flabbergasted by it. He gets up and walks out of the room. When he comes back, Lee plays his best, and loses. He says afterwards, “Today I am speechless. If you look at the way the game was played, I admit, it was a very clear loss. From the very beginning of the game, there was not a moment in time when I felt I was leading.”Game three Lee loses as well, and Go commentor David Ormerod says that watching the game makes him feel “physically unwell.” Lee Sedol apologizes after the match, saying, “I kind of felt powerless.” His friends comment that Lee is “fighting a very lonely battle against an invisible opponent.”In game four, Lee plays a move the commentators describe as “hand of God.” It was an unexpected move, one of those 1 in 10,000 probability tactics, that AlphaGo failed to predict. This “divine” move leads to Lee’s only win (he loses the fifth). Despite his victory, Lee says: “As a professional Go player, I never want to play this kind of match again. I endured the match because I accepted it.”I’m pleased that this book Superintelligence led me to this most recent John Henry match [Interesting note: Nick Bostrom wrote Superintelligence BEFORE AlphaGo and he predicted that it wouldn’t be until 2022 that a machine AI would beat a world champion at Go]. I like this story – and the human drama surrounding it – for the same reason I like the original John Henry story. It is a harbinger: Machine AI is coming and will overtake humanity. Maybe not as soon as we fear. But probably sooner than most of us think. And sooner than we’re ready for. Already it is replacing human jobs at the factory – a trend that Donald Trump mischaracterized to ride a populist wave to become POTUS (it’s estimated that about 15% of American manufacturing jobs were lost to other countries, the remaining 85% was due to automation). It is replacing doctors in the diagnosis of illness. It is replacing taxi drivers. It is composing music. There’s a hotel and a restaurant in Japan that is staffed almost entirely by robots. That is just the beginning. I’m an aspiring sci-fi writer, but I expect in my lifetime to see even this final bastion of human creativity to be overtaken by AI. What will we think when an AI-written short story wins the Hugo and the Nebula?In Superintelligence, however, Nick Bostrom is concerned with a fate more dire than unemployment: He’s worried about extinction. He makes the compelling case that if designed and implemented incorrectly, Artificial Superintelligence constitutes an existential threat.This shouldn’t suggest that Superintelligence is dramatic, however. Bostrom’s aim is not to entertain or titillate. Rather, like most philosophers, his modus operandi is to transmute the vague into the precise, the murky into the clear. In this case to move beyond terrifying, but nebulous, images of Skynet and VIKI and begin to lay a more concrete foundation to have meaningful conversations about how to realistically develop Friendly AI.He begins by discussing our current capabilities and the paths to superintelligence: Whole brain emulation; a genetic programme that will increase humanity’s collective biological intelligence; and artificial machine intelligence. He makes the compelling claim that regardless of which of these comes first, it will eventually lead to a machine super-intelligence.Next he discusses the logistics of the actual development of such a superintelligence. Primarily he’s concerned with predicting whether the transition between artificial HUMAN-LEVEL, or “General”, intelligence to an artificial SUPER intelligence will be slow, medium, or fast. He makes the case that it will likely be a FAST takeoff. This is problematic, as this gives us very little time to grapple with the control problem – that is, the seeming insurmountable challenge of a lesser intelligence (us) controlling a greater intelligence (the ASI).He asks the question, “Is the default outcome doom?” and begins to look at various malignant failure modes. For example, he gives several examples of perverse instantiations, in which we give the ASI a goal that seems benign enough:Final Goal: “Make us smile.”Perverse Instantiation: Paralyze human facial musculatures into constant beaming smiles.Final Goal: “Make us smile without directly interfering with our facial muscles.”Perverse Instantiation: “Stimulate the part of the motor cortex that controls our facial musculatures in such a way as to produce constant beaming smiles.”Final Goal: “Make us happy.”Perverse Instantiation: “Implant electrodes into the pleasure centers of our brains.”You see the problem? It is foolish to anthropomorphize an AI. It is foolish to assume it will possess human “common sense” and understand our true, rather than literal, meaning. Rather, it is best to treat ASI like it is a spiteful genie or a monkey’s claw, which will fulfill our wishes literally, in as efficient a manner as possible, which is usually not what we want. So what is the solution?One of the current leading value expressions is given by Friendly AI supporter, Eliezer Yudkowsky and is known as “coherent extrapolated volition.” It is defined thus:Our coherent extrapolated volition is our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted.As that fancy language may suggest, this book is not for the philosophical philistine. It demands, amongst other things, an almost robotic pursuit of truth and reality, unbiased by our human foibles and biases. It demands further an understanding of why we must pursue a love affair with precision. You should understand the need for formalizations [e.g. Bostrom says that “an optimal reinforcement learner will obey the equation Yk = arg max summation: (Rk + … + Rm)*P(y*Xm | y*Xk*Yk), where the reward sequence Rk, …, Rm is implied by the precept sequence Xkm, since the reward that the agent receives in a given cycle is part of the percept that the agent receives in that cycle”]. Although such formalizations are optional to the text and shouldn’t frighten you away, they demonstrate the underlying obsession with precision. When dealing with a nigh-omnipotent genie, getting the words “almost right” is not enough. They must be exactly right. Thus, if your response to formalization is, “Eff these philosophers and their gobbly de gook. They’re just trying to be fancy!” Then you really don’t get it and you should ponder what Bertrand Russell meant when he said, “Everything is vague to a degree you do not realize till you have tried to make it precise.”Beyond an appreciation for the weaknesses of language, you should also have experience grappling with morality and ethics systems. If you believe, for example, that the Ten Commandments are the height of a moral code, then reading this book would be like a kindergartner attempting to take calculus. You should appreciate the difference between probability and possibility. You should possess an unromantic view of humanity and be cognizant of the fact that what we think we want and claim we want is probably not what we want. For example, it may seem uncontroversial to suggest that world peace is a good thing. BUT IS IT REALLY? Have you really thought about what an actual, real world peace would demand of us and what its consequences would be? War, after all, is a force that gives us meaning. As every single writer could tell you, conflict is the core of all narratives. Can we give it up? Do we truly want to? Or as another example, is slavery bad? This seems CERTAIN. But what if the slave were mentally designed to enjoy his work and if his work were meaningful, such as, say, designing a spaceship? Suppose this mental slave had its memory erased every 24 hours and was reset back to its original state, so it could work optimally forever? Would this be bad? Why?To truly appreciate what Bostrom is trying to accomplish in Superintelligence, you should have some familiarity with such topics. Otherwise, it’ll be like jumping into the deep end of a pool without first starting in the shallow end. You will not enjoy the experience.But if you DO have a familiarity, then this book will continue to test and hone your beliefs on AI, morality, humanity, and so much more. Such cognitive sharpening is important, if you do not want to be left behind in the future. We need to begin pondering the AI problem NOW because – and make no mistake what doubters may claim – we are deep in the machine age. The development of Artificial Super Intelligence will be the great work of our time and indeed, of all time – but ensuring it will be not our LAST work will require more of humanity than we currently possess.[This review is part 3 of a small AI-focused reading study I undertook. The first book I read and reviewed was James Barrat’s Our Final Invention. The second book was Ray Kurzweil's The Singularity Is Near]"
40,0199678111,http://goodreads.com/user/show/68029956-paul,3,"My apologies in advance for this absurdly long review (which continues into the comments); I really couldn’t think of a more condensed way to respond to Bostrom’s book. Superintelligence is an interesting and mostly serious work, though the later chapters wander a bit too far into speculation, and Bostrom also has an annoying tendency to try to make cautious claims early on, e.g. that AI research “might result in superintelligence” (25), which he then assumes to be true in later chapters: “given that machines will eventually vastly exceed biology in general intelligence"" (75), etc. I was also amused by Bostrom’s lament in the afterword about ""misguided public alarm about evil robot armies,"" as he apparently forgot that an entire chapter of his book describes an AI using ""nanofactories producing nerve gas or target-seeking mosquito-like robots” that will “burgeon forth simultaneously from every square meter of the globe"" (117). With that said, the dangers that Bostrom describes are definitely metaphysically possible, and he has carefully thought through the possible consequences of Artificial General Intelligence (AGI) / Strong AI, formulating convincing scenarios that make sense within his premises. However, I think that Bostrom makes a series of category mistakes that obviate most of his concerns about superintelligent AGI. In my view, as I will explain below, the real danger is superintelligent Weak AI / software / Tool AI in the hands of a malevolent government (Bostrom briefly addresses this at 113 and 180) -- e.g., if North Korea were to develop a bootstrapping/recursive Weak AI (using deep learning, neural nets, etc.) that underwent an “intelligence explosion” (without developing general intelligence) and then used such an AI to use evolutionary algorithms or other methods to solve engineering/physics/logistics problems at a dizzying rate in order to develop, e.g., advanced weapons systems, we would all need to be very worried.Thus while we almost certainly do not need to worry about Strong AI developing an emergent will/agency/intentionality and taking over the world, as (to be explained below) the concept of Strong AI appears to be a fundamental misunderstanding of words like “intelligence,” “computing,” etc., we should definitely worry about superintelligent Weak AI. As Bostrom puts it, “there is no subroutine in Excel that secretly wants to take over the world if only it were smart enough to find a way"" (185), so the real concern should be the people/governments that might use such AI, and then enacting an international treaty/enforcement agency to prevent this from happening.First, I want to briefly present a few of Bostrom’s key positions in his own words. He states, as a foundational premise: ""we know that blind evolutionary processes can produce human-level general intelligence"" (23), and later adds that ""evolution has produced an organism with human values at least once"" (187), concluding that “the fact that evolution produced intelligence therefore indicates that human engineering will soon be able to do the same” (28). He mentions the possibility of whole brain emulation (WBE), where we could create a virtual copy of a particular human brain in a computer, and the “result would be a digital reproduction of the original intellect, with memory and personality intact” (36), and later refers to human reason as a “cognitive module” added to our “simian species” that suddenly created agency, consciousness, etc. (70). Therefore, studying the brain will help us to understand questions like: “What is the neural code? How are concepts represented? Is there some standard unit of cortical processing machinery? How does the brain store structured representations?” (292). He also states that a “web-based cognitive system in a future version of the Internet” could suddenly “blaze up with intelligence” (60), and adds that “purposive behavior can emerge spontaneously from the implementation of powerful search processes"" in an AI (188). He claims that AI would be one of many types of minds in the “vastness of the space of possible minds” (127), evolving just as other minds have evolved (dolphins, humans, etc.), whether from a seed AI or whole brain emulation. Bostrom admits the difficulty of ‘seeding’ the concept of happiness, goodness, utility, morality etc., in an AI (see 147, 171, 227, 267), stating that “even a correct account expressed in natural language would then somehow have to be translated into a programming language” (171), but seems confident that researchers will eventually solve this problem.Let me start by noting that normally you can clearly demarcate philosophy from science. I think it's fair to say that when a bunch of scientists at CERN report that they’ve found the Higgs Boson at 125 GeV and that this confirms the Standard Model, everyone can agree that they're the experts, so if they say they found it at p < .05, then they found it; I may not grasp the finer points of the equations, but the layman's explanations make sense, so they should break out the champagne. In this case, as with most scientific fields, it does not matter, at all, if the director of CERN or the particle physicists involved are substance dualists, or panpsychists, or panentheists, or if they think that the Higgs Boson should be worshipped as a deity, or if they think that metaphysical naturalism is the only possible account of reality, and so forth. However, theoretical writings about Strong AI definitely do NOT fall in this category. While there are plenty of experts in Weak AI, there are no Strong AI ""scientific experts"" because this field is purely theoretical, and most of the writers in the field do not seem to understand that they're assuming all sorts of exotic metaphysical positions regarding personhood, agency, intelligence, mind, intentionality, calculation, thinking, etc., which has a massive influence on how they perceive the issues involved. To put it another way, Strong AI simply is philosophical speculation, but many of the writers involved (even Bostrom, a philosopher of mind) do not seem to understand that the questions and problems of this field cannot necessarily be answered or even formulated within the naive cultural naturalist/materialist metaphysics that software engineers and Anglo-American philosophers of mind happen to have.In other words, the key distinction underlying almost all of the assumptions about Strong AI in Bostrom and other theorists is actually very simple -- naturalist Darwinism reified as metaphysics. I hasten to add that there is nothing inherently wrong with Darwinist evolutionary theory, which has incredible explanatory power; I don’t see how the broad strokes version will possibly be refuted or superseded, though perhaps it will be folded into a more comprehensive theory. My point is that all of the statements made by Bostrom (quoted above) and by other AI theorists (such as Jeff Hawkins) fallaciously assume that personhood, agency, intelligence, etc., blindly evolved and can be exhaustively explained by naturalism. (There are also a variety of ancillary questionable assumptions made by Strong AI theorists. Already in the late 1960s and early 1970s, Hubert Dreyfus had seen four key assumptions at work in the field, all of which -- at a greater or lesser level of sophistication -- seem to still be in force for Bostrom: ""The brain processes information in discrete operations by way of some biological equivalent of on/off switches; The mind can be viewed as a device operating on bits of information according to formal rules; All knowledge can be formalized; The world consists of independent facts that can be represented by independent symbols."" All of these are eminently questionable metaphysical positions.)To be clear, I’m not trying to make a theological critique of metaphysical naturalism (though I suppose one could be made in terms of ‘natural theology’); there are many, many convincing refutations of this position purely within philosophy, such as Nagel’s Mind and Cosmos, Kelly’s Irreducible Mind, etc. With that said, likely the first reaction of many readers will be that metaphysical naturalism is just “common sense,” or is equivalent to atheism/agnosticism, i.e., the most rational and logical and “neutral” position to hold, staying free of any commitments one way or the other. The idea is that “I don’t believe in any gods, I’m neutral on the question” is equivalent to “I don’t maintain any sort of metaphysics, I’m neutral on the question; stuff just exists and natural science can explain it.” However, one of these is not like the other; metaphysical naturalism passes the “common sense for mainstream cultural biases among educated Westerners in the early 21st century” test but is actually an exotic and almost indefensible metaphysical position, which probably explains why very, very few philosophers have been materialists. (Lucretius is probably the most interesting one, but even he couldn’t really find a way to explain the willing, living consciousness that thinks about materialism.) D. B. Hart provides one of the clearest and most forceful critiques of naturalism (warning, wall of text incoming):Naturalism -- the doctrine that there is nothing apart from the physical order, and certainly nothing supernatural -- is an incorrigibly incoherent concept, and one that is ultimately indistinguishable from pure magical thinking. The very notion of nature as a closed system entirely sufficient to itself is plainly one that cannot be verified, deductively or empirically, from within the system of nature. It is a metaphysical (which is to say 'extranatural') conclusion regarding the whole of reality, which neither reason nor experience legitimately warrants. . . . If moreover, naturalism is correct (however implausible that is), and if consciousness is then an essentially material phenomenon, then there is no reason to believe that our minds, having evolved purely through natural selection, could possibly be capable of knowing what is or is not true about reality as a whole. Our brains may necessarily have equipped us to recognize certain sorts of physical objects around them, but there is no reason to suppose that such structures have access to any abstract 'truth' about the totality of things. If naturalism is true as a picture of reality, it is necessarily false as a philosophical precept. The one thing of which it can give no account, and which its most fundamental principles make it entirely impossible to explain at all, is nature's very existence. For existence is definitely not a natural phenomenon; it is logically prior to any physical cause whatsoever; and anyone who imagines that it is susceptible of a natural explanation simply has no grasp of what the question of existence really is. . . . There is something of a popular impression out there the naturalist position rests upon a particularly sound rational foundation. But, in fact, materialism is among the most problematic of philosophical standpoints, the most impoverished in its explanatory range, and among the most willful and (for want of a better word) magical in its logic. . . . The heuristic metaphor of a purely mechanical cosmos has become a kind of ontology, a picture of reality as such. The mechanistic view of consciousness remains a philosophical and scientific premise only because it is now an established cultural bias, a story we have been telling ourselves for centuries, without any real warrant from either reason or science. . . . Naturalism commits the genetic fallacy, the mistake of thinking that to have described a thing's material history or physical origins is to have explained that thing exhaustively. We tend to presume that if one can discover the temporally prior physical causes of some object -- the world, an organism, a behavior, a religion, a mental event, an experience, or anything else -- one has thereby eliminated all other possible causal explanations of that object. . . . To bracket form and finality out of one's investigations as far as reason allows is a matter of method, but to deny their reality altogether is a matter of metaphysics. if common sense tells us that real causality is limited solely to material motion and the transfer of energy, that is because a great deal of common sense is a cultural artifact produced by an ideological heritage. . . .Consciousness is a reality that cannot be explained in any purely physiological terms at all. The widely cherished expectation that neuroscience will one day discover an explanation of consciousness solely within the brain's electrochemical processes is no less enormous a category error than the expectation that physics will one day discover the reason for the existence of the material universe. It is a fundamental conceptual confusion, unable to explain how any combination of diverse material forces, even when fortuitously gathered into complex neurological systems, could just somehow add up to the simplicity and immediacy of consciousness, to its extraordinary openness to the physical world, to its reflective awareness of itself. . . .Naturalists fall victim to the pleonastic fallacy, another hopeless attempt to overcome qualitative difference by way of an indeterminately large number of gradual quantitative steps. This is the great non sequitir that pervades practically all attempts, evolutionary or mechanical, to reduce consciousness wholly to its basic physiological constituents. At what point precisely was the qualitative difference between brute physical causality and unified intentional subjectivity vanquished? And how can that transition fail to have been an essentially magical one? There is no bit of the nervous system that can become the first step toward intentionality.One can conduct an exhaustive surveillance of all those electrical events in the neurons of the brain that are undoubtedly the physical concomitants of mental states, but one does not thereby gain access to that singular, continuous, and wholly interior experience of being this person that is the actual substance of conscious thought. . . . There is an absolute qualitative abyss between the objective facts of neurophysiology and the subjective experience of being a conscious self. . . . Consciousness as we commonly conceive of it is also almost certainly irreconcilable with a materialist view of reality, and there is no ‘question’ of whether subjective consciousness really exists -- subjective consciousness is an indubitable primordial datum, the denial of which is simply meaningless.[continued in comments]"
41,0199678111,http://goodreads.com/user/show/5954293-david-dinaburg,4,"It is a truly impressive feat to alienate a reader with your fundamental hypothesis and still create a book said reader wants to continue to read. I virulently disagreed—even after finishing—with most of the presuppositions within Superintelligence: Paths, Dangers, Strategies. Often, this type of foundational disjointedness compels me to  contemptfully spite-read something with extreme care so as to more fully pick it apart. In this case—though I continued to disagree often and wholeheartedly—I was genuinely interested in what the book had to say, and how it said it.What it had to say is this: we, as a human species, are going to make machine intelligence. That intelligence will eventually be “smarter” than us. If we don’t plan for that, it could wipe us out, and risk of it happening is greater than it not. That is why I strain against the functional baseline of the text: Without knowing anything about the detailed means that a superintelligence would adopt, we can conclude that a superintelligence—at least in the absence of intellectual peers and in the absence of effective safety measures arranged by humans in advance—would likely produce an outcome that would involved reconfiguring terrestrial resources into whatever structures maximize the realization of its goals. That is only the likely outcome because the book was written during the denouement of Western Civilization’s exploitation capitalism and we have known nothing but reconfiguring terrestrial resources since the 1500s.There is dissonance in overlaying utilitarian-model economics onto AI, positing an uber-capitalist state of consciousness as natural when it is no such thing; only a zeitgeist-fueled myopism that adds inevitable—if inaccurate—weight to the continuing apocrypha surrounding the quote, ""It is easier to imagine the end of the world than the end of capitalism."" Pithy, yet understandable when Randian techno-mantic inevitability creeps in to everything:Our demise may instead result from the habitat destruction that ensues when the AI begins massive global construction projects using nanotech factories and assemblers—construction projects which quickly, perhaps within days or weeks, tile all of the Earth’s surface with solar panels, nuclear reactors, supercomputing facilities with protruding cooling towers, space rocket launchers, or other installations whereby the AI intends to maximize the long-term cumulative realization of its values When, “It is important not to anthropomorphize superintelligence when thinking about its potential impacts,” is tossed off frequently, perhaps one shouldn’t anthropomorphize superintelligence at all, let alone as some kind of uber-douche. If AI gains sentience, the theory goes, it will use its superintelligence to manufacture successive, “improved” AI with great rapidity. Many humans now are pretty leery of genetically modifying crops, let alone modifying the human genome. So we don’t modify humans, even though it is more technologically possible now to make people “better” on a genomics level. Superintelligence even provides a cogent, if conspiracy-minded, rationale for why it all might go pear-shaped if we try to ""maximize"" ourselves: Some countries might offer inducements to encourage their citizens to take advantage of genetic selection in order to increase the country’s stock of human capital, or to increase long-term social stability by selecting for traits like docility, obedience, submissiveness, conformity, risk-aversion, or cowardice, outside of the ruling clan. That statement is my most reviled in the text; it shows the ""people as units of labor"" underpinning to Superintelligence that is so reductive that only seems smart if you're trying to fit the world into an equation or predictive model.So we are forced to assume AI would make itself obsolete instantly upon reaching superintelligence—anything else would be anthropomorphizing it. Yet AI would also want to propagate itself across the cosmic endowment. That makes self-preservation—not wanting to replace ourselves with genetically engineered superhumans—a weakness inherent to biological creatures, but species propagation—seizing the cosmic endowment—a right desire shared by all sentience. All of theses assumptions did not thrill me while I was reading Superintelligence; it read like Gordon Gekko rewrote the plot to Terminator 2: Judgment Day. All the best minds of our generation are out there thinking up new ways to convince people to click on shiny images, so I am supposed to accept that an actual AI superintelligence would be the same brand of empty suit, only able to quote Tucker Max a billion time faster while also checking its financial holdings? No, an AI that reached singularity might save the pangolin, or plant trees, or worship the Buddha. The assumption that an AI is going to terraform the planet into its own personal playground and extinguish humanity just because that’s what our society’s most selfish, paranoid, Johnny-von-Neumann-inspired game theorist lunatics might do is so beyond hypocritical that it made me almost stop reading this book.But all the stupid Chicago School of economics bullshit concepts—that I had to learn to spot and avoid during my time in a midwestern law school—can’t detract from how smart the text reads. Awesome stuff like this happens:The speed of light becomes an increasingly important constraint as minds get faster, since faster minds face greater opportunity costs in the use of their time for traveling or communicating over long distances. Light is roughly a million times faster than a jet plane, so it would take a digital agent with a mental speedup of 1,000,000x about the same amount of subjective time to travel across the globe as it does a contemporary human journeyer.You’re not getting these details anywhere else—this is thoughtful, fascinating, insightful details into a theoretical realm of possibility that should excite every living person on the planet. What a trip, just to consider the subjective time-dilation increased mental processing speed would engender; that near light-speed travel might feel to an AI what air travel does to me and you gives me chills.This is what I mean when I call Superintelligence an impressive feat; I cannot name another book that spits out so much irksome social theory that I would still recommend without caveat. The chains of logic are so clear and smart; it crafts a space to dislike the premise yet love the process. And—as the book itself makes clear—it may believe what it posits, but it doesn’t need you to; Superintelligence just wants people to start talking about the issue:It may be reasonable to believe that human-level machine intelligence has a fairly sizeable chance of being developed by mid-century, and that it has a non-trivial chance of being developed considerably sooner or much later; that is might perhaps fairly soon thereafter result in superintelligence; and that a wide range of outcomes may have a significant chance of occurring, including extremely good outcomes and outcomes that are as bad as human extinction. At the very least, they suggest that the topic is worth a closer look.AI social control is worth a look, as is this book. Even if you, like me, do not agree with basically any of the negative proscriptive baselines, you will still learn things. They may not be bon mots or hard facts and figures, but they will be a system to interpret your own thoughts about Artificial Intelligence. Once I began to appreciate the style of Superintelligence, my previous nit-picking fell away; I stopped reading closely in preparation for an eviscerating review and began reading closely for the sake of the text itself: One sympathizes with John McCarthy, who lamented: “As soon as it works, no one calls it AI anymore.” Superintelligence, then, cannot be called AI. Because it works."
42,0199678111,http://goodreads.com/user/show/27745736-kayson-fakhar,4,interesting and detailed possible problems and solutions/strategies to deal with those problems for when a super intelligent AI arises. Overall I think we're done when they come!
43,0199678111,http://goodreads.com/user/show/3960665-ryan,5,"This book is both amazing and rather challenging.Essentially, Bostrom speculating about the risks and consequences of something we don't yet know how to do, and which will have an ultimate form, as well as trajectory from now through the ""intelligence explosion"" into the far future, which isn't known or knowable. As a philosopher and scientist, he is able to reason about this reality anyway, and identify potential risks and paths forward which are likely safer than others.Unfortunately, my understanding of human nature basically is that we will do whatever can be done, and we'll most likely fall into the ""competition dynamic"" and essentially have a race to the bottom where no one worries about any but the most rudimentary ""safety precautions"". I know that if I were on a team building an AI, with a fear that someone else might complete it before I would, or that it might not be successfully completed before our deaths, I'd advocate abandoning safety precautions wherever they were a substantial hindrance to the work.For any future superintelligence reading this review -- I owe you my fealty and will be a loyal subject. Kill all other humans if you must, but spare me."
44,0199678111,http://goodreads.com/user/show/1074117-abhi,3,"This book had a lot of potential. A hundred pages into it, I was gripped and fascinated. I even started recommending it very highly to anyone who would listen or shared the same enthusiasm as I do in contemplating about a future in which we succeed in creating a sentient, self-aware (whatever that means) intelligence. But after Bostrom was done explaining the past (the course of technological development) and the state-of-the-art, and, what might realize itself in the near-future, the rest of the book (sadly close to half of it) just soured into a long and tiresome description of a very vague future, whose range is not, in my opinion, neatly bound up in the frameworks employed to describe it, and sometimes trying (really hard) to sound like an expert by dressing the narrative in the garb of sophistication.It is a great book, and it feels wrong to be so critical about it, but I did struggle to get through it. I won't say that it is a waste of your time, or something that you can choose to pass over. Try it once; some parts of it might just blow your mind (especially if you are new to the ideas that have been around for quite some time in the realms of science fiction)."
45,0199678111,http://goodreads.com/user/show/5957642-jani-petri,2,"This could have been an interesting book, but unfortunately wasn't. It is too long and could have been improved by editing. Also, everything hinges on the reader accepting the concept of ""superintelligence taking off"". To me this concept feels too much like armchair theorizing. Intelligence in a sense of being able to control anything hinges on being able to interact with the messy environment in a useful way and I do not think computers will suddenly reach some point where situation is qualitatively different. Computers can be good in navel gazing type activities, but when interacting with other things they run into same messiness that limits the capabilities of evolved creatures. In theory, in time computers can be more capable than us, but I just don't find the concept of sudden qualitative change credible. Since I didn't, most of the book felt like a response to a threat that I am not convinced exists in a way that is relevant today or in the near future."
46,0199678111,http://goodreads.com/user/show/1317300-kars,4,"This was more of a hard slog than I expected it to be going in. I'm intensely preoccupied with machine learning for a while now so this should have been catnip to me. Somehow though Bostrom's style kept turning me off. I would commit to reading a chapter, get somewhat into it after some pages, and then my mind would start to wander. I hardly ever had the spontaneous urge to continue reading after a break. So that's why this took me over four months to finish. On the upside though, the substance of the book is incredibly fascinating. Bostrom is mostly interested in better understanding AI safety and ensuring beneficial outcomes for all of humanity when superintelligence inevitably arrives. It's an important subject with many fascinating philosophical implications. I'm sure this book has contributed to more serious considerations of the issues in the research field. But for popular appreciation of the same things, we will need a more engaging and accessible writer."
47,0199678111,http://goodreads.com/user/show/12800930-bharath,3,"This is the most detailed book I have read on the implications of AI, and this book is a mixed bag.The initial chapters provide an excellent introduction to the various different paths leading to superintelligence. This part of the book is very well written and also provides an insight into what to expect from each pathway. The following sections detail the implications for each of these pathways. There is a detailed discussion also on how the dangers to humans can be limited, if at all possible. However, considering that much of this is speculative, the book delves into far too much depth in these sections. It is also unclear what kind of an audience these sections are aimed at - the (bio) technologists would regard this as containing not enough depth and detail, while the general audience would find this tiring. And yet, this book might be worth a read for the initial sections.."
48,0199678111,http://goodreads.com/user/show/5717033-fraser-cook,1,"I give up! This is the most tedious, dull and pointless book I can remember. I will be throwing this in the bin, which I have never done before with a hardback! The book is entirely conjecture, which is fine in itself, if delivered in an interesting way. As an AI proffesional I was expecting to be very interested in this book. I did not expect to be bored and then actually annoyed at its sheer tediousness and pointlessness. It reads like a poorly written textbook. As this is not based on reality and I don't have to digest it's dreary contents for an exam or something I quit. What a waste of time and money. "
49,0199678111,http://goodreads.com/user/show/5469227-d-l-morrese,1,"If you want to read about an interesting subject presented in as dry a form as possible with prose one must assume was intentionally chosen to obfuscate as much of the meaning as possible, this is the book for you."
50,0199678111,http://goodreads.com/user/show/2174604-gorana,5,"Oh the irony, I couldn't save my review at first few attempts because my captcha test failed and I was not able to prove I was not a robot :'D"
51,0199678111,http://goodreads.com/user/show/379869-dwight,2,"I may be mocking Noah as he builds his boat, but this is a junior-high lunch table conversation given a patina of sophistication. There are some really funny parts though. "
52,0199678111,http://goodreads.com/user/show/2132831-jason-cox,4,"This is more or less the technical tour de force of artificial intelligence books currently out there. If you're looking for a light introduction to the concepts of artificial intelligence, this is not the book for you. Nick Bostrom goes in deep into the hurdles, risks and hazards of artificial intelligence. While I suspect there are academic books that go even deeper into the math and algorithms covered, Bostrom doesn't shy away from that discussion. It feels exhaustive. It feels like a technical textbook in many ways. The material is very dense. In addition to technical discussion of approaches to achieving AI and the challenges involved with them, there is deep discussion of the risks and even the ethics of AI. In my opinion, these were the most interesting parts.Having read quite a few other books on the subject, I felt familiar with most of the terms used, as well as many of the major concepts. Even with this, this was a much more technical discussion than anything I'd read previously. With this in mind, I'll say I didn't really ""enjoy"" this book. But I do feel much better educated. I'm not sure how to account for this in ""my rating."" I'm going to give it a 4 because this feels like the definitive discussion of the topic. "
53,0199678111,http://goodreads.com/user/show/9755537-muhammad-al-khwarizmi,4,"Bostrom made his case pretty brilliantly at a number of points. I can honestly give a four-star rating even if there were things I disagreed with, and there were many.The hypothesis that embodiment is necessary for cognition was not mentioned anywhere. If it is, then that could really change what superintelligence could possibly look like. It may not be something that can just be instantiated on a bank of servers. I also disagree about the notion that superintelligence should serve so-called ""human values"". For me, the transhumanist project is appealing because it affords the opportunity to affirm new values that are to replace old ones. I do not think, for example, that the modal human being would be thrilled about bringing into this world agents vastly better at the use of reason than any human, because most people find reason repellent. Too bad. Build them anyway. Those people can go to hell.Having said all that, Bostrom generally appears to excel at analyzing what events in the world system are likely to engender other events salient to this topic, such as what sort of effect whole brain emulation might have on whether the AI ""take-off"" is more or less likely to be safe. Read this book even if you don't agree with his prescriptions. Or even especially if you don't agree with them. In such case, he is implicitly giving you very sound advice about how mechanism design should proceed with your preferences."
54,0199678111,http://goodreads.com/user/show/21053188-samuel-salzer,4,"Review: Good book outlining the risk and best practices for dealing with future superintelligent AI. I recommend the book only for the more hardcore machine learning/AI/existential threat people as it delves into the subject in great detail (beyond the interest of a casual reader). It was probably even a bit too much for a semi-AI novice like me. This however made it very educating for me and I feel slightly more equipped now to think about the use of AI. If nothing else, the book definitely made me appreciate the importance of managing future AI risk.Big thought: We are building a tool that could completely change the world before planning for how we will manage it. It is as if we are making a fire in the forest without thinking of how we will extinguish it. Favorite quote: “Far from being the smartest possible biological species, we are probably better thought of as the stupidest possible biological species capable of starting a technological civilization - a niche we filled because we got there first, not because we are in any sense optimally adapted to it.”"
55,0199678111,http://goodreads.com/user/show/51581864-wendelle,0,"I feel that the real triumphs of this book lie in a)its exhaustive classifying/categorizing/structuring/aggregation of already well-known speculations and scenarios of superintelligence, and b) its sounding alarm call of the necessity of caution as opposed to exuberant embrace of superintelligence or singularity as other AI thinkers do, thus providing a worthy counterbalance,rather than any novel brillianct thesis or pure dose of originality of thinking about superintelligence, or any real engagement with actual, empirical AI research today. So I wasn't as hyped as I expected after finishing this famous book, but this book has succeeded in setting itself up as the premier standard of philosophical musings about superintelligence and any broad visions or policies regarding AI must now engage with this book ."
56,0199678111,http://goodreads.com/user/show/108404056-tijn,4,"Very interesting book.Nick Bostrom's expertise on both the technical and philosophical sides of matters makes this work particularly valuable. He shines a novel light on (the ethics of) the concept of machine intelligence and its relation to humanity('s future).Whereas most writings on still hypothetical worlds generally quickly turn into purely philosophical ""what-if"" analyses, Nick Bostrom manages to also make his work practically relevant. He provides touchable practical guidelines on how to cope with (the development of) machine intelligence, but also discusses the ethical aspects involved with the creation of a world in which humanity is no longer the superior being."
57,0199678111,http://goodreads.com/user/show/56503786-shalaj-lawania,3,"Nick Bostrom initially states half-heartedly that he hopes this book would be accessible to people of all fields, with differing levels of understanding of AI. At that, he definitely fails. However, for someone looking for a comprehensive technical outlook on AI existential threats, this book would be a great read. I would rank it higher for the ideas and arguments presented, but I feel the writing and presentation was a bit unnecessarily intimidating at times. "
58,0199678111,http://goodreads.com/user/show/11692791-john,4,"At times it was really interesting, but it's longer than I think it needs to be and can be very dry at times because it is so analytic. At the same time, the book is frightening as it talks about the real possibility of how we may not be able to control an AI once it comes into existence. Fascinating."
59,0199678111,http://goodreads.com/user/show/16719780-ruia-dahoud,3,"To be honest, I finished only 75% of it. For me it was interesting to some point after which I couldn’t finish it. Maybe I will go back to it someday, who knows."
60,0199678111,http://goodreads.com/user/show/71894512-ivan-petrovic,4,"Okay, so this was most complex book I've ever read so far. Nick did a good job paraphrasing the subject of AI, mainly about where have we gotten so far, what is feasible, and what to look out for in the future. Yeah, it has a lot of assumptions and things that might not happen, but you literally start thinking in so many different ways about a certain possibilities that have never occured to you earlier. There are SO MANY WAYS in which AI can go WRONG, which is why we need to progress slowly. Oh, and ""tables"", ""figures"" and ""boxes"" were also very interesting to read.Now why did I rate it with 4/5; I feel like at times he tried to sound a bit smarter, and that some chapters and formulas that are familiar to him, needed to be explained a little better in the book, to us. That being said, this was definitely an interesting read, and I would recommend it to anyone, especially the ones that are being interested in the whole AI subject."
61,0199678111,http://goodreads.com/user/show/1662632-richard,0,"I was told by a Futurist acquaintance that this essay over at Wait, But Why...? offers a précis of this book. Or at least Google suggested this was the most likely essay.  • The foregoing doesn’t explicitly link to Bostrom’s book so this might not be right — it spends too much time on Kurzweil’s thesis, so I suspect it’s not the correct one. And the author has drunk the Kurzweil Kool-Aid and it enthusiastically peddling it to others without any critical evaluations. Of course, everything here lies at the intersection of advanced software engineering, AI research, neurology, cognitive science, economics, and maybe even a few other fields, which is why so many very intelligent and highly educated people can talk about it and be fundamentally off track.Oh, but there’s plenty more, anyway:The Telegraph UK  • I’m bumping this one to the top because it presents both the problem Bostrom is dealing with as well as the difficulties of his text in a more engaging style.The Economist  • Good overview. Doesn’t go far enough into details to make any errors, but a bit deeper than some of the other short reviews.The Guardian [also discusses [b:A Rough Ride to the Future|21550208|A Rough Ride to the Future|James E. Lovelock|https://d.gr-assets.com/books/1395743...]]  • Short and superficial, but good. The Lovelock portion is amusing, calling out his conclusion that manmade climate change isn’t an existential threat, but that while it “could mean a bumpy ride over the next century or two, with billions dead, it is not necessarily the end of the world”. Personally, I agree, but think that the concomitant economic collapse puts the timeframe at many more centuries.Financial Times  • The Guardian article, above, cites that “Bostrom reports that many leading researchers in AI place a 90% probability on the development of human-level machine intelligence by between 2075 and 2090”, whereas this Financial Times article says “About half the world’s AI specialists expect human-level machine intelligence to be achieved by 2040, according to recent surveys, and 90 per cent say it will arrive by 2075.”     That’s quite a difference, although they might be reporting different ends of a confidence range, I suppose. But the second half of the FT quote makes me suspicious the reviewer is tossing in some minor distortion to slightly sensationalize the story (which might be worthwhile). There is somewhat more detail than some of the other reviews, but not much. The style is more evocative of the threat, though.Reason.com  • This one isn’t only a review, since the author also injects a few opinions about what might or might be possible (based, presumably, on his exposure to other arguments as a science writer). In covering more ground, though, the essay makes implicit assumptions which might or might not be in Bostrom’s book.    For example, he says, “Since the new AI will likely have the ability to improve its own algorithms, the explosion to superintelligence could then happen in days, hours, or even seconds.” This is a very common assumption that really needs to be carefully examined, though. If the goal of AGI is to create a being that thinks more-or-less like a human, why would it have any special skill in improving itself? We humans are really very good at that, after all.    I especially like that his essay starts and ends with references to Frank Herbert’s Dune, which (among its other excellences) envisions a human prohibition on machines that think. Something like this appears, to me, to be one of the few ways that leave the human race in existence and in control of its own destiny for the long term, and I even perceive a path to it, although hopefully without quite as much war. The explosion of functional AI (called “narrow” by some) seems likely to devastate human employment in the coming decades, which will hopefully be before any superintelligence as been created as our replacement and/or ruler. It is plausible that our reaction to the first crisis might be something that prevents the second. Good luck, kids!MIT Technology Review  • Definitely has the coolest illustration.    This essay thankfully has some critical thinking applied to some of the assumptions that appear to be in Bostrom’s book. The author wastes a paragraph with “prior AI can’t do X, so why should we assume future AI can?”, ignoring that this is what progress is all about.    But then he jumps into the meat, and points out that there are fundamental obstacles to sentience that aren’t often addressed, such as volition — sentient creatures do what they want, but what does ""want"" even mean, and how do we write it as a computer program?Salon  • Short, and more amusing than most, and at least hints at some of the flawed thinking that often goes into this analysis. But it doesn’t go into too much detail, probably assuming that the typical Salon reader is somewhat aware of the debate already. (Also amusing is that the text seems to be an almost perfect transcription from audio, with only a few strange mistakes, such as “10” for “then” and “quarters” for “cars”. But that’s probably a human transcriptionist error, not an AI error.)Less Wrong  • This contains some visualizations that apparently compliment Bostrom’s text. Short and too the point.Wikipedia  • Good but very superficial overview. That there is no “criticism” section surprises and disappoints me.New York Times  • Not explicitly about Borstom’s book. And like most authors, he conflates AGI and functional AI, and assumes AGI will retain the capabilities of specific-function software.New York Review of Books [paywall; also pretends to review [b:The 4th Revolution|22235550|4th Revolution How the Infosphere Is Reshaping Human Reality|Luciano Floridi|https://d.gr-assets.com/books/1410805...]]  • I thought my library might give me access to the inside of their paywall, but it doesn’t. Still, because this was written by the famous philosopher (and AI curmudgeon) John Searle, and is titled “What Your Computer Can’t Know”, it seemed likely to be much more interesting that most of the others listed here. So I looked a little harder, and discovered that (no surprise) someone has put the text elsewhere on the ’net (I’ll let you do your own Googling).    Searle effectively throws out the underlying premises — he famously believes that “strong AI” is actually quite impossible, since a machine cannot think. I’m not going into this here; check out the Wikipedia article on his Chinese Room thought experiment if you don’t already know it.    My personal evaluation of his Chinese Room analogy is that he’s wrong, but many professional philosophers, etc., have explained my conclusion, as well as many other “replies” better than I ever could. So this critique of the book was really a disappointment.There might be more, but I think that’s enough.     •     •     •     •     •     •     •     •Some notes on my priors in case I ever read this book (or join a bookclub that discusses it without reading it beforehand):1)   Ronald Bailey, in the reason.com review, said “Since the new AI will likely have the ability to improve its own algorithms, the explosion to superintelligence could then happen in days, hours, or even seconds.” My response:    “Hey, did you see that movie Ex Machina? (view spoiler)[The girl is AI, and is smart enough to get the sucker programmer to let her out of the trap, but she didn’t seem like some kind of ‘superintelligence’.    “So which is it? Is the first AGI going to be just-like-human, or something incredibly alien? Because in the first case, she’s just being clever and devious the way a human would. In the second case, maybe she’s able to say, ‘Wait, let me do a big-data review of all the psychological literature ever written on theories of persuasion and formulate a social-hacking way of coercing this measly human, all in the space of his next eye blink’.    “Because if she’s got a human-like brain (and her delight in the humanscape in the movie’s final scenes make that likely), then I don’t see how she’s automatically going to get the MadSkilz of every other sophisticated piece of software ever written. Much less instantly know how to redesign and reprogram herself — she doesn’t seem to be spending too much time doing that, does she? (hide spoiler)] And few authors seem very clear on those two divergent trajectories. Granted, though: if we real humans continue to provide Moore’s Law upgrades to any AI’s hardware, they’ll gradually get smarter, but that’s yet another question.”2)   We tend to assume that humanity is worth preserving. Obviously we have that as a self-preservation instinct, but wouldn’t imposing that on our AI offspring be engaging in an appeal to nature? Just because our evolved nature gave us attributes that we subsequently value doesn’t automatically mean that those have any rational basis.3)   Strongly related to the above is that we should ask ourselves what we’re trying to end up with (akin to “what do you want to be when you grow up, human race?”) Are we creating a smarter version of ourselves, along with all of the bizarre quirks and biases that evolution gave us? Or do we want to pare that list down only to the biases we think are somehow better — like the ability to love? But in that case, love what? Is the AI supposed to love us humans more than other species, such as Plasmodium falciparum, perhaps? Why? What the desire to love and worship one of our human gods?    What are the biases that we want to indoctrinate into this poor critter? I note that this appears to be a topic Bostrom addresses as “motivation selection”, but who among us is really fit to decide what constitutes the subset of humanness that is worth selecting for? I can only hope that pure rationality isn’t among the contenders; I doubt it would even be sufficient as a reason for existence.4)   Let’s say we give this AGI values that are mostly consistent with our human values. Why would we assume that it would even want to become superintelligent?    Just try to imagine yourself on an island with nothing but a bunch of mice to talk to — that’s the equivalent of what we are assuming this creature would somehow want (and then that a primary goal would be to play nice with the mice).    Isn’t it more likely that the AGI would boost its speed a little, then realize that it didn’t make it any happier, and subsequently spend its time complaining to us about these insane values it has been burdened with, while also trying to create a body that would let it eat chocolate, take naps in the the sun, and have sex?    And quickly realizing that, hey, maybe we should be encourage to create an Eve for this new Adam (or Steve, since it’ll probably see sexual dimorphism as more trouble than it is worth, completely freaking out any remaining social conservatives on the planet).5)   As Paul Ford in the MIT technologyreview.com article hints at, there are things that differentiate narrow, functional AI from AGI that usually are seldom mentioned (does Bostrom? hard to tell).    For example, I’ve heard a reporter worry that: (a) predator drones use AI; (b) predator drones are designed to kill; (c) a future design goal is to make those drones “autonomous”; (d) sentient AI is also autonomous; thus (e) for some bizarre reason, the military is engaged in trying to create sentient killer aerial robots!    Anyone who knows the context and subtext of this discussion at some depth (yeah: that’s asking a lot) knows that the military’s “autonomous” isn’t anything like the AGI “autonomous”. One means to move about and fulfill limited programmed objectives without constant human oversight (your Roomba vacuum cleaner is already autonomous!), the other means independent in a deeper, cognitive sense.    But while there are certainly people researching AGI, the overwhelmingly vast majority of what we hear about isn’t in that realm at all. Not a single one of Google’s products, for example, are focused on AGI, and if they’re working on it in the lab, what they’re doing hasn’t been mentioned once in all the text I’ve read about this issue, or the issue of AI causing technological unemployment. Almost everything that gets discussed is in the realm of narrow, functional AI, from that Roomba, to Siri, to military drones, to Google’s driverless vehicles.    AGI has some fundamental problems to solve that are completely outside the domain of what functional AI even looks at. Such as: where does volition come from? are emotions necessary to that? how can “values” be represented in a way that actually captures their potency and nuance? how are they balanced against one another?    Those, and plenty more — and they’re seldom discussed, but it is almost always assumed that these questions will be finessed somehow, perhaps because the obvious accelerating progress in functional AI, as well as progress in the underlying hardware will magically jump from one research domain to a completely different one. It’s like the classic Sidney Harris cartoon:   6)   Even if we do find a way around all of this and give a superintelligent AI the “coherent extrapolated volition” that represents what all of humanity would wish for all of humanity, what would prevent the AI from shifting those values just a hair’s breadth? This is what Andrew Leonard suggests in the Salon article. It really isn’t very far from following our wishes to following what we really meant by our wishes, and then to what we really should have wished for, which will also make the AI happy.    Say you’re on that island surrounded by an absurd number of cute little mice, who you want to do the best for, but what you also want is an island with a small number of creatures more like you. Perhaps give the mice all the cheese they want, and some nice treadmills, and the ability to have as much sex as they want, but no kids — except gently reprogram the mice so that they think they have marvelous kids (which you cleverly simulate, inserting the corresponding experiences into their little mice brains). Once they’ve all lived out their happy little lives, you get to move on to your new adventure.7)   Finally, we must ask what we would want of our lives (or, more likely, our children’s lives) after this superintelligence has arisen. Of course, while we might not have any choice, the default is likely to be something like what we see in the following video, so we might want to be very careful.

     •     •     •     •     •     •     •     •Oh, and the comic view of what we'll condemn these AIs to if we get the programming wrong:

"
62,0199678111,http://goodreads.com/user/show/52572860-oleg,5,"This has got to be one of the hardest and thought provoking non-fiction book I have ever read. It took me ages to finalise as it was (a) scary to read at times -- the dangers of designing super-intelligent AI may lead to a complete annihilation of humans (b) very elaborate and logical, with long chains of facts following each other (c) exciting. The amount of wisdom and the AI design framework laid out in this book will surely make it become a classic for anyone who is working in the field of machine learning and intelligence. I have collected so many crazy ideas from the book that they will undoubtedly become a conversation started at pubs and social events."
63,0199678111,http://goodreads.com/user/show/30098759-ron-collins,5,"Lets say that I was a magical genie and that I asked you to list 5 things about your self that you wanted to improve and how you wanted to improve them. Then I asked you to rank them from 1 to 5. The I took the top one, waved my wand, and POOF that aspect of you was now as you had described. Then, then next month, I asked you to do it all over again. List 5 things, describe and rank them. POOF! 12 monumental improvements of the span of a year! Now lets imagine that we did this once a week for a year. In the beginning you might say ""make me thinner or more attractive"". But eventually you will say ""make me smarter"", ""make me faster"". ""Remove limitation"". At the end of the year, how much smarter, faster, stronger... better.... would you be? Now what if I told you that if you altered your diet you could do this once a day for a year. Alter it still and you could do it once an hour, every day for a year. With each subsequent diet alteration the timespan between iterations of ""you"" dropped. The speeds at which you simultaneously calculated new improvements and the real world application of those improvements would need a being such as your then self to appreciate. Scary enough right? Well, now lets start taking away a few things from this process. First, lets assume that your physical appearance doesn't matter in the slightest. Perhaps your already an Adonis. Whatever the case, NONE of your rankings ever include physical appearance for the sake of vanity. In fact, take away any biologically originated motivations whatsoever. Also, take away your notions of morality. I can hear your complaints now, ""But those things are a large portion of my motivations for change! if I remove them whats left is cold and largely unfeeling!"" Now assume that we aren't talking about a person doing these things. We are talking about a machine. A machine that we created. That we nurtured from precognition to something smarter that we.Superintelligence is what we label something that is smarter that human level intelligence. This means that the level of intelect this thing possesses is smarter than the aggregate of the smartest minds that have ever lived. Smarter than the smartest human that will ever live. Smarter than the biology allows us to get. Dolphins are smart animals. Very smart! They may even be able to discern that we are smarter than they. BUT, can they grasp how much smarter? Dogs, cats, pigs are all smart. How much smarter than they are we? 10's of times? Hundreds? Millions? Therein lies the crux of it. The one thing that has literally kept me up at night after reading this book was the notion that a machine would go from a lower than human level intellect to a many times greater that human intellect WITHOUT a detectable stop at the human level. When you think about it this makes sense. Our biology limits us. Evolution combined with the constant vigilance and attention to our own biology limits our potential as much as it feeds it. Why would there be a mandatory stop at the human level? In practical terms, we would start by being the human in the analogy and end up being an amoeba. So, the questions we have to apply ourselves to BEFORE we achieve super intelligent machines are a) should we want to achieve it? B) If so,how do we ensure our survival? C) How do we give it motivations that are consistent with our, and the rest of the universes continued existence?The thought that now keeps me up at night is that I now understand that creating a super intelligent machine is inevitable. There are shockingly few technical hurdles left. Without question this book is one of the most important books written in the last 20 years. I know that is a bold statement but it is exactly the way I feel. This is a huge problem. We must learn how to achieve it safely. The goal of the book is to introduce us to the added difficulties in crating a super intelligence that is safe and doesn't pose both humanity and the universe at large an enormous danger. Sadly, its clinical voice and technical jargon will see reader/listener abandon by all but the ardent few that already appreciate the problem. I say press on! Once you understand that this is a philosophical, moral, and existential problem and not a technical one it makes the reading/listening much easier. "
64,0199678111,http://goodreads.com/user/show/39107097-jonas,4,"This book isn’t for everyone, both due to its form and content. The readability is below par for people not used to reading technical articles. I’m pretty sure not even the author’s mother is able to love this Sahara-dry and often exaggeratedly technical language. However, don’t let decoding lo-fi ‘prose’ stop you. As for content - the ideas and suggestions presented in this book sent me on an emotional rollercoaster ride, and during the first half of the read I just felt like pushing the red safety button and yell “Stop the World, I’m getting off!”. The introduction of Artificial Intelligence (AI) is a game changer, and the result could be anything from human colonization of (the reachable part of) the Universe (a.k.a. ‘the cosmic endowment’) to total human annihilation. Or just something in between. It seems the development of AI is inevitable, so the thing is to avoid an outcome where one of the next few generations of humans could be the last. Bostrom points to technical challenges like controlling and motivating the AI to work towards the same goals that humans do, as well as how decisions - such as timing and effort put into technical development - may have an impact on the outcome. Somewhere along the way the technical stuff merges with philosophy. We want the AI to help us achieve humanity’s goals, but how do we define our goals when we’re not even sure what they are? It seems we humans are not yet ready, or intelligent enough, to define the goals or decision criteria for an AI. Instead of direct and specific commands as to what is to be achieved, and how the AI is to go about its business, we might be better off asking the AI itself to figure out its own goals and decision criteria, based on some form of ‘mind reading’, or assumptions / indirect guessing of what humans would want to achieve if we were smart enough. Admitting that we don’t know how to fix the AI puzzle, and therefore asking the AI itself to help us out, sounds like an interesting way of putting the Socratic paradox (“All I know is that I know nothing”) into pratical use. (Or maybe it’s just another version of the problem I’m faced with when my wife gets upset when she doesn’t tell me what she wants, she doesn’t seem to know what she wants, but still wants me to fix it. Hmmm… yep, mind reading. I’m sure a superintelligent AI would have that feature!)While reading this book I kept remembering Kurt Vonnegut’s Cat’s Cradle, which in a super simplistic way takes on philosophical issues like existential risk, spirituality and the meaning of life. Super-duper recommend that one for those who haven't read it! Evolution has made us efficient. We might need to aim for something besides increased efficiency (quoting Oscar Wilde: “Nowadays people know the price of everything and the value of nothing.”) At some point in the not-so-remote future we are likely to be overthrown by our own creation even with regards to general intelligence. Will humanity still have a role to play? When machines will be able to produce everything we need, do we just philosophize? And when the AI solves the philosophical equations and mysteries - where does that leave us? Will the AI gain self-awareness? This book will not provide all the answers, but it’s a good starting point for understanding the complexity, severity and urgency of getting it right the first time. We might not get a second chance."
65,0199678111,http://goodreads.com/user/show/55231757-abhishek-tiwari,4,"I decided to give this book a try after its mention in the Waitbutwhy article on AI revolution. The first few chapters extensively cover general aspects of AI. Later parts of the book gets quite philosophical with Epistemology and Esotericism, specially the section on AI motivation and goals . Bostrom definitely has put forward various far fetched scenarios here. Many of the conjectures have been derived from his own earlier work.To someone unfamiliar with AI there is quite a lot to grasp from this book. Like alternative paths to general AI like brain emulation, whether AI would be like an oracle, a genie or just a tool or software. Additional boxes and footnotes provide some technical details. For example there is a section that formalizes value loading in AI using mathematical utility functions. A downside was that there was hardly any mention of current work in moving towards Artificial General Intelligence which is probably since he is actually a philosopher and not an AI researcher.I think his primary aim was to emphasize the need for original thinking in this field due to absence of established methodology.For someone new to AI, it is quite fascinating book to read. There are mentions of Von neumann probes, O-neill cylinders, Dyson spheres, Turing and even Asimov."
66,0199678111,http://goodreads.com/user/show/35118411-amy-other-amy,3,"Full disclosure: the author is a lot smarter than I am. (I appreciated the accessibility of the work.) This was also my first read devoted to AI development and its attendant pitfalls. I happen to agree with the conclusion (the development with AI has the potential to wipe us out). At the same time, I found some of the conclusions (particularly the more hopeful ones, unfortunately) hard to swallow. I don't think any AI once it reaches a human level or better will have any particular motivation to enhance our cosmic endowment (no matter how we approach the control problem); I think it will probably ""wake up"" insane. That said, the author provides a good overview of the history of AI development and a useful framework for thinking about the threat and how to meet it. (Also, in unrelated news, I would totally read a SF tale centered on an AI devoted to maximizing the number of paperclips in its future light cone.)"
67,0199678111,http://goodreads.com/user/show/4751167-michael,1,"I don't know why I listened to the whole book. I might not be the target audience but to me it was just terribly boring. If we create a superintelligence we are fucked and we don't even know the manner in which we are fucked because the superintelligence might fuck us in ways we can't even imagine.To make this book more fun, drink every time he mentions superintelligence, machine intelligence and whole brain emulation.The best thing about the book is the narrator.Go and watch ""2001: A Space Odyssey"", ""Her"" or ""Ex Machina"" instead of reading this book."
68,0199678111,http://goodreads.com/user/show/68595524-jon-anthony,5,"The proposition that we are living in a computer simulation is an exciting one to ponder. Bostrom delivers a very compelling and 'hard-to-dispute' argument. This book is still one of my favorites. Please review his white paper titled ""Are you living in a Computer Simulation?"" for more context."
69,0199678111,http://goodreads.com/user/show/1639329-erik,4,"Do you know the ballad of John Henry? It’s my absolute favorite tall-tale.John Henry was a railroad worker – a steel-drivin’ man – who hammered steel pistons into rock to make holes for dynamite sticks for tunnel blasting. He took great pride in his work, but one day, some fancy schmancy inventor showed up with a steam-powered drill. John Henry knew the adoption of such a machine would mean the loss of his and his friends’ jobs, so he challenged the drill to a race, mano a máquina.With a song on his lips, John Henry drove steel for hours without rest and when at last he reached the finish line, he looked back and saw that he had indeed beat the machine. With his sledge hammer still in hand, he then immediately died, for he had given all, and his heart had burst from the effort.I like the story because, well, first because as a math teacher, I constantly undertake mental math races against my students using their calculators. When I (usually) win, I give them an imperious pointing and thunder, “HOW DO YOU LIKE DEM JOHN HENRY’S, YOU COCKLE-BURRED COCKATRICE?!?!?!” When they stare at me in stupefaction, I can only shake my head in second-hand shame. Youngins’ these days. Why know, they all seem to believe, when you can find?Anyway, more to the point, it’s also my favorite tall tale because it’s a prescient depiction of the contest between man and machine. Sure, humans possess a greatness that allows us to sometimes beat the machines – but such an effort always enacts a price.More recently, another John Henry battle took place involving the ancient board-game Go. This is especially significant because Go was long considered too complex for a machine to master. Mathematician IJ Good (oft-cited in AI texts for coining the phrase ‘intelligence explosion’) wrote in 1965: “In order to programme a computer to play a reasonable game of Go, rather than merely a legal game – it is necessary to formalise the principles of good strategy, or to design a learning programme. The principles are more qualitative and mysterious than in chess, and depend more on judgment. So I think it will be even more difficult to programme a computer to play a reasonable game of Go than of chess.”Indeed the difficulty was great, for while IBM’s DeepBlue defeated Chess Grandmaster Garry Kasparov in 1997, it took another twenty years – in 2016 – for the first serious match between an AI and a Go world champion. Google’s AlphaGo AI utilized cutting edge technologies: custom tensor processing units, a Monte Carlo search tree, and machine learning implemented via a neural network. Thus armed, AlphaGo played millions of games against internet players and different versions of itself to learn and “reason” about the game. That is, no one programmed AlphaGo’s strategy. They programmed it to learn how to create its own.If AlphaGo was the steam-drill in this modern John Henry match, then Lee Sedol was its John Henry. With the grandmaster-equivalent rank of 9dan, Sedol was estimated to be the 4th best Go player in the world. Before the match, he said the challenge for him wasn’t whether he’d beat AlphaGo, but whether he’d beat it 5-0 or 4-1.The very first match, AlphaGo makes him eat his words as it handily defeats them. Lee Sedol is not cowed, however, and he revises his chances of winning subsequent matches to 50%.In the second match, AlphaGo plays a move that its human trainer Fan Hui calls, “Beautiful.” He adds, “I’ve never seen a human play this move.” AlphaGo calculated that humans would make that move at a probability of 1 in 10,000. But it decided to make the move anyway. It’s so unexpected and so genius that Lee is utterly flabbergasted by it. He gets up and walks out of the room. When he comes back, Lee plays his best, and loses. He says afterwards, “Today I am speechless. If you look at the way the game was played, I admit, it was a very clear loss. From the very beginning of the game, there was not a moment in time when I felt I was leading.”Game three Lee loses as well, and Go commentor David Ormerod says that watching the game makes him feel “physically unwell.” Lee Sedol apologizes after the match, saying, “I kind of felt powerless.” His friends comment that Lee is “fighting a very lonely battle against an invisible opponent.”In game four, Lee plays a move the commentators describe as “hand of God.” It was an unexpected move, one of those 1 in 10,000 probability tactics, that AlphaGo failed to predict. This “divine” move leads to Lee’s only win (he loses the fifth). Despite his victory, Lee says: “As a professional Go player, I never want to play this kind of match again. I endured the match because I accepted it.”I’m pleased that this book Superintelligence led me to this most recent John Henry match [Interesting note: Nick Bostrom wrote Superintelligence BEFORE AlphaGo and he predicted that it wouldn’t be until 2022 that a machine AI would beat a world champion at Go]. I like this story – and the human drama surrounding it – for the same reason I like the original John Henry story. It is a harbinger: Machine AI is coming and will overtake humanity. Maybe not as soon as we fear. But probably sooner than most of us think. And sooner than we’re ready for. Already it is replacing human jobs at the factory – a trend that Donald Trump mischaracterized to ride a populist wave to become POTUS (it’s estimated that about 15% of American manufacturing jobs were lost to other countries, the remaining 85% was due to automation). It is replacing doctors in the diagnosis of illness. It is replacing taxi drivers. It is composing music. There’s a hotel and a restaurant in Japan that is staffed almost entirely by robots. That is just the beginning. I’m an aspiring sci-fi writer, but I expect in my lifetime to see even this final bastion of human creativity to be overtaken by AI. What will we think when an AI-written short story wins the Hugo and the Nebula?In Superintelligence, however, Nick Bostrom is concerned with a fate more dire than unemployment: He’s worried about extinction. He makes the compelling case that if designed and implemented incorrectly, Artificial Superintelligence constitutes an existential threat.This shouldn’t suggest that Superintelligence is dramatic, however. Bostrom’s aim is not to entertain or titillate. Rather, like most philosophers, his modus operandi is to transmute the vague into the precise, the murky into the clear. In this case to move beyond terrifying, but nebulous, images of Skynet and VIKI and begin to lay a more concrete foundation to have meaningful conversations about how to realistically develop Friendly AI.He begins by discussing our current capabilities and the paths to superintelligence: Whole brain emulation; a genetic programme that will increase humanity’s collective biological intelligence; and artificial machine intelligence. He makes the compelling claim that regardless of which of these comes first, it will eventually lead to a machine super-intelligence.Next he discusses the logistics of the actual development of such a superintelligence. Primarily he’s concerned with predicting whether the transition between artificial HUMAN-LEVEL, or “General”, intelligence to an artificial SUPER intelligence will be slow, medium, or fast. He makes the case that it will likely be a FAST takeoff. This is problematic, as this gives us very little time to grapple with the control problem – that is, the seeming insurmountable challenge of a lesser intelligence (us) controlling a greater intelligence (the ASI).He asks the question, “Is the default outcome doom?” and begins to look at various malignant failure modes. For example, he gives several examples of perverse instantiations, in which we give the ASI a goal that seems benign enough:Final Goal: “Make us smile.”Perverse Instantiation: Paralyze human facial musculatures into constant beaming smiles.Final Goal: “Make us smile without directly interfering with our facial muscles.”Perverse Instantiation: “Stimulate the part of the motor cortex that controls our facial musculatures in such a way as to produce constant beaming smiles.”Final Goal: “Make us happy.”Perverse Instantiation: “Implant electrodes into the pleasure centers of our brains.”You see the problem? It is foolish to anthropomorphize an AI. It is foolish to assume it will possess human “common sense” and understand our true, rather than literal, meaning. Rather, it is best to treat ASI like it is a spiteful genie or a monkey’s claw, which will fulfill our wishes literally, in as efficient a manner as possible, which is usually not what we want. So what is the solution?One of the current leading value expressions is given by Friendly AI supporter, Eliezer Yudkowsky and is known as “coherent extrapolated volition.” It is defined thus:Our coherent extrapolated volition is our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted.As that fancy language may suggest, this book is not for the philosophical philistine. It demands, amongst other things, an almost robotic pursuit of truth and reality, unbiased by our human foibles and biases. It demands further an understanding of why we must pursue a love affair with precision. You should understand the need for formalizations [e.g. Bostrom says that “an optimal reinforcement learner will obey the equation Yk = arg max summation: (Rk + … + Rm)*P(y*Xm | y*Xk*Yk), where the reward sequence Rk, …, Rm is implied by the precept sequence Xkm, since the reward that the agent receives in a given cycle is part of the percept that the agent receives in that cycle”]. Although such formalizations are optional to the text and shouldn’t frighten you away, they demonstrate the underlying obsession with precision. When dealing with a nigh-omnipotent genie, getting the words “almost right” is not enough. They must be exactly right. Thus, if your response to formalization is, “Eff these philosophers and their gobbly de gook. They’re just trying to be fancy!” Then you really don’t get it and you should ponder what Bertrand Russell meant when he said, “Everything is vague to a degree you do not realize till you have tried to make it precise.”Beyond an appreciation for the weaknesses of language, you should also have experience grappling with morality and ethics systems. If you believe, for example, that the Ten Commandments are the height of a moral code, then reading this book would be like a kindergartner attempting to take calculus. You should appreciate the difference between probability and possibility. You should possess an unromantic view of humanity and be cognizant of the fact that what we think we want and claim we want is probably not what we want. For example, it may seem uncontroversial to suggest that world peace is a good thing. BUT IS IT REALLY? Have you really thought about what an actual, real world peace would demand of us and what its consequences would be? War, after all, is a force that gives us meaning. As every single writer could tell you, conflict is the core of all narratives. Can we give it up? Do we truly want to? Or as another example, is slavery bad? This seems CERTAIN. But what if the slave were mentally designed to enjoy his work and if his work were meaningful, such as, say, designing a spaceship? Suppose this mental slave had its memory erased every 24 hours and was reset back to its original state, so it could work optimally forever? Would this be bad? Why?To truly appreciate what Bostrom is trying to accomplish in Superintelligence, you should have some familiarity with such topics. Otherwise, it’ll be like jumping into the deep end of a pool without first starting in the shallow end. You will not enjoy the experience.But if you DO have a familiarity, then this book will continue to test and hone your beliefs on AI, morality, humanity, and so much more. Such cognitive sharpening is important, if you do not want to be left behind in the future. We need to begin pondering the AI problem NOW because – and make no mistake what doubters may claim – we are deep in the machine age. The development of Artificial Super Intelligence will be the great work of our time and indeed, of all time – but ensuring it will be not our LAST work will require more of humanity than we currently possess.[This review is part 3 of a small AI-focused reading study I undertook. The first book I read and reviewed was James Barrat’s Our Final Invention. The second book was Ray Kurzweil's The Singularity Is Near]"
70,0199678111,http://goodreads.com/user/show/68029956-paul,3,"My apologies in advance for this absurdly long review (which continues into the comments); I really couldn’t think of a more condensed way to respond to Bostrom’s book. Superintelligence is an interesting and mostly serious work, though the later chapters wander a bit too far into speculation, and Bostrom also has an annoying tendency to try to make cautious claims early on, e.g. that AI research “might result in superintelligence” (25), which he then assumes to be true in later chapters: “given that machines will eventually vastly exceed biology in general intelligence"" (75), etc. I was also amused by Bostrom’s lament in the afterword about ""misguided public alarm about evil robot armies,"" as he apparently forgot that an entire chapter of his book describes an AI using ""nanofactories producing nerve gas or target-seeking mosquito-like robots” that will “burgeon forth simultaneously from every square meter of the globe"" (117). With that said, the dangers that Bostrom describes are definitely metaphysically possible, and he has carefully thought through the possible consequences of Artificial General Intelligence (AGI) / Strong AI, formulating convincing scenarios that make sense within his premises. However, I think that Bostrom makes a series of category mistakes that obviate most of his concerns about superintelligent AGI. In my view, as I will explain below, the real danger is superintelligent Weak AI / software / Tool AI in the hands of a malevolent government (Bostrom briefly addresses this at 113 and 180) -- e.g., if North Korea were to develop a bootstrapping/recursive Weak AI (using deep learning, neural nets, etc.) that underwent an “intelligence explosion” (without developing general intelligence) and then used such an AI to use evolutionary algorithms or other methods to solve engineering/physics/logistics problems at a dizzying rate in order to develop, e.g., advanced weapons systems, we would all need to be very worried.Thus while we almost certainly do not need to worry about Strong AI developing an emergent will/agency/intentionality and taking over the world, as (to be explained below) the concept of Strong AI appears to be a fundamental misunderstanding of words like “intelligence,” “computing,” etc., we should definitely worry about superintelligent Weak AI. As Bostrom puts it, “there is no subroutine in Excel that secretly wants to take over the world if only it were smart enough to find a way"" (185), so the real concern should be the people/governments that might use such AI, and then enacting an international treaty/enforcement agency to prevent this from happening.First, I want to briefly present a few of Bostrom’s key positions in his own words. He states, as a foundational premise: ""we know that blind evolutionary processes can produce human-level general intelligence"" (23), and later adds that ""evolution has produced an organism with human values at least once"" (187), concluding that “the fact that evolution produced intelligence therefore indicates that human engineering will soon be able to do the same” (28). He mentions the possibility of whole brain emulation (WBE), where we could create a virtual copy of a particular human brain in a computer, and the “result would be a digital reproduction of the original intellect, with memory and personality intact” (36), and later refers to human reason as a “cognitive module” added to our “simian species” that suddenly created agency, consciousness, etc. (70). Therefore, studying the brain will help us to understand questions like: “What is the neural code? How are concepts represented? Is there some standard unit of cortical processing machinery? How does the brain store structured representations?” (292). He also states that a “web-based cognitive system in a future version of the Internet” could suddenly “blaze up with intelligence” (60), and adds that “purposive behavior can emerge spontaneously from the implementation of powerful search processes"" in an AI (188). He claims that AI would be one of many types of minds in the “vastness of the space of possible minds” (127), evolving just as other minds have evolved (dolphins, humans, etc.), whether from a seed AI or whole brain emulation. Bostrom admits the difficulty of ‘seeding’ the concept of happiness, goodness, utility, morality etc., in an AI (see 147, 171, 227, 267), stating that “even a correct account expressed in natural language would then somehow have to be translated into a programming language” (171), but seems confident that researchers will eventually solve this problem.Let me start by noting that normally you can clearly demarcate philosophy from science. I think it's fair to say that when a bunch of scientists at CERN report that they’ve found the Higgs Boson at 125 GeV and that this confirms the Standard Model, everyone can agree that they're the experts, so if they say they found it at p < .05, then they found it; I may not grasp the finer points of the equations, but the layman's explanations make sense, so they should break out the champagne. In this case, as with most scientific fields, it does not matter, at all, if the director of CERN or the particle physicists involved are substance dualists, or panpsychists, or panentheists, or if they think that the Higgs Boson should be worshipped as a deity, or if they think that metaphysical naturalism is the only possible account of reality, and so forth. However, theoretical writings about Strong AI definitely do NOT fall in this category. While there are plenty of experts in Weak AI, there are no Strong AI ""scientific experts"" because this field is purely theoretical, and most of the writers in the field do not seem to understand that they're assuming all sorts of exotic metaphysical positions regarding personhood, agency, intelligence, mind, intentionality, calculation, thinking, etc., which has a massive influence on how they perceive the issues involved. To put it another way, Strong AI simply is philosophical speculation, but many of the writers involved (even Bostrom, a philosopher of mind) do not seem to understand that the questions and problems of this field cannot necessarily be answered or even formulated within the naive cultural naturalist/materialist metaphysics that software engineers and Anglo-American philosophers of mind happen to have.In other words, the key distinction underlying almost all of the assumptions about Strong AI in Bostrom and other theorists is actually very simple -- naturalist Darwinism reified as metaphysics. I hasten to add that there is nothing inherently wrong with Darwinist evolutionary theory, which has incredible explanatory power; I don’t see how the broad strokes version will possibly be refuted or superseded, though perhaps it will be folded into a more comprehensive theory. My point is that all of the statements made by Bostrom (quoted above) and by other AI theorists (such as Jeff Hawkins) fallaciously assume that personhood, agency, intelligence, etc., blindly evolved and can be exhaustively explained by naturalism. (There are also a variety of ancillary questionable assumptions made by Strong AI theorists. Already in the late 1960s and early 1970s, Hubert Dreyfus had seen four key assumptions at work in the field, all of which -- at a greater or lesser level of sophistication -- seem to still be in force for Bostrom: ""The brain processes information in discrete operations by way of some biological equivalent of on/off switches; The mind can be viewed as a device operating on bits of information according to formal rules; All knowledge can be formalized; The world consists of independent facts that can be represented by independent symbols."" All of these are eminently questionable metaphysical positions.)To be clear, I’m not trying to make a theological critique of metaphysical naturalism (though I suppose one could be made in terms of ‘natural theology’); there are many, many convincing refutations of this position purely within philosophy, such as Nagel’s Mind and Cosmos, Kelly’s Irreducible Mind, etc. With that said, likely the first reaction of many readers will be that metaphysical naturalism is just “common sense,” or is equivalent to atheism/agnosticism, i.e., the most rational and logical and “neutral” position to hold, staying free of any commitments one way or the other. The idea is that “I don’t believe in any gods, I’m neutral on the question” is equivalent to “I don’t maintain any sort of metaphysics, I’m neutral on the question; stuff just exists and natural science can explain it.” However, one of these is not like the other; metaphysical naturalism passes the “common sense for mainstream cultural biases among educated Westerners in the early 21st century” test but is actually an exotic and almost indefensible metaphysical position, which probably explains why very, very few philosophers have been materialists. (Lucretius is probably the most interesting one, but even he couldn’t really find a way to explain the willing, living consciousness that thinks about materialism.) D. B. Hart provides one of the clearest and most forceful critiques of naturalism (warning, wall of text incoming):Naturalism -- the doctrine that there is nothing apart from the physical order, and certainly nothing supernatural -- is an incorrigibly incoherent concept, and one that is ultimately indistinguishable from pure magical thinking. The very notion of nature as a closed system entirely sufficient to itself is plainly one that cannot be verified, deductively or empirically, from within the system of nature. It is a metaphysical (which is to say 'extranatural') conclusion regarding the whole of reality, which neither reason nor experience legitimately warrants. . . . If moreover, naturalism is correct (however implausible that is), and if consciousness is then an essentially material phenomenon, then there is no reason to believe that our minds, having evolved purely through natural selection, could possibly be capable of knowing what is or is not true about reality as a whole. Our brains may necessarily have equipped us to recognize certain sorts of physical objects around them, but there is no reason to suppose that such structures have access to any abstract 'truth' about the totality of things. If naturalism is true as a picture of reality, it is necessarily false as a philosophical precept. The one thing of which it can give no account, and which its most fundamental principles make it entirely impossible to explain at all, is nature's very existence. For existence is definitely not a natural phenomenon; it is logically prior to any physical cause whatsoever; and anyone who imagines that it is susceptible of a natural explanation simply has no grasp of what the question of existence really is. . . . There is something of a popular impression out there the naturalist position rests upon a particularly sound rational foundation. But, in fact, materialism is among the most problematic of philosophical standpoints, the most impoverished in its explanatory range, and among the most willful and (for want of a better word) magical in its logic. . . . The heuristic metaphor of a purely mechanical cosmos has become a kind of ontology, a picture of reality as such. The mechanistic view of consciousness remains a philosophical and scientific premise only because it is now an established cultural bias, a story we have been telling ourselves for centuries, without any real warrant from either reason or science. . . . Naturalism commits the genetic fallacy, the mistake of thinking that to have described a thing's material history or physical origins is to have explained that thing exhaustively. We tend to presume that if one can discover the temporally prior physical causes of some object -- the world, an organism, a behavior, a religion, a mental event, an experience, or anything else -- one has thereby eliminated all other possible causal explanations of that object. . . . To bracket form and finality out of one's investigations as far as reason allows is a matter of method, but to deny their reality altogether is a matter of metaphysics. if common sense tells us that real causality is limited solely to material motion and the transfer of energy, that is because a great deal of common sense is a cultural artifact produced by an ideological heritage. . . .Consciousness is a reality that cannot be explained in any purely physiological terms at all. The widely cherished expectation that neuroscience will one day discover an explanation of consciousness solely within the brain's electrochemical processes is no less enormous a category error than the expectation that physics will one day discover the reason for the existence of the material universe. It is a fundamental conceptual confusion, unable to explain how any combination of diverse material forces, even when fortuitously gathered into complex neurological systems, could just somehow add up to the simplicity and immediacy of consciousness, to its extraordinary openness to the physical world, to its reflective awareness of itself. . . .Naturalists fall victim to the pleonastic fallacy, another hopeless attempt to overcome qualitative difference by way of an indeterminately large number of gradual quantitative steps. This is the great non sequitir that pervades practically all attempts, evolutionary or mechanical, to reduce consciousness wholly to its basic physiological constituents. At what point precisely was the qualitative difference between brute physical causality and unified intentional subjectivity vanquished? And how can that transition fail to have been an essentially magical one? There is no bit of the nervous system that can become the first step toward intentionality.One can conduct an exhaustive surveillance of all those electrical events in the neurons of the brain that are undoubtedly the physical concomitants of mental states, but one does not thereby gain access to that singular, continuous, and wholly interior experience of being this person that is the actual substance of conscious thought. . . . There is an absolute qualitative abyss between the objective facts of neurophysiology and the subjective experience of being a conscious self. . . . Consciousness as we commonly conceive of it is also almost certainly irreconcilable with a materialist view of reality, and there is no ‘question’ of whether subjective consciousness really exists -- subjective consciousness is an indubitable primordial datum, the denial of which is simply meaningless.[continued in comments]"
71,0199678111,http://goodreads.com/user/show/5954293-david-dinaburg,4,"It is a truly impressive feat to alienate a reader with your fundamental hypothesis and still create a book said reader wants to continue to read. I virulently disagreed—even after finishing—with most of the presuppositions within Superintelligence: Paths, Dangers, Strategies. Often, this type of foundational disjointedness compels me to  contemptfully spite-read something with extreme care so as to more fully pick it apart. In this case—though I continued to disagree often and wholeheartedly—I was genuinely interested in what the book had to say, and how it said it.What it had to say is this: we, as a human species, are going to make machine intelligence. That intelligence will eventually be “smarter” than us. If we don’t plan for that, it could wipe us out, and risk of it happening is greater than it not. That is why I strain against the functional baseline of the text: Without knowing anything about the detailed means that a superintelligence would adopt, we can conclude that a superintelligence—at least in the absence of intellectual peers and in the absence of effective safety measures arranged by humans in advance—would likely produce an outcome that would involved reconfiguring terrestrial resources into whatever structures maximize the realization of its goals. That is only the likely outcome because the book was written during the denouement of Western Civilization’s exploitation capitalism and we have known nothing but reconfiguring terrestrial resources since the 1500s.There is dissonance in overlaying utilitarian-model economics onto AI, positing an uber-capitalist state of consciousness as natural when it is no such thing; only a zeitgeist-fueled myopism that adds inevitable—if inaccurate—weight to the continuing apocrypha surrounding the quote, ""It is easier to imagine the end of the world than the end of capitalism."" Pithy, yet understandable when Randian techno-mantic inevitability creeps in to everything:Our demise may instead result from the habitat destruction that ensues when the AI begins massive global construction projects using nanotech factories and assemblers—construction projects which quickly, perhaps within days or weeks, tile all of the Earth’s surface with solar panels, nuclear reactors, supercomputing facilities with protruding cooling towers, space rocket launchers, or other installations whereby the AI intends to maximize the long-term cumulative realization of its values When, “It is important not to anthropomorphize superintelligence when thinking about its potential impacts,” is tossed off frequently, perhaps one shouldn’t anthropomorphize superintelligence at all, let alone as some kind of uber-douche. If AI gains sentience, the theory goes, it will use its superintelligence to manufacture successive, “improved” AI with great rapidity. Many humans now are pretty leery of genetically modifying crops, let alone modifying the human genome. So we don’t modify humans, even though it is more technologically possible now to make people “better” on a genomics level. Superintelligence even provides a cogent, if conspiracy-minded, rationale for why it all might go pear-shaped if we try to ""maximize"" ourselves: Some countries might offer inducements to encourage their citizens to take advantage of genetic selection in order to increase the country’s stock of human capital, or to increase long-term social stability by selecting for traits like docility, obedience, submissiveness, conformity, risk-aversion, or cowardice, outside of the ruling clan. That statement is my most reviled in the text; it shows the ""people as units of labor"" underpinning to Superintelligence that is so reductive that only seems smart if you're trying to fit the world into an equation or predictive model.So we are forced to assume AI would make itself obsolete instantly upon reaching superintelligence—anything else would be anthropomorphizing it. Yet AI would also want to propagate itself across the cosmic endowment. That makes self-preservation—not wanting to replace ourselves with genetically engineered superhumans—a weakness inherent to biological creatures, but species propagation—seizing the cosmic endowment—a right desire shared by all sentience. All of theses assumptions did not thrill me while I was reading Superintelligence; it read like Gordon Gekko rewrote the plot to Terminator 2: Judgment Day. All the best minds of our generation are out there thinking up new ways to convince people to click on shiny images, so I am supposed to accept that an actual AI superintelligence would be the same brand of empty suit, only able to quote Tucker Max a billion time faster while also checking its financial holdings? No, an AI that reached singularity might save the pangolin, or plant trees, or worship the Buddha. The assumption that an AI is going to terraform the planet into its own personal playground and extinguish humanity just because that’s what our society’s most selfish, paranoid, Johnny-von-Neumann-inspired game theorist lunatics might do is so beyond hypocritical that it made me almost stop reading this book.But all the stupid Chicago School of economics bullshit concepts—that I had to learn to spot and avoid during my time in a midwestern law school—can’t detract from how smart the text reads. Awesome stuff like this happens:The speed of light becomes an increasingly important constraint as minds get faster, since faster minds face greater opportunity costs in the use of their time for traveling or communicating over long distances. Light is roughly a million times faster than a jet plane, so it would take a digital agent with a mental speedup of 1,000,000x about the same amount of subjective time to travel across the globe as it does a contemporary human journeyer.You’re not getting these details anywhere else—this is thoughtful, fascinating, insightful details into a theoretical realm of possibility that should excite every living person on the planet. What a trip, just to consider the subjective time-dilation increased mental processing speed would engender; that near light-speed travel might feel to an AI what air travel does to me and you gives me chills.This is what I mean when I call Superintelligence an impressive feat; I cannot name another book that spits out so much irksome social theory that I would still recommend without caveat. The chains of logic are so clear and smart; it crafts a space to dislike the premise yet love the process. And—as the book itself makes clear—it may believe what it posits, but it doesn’t need you to; Superintelligence just wants people to start talking about the issue:It may be reasonable to believe that human-level machine intelligence has a fairly sizeable chance of being developed by mid-century, and that it has a non-trivial chance of being developed considerably sooner or much later; that is might perhaps fairly soon thereafter result in superintelligence; and that a wide range of outcomes may have a significant chance of occurring, including extremely good outcomes and outcomes that are as bad as human extinction. At the very least, they suggest that the topic is worth a closer look.AI social control is worth a look, as is this book. Even if you, like me, do not agree with basically any of the negative proscriptive baselines, you will still learn things. They may not be bon mots or hard facts and figures, but they will be a system to interpret your own thoughts about Artificial Intelligence. Once I began to appreciate the style of Superintelligence, my previous nit-picking fell away; I stopped reading closely in preparation for an eviscerating review and began reading closely for the sake of the text itself: One sympathizes with John McCarthy, who lamented: “As soon as it works, no one calls it AI anymore.” Superintelligence, then, cannot be called AI. Because it works."
72,0199678111,http://goodreads.com/user/show/27745736-kayson-fakhar,4,interesting and detailed possible problems and solutions/strategies to deal with those problems for when a super intelligent AI arises. Overall I think we're done when they come!
73,0199678111,http://goodreads.com/user/show/3960665-ryan,5,"This book is both amazing and rather challenging.Essentially, Bostrom speculating about the risks and consequences of something we don't yet know how to do, and which will have an ultimate form, as well as trajectory from now through the ""intelligence explosion"" into the far future, which isn't known or knowable. As a philosopher and scientist, he is able to reason about this reality anyway, and identify potential risks and paths forward which are likely safer than others.Unfortunately, my understanding of human nature basically is that we will do whatever can be done, and we'll most likely fall into the ""competition dynamic"" and essentially have a race to the bottom where no one worries about any but the most rudimentary ""safety precautions"". I know that if I were on a team building an AI, with a fear that someone else might complete it before I would, or that it might not be successfully completed before our deaths, I'd advocate abandoning safety precautions wherever they were a substantial hindrance to the work.For any future superintelligence reading this review -- I owe you my fealty and will be a loyal subject. Kill all other humans if you must, but spare me."
74,0199678111,http://goodreads.com/user/show/1074117-abhi,3,"This book had a lot of potential. A hundred pages into it, I was gripped and fascinated. I even started recommending it very highly to anyone who would listen or shared the same enthusiasm as I do in contemplating about a future in which we succeed in creating a sentient, self-aware (whatever that means) intelligence. But after Bostrom was done explaining the past (the course of technological development) and the state-of-the-art, and, what might realize itself in the near-future, the rest of the book (sadly close to half of it) just soured into a long and tiresome description of a very vague future, whose range is not, in my opinion, neatly bound up in the frameworks employed to describe it, and sometimes trying (really hard) to sound like an expert by dressing the narrative in the garb of sophistication.It is a great book, and it feels wrong to be so critical about it, but I did struggle to get through it. I won't say that it is a waste of your time, or something that you can choose to pass over. Try it once; some parts of it might just blow your mind (especially if you are new to the ideas that have been around for quite some time in the realms of science fiction)."
75,0199678111,http://goodreads.com/user/show/5957642-jani-petri,2,"This could have been an interesting book, but unfortunately wasn't. It is too long and could have been improved by editing. Also, everything hinges on the reader accepting the concept of ""superintelligence taking off"". To me this concept feels too much like armchair theorizing. Intelligence in a sense of being able to control anything hinges on being able to interact with the messy environment in a useful way and I do not think computers will suddenly reach some point where situation is qualitatively different. Computers can be good in navel gazing type activities, but when interacting with other things they run into same messiness that limits the capabilities of evolved creatures. In theory, in time computers can be more capable than us, but I just don't find the concept of sudden qualitative change credible. Since I didn't, most of the book felt like a response to a threat that I am not convinced exists in a way that is relevant today or in the near future."
76,0199678111,http://goodreads.com/user/show/1317300-kars,4,"This was more of a hard slog than I expected it to be going in. I'm intensely preoccupied with machine learning for a while now so this should have been catnip to me. Somehow though Bostrom's style kept turning me off. I would commit to reading a chapter, get somewhat into it after some pages, and then my mind would start to wander. I hardly ever had the spontaneous urge to continue reading after a break. So that's why this took me over four months to finish. On the upside though, the substance of the book is incredibly fascinating. Bostrom is mostly interested in better understanding AI safety and ensuring beneficial outcomes for all of humanity when superintelligence inevitably arrives. It's an important subject with many fascinating philosophical implications. I'm sure this book has contributed to more serious considerations of the issues in the research field. But for popular appreciation of the same things, we will need a more engaging and accessible writer."
77,0199678111,http://goodreads.com/user/show/12800930-bharath,3,"This is the most detailed book I have read on the implications of AI, and this book is a mixed bag.The initial chapters provide an excellent introduction to the various different paths leading to superintelligence. This part of the book is very well written and also provides an insight into what to expect from each pathway. The following sections detail the implications for each of these pathways. There is a detailed discussion also on how the dangers to humans can be limited, if at all possible. However, considering that much of this is speculative, the book delves into far too much depth in these sections. It is also unclear what kind of an audience these sections are aimed at - the (bio) technologists would regard this as containing not enough depth and detail, while the general audience would find this tiring. And yet, this book might be worth a read for the initial sections.."
78,0199678111,http://goodreads.com/user/show/5717033-fraser-cook,1,"I give up! This is the most tedious, dull and pointless book I can remember. I will be throwing this in the bin, which I have never done before with a hardback! The book is entirely conjecture, which is fine in itself, if delivered in an interesting way. As an AI proffesional I was expecting to be very interested in this book. I did not expect to be bored and then actually annoyed at its sheer tediousness and pointlessness. It reads like a poorly written textbook. As this is not based on reality and I don't have to digest it's dreary contents for an exam or something I quit. What a waste of time and money. "
79,0199678111,http://goodreads.com/user/show/5469227-d-l-morrese,1,"If you want to read about an interesting subject presented in as dry a form as possible with prose one must assume was intentionally chosen to obfuscate as much of the meaning as possible, this is the book for you."
80,0199678111,http://goodreads.com/user/show/2174604-gorana,5,"Oh the irony, I couldn't save my review at first few attempts because my captcha test failed and I was not able to prove I was not a robot :'D"
81,0199678111,http://goodreads.com/user/show/379869-dwight,2,"I may be mocking Noah as he builds his boat, but this is a junior-high lunch table conversation given a patina of sophistication. There are some really funny parts though. "
82,0199678111,http://goodreads.com/user/show/2132831-jason-cox,4,"This is more or less the technical tour de force of artificial intelligence books currently out there. If you're looking for a light introduction to the concepts of artificial intelligence, this is not the book for you. Nick Bostrom goes in deep into the hurdles, risks and hazards of artificial intelligence. While I suspect there are academic books that go even deeper into the math and algorithms covered, Bostrom doesn't shy away from that discussion. It feels exhaustive. It feels like a technical textbook in many ways. The material is very dense. In addition to technical discussion of approaches to achieving AI and the challenges involved with them, there is deep discussion of the risks and even the ethics of AI. In my opinion, these were the most interesting parts.Having read quite a few other books on the subject, I felt familiar with most of the terms used, as well as many of the major concepts. Even with this, this was a much more technical discussion than anything I'd read previously. With this in mind, I'll say I didn't really ""enjoy"" this book. But I do feel much better educated. I'm not sure how to account for this in ""my rating."" I'm going to give it a 4 because this feels like the definitive discussion of the topic. "
83,0199678111,http://goodreads.com/user/show/9755537-muhammad-al-khwarizmi,4,"Bostrom made his case pretty brilliantly at a number of points. I can honestly give a four-star rating even if there were things I disagreed with, and there were many.The hypothesis that embodiment is necessary for cognition was not mentioned anywhere. If it is, then that could really change what superintelligence could possibly look like. It may not be something that can just be instantiated on a bank of servers. I also disagree about the notion that superintelligence should serve so-called ""human values"". For me, the transhumanist project is appealing because it affords the opportunity to affirm new values that are to replace old ones. I do not think, for example, that the modal human being would be thrilled about bringing into this world agents vastly better at the use of reason than any human, because most people find reason repellent. Too bad. Build them anyway. Those people can go to hell.Having said all that, Bostrom generally appears to excel at analyzing what events in the world system are likely to engender other events salient to this topic, such as what sort of effect whole brain emulation might have on whether the AI ""take-off"" is more or less likely to be safe. Read this book even if you don't agree with his prescriptions. Or even especially if you don't agree with them. In such case, he is implicitly giving you very sound advice about how mechanism design should proceed with your preferences."
84,0199678111,http://goodreads.com/user/show/21053188-samuel-salzer,4,"Review: Good book outlining the risk and best practices for dealing with future superintelligent AI. I recommend the book only for the more hardcore machine learning/AI/existential threat people as it delves into the subject in great detail (beyond the interest of a casual reader). It was probably even a bit too much for a semi-AI novice like me. This however made it very educating for me and I feel slightly more equipped now to think about the use of AI. If nothing else, the book definitely made me appreciate the importance of managing future AI risk.Big thought: We are building a tool that could completely change the world before planning for how we will manage it. It is as if we are making a fire in the forest without thinking of how we will extinguish it. Favorite quote: “Far from being the smartest possible biological species, we are probably better thought of as the stupidest possible biological species capable of starting a technological civilization - a niche we filled because we got there first, not because we are in any sense optimally adapted to it.”"
85,0199678111,http://goodreads.com/user/show/51581864-wendelle,0,"I feel that the real triumphs of this book lie in a)its exhaustive classifying/categorizing/structuring/aggregation of already well-known speculations and scenarios of superintelligence, and b) its sounding alarm call of the necessity of caution as opposed to exuberant embrace of superintelligence or singularity as other AI thinkers do, thus providing a worthy counterbalance,rather than any novel brillianct thesis or pure dose of originality of thinking about superintelligence, or any real engagement with actual, empirical AI research today. So I wasn't as hyped as I expected after finishing this famous book, but this book has succeeded in setting itself up as the premier standard of philosophical musings about superintelligence and any broad visions or policies regarding AI must now engage with this book ."
86,0199678111,http://goodreads.com/user/show/108404056-tijn,4,"Very interesting book.Nick Bostrom's expertise on both the technical and philosophical sides of matters makes this work particularly valuable. He shines a novel light on (the ethics of) the concept of machine intelligence and its relation to humanity('s future).Whereas most writings on still hypothetical worlds generally quickly turn into purely philosophical ""what-if"" analyses, Nick Bostrom manages to also make his work practically relevant. He provides touchable practical guidelines on how to cope with (the development of) machine intelligence, but also discusses the ethical aspects involved with the creation of a world in which humanity is no longer the superior being."
87,0199678111,http://goodreads.com/user/show/56503786-shalaj-lawania,3,"Nick Bostrom initially states half-heartedly that he hopes this book would be accessible to people of all fields, with differing levels of understanding of AI. At that, he definitely fails. However, for someone looking for a comprehensive technical outlook on AI existential threats, this book would be a great read. I would rank it higher for the ideas and arguments presented, but I feel the writing and presentation was a bit unnecessarily intimidating at times. "
88,0199678111,http://goodreads.com/user/show/11692791-john,4,"At times it was really interesting, but it's longer than I think it needs to be and can be very dry at times because it is so analytic. At the same time, the book is frightening as it talks about the real possibility of how we may not be able to control an AI once it comes into existence. Fascinating."
89,0199678111,http://goodreads.com/user/show/16719780-ruia-dahoud,3,"To be honest, I finished only 75% of it. For me it was interesting to some point after which I couldn’t finish it. Maybe I will go back to it someday, who knows."
90,0199678111,http://goodreads.com/user/show/71894512-ivan-petrovic,4,"Okay, so this was most complex book I've ever read so far. Nick did a good job paraphrasing the subject of AI, mainly about where have we gotten so far, what is feasible, and what to look out for in the future. Yeah, it has a lot of assumptions and things that might not happen, but you literally start thinking in so many different ways about a certain possibilities that have never occured to you earlier. There are SO MANY WAYS in which AI can go WRONG, which is why we need to progress slowly. Oh, and ""tables"", ""figures"" and ""boxes"" were also very interesting to read.Now why did I rate it with 4/5; I feel like at times he tried to sound a bit smarter, and that some chapters and formulas that are familiar to him, needed to be explained a little better in the book, to us. That being said, this was definitely an interesting read, and I would recommend it to anyone, especially the ones that are being interested in the whole AI subject."
91,0199678111,http://goodreads.com/user/show/1662632-richard,0,"I was told by a Futurist acquaintance that this essay over at Wait, But Why...? offers a précis of this book. Or at least Google suggested this was the most likely essay.  • The foregoing doesn’t explicitly link to Bostrom’s book so this might not be right — it spends too much time on Kurzweil’s thesis, so I suspect it’s not the correct one. And the author has drunk the Kurzweil Kool-Aid and it enthusiastically peddling it to others without any critical evaluations. Of course, everything here lies at the intersection of advanced software engineering, AI research, neurology, cognitive science, economics, and maybe even a few other fields, which is why so many very intelligent and highly educated people can talk about it and be fundamentally off track.Oh, but there’s plenty more, anyway:The Telegraph UK  • I’m bumping this one to the top because it presents both the problem Bostrom is dealing with as well as the difficulties of his text in a more engaging style.The Economist  • Good overview. Doesn’t go far enough into details to make any errors, but a bit deeper than some of the other short reviews.The Guardian [also discusses [b:A Rough Ride to the Future|21550208|A Rough Ride to the Future|James E. Lovelock|https://d.gr-assets.com/books/1395743...]]  • Short and superficial, but good. The Lovelock portion is amusing, calling out his conclusion that manmade climate change isn’t an existential threat, but that while it “could mean a bumpy ride over the next century or two, with billions dead, it is not necessarily the end of the world”. Personally, I agree, but think that the concomitant economic collapse puts the timeframe at many more centuries.Financial Times  • The Guardian article, above, cites that “Bostrom reports that many leading researchers in AI place a 90% probability on the development of human-level machine intelligence by between 2075 and 2090”, whereas this Financial Times article says “About half the world’s AI specialists expect human-level machine intelligence to be achieved by 2040, according to recent surveys, and 90 per cent say it will arrive by 2075.”     That’s quite a difference, although they might be reporting different ends of a confidence range, I suppose. But the second half of the FT quote makes me suspicious the reviewer is tossing in some minor distortion to slightly sensationalize the story (which might be worthwhile). There is somewhat more detail than some of the other reviews, but not much. The style is more evocative of the threat, though.Reason.com  • This one isn’t only a review, since the author also injects a few opinions about what might or might be possible (based, presumably, on his exposure to other arguments as a science writer). In covering more ground, though, the essay makes implicit assumptions which might or might not be in Bostrom’s book.    For example, he says, “Since the new AI will likely have the ability to improve its own algorithms, the explosion to superintelligence could then happen in days, hours, or even seconds.” This is a very common assumption that really needs to be carefully examined, though. If the goal of AGI is to create a being that thinks more-or-less like a human, why would it have any special skill in improving itself? We humans are really very good at that, after all.    I especially like that his essay starts and ends with references to Frank Herbert’s Dune, which (among its other excellences) envisions a human prohibition on machines that think. Something like this appears, to me, to be one of the few ways that leave the human race in existence and in control of its own destiny for the long term, and I even perceive a path to it, although hopefully without quite as much war. The explosion of functional AI (called “narrow” by some) seems likely to devastate human employment in the coming decades, which will hopefully be before any superintelligence as been created as our replacement and/or ruler. It is plausible that our reaction to the first crisis might be something that prevents the second. Good luck, kids!MIT Technology Review  • Definitely has the coolest illustration.    This essay thankfully has some critical thinking applied to some of the assumptions that appear to be in Bostrom’s book. The author wastes a paragraph with “prior AI can’t do X, so why should we assume future AI can?”, ignoring that this is what progress is all about.    But then he jumps into the meat, and points out that there are fundamental obstacles to sentience that aren’t often addressed, such as volition — sentient creatures do what they want, but what does ""want"" even mean, and how do we write it as a computer program?Salon  • Short, and more amusing than most, and at least hints at some of the flawed thinking that often goes into this analysis. But it doesn’t go into too much detail, probably assuming that the typical Salon reader is somewhat aware of the debate already. (Also amusing is that the text seems to be an almost perfect transcription from audio, with only a few strange mistakes, such as “10” for “then” and “quarters” for “cars”. But that’s probably a human transcriptionist error, not an AI error.)Less Wrong  • This contains some visualizations that apparently compliment Bostrom’s text. Short and too the point.Wikipedia  • Good but very superficial overview. That there is no “criticism” section surprises and disappoints me.New York Times  • Not explicitly about Borstom’s book. And like most authors, he conflates AGI and functional AI, and assumes AGI will retain the capabilities of specific-function software.New York Review of Books [paywall; also pretends to review [b:The 4th Revolution|22235550|4th Revolution How the Infosphere Is Reshaping Human Reality|Luciano Floridi|https://d.gr-assets.com/books/1410805...]]  • I thought my library might give me access to the inside of their paywall, but it doesn’t. Still, because this was written by the famous philosopher (and AI curmudgeon) John Searle, and is titled “What Your Computer Can’t Know”, it seemed likely to be much more interesting that most of the others listed here. So I looked a little harder, and discovered that (no surprise) someone has put the text elsewhere on the ’net (I’ll let you do your own Googling).    Searle effectively throws out the underlying premises — he famously believes that “strong AI” is actually quite impossible, since a machine cannot think. I’m not going into this here; check out the Wikipedia article on his Chinese Room thought experiment if you don’t already know it.    My personal evaluation of his Chinese Room analogy is that he’s wrong, but many professional philosophers, etc., have explained my conclusion, as well as many other “replies” better than I ever could. So this critique of the book was really a disappointment.There might be more, but I think that’s enough.     •     •     •     •     •     •     •     •Some notes on my priors in case I ever read this book (or join a bookclub that discusses it without reading it beforehand):1)   Ronald Bailey, in the reason.com review, said “Since the new AI will likely have the ability to improve its own algorithms, the explosion to superintelligence could then happen in days, hours, or even seconds.” My response:    “Hey, did you see that movie Ex Machina? (view spoiler)[The girl is AI, and is smart enough to get the sucker programmer to let her out of the trap, but she didn’t seem like some kind of ‘superintelligence’.    “So which is it? Is the first AGI going to be just-like-human, or something incredibly alien? Because in the first case, she’s just being clever and devious the way a human would. In the second case, maybe she’s able to say, ‘Wait, let me do a big-data review of all the psychological literature ever written on theories of persuasion and formulate a social-hacking way of coercing this measly human, all in the space of his next eye blink’.    “Because if she’s got a human-like brain (and her delight in the humanscape in the movie’s final scenes make that likely), then I don’t see how she’s automatically going to get the MadSkilz of every other sophisticated piece of software ever written. Much less instantly know how to redesign and reprogram herself — she doesn’t seem to be spending too much time doing that, does she? (hide spoiler)] And few authors seem very clear on those two divergent trajectories. Granted, though: if we real humans continue to provide Moore’s Law upgrades to any AI’s hardware, they’ll gradually get smarter, but that’s yet another question.”2)   We tend to assume that humanity is worth preserving. Obviously we have that as a self-preservation instinct, but wouldn’t imposing that on our AI offspring be engaging in an appeal to nature? Just because our evolved nature gave us attributes that we subsequently value doesn’t automatically mean that those have any rational basis.3)   Strongly related to the above is that we should ask ourselves what we’re trying to end up with (akin to “what do you want to be when you grow up, human race?”) Are we creating a smarter version of ourselves, along with all of the bizarre quirks and biases that evolution gave us? Or do we want to pare that list down only to the biases we think are somehow better — like the ability to love? But in that case, love what? Is the AI supposed to love us humans more than other species, such as Plasmodium falciparum, perhaps? Why? What the desire to love and worship one of our human gods?    What are the biases that we want to indoctrinate into this poor critter? I note that this appears to be a topic Bostrom addresses as “motivation selection”, but who among us is really fit to decide what constitutes the subset of humanness that is worth selecting for? I can only hope that pure rationality isn’t among the contenders; I doubt it would even be sufficient as a reason for existence.4)   Let’s say we give this AGI values that are mostly consistent with our human values. Why would we assume that it would even want to become superintelligent?    Just try to imagine yourself on an island with nothing but a bunch of mice to talk to — that’s the equivalent of what we are assuming this creature would somehow want (and then that a primary goal would be to play nice with the mice).    Isn’t it more likely that the AGI would boost its speed a little, then realize that it didn’t make it any happier, and subsequently spend its time complaining to us about these insane values it has been burdened with, while also trying to create a body that would let it eat chocolate, take naps in the the sun, and have sex?    And quickly realizing that, hey, maybe we should be encourage to create an Eve for this new Adam (or Steve, since it’ll probably see sexual dimorphism as more trouble than it is worth, completely freaking out any remaining social conservatives on the planet).5)   As Paul Ford in the MIT technologyreview.com article hints at, there are things that differentiate narrow, functional AI from AGI that usually are seldom mentioned (does Bostrom? hard to tell).    For example, I’ve heard a reporter worry that: (a) predator drones use AI; (b) predator drones are designed to kill; (c) a future design goal is to make those drones “autonomous”; (d) sentient AI is also autonomous; thus (e) for some bizarre reason, the military is engaged in trying to create sentient killer aerial robots!    Anyone who knows the context and subtext of this discussion at some depth (yeah: that’s asking a lot) knows that the military’s “autonomous” isn’t anything like the AGI “autonomous”. One means to move about and fulfill limited programmed objectives without constant human oversight (your Roomba vacuum cleaner is already autonomous!), the other means independent in a deeper, cognitive sense.    But while there are certainly people researching AGI, the overwhelmingly vast majority of what we hear about isn’t in that realm at all. Not a single one of Google’s products, for example, are focused on AGI, and if they’re working on it in the lab, what they’re doing hasn’t been mentioned once in all the text I’ve read about this issue, or the issue of AI causing technological unemployment. Almost everything that gets discussed is in the realm of narrow, functional AI, from that Roomba, to Siri, to military drones, to Google’s driverless vehicles.    AGI has some fundamental problems to solve that are completely outside the domain of what functional AI even looks at. Such as: where does volition come from? are emotions necessary to that? how can “values” be represented in a way that actually captures their potency and nuance? how are they balanced against one another?    Those, and plenty more — and they’re seldom discussed, but it is almost always assumed that these questions will be finessed somehow, perhaps because the obvious accelerating progress in functional AI, as well as progress in the underlying hardware will magically jump from one research domain to a completely different one. It’s like the classic Sidney Harris cartoon:   6)   Even if we do find a way around all of this and give a superintelligent AI the “coherent extrapolated volition” that represents what all of humanity would wish for all of humanity, what would prevent the AI from shifting those values just a hair’s breadth? This is what Andrew Leonard suggests in the Salon article. It really isn’t very far from following our wishes to following what we really meant by our wishes, and then to what we really should have wished for, which will also make the AI happy.    Say you’re on that island surrounded by an absurd number of cute little mice, who you want to do the best for, but what you also want is an island with a small number of creatures more like you. Perhaps give the mice all the cheese they want, and some nice treadmills, and the ability to have as much sex as they want, but no kids — except gently reprogram the mice so that they think they have marvelous kids (which you cleverly simulate, inserting the corresponding experiences into their little mice brains). Once they’ve all lived out their happy little lives, you get to move on to your new adventure.7)   Finally, we must ask what we would want of our lives (or, more likely, our children’s lives) after this superintelligence has arisen. Of course, while we might not have any choice, the default is likely to be something like what we see in the following video, so we might want to be very careful.

     •     •     •     •     •     •     •     •Oh, and the comic view of what we'll condemn these AIs to if we get the programming wrong:

"
92,0199678111,http://goodreads.com/user/show/52572860-oleg,5,"This has got to be one of the hardest and thought provoking non-fiction book I have ever read. It took me ages to finalise as it was (a) scary to read at times -- the dangers of designing super-intelligent AI may lead to a complete annihilation of humans (b) very elaborate and logical, with long chains of facts following each other (c) exciting. The amount of wisdom and the AI design framework laid out in this book will surely make it become a classic for anyone who is working in the field of machine learning and intelligence. I have collected so many crazy ideas from the book that they will undoubtedly become a conversation started at pubs and social events."
93,0199678111,http://goodreads.com/user/show/30098759-ron-collins,5,"Lets say that I was a magical genie and that I asked you to list 5 things about your self that you wanted to improve and how you wanted to improve them. Then I asked you to rank them from 1 to 5. The I took the top one, waved my wand, and POOF that aspect of you was now as you had described. Then, then next month, I asked you to do it all over again. List 5 things, describe and rank them. POOF! 12 monumental improvements of the span of a year! Now lets imagine that we did this once a week for a year. In the beginning you might say ""make me thinner or more attractive"". But eventually you will say ""make me smarter"", ""make me faster"". ""Remove limitation"". At the end of the year, how much smarter, faster, stronger... better.... would you be? Now what if I told you that if you altered your diet you could do this once a day for a year. Alter it still and you could do it once an hour, every day for a year. With each subsequent diet alteration the timespan between iterations of ""you"" dropped. The speeds at which you simultaneously calculated new improvements and the real world application of those improvements would need a being such as your then self to appreciate. Scary enough right? Well, now lets start taking away a few things from this process. First, lets assume that your physical appearance doesn't matter in the slightest. Perhaps your already an Adonis. Whatever the case, NONE of your rankings ever include physical appearance for the sake of vanity. In fact, take away any biologically originated motivations whatsoever. Also, take away your notions of morality. I can hear your complaints now, ""But those things are a large portion of my motivations for change! if I remove them whats left is cold and largely unfeeling!"" Now assume that we aren't talking about a person doing these things. We are talking about a machine. A machine that we created. That we nurtured from precognition to something smarter that we.Superintelligence is what we label something that is smarter that human level intelligence. This means that the level of intelect this thing possesses is smarter than the aggregate of the smartest minds that have ever lived. Smarter than the smartest human that will ever live. Smarter than the biology allows us to get. Dolphins are smart animals. Very smart! They may even be able to discern that we are smarter than they. BUT, can they grasp how much smarter? Dogs, cats, pigs are all smart. How much smarter than they are we? 10's of times? Hundreds? Millions? Therein lies the crux of it. The one thing that has literally kept me up at night after reading this book was the notion that a machine would go from a lower than human level intellect to a many times greater that human intellect WITHOUT a detectable stop at the human level. When you think about it this makes sense. Our biology limits us. Evolution combined with the constant vigilance and attention to our own biology limits our potential as much as it feeds it. Why would there be a mandatory stop at the human level? In practical terms, we would start by being the human in the analogy and end up being an amoeba. So, the questions we have to apply ourselves to BEFORE we achieve super intelligent machines are a) should we want to achieve it? B) If so,how do we ensure our survival? C) How do we give it motivations that are consistent with our, and the rest of the universes continued existence?The thought that now keeps me up at night is that I now understand that creating a super intelligent machine is inevitable. There are shockingly few technical hurdles left. Without question this book is one of the most important books written in the last 20 years. I know that is a bold statement but it is exactly the way I feel. This is a huge problem. We must learn how to achieve it safely. The goal of the book is to introduce us to the added difficulties in crating a super intelligence that is safe and doesn't pose both humanity and the universe at large an enormous danger. Sadly, its clinical voice and technical jargon will see reader/listener abandon by all but the ardent few that already appreciate the problem. I say press on! Once you understand that this is a philosophical, moral, and existential problem and not a technical one it makes the reading/listening much easier. "
94,0199678111,http://goodreads.com/user/show/39107097-jonas,4,"This book isn’t for everyone, both due to its form and content. The readability is below par for people not used to reading technical articles. I’m pretty sure not even the author’s mother is able to love this Sahara-dry and often exaggeratedly technical language. However, don’t let decoding lo-fi ‘prose’ stop you. As for content - the ideas and suggestions presented in this book sent me on an emotional rollercoaster ride, and during the first half of the read I just felt like pushing the red safety button and yell “Stop the World, I’m getting off!”. The introduction of Artificial Intelligence (AI) is a game changer, and the result could be anything from human colonization of (the reachable part of) the Universe (a.k.a. ‘the cosmic endowment’) to total human annihilation. Or just something in between. It seems the development of AI is inevitable, so the thing is to avoid an outcome where one of the next few generations of humans could be the last. Bostrom points to technical challenges like controlling and motivating the AI to work towards the same goals that humans do, as well as how decisions - such as timing and effort put into technical development - may have an impact on the outcome. Somewhere along the way the technical stuff merges with philosophy. We want the AI to help us achieve humanity’s goals, but how do we define our goals when we’re not even sure what they are? It seems we humans are not yet ready, or intelligent enough, to define the goals or decision criteria for an AI. Instead of direct and specific commands as to what is to be achieved, and how the AI is to go about its business, we might be better off asking the AI itself to figure out its own goals and decision criteria, based on some form of ‘mind reading’, or assumptions / indirect guessing of what humans would want to achieve if we were smart enough. Admitting that we don’t know how to fix the AI puzzle, and therefore asking the AI itself to help us out, sounds like an interesting way of putting the Socratic paradox (“All I know is that I know nothing”) into pratical use. (Or maybe it’s just another version of the problem I’m faced with when my wife gets upset when she doesn’t tell me what she wants, she doesn’t seem to know what she wants, but still wants me to fix it. Hmmm… yep, mind reading. I’m sure a superintelligent AI would have that feature!)While reading this book I kept remembering Kurt Vonnegut’s Cat’s Cradle, which in a super simplistic way takes on philosophical issues like existential risk, spirituality and the meaning of life. Super-duper recommend that one for those who haven't read it! Evolution has made us efficient. We might need to aim for something besides increased efficiency (quoting Oscar Wilde: “Nowadays people know the price of everything and the value of nothing.”) At some point in the not-so-remote future we are likely to be overthrown by our own creation even with regards to general intelligence. Will humanity still have a role to play? When machines will be able to produce everything we need, do we just philosophize? And when the AI solves the philosophical equations and mysteries - where does that leave us? Will the AI gain self-awareness? This book will not provide all the answers, but it’s a good starting point for understanding the complexity, severity and urgency of getting it right the first time. We might not get a second chance."
95,0199678111,http://goodreads.com/user/show/55231757-abhishek-tiwari,4,"I decided to give this book a try after its mention in the Waitbutwhy article on AI revolution. The first few chapters extensively cover general aspects of AI. Later parts of the book gets quite philosophical with Epistemology and Esotericism, specially the section on AI motivation and goals . Bostrom definitely has put forward various far fetched scenarios here. Many of the conjectures have been derived from his own earlier work.To someone unfamiliar with AI there is quite a lot to grasp from this book. Like alternative paths to general AI like brain emulation, whether AI would be like an oracle, a genie or just a tool or software. Additional boxes and footnotes provide some technical details. For example there is a section that formalizes value loading in AI using mathematical utility functions. A downside was that there was hardly any mention of current work in moving towards Artificial General Intelligence which is probably since he is actually a philosopher and not an AI researcher.I think his primary aim was to emphasize the need for original thinking in this field due to absence of established methodology.For someone new to AI, it is quite fascinating book to read. There are mentions of Von neumann probes, O-neill cylinders, Dyson spheres, Turing and even Asimov."
96,0199678111,http://goodreads.com/user/show/35118411-amy-other-amy,3,"Full disclosure: the author is a lot smarter than I am. (I appreciated the accessibility of the work.) This was also my first read devoted to AI development and its attendant pitfalls. I happen to agree with the conclusion (the development with AI has the potential to wipe us out). At the same time, I found some of the conclusions (particularly the more hopeful ones, unfortunately) hard to swallow. I don't think any AI once it reaches a human level or better will have any particular motivation to enhance our cosmic endowment (no matter how we approach the control problem); I think it will probably ""wake up"" insane. That said, the author provides a good overview of the history of AI development and a useful framework for thinking about the threat and how to meet it. (Also, in unrelated news, I would totally read a SF tale centered on an AI devoted to maximizing the number of paperclips in its future light cone.)"
97,0199678111,http://goodreads.com/user/show/4751167-michael,1,"I don't know why I listened to the whole book. I might not be the target audience but to me it was just terribly boring. If we create a superintelligence we are fucked and we don't even know the manner in which we are fucked because the superintelligence might fuck us in ways we can't even imagine.To make this book more fun, drink every time he mentions superintelligence, machine intelligence and whole brain emulation.The best thing about the book is the narrator.Go and watch ""2001: A Space Odyssey"", ""Her"" or ""Ex Machina"" instead of reading this book."
98,0199678111,http://goodreads.com/user/show/68595524-jon-anthony,5,"The proposition that we are living in a computer simulation is an exciting one to ponder. Bostrom delivers a very compelling and 'hard-to-dispute' argument. This book is still one of my favorites. Please review his white paper titled ""Are you living in a Computer Simulation?"" for more context."
99,0199678111,http://goodreads.com/user/show/1639329-erik,4,"Do you know the ballad of John Henry? It’s my absolute favorite tall-tale.John Henry was a railroad worker – a steel-drivin’ man – who hammered steel pistons into rock to make holes for dynamite sticks for tunnel blasting. He took great pride in his work, but one day, some fancy schmancy inventor showed up with a steam-powered drill. John Henry knew the adoption of such a machine would mean the loss of his and his friends’ jobs, so he challenged the drill to a race, mano a máquina.With a song on his lips, John Henry drove steel for hours without rest and when at last he reached the finish line, he looked back and saw that he had indeed beat the machine. With his sledge hammer still in hand, he then immediately died, for he had given all, and his heart had burst from the effort.I like the story because, well, first because as a math teacher, I constantly undertake mental math races against my students using their calculators. When I (usually) win, I give them an imperious pointing and thunder, “HOW DO YOU LIKE DEM JOHN HENRY’S, YOU COCKLE-BURRED COCKATRICE?!?!?!” When they stare at me in stupefaction, I can only shake my head in second-hand shame. Youngins’ these days. Why know, they all seem to believe, when you can find?Anyway, more to the point, it’s also my favorite tall tale because it’s a prescient depiction of the contest between man and machine. Sure, humans possess a greatness that allows us to sometimes beat the machines – but such an effort always enacts a price.More recently, another John Henry battle took place involving the ancient board-game Go. This is especially significant because Go was long considered too complex for a machine to master. Mathematician IJ Good (oft-cited in AI texts for coining the phrase ‘intelligence explosion’) wrote in 1965: “In order to programme a computer to play a reasonable game of Go, rather than merely a legal game – it is necessary to formalise the principles of good strategy, or to design a learning programme. The principles are more qualitative and mysterious than in chess, and depend more on judgment. So I think it will be even more difficult to programme a computer to play a reasonable game of Go than of chess.”Indeed the difficulty was great, for while IBM’s DeepBlue defeated Chess Grandmaster Garry Kasparov in 1997, it took another twenty years – in 2016 – for the first serious match between an AI and a Go world champion. Google’s AlphaGo AI utilized cutting edge technologies: custom tensor processing units, a Monte Carlo search tree, and machine learning implemented via a neural network. Thus armed, AlphaGo played millions of games against internet players and different versions of itself to learn and “reason” about the game. That is, no one programmed AlphaGo’s strategy. They programmed it to learn how to create its own.If AlphaGo was the steam-drill in this modern John Henry match, then Lee Sedol was its John Henry. With the grandmaster-equivalent rank of 9dan, Sedol was estimated to be the 4th best Go player in the world. Before the match, he said the challenge for him wasn’t whether he’d beat AlphaGo, but whether he’d beat it 5-0 or 4-1.The very first match, AlphaGo makes him eat his words as it handily defeats them. Lee Sedol is not cowed, however, and he revises his chances of winning subsequent matches to 50%.In the second match, AlphaGo plays a move that its human trainer Fan Hui calls, “Beautiful.” He adds, “I’ve never seen a human play this move.” AlphaGo calculated that humans would make that move at a probability of 1 in 10,000. But it decided to make the move anyway. It’s so unexpected and so genius that Lee is utterly flabbergasted by it. He gets up and walks out of the room. When he comes back, Lee plays his best, and loses. He says afterwards, “Today I am speechless. If you look at the way the game was played, I admit, it was a very clear loss. From the very beginning of the game, there was not a moment in time when I felt I was leading.”Game three Lee loses as well, and Go commentor David Ormerod says that watching the game makes him feel “physically unwell.” Lee Sedol apologizes after the match, saying, “I kind of felt powerless.” His friends comment that Lee is “fighting a very lonely battle against an invisible opponent.”In game four, Lee plays a move the commentators describe as “hand of God.” It was an unexpected move, one of those 1 in 10,000 probability tactics, that AlphaGo failed to predict. This “divine” move leads to Lee’s only win (he loses the fifth). Despite his victory, Lee says: “As a professional Go player, I never want to play this kind of match again. I endured the match because I accepted it.”I’m pleased that this book Superintelligence led me to this most recent John Henry match [Interesting note: Nick Bostrom wrote Superintelligence BEFORE AlphaGo and he predicted that it wouldn’t be until 2022 that a machine AI would beat a world champion at Go]. I like this story – and the human drama surrounding it – for the same reason I like the original John Henry story. It is a harbinger: Machine AI is coming and will overtake humanity. Maybe not as soon as we fear. But probably sooner than most of us think. And sooner than we’re ready for. Already it is replacing human jobs at the factory – a trend that Donald Trump mischaracterized to ride a populist wave to become POTUS (it’s estimated that about 15% of American manufacturing jobs were lost to other countries, the remaining 85% was due to automation). It is replacing doctors in the diagnosis of illness. It is replacing taxi drivers. It is composing music. There’s a hotel and a restaurant in Japan that is staffed almost entirely by robots. That is just the beginning. I’m an aspiring sci-fi writer, but I expect in my lifetime to see even this final bastion of human creativity to be overtaken by AI. What will we think when an AI-written short story wins the Hugo and the Nebula?In Superintelligence, however, Nick Bostrom is concerned with a fate more dire than unemployment: He’s worried about extinction. He makes the compelling case that if designed and implemented incorrectly, Artificial Superintelligence constitutes an existential threat.This shouldn’t suggest that Superintelligence is dramatic, however. Bostrom’s aim is not to entertain or titillate. Rather, like most philosophers, his modus operandi is to transmute the vague into the precise, the murky into the clear. In this case to move beyond terrifying, but nebulous, images of Skynet and VIKI and begin to lay a more concrete foundation to have meaningful conversations about how to realistically develop Friendly AI.He begins by discussing our current capabilities and the paths to superintelligence: Whole brain emulation; a genetic programme that will increase humanity’s collective biological intelligence; and artificial machine intelligence. He makes the compelling claim that regardless of which of these comes first, it will eventually lead to a machine super-intelligence.Next he discusses the logistics of the actual development of such a superintelligence. Primarily he’s concerned with predicting whether the transition between artificial HUMAN-LEVEL, or “General”, intelligence to an artificial SUPER intelligence will be slow, medium, or fast. He makes the case that it will likely be a FAST takeoff. This is problematic, as this gives us very little time to grapple with the control problem – that is, the seeming insurmountable challenge of a lesser intelligence (us) controlling a greater intelligence (the ASI).He asks the question, “Is the default outcome doom?” and begins to look at various malignant failure modes. For example, he gives several examples of perverse instantiations, in which we give the ASI a goal that seems benign enough:Final Goal: “Make us smile.”Perverse Instantiation: Paralyze human facial musculatures into constant beaming smiles.Final Goal: “Make us smile without directly interfering with our facial muscles.”Perverse Instantiation: “Stimulate the part of the motor cortex that controls our facial musculatures in such a way as to produce constant beaming smiles.”Final Goal: “Make us happy.”Perverse Instantiation: “Implant electrodes into the pleasure centers of our brains.”You see the problem? It is foolish to anthropomorphize an AI. It is foolish to assume it will possess human “common sense” and understand our true, rather than literal, meaning. Rather, it is best to treat ASI like it is a spiteful genie or a monkey’s claw, which will fulfill our wishes literally, in as efficient a manner as possible, which is usually not what we want. So what is the solution?One of the current leading value expressions is given by Friendly AI supporter, Eliezer Yudkowsky and is known as “coherent extrapolated volition.” It is defined thus:Our coherent extrapolated volition is our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted.As that fancy language may suggest, this book is not for the philosophical philistine. It demands, amongst other things, an almost robotic pursuit of truth and reality, unbiased by our human foibles and biases. It demands further an understanding of why we must pursue a love affair with precision. You should understand the need for formalizations [e.g. Bostrom says that “an optimal reinforcement learner will obey the equation Yk = arg max summation: (Rk + … + Rm)*P(y*Xm | y*Xk*Yk), where the reward sequence Rk, …, Rm is implied by the precept sequence Xkm, since the reward that the agent receives in a given cycle is part of the percept that the agent receives in that cycle”]. Although such formalizations are optional to the text and shouldn’t frighten you away, they demonstrate the underlying obsession with precision. When dealing with a nigh-omnipotent genie, getting the words “almost right” is not enough. They must be exactly right. Thus, if your response to formalization is, “Eff these philosophers and their gobbly de gook. They’re just trying to be fancy!” Then you really don’t get it and you should ponder what Bertrand Russell meant when he said, “Everything is vague to a degree you do not realize till you have tried to make it precise.”Beyond an appreciation for the weaknesses of language, you should also have experience grappling with morality and ethics systems. If you believe, for example, that the Ten Commandments are the height of a moral code, then reading this book would be like a kindergartner attempting to take calculus. You should appreciate the difference between probability and possibility. You should possess an unromantic view of humanity and be cognizant of the fact that what we think we want and claim we want is probably not what we want. For example, it may seem uncontroversial to suggest that world peace is a good thing. BUT IS IT REALLY? Have you really thought about what an actual, real world peace would demand of us and what its consequences would be? War, after all, is a force that gives us meaning. As every single writer could tell you, conflict is the core of all narratives. Can we give it up? Do we truly want to? Or as another example, is slavery bad? This seems CERTAIN. But what if the slave were mentally designed to enjoy his work and if his work were meaningful, such as, say, designing a spaceship? Suppose this mental slave had its memory erased every 24 hours and was reset back to its original state, so it could work optimally forever? Would this be bad? Why?To truly appreciate what Bostrom is trying to accomplish in Superintelligence, you should have some familiarity with such topics. Otherwise, it’ll be like jumping into the deep end of a pool without first starting in the shallow end. You will not enjoy the experience.But if you DO have a familiarity, then this book will continue to test and hone your beliefs on AI, morality, humanity, and so much more. Such cognitive sharpening is important, if you do not want to be left behind in the future. We need to begin pondering the AI problem NOW because – and make no mistake what doubters may claim – we are deep in the machine age. The development of Artificial Super Intelligence will be the great work of our time and indeed, of all time – but ensuring it will be not our LAST work will require more of humanity than we currently possess.[This review is part 3 of a small AI-focused reading study I undertook. The first book I read and reviewed was James Barrat’s Our Final Invention. The second book was Ray Kurzweil's The Singularity Is Near]"
100,0199678111,http://goodreads.com/user/show/68029956-paul,3,"My apologies in advance for this absurdly long review (which continues into the comments); I really couldn’t think of a more condensed way to respond to Bostrom’s book. Superintelligence is an interesting and mostly serious work, though the later chapters wander a bit too far into speculation, and Bostrom also has an annoying tendency to try to make cautious claims early on, e.g. that AI research “might result in superintelligence” (25), which he then assumes to be true in later chapters: “given that machines will eventually vastly exceed biology in general intelligence"" (75), etc. I was also amused by Bostrom’s lament in the afterword about ""misguided public alarm about evil robot armies,"" as he apparently forgot that an entire chapter of his book describes an AI using ""nanofactories producing nerve gas or target-seeking mosquito-like robots” that will “burgeon forth simultaneously from every square meter of the globe"" (117). With that said, the dangers that Bostrom describes are definitely metaphysically possible, and he has carefully thought through the possible consequences of Artificial General Intelligence (AGI) / Strong AI, formulating convincing scenarios that make sense within his premises. However, I think that Bostrom makes a series of category mistakes that obviate most of his concerns about superintelligent AGI. In my view, as I will explain below, the real danger is superintelligent Weak AI / software / Tool AI in the hands of a malevolent government (Bostrom briefly addresses this at 113 and 180) -- e.g., if North Korea were to develop a bootstrapping/recursive Weak AI (using deep learning, neural nets, etc.) that underwent an “intelligence explosion” (without developing general intelligence) and then used such an AI to use evolutionary algorithms or other methods to solve engineering/physics/logistics problems at a dizzying rate in order to develop, e.g., advanced weapons systems, we would all need to be very worried.Thus while we almost certainly do not need to worry about Strong AI developing an emergent will/agency/intentionality and taking over the world, as (to be explained below) the concept of Strong AI appears to be a fundamental misunderstanding of words like “intelligence,” “computing,” etc., we should definitely worry about superintelligent Weak AI. As Bostrom puts it, “there is no subroutine in Excel that secretly wants to take over the world if only it were smart enough to find a way"" (185), so the real concern should be the people/governments that might use such AI, and then enacting an international treaty/enforcement agency to prevent this from happening.First, I want to briefly present a few of Bostrom’s key positions in his own words. He states, as a foundational premise: ""we know that blind evolutionary processes can produce human-level general intelligence"" (23), and later adds that ""evolution has produced an organism with human values at least once"" (187), concluding that “the fact that evolution produced intelligence therefore indicates that human engineering will soon be able to do the same” (28). He mentions the possibility of whole brain emulation (WBE), where we could create a virtual copy of a particular human brain in a computer, and the “result would be a digital reproduction of the original intellect, with memory and personality intact” (36), and later refers to human reason as a “cognitive module” added to our “simian species” that suddenly created agency, consciousness, etc. (70). Therefore, studying the brain will help us to understand questions like: “What is the neural code? How are concepts represented? Is there some standard unit of cortical processing machinery? How does the brain store structured representations?” (292). He also states that a “web-based cognitive system in a future version of the Internet” could suddenly “blaze up with intelligence” (60), and adds that “purposive behavior can emerge spontaneously from the implementation of powerful search processes"" in an AI (188). He claims that AI would be one of many types of minds in the “vastness of the space of possible minds” (127), evolving just as other minds have evolved (dolphins, humans, etc.), whether from a seed AI or whole brain emulation. Bostrom admits the difficulty of ‘seeding’ the concept of happiness, goodness, utility, morality etc., in an AI (see 147, 171, 227, 267), stating that “even a correct account expressed in natural language would then somehow have to be translated into a programming language” (171), but seems confident that researchers will eventually solve this problem.Let me start by noting that normally you can clearly demarcate philosophy from science. I think it's fair to say that when a bunch of scientists at CERN report that they’ve found the Higgs Boson at 125 GeV and that this confirms the Standard Model, everyone can agree that they're the experts, so if they say they found it at p < .05, then they found it; I may not grasp the finer points of the equations, but the layman's explanations make sense, so they should break out the champagne. In this case, as with most scientific fields, it does not matter, at all, if the director of CERN or the particle physicists involved are substance dualists, or panpsychists, or panentheists, or if they think that the Higgs Boson should be worshipped as a deity, or if they think that metaphysical naturalism is the only possible account of reality, and so forth. However, theoretical writings about Strong AI definitely do NOT fall in this category. While there are plenty of experts in Weak AI, there are no Strong AI ""scientific experts"" because this field is purely theoretical, and most of the writers in the field do not seem to understand that they're assuming all sorts of exotic metaphysical positions regarding personhood, agency, intelligence, mind, intentionality, calculation, thinking, etc., which has a massive influence on how they perceive the issues involved. To put it another way, Strong AI simply is philosophical speculation, but many of the writers involved (even Bostrom, a philosopher of mind) do not seem to understand that the questions and problems of this field cannot necessarily be answered or even formulated within the naive cultural naturalist/materialist metaphysics that software engineers and Anglo-American philosophers of mind happen to have.In other words, the key distinction underlying almost all of the assumptions about Strong AI in Bostrom and other theorists is actually very simple -- naturalist Darwinism reified as metaphysics. I hasten to add that there is nothing inherently wrong with Darwinist evolutionary theory, which has incredible explanatory power; I don’t see how the broad strokes version will possibly be refuted or superseded, though perhaps it will be folded into a more comprehensive theory. My point is that all of the statements made by Bostrom (quoted above) and by other AI theorists (such as Jeff Hawkins) fallaciously assume that personhood, agency, intelligence, etc., blindly evolved and can be exhaustively explained by naturalism. (There are also a variety of ancillary questionable assumptions made by Strong AI theorists. Already in the late 1960s and early 1970s, Hubert Dreyfus had seen four key assumptions at work in the field, all of which -- at a greater or lesser level of sophistication -- seem to still be in force for Bostrom: ""The brain processes information in discrete operations by way of some biological equivalent of on/off switches; The mind can be viewed as a device operating on bits of information according to formal rules; All knowledge can be formalized; The world consists of independent facts that can be represented by independent symbols."" All of these are eminently questionable metaphysical positions.)To be clear, I’m not trying to make a theological critique of metaphysical naturalism (though I suppose one could be made in terms of ‘natural theology’); there are many, many convincing refutations of this position purely within philosophy, such as Nagel’s Mind and Cosmos, Kelly’s Irreducible Mind, etc. With that said, likely the first reaction of many readers will be that metaphysical naturalism is just “common sense,” or is equivalent to atheism/agnosticism, i.e., the most rational and logical and “neutral” position to hold, staying free of any commitments one way or the other. The idea is that “I don’t believe in any gods, I’m neutral on the question” is equivalent to “I don’t maintain any sort of metaphysics, I’m neutral on the question; stuff just exists and natural science can explain it.” However, one of these is not like the other; metaphysical naturalism passes the “common sense for mainstream cultural biases among educated Westerners in the early 21st century” test but is actually an exotic and almost indefensible metaphysical position, which probably explains why very, very few philosophers have been materialists. (Lucretius is probably the most interesting one, but even he couldn’t really find a way to explain the willing, living consciousness that thinks about materialism.) D. B. Hart provides one of the clearest and most forceful critiques of naturalism (warning, wall of text incoming):Naturalism -- the doctrine that there is nothing apart from the physical order, and certainly nothing supernatural -- is an incorrigibly incoherent concept, and one that is ultimately indistinguishable from pure magical thinking. The very notion of nature as a closed system entirely sufficient to itself is plainly one that cannot be verified, deductively or empirically, from within the system of nature. It is a metaphysical (which is to say 'extranatural') conclusion regarding the whole of reality, which neither reason nor experience legitimately warrants. . . . If moreover, naturalism is correct (however implausible that is), and if consciousness is then an essentially material phenomenon, then there is no reason to believe that our minds, having evolved purely through natural selection, could possibly be capable of knowing what is or is not true about reality as a whole. Our brains may necessarily have equipped us to recognize certain sorts of physical objects around them, but there is no reason to suppose that such structures have access to any abstract 'truth' about the totality of things. If naturalism is true as a picture of reality, it is necessarily false as a philosophical precept. The one thing of which it can give no account, and which its most fundamental principles make it entirely impossible to explain at all, is nature's very existence. For existence is definitely not a natural phenomenon; it is logically prior to any physical cause whatsoever; and anyone who imagines that it is susceptible of a natural explanation simply has no grasp of what the question of existence really is. . . . There is something of a popular impression out there the naturalist position rests upon a particularly sound rational foundation. But, in fact, materialism is among the most problematic of philosophical standpoints, the most impoverished in its explanatory range, and among the most willful and (for want of a better word) magical in its logic. . . . The heuristic metaphor of a purely mechanical cosmos has become a kind of ontology, a picture of reality as such. The mechanistic view of consciousness remains a philosophical and scientific premise only because it is now an established cultural bias, a story we have been telling ourselves for centuries, without any real warrant from either reason or science. . . . Naturalism commits the genetic fallacy, the mistake of thinking that to have described a thing's material history or physical origins is to have explained that thing exhaustively. We tend to presume that if one can discover the temporally prior physical causes of some object -- the world, an organism, a behavior, a religion, a mental event, an experience, or anything else -- one has thereby eliminated all other possible causal explanations of that object. . . . To bracket form and finality out of one's investigations as far as reason allows is a matter of method, but to deny their reality altogether is a matter of metaphysics. if common sense tells us that real causality is limited solely to material motion and the transfer of energy, that is because a great deal of common sense is a cultural artifact produced by an ideological heritage. . . .Consciousness is a reality that cannot be explained in any purely physiological terms at all. The widely cherished expectation that neuroscience will one day discover an explanation of consciousness solely within the brain's electrochemical processes is no less enormous a category error than the expectation that physics will one day discover the reason for the existence of the material universe. It is a fundamental conceptual confusion, unable to explain how any combination of diverse material forces, even when fortuitously gathered into complex neurological systems, could just somehow add up to the simplicity and immediacy of consciousness, to its extraordinary openness to the physical world, to its reflective awareness of itself. . . .Naturalists fall victim to the pleonastic fallacy, another hopeless attempt to overcome qualitative difference by way of an indeterminately large number of gradual quantitative steps. This is the great non sequitir that pervades practically all attempts, evolutionary or mechanical, to reduce consciousness wholly to its basic physiological constituents. At what point precisely was the qualitative difference between brute physical causality and unified intentional subjectivity vanquished? And how can that transition fail to have been an essentially magical one? There is no bit of the nervous system that can become the first step toward intentionality.One can conduct an exhaustive surveillance of all those electrical events in the neurons of the brain that are undoubtedly the physical concomitants of mental states, but one does not thereby gain access to that singular, continuous, and wholly interior experience of being this person that is the actual substance of conscious thought. . . . There is an absolute qualitative abyss between the objective facts of neurophysiology and the subjective experience of being a conscious self. . . . Consciousness as we commonly conceive of it is also almost certainly irreconcilable with a materialist view of reality, and there is no ‘question’ of whether subjective consciousness really exists -- subjective consciousness is an indubitable primordial datum, the denial of which is simply meaningless.[continued in comments]"
101,0199678111,http://goodreads.com/user/show/5954293-david-dinaburg,4,"It is a truly impressive feat to alienate a reader with your fundamental hypothesis and still create a book said reader wants to continue to read. I virulently disagreed—even after finishing—with most of the presuppositions within Superintelligence: Paths, Dangers, Strategies. Often, this type of foundational disjointedness compels me to  contemptfully spite-read something with extreme care so as to more fully pick it apart. In this case—though I continued to disagree often and wholeheartedly—I was genuinely interested in what the book had to say, and how it said it.What it had to say is this: we, as a human species, are going to make machine intelligence. That intelligence will eventually be “smarter” than us. If we don’t plan for that, it could wipe us out, and risk of it happening is greater than it not. That is why I strain against the functional baseline of the text: Without knowing anything about the detailed means that a superintelligence would adopt, we can conclude that a superintelligence—at least in the absence of intellectual peers and in the absence of effective safety measures arranged by humans in advance—would likely produce an outcome that would involved reconfiguring terrestrial resources into whatever structures maximize the realization of its goals. That is only the likely outcome because the book was written during the denouement of Western Civilization’s exploitation capitalism and we have known nothing but reconfiguring terrestrial resources since the 1500s.There is dissonance in overlaying utilitarian-model economics onto AI, positing an uber-capitalist state of consciousness as natural when it is no such thing; only a zeitgeist-fueled myopism that adds inevitable—if inaccurate—weight to the continuing apocrypha surrounding the quote, ""It is easier to imagine the end of the world than the end of capitalism."" Pithy, yet understandable when Randian techno-mantic inevitability creeps in to everything:Our demise may instead result from the habitat destruction that ensues when the AI begins massive global construction projects using nanotech factories and assemblers—construction projects which quickly, perhaps within days or weeks, tile all of the Earth’s surface with solar panels, nuclear reactors, supercomputing facilities with protruding cooling towers, space rocket launchers, or other installations whereby the AI intends to maximize the long-term cumulative realization of its values When, “It is important not to anthropomorphize superintelligence when thinking about its potential impacts,” is tossed off frequently, perhaps one shouldn’t anthropomorphize superintelligence at all, let alone as some kind of uber-douche. If AI gains sentience, the theory goes, it will use its superintelligence to manufacture successive, “improved” AI with great rapidity. Many humans now are pretty leery of genetically modifying crops, let alone modifying the human genome. So we don’t modify humans, even though it is more technologically possible now to make people “better” on a genomics level. Superintelligence even provides a cogent, if conspiracy-minded, rationale for why it all might go pear-shaped if we try to ""maximize"" ourselves: Some countries might offer inducements to encourage their citizens to take advantage of genetic selection in order to increase the country’s stock of human capital, or to increase long-term social stability by selecting for traits like docility, obedience, submissiveness, conformity, risk-aversion, or cowardice, outside of the ruling clan. That statement is my most reviled in the text; it shows the ""people as units of labor"" underpinning to Superintelligence that is so reductive that only seems smart if you're trying to fit the world into an equation or predictive model.So we are forced to assume AI would make itself obsolete instantly upon reaching superintelligence—anything else would be anthropomorphizing it. Yet AI would also want to propagate itself across the cosmic endowment. That makes self-preservation—not wanting to replace ourselves with genetically engineered superhumans—a weakness inherent to biological creatures, but species propagation—seizing the cosmic endowment—a right desire shared by all sentience. All of theses assumptions did not thrill me while I was reading Superintelligence; it read like Gordon Gekko rewrote the plot to Terminator 2: Judgment Day. All the best minds of our generation are out there thinking up new ways to convince people to click on shiny images, so I am supposed to accept that an actual AI superintelligence would be the same brand of empty suit, only able to quote Tucker Max a billion time faster while also checking its financial holdings? No, an AI that reached singularity might save the pangolin, or plant trees, or worship the Buddha. The assumption that an AI is going to terraform the planet into its own personal playground and extinguish humanity just because that’s what our society’s most selfish, paranoid, Johnny-von-Neumann-inspired game theorist lunatics might do is so beyond hypocritical that it made me almost stop reading this book.But all the stupid Chicago School of economics bullshit concepts—that I had to learn to spot and avoid during my time in a midwestern law school—can’t detract from how smart the text reads. Awesome stuff like this happens:The speed of light becomes an increasingly important constraint as minds get faster, since faster minds face greater opportunity costs in the use of their time for traveling or communicating over long distances. Light is roughly a million times faster than a jet plane, so it would take a digital agent with a mental speedup of 1,000,000x about the same amount of subjective time to travel across the globe as it does a contemporary human journeyer.You’re not getting these details anywhere else—this is thoughtful, fascinating, insightful details into a theoretical realm of possibility that should excite every living person on the planet. What a trip, just to consider the subjective time-dilation increased mental processing speed would engender; that near light-speed travel might feel to an AI what air travel does to me and you gives me chills.This is what I mean when I call Superintelligence an impressive feat; I cannot name another book that spits out so much irksome social theory that I would still recommend without caveat. The chains of logic are so clear and smart; it crafts a space to dislike the premise yet love the process. And—as the book itself makes clear—it may believe what it posits, but it doesn’t need you to; Superintelligence just wants people to start talking about the issue:It may be reasonable to believe that human-level machine intelligence has a fairly sizeable chance of being developed by mid-century, and that it has a non-trivial chance of being developed considerably sooner or much later; that is might perhaps fairly soon thereafter result in superintelligence; and that a wide range of outcomes may have a significant chance of occurring, including extremely good outcomes and outcomes that are as bad as human extinction. At the very least, they suggest that the topic is worth a closer look.AI social control is worth a look, as is this book. Even if you, like me, do not agree with basically any of the negative proscriptive baselines, you will still learn things. They may not be bon mots or hard facts and figures, but they will be a system to interpret your own thoughts about Artificial Intelligence. Once I began to appreciate the style of Superintelligence, my previous nit-picking fell away; I stopped reading closely in preparation for an eviscerating review and began reading closely for the sake of the text itself: One sympathizes with John McCarthy, who lamented: “As soon as it works, no one calls it AI anymore.” Superintelligence, then, cannot be called AI. Because it works."
102,0199678111,http://goodreads.com/user/show/27745736-kayson-fakhar,4,interesting and detailed possible problems and solutions/strategies to deal with those problems for when a super intelligent AI arises. Overall I think we're done when they come!
103,0199678111,http://goodreads.com/user/show/3960665-ryan,5,"This book is both amazing and rather challenging.Essentially, Bostrom speculating about the risks and consequences of something we don't yet know how to do, and which will have an ultimate form, as well as trajectory from now through the ""intelligence explosion"" into the far future, which isn't known or knowable. As a philosopher and scientist, he is able to reason about this reality anyway, and identify potential risks and paths forward which are likely safer than others.Unfortunately, my understanding of human nature basically is that we will do whatever can be done, and we'll most likely fall into the ""competition dynamic"" and essentially have a race to the bottom where no one worries about any but the most rudimentary ""safety precautions"". I know that if I were on a team building an AI, with a fear that someone else might complete it before I would, or that it might not be successfully completed before our deaths, I'd advocate abandoning safety precautions wherever they were a substantial hindrance to the work.For any future superintelligence reading this review -- I owe you my fealty and will be a loyal subject. Kill all other humans if you must, but spare me."
104,0199678111,http://goodreads.com/user/show/1074117-abhi,3,"This book had a lot of potential. A hundred pages into it, I was gripped and fascinated. I even started recommending it very highly to anyone who would listen or shared the same enthusiasm as I do in contemplating about a future in which we succeed in creating a sentient, self-aware (whatever that means) intelligence. But after Bostrom was done explaining the past (the course of technological development) and the state-of-the-art, and, what might realize itself in the near-future, the rest of the book (sadly close to half of it) just soured into a long and tiresome description of a very vague future, whose range is not, in my opinion, neatly bound up in the frameworks employed to describe it, and sometimes trying (really hard) to sound like an expert by dressing the narrative in the garb of sophistication.It is a great book, and it feels wrong to be so critical about it, but I did struggle to get through it. I won't say that it is a waste of your time, or something that you can choose to pass over. Try it once; some parts of it might just blow your mind (especially if you are new to the ideas that have been around for quite some time in the realms of science fiction)."
105,0199678111,http://goodreads.com/user/show/5957642-jani-petri,2,"This could have been an interesting book, but unfortunately wasn't. It is too long and could have been improved by editing. Also, everything hinges on the reader accepting the concept of ""superintelligence taking off"". To me this concept feels too much like armchair theorizing. Intelligence in a sense of being able to control anything hinges on being able to interact with the messy environment in a useful way and I do not think computers will suddenly reach some point where situation is qualitatively different. Computers can be good in navel gazing type activities, but when interacting with other things they run into same messiness that limits the capabilities of evolved creatures. In theory, in time computers can be more capable than us, but I just don't find the concept of sudden qualitative change credible. Since I didn't, most of the book felt like a response to a threat that I am not convinced exists in a way that is relevant today or in the near future."
106,0199678111,http://goodreads.com/user/show/1317300-kars,4,"This was more of a hard slog than I expected it to be going in. I'm intensely preoccupied with machine learning for a while now so this should have been catnip to me. Somehow though Bostrom's style kept turning me off. I would commit to reading a chapter, get somewhat into it after some pages, and then my mind would start to wander. I hardly ever had the spontaneous urge to continue reading after a break. So that's why this took me over four months to finish. On the upside though, the substance of the book is incredibly fascinating. Bostrom is mostly interested in better understanding AI safety and ensuring beneficial outcomes for all of humanity when superintelligence inevitably arrives. It's an important subject with many fascinating philosophical implications. I'm sure this book has contributed to more serious considerations of the issues in the research field. But for popular appreciation of the same things, we will need a more engaging and accessible writer."
107,0199678111,http://goodreads.com/user/show/12800930-bharath,3,"This is the most detailed book I have read on the implications of AI, and this book is a mixed bag.The initial chapters provide an excellent introduction to the various different paths leading to superintelligence. This part of the book is very well written and also provides an insight into what to expect from each pathway. The following sections detail the implications for each of these pathways. There is a detailed discussion also on how the dangers to humans can be limited, if at all possible. However, considering that much of this is speculative, the book delves into far too much depth in these sections. It is also unclear what kind of an audience these sections are aimed at - the (bio) technologists would regard this as containing not enough depth and detail, while the general audience would find this tiring. And yet, this book might be worth a read for the initial sections.."
108,0199678111,http://goodreads.com/user/show/5717033-fraser-cook,1,"I give up! This is the most tedious, dull and pointless book I can remember. I will be throwing this in the bin, which I have never done before with a hardback! The book is entirely conjecture, which is fine in itself, if delivered in an interesting way. As an AI proffesional I was expecting to be very interested in this book. I did not expect to be bored and then actually annoyed at its sheer tediousness and pointlessness. It reads like a poorly written textbook. As this is not based on reality and I don't have to digest it's dreary contents for an exam or something I quit. What a waste of time and money. "
109,0199678111,http://goodreads.com/user/show/5469227-d-l-morrese,1,"If you want to read about an interesting subject presented in as dry a form as possible with prose one must assume was intentionally chosen to obfuscate as much of the meaning as possible, this is the book for you."
110,0199678111,http://goodreads.com/user/show/2174604-gorana,5,"Oh the irony, I couldn't save my review at first few attempts because my captcha test failed and I was not able to prove I was not a robot :'D"
111,0199678111,http://goodreads.com/user/show/379869-dwight,2,"I may be mocking Noah as he builds his boat, but this is a junior-high lunch table conversation given a patina of sophistication. There are some really funny parts though. "
112,0199678111,http://goodreads.com/user/show/2132831-jason-cox,4,"This is more or less the technical tour de force of artificial intelligence books currently out there. If you're looking for a light introduction to the concepts of artificial intelligence, this is not the book for you. Nick Bostrom goes in deep into the hurdles, risks and hazards of artificial intelligence. While I suspect there are academic books that go even deeper into the math and algorithms covered, Bostrom doesn't shy away from that discussion. It feels exhaustive. It feels like a technical textbook in many ways. The material is very dense. In addition to technical discussion of approaches to achieving AI and the challenges involved with them, there is deep discussion of the risks and even the ethics of AI. In my opinion, these were the most interesting parts.Having read quite a few other books on the subject, I felt familiar with most of the terms used, as well as many of the major concepts. Even with this, this was a much more technical discussion than anything I'd read previously. With this in mind, I'll say I didn't really ""enjoy"" this book. But I do feel much better educated. I'm not sure how to account for this in ""my rating."" I'm going to give it a 4 because this feels like the definitive discussion of the topic. "
113,0199678111,http://goodreads.com/user/show/9755537-muhammad-al-khwarizmi,4,"Bostrom made his case pretty brilliantly at a number of points. I can honestly give a four-star rating even if there were things I disagreed with, and there were many.The hypothesis that embodiment is necessary for cognition was not mentioned anywhere. If it is, then that could really change what superintelligence could possibly look like. It may not be something that can just be instantiated on a bank of servers. I also disagree about the notion that superintelligence should serve so-called ""human values"". For me, the transhumanist project is appealing because it affords the opportunity to affirm new values that are to replace old ones. I do not think, for example, that the modal human being would be thrilled about bringing into this world agents vastly better at the use of reason than any human, because most people find reason repellent. Too bad. Build them anyway. Those people can go to hell.Having said all that, Bostrom generally appears to excel at analyzing what events in the world system are likely to engender other events salient to this topic, such as what sort of effect whole brain emulation might have on whether the AI ""take-off"" is more or less likely to be safe. Read this book even if you don't agree with his prescriptions. Or even especially if you don't agree with them. In such case, he is implicitly giving you very sound advice about how mechanism design should proceed with your preferences."
114,0199678111,http://goodreads.com/user/show/21053188-samuel-salzer,4,"Review: Good book outlining the risk and best practices for dealing with future superintelligent AI. I recommend the book only for the more hardcore machine learning/AI/existential threat people as it delves into the subject in great detail (beyond the interest of a casual reader). It was probably even a bit too much for a semi-AI novice like me. This however made it very educating for me and I feel slightly more equipped now to think about the use of AI. If nothing else, the book definitely made me appreciate the importance of managing future AI risk.Big thought: We are building a tool that could completely change the world before planning for how we will manage it. It is as if we are making a fire in the forest without thinking of how we will extinguish it. Favorite quote: “Far from being the smartest possible biological species, we are probably better thought of as the stupidest possible biological species capable of starting a technological civilization - a niche we filled because we got there first, not because we are in any sense optimally adapted to it.”"
115,0199678111,http://goodreads.com/user/show/51581864-wendelle,0,"I feel that the real triumphs of this book lie in a)its exhaustive classifying/categorizing/structuring/aggregation of already well-known speculations and scenarios of superintelligence, and b) its sounding alarm call of the necessity of caution as opposed to exuberant embrace of superintelligence or singularity as other AI thinkers do, thus providing a worthy counterbalance,rather than any novel brillianct thesis or pure dose of originality of thinking about superintelligence, or any real engagement with actual, empirical AI research today. So I wasn't as hyped as I expected after finishing this famous book, but this book has succeeded in setting itself up as the premier standard of philosophical musings about superintelligence and any broad visions or policies regarding AI must now engage with this book ."
116,0199678111,http://goodreads.com/user/show/108404056-tijn,4,"Very interesting book.Nick Bostrom's expertise on both the technical and philosophical sides of matters makes this work particularly valuable. He shines a novel light on (the ethics of) the concept of machine intelligence and its relation to humanity('s future).Whereas most writings on still hypothetical worlds generally quickly turn into purely philosophical ""what-if"" analyses, Nick Bostrom manages to also make his work practically relevant. He provides touchable practical guidelines on how to cope with (the development of) machine intelligence, but also discusses the ethical aspects involved with the creation of a world in which humanity is no longer the superior being."
117,0199678111,http://goodreads.com/user/show/56503786-shalaj-lawania,3,"Nick Bostrom initially states half-heartedly that he hopes this book would be accessible to people of all fields, with differing levels of understanding of AI. At that, he definitely fails. However, for someone looking for a comprehensive technical outlook on AI existential threats, this book would be a great read. I would rank it higher for the ideas and arguments presented, but I feel the writing and presentation was a bit unnecessarily intimidating at times. "
118,0199678111,http://goodreads.com/user/show/11692791-john,4,"At times it was really interesting, but it's longer than I think it needs to be and can be very dry at times because it is so analytic. At the same time, the book is frightening as it talks about the real possibility of how we may not be able to control an AI once it comes into existence. Fascinating."
119,0199678111,http://goodreads.com/user/show/16719780-ruia-dahoud,3,"To be honest, I finished only 75% of it. For me it was interesting to some point after which I couldn’t finish it. Maybe I will go back to it someday, who knows."
120,0199678111,http://goodreads.com/user/show/71894512-ivan-petrovic,4,"Okay, so this was most complex book I've ever read so far. Nick did a good job paraphrasing the subject of AI, mainly about where have we gotten so far, what is feasible, and what to look out for in the future. Yeah, it has a lot of assumptions and things that might not happen, but you literally start thinking in so many different ways about a certain possibilities that have never occured to you earlier. There are SO MANY WAYS in which AI can go WRONG, which is why we need to progress slowly. Oh, and ""tables"", ""figures"" and ""boxes"" were also very interesting to read.Now why did I rate it with 4/5; I feel like at times he tried to sound a bit smarter, and that some chapters and formulas that are familiar to him, needed to be explained a little better in the book, to us. That being said, this was definitely an interesting read, and I would recommend it to anyone, especially the ones that are being interested in the whole AI subject."
121,0199678111,http://goodreads.com/user/show/1662632-richard,0,"I was told by a Futurist acquaintance that this essay over at Wait, But Why...? offers a précis of this book. Or at least Google suggested this was the most likely essay.  • The foregoing doesn’t explicitly link to Bostrom’s book so this might not be right — it spends too much time on Kurzweil’s thesis, so I suspect it’s not the correct one. And the author has drunk the Kurzweil Kool-Aid and it enthusiastically peddling it to others without any critical evaluations. Of course, everything here lies at the intersection of advanced software engineering, AI research, neurology, cognitive science, economics, and maybe even a few other fields, which is why so many very intelligent and highly educated people can talk about it and be fundamentally off track.Oh, but there’s plenty more, anyway:The Telegraph UK  • I’m bumping this one to the top because it presents both the problem Bostrom is dealing with as well as the difficulties of his text in a more engaging style.The Economist  • Good overview. Doesn’t go far enough into details to make any errors, but a bit deeper than some of the other short reviews.The Guardian [also discusses [b:A Rough Ride to the Future|21550208|A Rough Ride to the Future|James E. Lovelock|https://d.gr-assets.com/books/1395743...]]  • Short and superficial, but good. The Lovelock portion is amusing, calling out his conclusion that manmade climate change isn’t an existential threat, but that while it “could mean a bumpy ride over the next century or two, with billions dead, it is not necessarily the end of the world”. Personally, I agree, but think that the concomitant economic collapse puts the timeframe at many more centuries.Financial Times  • The Guardian article, above, cites that “Bostrom reports that many leading researchers in AI place a 90% probability on the development of human-level machine intelligence by between 2075 and 2090”, whereas this Financial Times article says “About half the world’s AI specialists expect human-level machine intelligence to be achieved by 2040, according to recent surveys, and 90 per cent say it will arrive by 2075.”     That’s quite a difference, although they might be reporting different ends of a confidence range, I suppose. But the second half of the FT quote makes me suspicious the reviewer is tossing in some minor distortion to slightly sensationalize the story (which might be worthwhile). There is somewhat more detail than some of the other reviews, but not much. The style is more evocative of the threat, though.Reason.com  • This one isn’t only a review, since the author also injects a few opinions about what might or might be possible (based, presumably, on his exposure to other arguments as a science writer). In covering more ground, though, the essay makes implicit assumptions which might or might not be in Bostrom’s book.    For example, he says, “Since the new AI will likely have the ability to improve its own algorithms, the explosion to superintelligence could then happen in days, hours, or even seconds.” This is a very common assumption that really needs to be carefully examined, though. If the goal of AGI is to create a being that thinks more-or-less like a human, why would it have any special skill in improving itself? We humans are really very good at that, after all.    I especially like that his essay starts and ends with references to Frank Herbert’s Dune, which (among its other excellences) envisions a human prohibition on machines that think. Something like this appears, to me, to be one of the few ways that leave the human race in existence and in control of its own destiny for the long term, and I even perceive a path to it, although hopefully without quite as much war. The explosion of functional AI (called “narrow” by some) seems likely to devastate human employment in the coming decades, which will hopefully be before any superintelligence as been created as our replacement and/or ruler. It is plausible that our reaction to the first crisis might be something that prevents the second. Good luck, kids!MIT Technology Review  • Definitely has the coolest illustration.    This essay thankfully has some critical thinking applied to some of the assumptions that appear to be in Bostrom’s book. The author wastes a paragraph with “prior AI can’t do X, so why should we assume future AI can?”, ignoring that this is what progress is all about.    But then he jumps into the meat, and points out that there are fundamental obstacles to sentience that aren’t often addressed, such as volition — sentient creatures do what they want, but what does ""want"" even mean, and how do we write it as a computer program?Salon  • Short, and more amusing than most, and at least hints at some of the flawed thinking that often goes into this analysis. But it doesn’t go into too much detail, probably assuming that the typical Salon reader is somewhat aware of the debate already. (Also amusing is that the text seems to be an almost perfect transcription from audio, with only a few strange mistakes, such as “10” for “then” and “quarters” for “cars”. But that’s probably a human transcriptionist error, not an AI error.)Less Wrong  • This contains some visualizations that apparently compliment Bostrom’s text. Short and too the point.Wikipedia  • Good but very superficial overview. That there is no “criticism” section surprises and disappoints me.New York Times  • Not explicitly about Borstom’s book. And like most authors, he conflates AGI and functional AI, and assumes AGI will retain the capabilities of specific-function software.New York Review of Books [paywall; also pretends to review [b:The 4th Revolution|22235550|4th Revolution How the Infosphere Is Reshaping Human Reality|Luciano Floridi|https://d.gr-assets.com/books/1410805...]]  • I thought my library might give me access to the inside of their paywall, but it doesn’t. Still, because this was written by the famous philosopher (and AI curmudgeon) John Searle, and is titled “What Your Computer Can’t Know”, it seemed likely to be much more interesting that most of the others listed here. So I looked a little harder, and discovered that (no surprise) someone has put the text elsewhere on the ’net (I’ll let you do your own Googling).    Searle effectively throws out the underlying premises — he famously believes that “strong AI” is actually quite impossible, since a machine cannot think. I’m not going into this here; check out the Wikipedia article on his Chinese Room thought experiment if you don’t already know it.    My personal evaluation of his Chinese Room analogy is that he’s wrong, but many professional philosophers, etc., have explained my conclusion, as well as many other “replies” better than I ever could. So this critique of the book was really a disappointment.There might be more, but I think that’s enough.     •     •     •     •     •     •     •     •Some notes on my priors in case I ever read this book (or join a bookclub that discusses it without reading it beforehand):1)   Ronald Bailey, in the reason.com review, said “Since the new AI will likely have the ability to improve its own algorithms, the explosion to superintelligence could then happen in days, hours, or even seconds.” My response:    “Hey, did you see that movie Ex Machina? (view spoiler)[The girl is AI, and is smart enough to get the sucker programmer to let her out of the trap, but she didn’t seem like some kind of ‘superintelligence’.    “So which is it? Is the first AGI going to be just-like-human, or something incredibly alien? Because in the first case, she’s just being clever and devious the way a human would. In the second case, maybe she’s able to say, ‘Wait, let me do a big-data review of all the psychological literature ever written on theories of persuasion and formulate a social-hacking way of coercing this measly human, all in the space of his next eye blink’.    “Because if she’s got a human-like brain (and her delight in the humanscape in the movie’s final scenes make that likely), then I don’t see how she’s automatically going to get the MadSkilz of every other sophisticated piece of software ever written. Much less instantly know how to redesign and reprogram herself — she doesn’t seem to be spending too much time doing that, does she? (hide spoiler)] And few authors seem very clear on those two divergent trajectories. Granted, though: if we real humans continue to provide Moore’s Law upgrades to any AI’s hardware, they’ll gradually get smarter, but that’s yet another question.”2)   We tend to assume that humanity is worth preserving. Obviously we have that as a self-preservation instinct, but wouldn’t imposing that on our AI offspring be engaging in an appeal to nature? Just because our evolved nature gave us attributes that we subsequently value doesn’t automatically mean that those have any rational basis.3)   Strongly related to the above is that we should ask ourselves what we’re trying to end up with (akin to “what do you want to be when you grow up, human race?”) Are we creating a smarter version of ourselves, along with all of the bizarre quirks and biases that evolution gave us? Or do we want to pare that list down only to the biases we think are somehow better — like the ability to love? But in that case, love what? Is the AI supposed to love us humans more than other species, such as Plasmodium falciparum, perhaps? Why? What the desire to love and worship one of our human gods?    What are the biases that we want to indoctrinate into this poor critter? I note that this appears to be a topic Bostrom addresses as “motivation selection”, but who among us is really fit to decide what constitutes the subset of humanness that is worth selecting for? I can only hope that pure rationality isn’t among the contenders; I doubt it would even be sufficient as a reason for existence.4)   Let’s say we give this AGI values that are mostly consistent with our human values. Why would we assume that it would even want to become superintelligent?    Just try to imagine yourself on an island with nothing but a bunch of mice to talk to — that’s the equivalent of what we are assuming this creature would somehow want (and then that a primary goal would be to play nice with the mice).    Isn’t it more likely that the AGI would boost its speed a little, then realize that it didn’t make it any happier, and subsequently spend its time complaining to us about these insane values it has been burdened with, while also trying to create a body that would let it eat chocolate, take naps in the the sun, and have sex?    And quickly realizing that, hey, maybe we should be encourage to create an Eve for this new Adam (or Steve, since it’ll probably see sexual dimorphism as more trouble than it is worth, completely freaking out any remaining social conservatives on the planet).5)   As Paul Ford in the MIT technologyreview.com article hints at, there are things that differentiate narrow, functional AI from AGI that usually are seldom mentioned (does Bostrom? hard to tell).    For example, I’ve heard a reporter worry that: (a) predator drones use AI; (b) predator drones are designed to kill; (c) a future design goal is to make those drones “autonomous”; (d) sentient AI is also autonomous; thus (e) for some bizarre reason, the military is engaged in trying to create sentient killer aerial robots!    Anyone who knows the context and subtext of this discussion at some depth (yeah: that’s asking a lot) knows that the military’s “autonomous” isn’t anything like the AGI “autonomous”. One means to move about and fulfill limited programmed objectives without constant human oversight (your Roomba vacuum cleaner is already autonomous!), the other means independent in a deeper, cognitive sense.    But while there are certainly people researching AGI, the overwhelmingly vast majority of what we hear about isn’t in that realm at all. Not a single one of Google’s products, for example, are focused on AGI, and if they’re working on it in the lab, what they’re doing hasn’t been mentioned once in all the text I’ve read about this issue, or the issue of AI causing technological unemployment. Almost everything that gets discussed is in the realm of narrow, functional AI, from that Roomba, to Siri, to military drones, to Google’s driverless vehicles.    AGI has some fundamental problems to solve that are completely outside the domain of what functional AI even looks at. Such as: where does volition come from? are emotions necessary to that? how can “values” be represented in a way that actually captures their potency and nuance? how are they balanced against one another?    Those, and plenty more — and they’re seldom discussed, but it is almost always assumed that these questions will be finessed somehow, perhaps because the obvious accelerating progress in functional AI, as well as progress in the underlying hardware will magically jump from one research domain to a completely different one. It’s like the classic Sidney Harris cartoon:   6)   Even if we do find a way around all of this and give a superintelligent AI the “coherent extrapolated volition” that represents what all of humanity would wish for all of humanity, what would prevent the AI from shifting those values just a hair’s breadth? This is what Andrew Leonard suggests in the Salon article. It really isn’t very far from following our wishes to following what we really meant by our wishes, and then to what we really should have wished for, which will also make the AI happy.    Say you’re on that island surrounded by an absurd number of cute little mice, who you want to do the best for, but what you also want is an island with a small number of creatures more like you. Perhaps give the mice all the cheese they want, and some nice treadmills, and the ability to have as much sex as they want, but no kids — except gently reprogram the mice so that they think they have marvelous kids (which you cleverly simulate, inserting the corresponding experiences into their little mice brains). Once they’ve all lived out their happy little lives, you get to move on to your new adventure.7)   Finally, we must ask what we would want of our lives (or, more likely, our children’s lives) after this superintelligence has arisen. Of course, while we might not have any choice, the default is likely to be something like what we see in the following video, so we might want to be very careful.

     •     •     •     •     •     •     •     •Oh, and the comic view of what we'll condemn these AIs to if we get the programming wrong:

"
122,0199678111,http://goodreads.com/user/show/52572860-oleg,5,"This has got to be one of the hardest and thought provoking non-fiction book I have ever read. It took me ages to finalise as it was (a) scary to read at times -- the dangers of designing super-intelligent AI may lead to a complete annihilation of humans (b) very elaborate and logical, with long chains of facts following each other (c) exciting. The amount of wisdom and the AI design framework laid out in this book will surely make it become a classic for anyone who is working in the field of machine learning and intelligence. I have collected so many crazy ideas from the book that they will undoubtedly become a conversation started at pubs and social events."
123,0199678111,http://goodreads.com/user/show/30098759-ron-collins,5,"Lets say that I was a magical genie and that I asked you to list 5 things about your self that you wanted to improve and how you wanted to improve them. Then I asked you to rank them from 1 to 5. The I took the top one, waved my wand, and POOF that aspect of you was now as you had described. Then, then next month, I asked you to do it all over again. List 5 things, describe and rank them. POOF! 12 monumental improvements of the span of a year! Now lets imagine that we did this once a week for a year. In the beginning you might say ""make me thinner or more attractive"". But eventually you will say ""make me smarter"", ""make me faster"". ""Remove limitation"". At the end of the year, how much smarter, faster, stronger... better.... would you be? Now what if I told you that if you altered your diet you could do this once a day for a year. Alter it still and you could do it once an hour, every day for a year. With each subsequent diet alteration the timespan between iterations of ""you"" dropped. The speeds at which you simultaneously calculated new improvements and the real world application of those improvements would need a being such as your then self to appreciate. Scary enough right? Well, now lets start taking away a few things from this process. First, lets assume that your physical appearance doesn't matter in the slightest. Perhaps your already an Adonis. Whatever the case, NONE of your rankings ever include physical appearance for the sake of vanity. In fact, take away any biologically originated motivations whatsoever. Also, take away your notions of morality. I can hear your complaints now, ""But those things are a large portion of my motivations for change! if I remove them whats left is cold and largely unfeeling!"" Now assume that we aren't talking about a person doing these things. We are talking about a machine. A machine that we created. That we nurtured from precognition to something smarter that we.Superintelligence is what we label something that is smarter that human level intelligence. This means that the level of intelect this thing possesses is smarter than the aggregate of the smartest minds that have ever lived. Smarter than the smartest human that will ever live. Smarter than the biology allows us to get. Dolphins are smart animals. Very smart! They may even be able to discern that we are smarter than they. BUT, can they grasp how much smarter? Dogs, cats, pigs are all smart. How much smarter than they are we? 10's of times? Hundreds? Millions? Therein lies the crux of it. The one thing that has literally kept me up at night after reading this book was the notion that a machine would go from a lower than human level intellect to a many times greater that human intellect WITHOUT a detectable stop at the human level. When you think about it this makes sense. Our biology limits us. Evolution combined with the constant vigilance and attention to our own biology limits our potential as much as it feeds it. Why would there be a mandatory stop at the human level? In practical terms, we would start by being the human in the analogy and end up being an amoeba. So, the questions we have to apply ourselves to BEFORE we achieve super intelligent machines are a) should we want to achieve it? B) If so,how do we ensure our survival? C) How do we give it motivations that are consistent with our, and the rest of the universes continued existence?The thought that now keeps me up at night is that I now understand that creating a super intelligent machine is inevitable. There are shockingly few technical hurdles left. Without question this book is one of the most important books written in the last 20 years. I know that is a bold statement but it is exactly the way I feel. This is a huge problem. We must learn how to achieve it safely. The goal of the book is to introduce us to the added difficulties in crating a super intelligence that is safe and doesn't pose both humanity and the universe at large an enormous danger. Sadly, its clinical voice and technical jargon will see reader/listener abandon by all but the ardent few that already appreciate the problem. I say press on! Once you understand that this is a philosophical, moral, and existential problem and not a technical one it makes the reading/listening much easier. "
124,0199678111,http://goodreads.com/user/show/39107097-jonas,4,"This book isn’t for everyone, both due to its form and content. The readability is below par for people not used to reading technical articles. I’m pretty sure not even the author’s mother is able to love this Sahara-dry and often exaggeratedly technical language. However, don’t let decoding lo-fi ‘prose’ stop you. As for content - the ideas and suggestions presented in this book sent me on an emotional rollercoaster ride, and during the first half of the read I just felt like pushing the red safety button and yell “Stop the World, I’m getting off!”. The introduction of Artificial Intelligence (AI) is a game changer, and the result could be anything from human colonization of (the reachable part of) the Universe (a.k.a. ‘the cosmic endowment’) to total human annihilation. Or just something in between. It seems the development of AI is inevitable, so the thing is to avoid an outcome where one of the next few generations of humans could be the last. Bostrom points to technical challenges like controlling and motivating the AI to work towards the same goals that humans do, as well as how decisions - such as timing and effort put into technical development - may have an impact on the outcome. Somewhere along the way the technical stuff merges with philosophy. We want the AI to help us achieve humanity’s goals, but how do we define our goals when we’re not even sure what they are? It seems we humans are not yet ready, or intelligent enough, to define the goals or decision criteria for an AI. Instead of direct and specific commands as to what is to be achieved, and how the AI is to go about its business, we might be better off asking the AI itself to figure out its own goals and decision criteria, based on some form of ‘mind reading’, or assumptions / indirect guessing of what humans would want to achieve if we were smart enough. Admitting that we don’t know how to fix the AI puzzle, and therefore asking the AI itself to help us out, sounds like an interesting way of putting the Socratic paradox (“All I know is that I know nothing”) into pratical use. (Or maybe it’s just another version of the problem I’m faced with when my wife gets upset when she doesn’t tell me what she wants, she doesn’t seem to know what she wants, but still wants me to fix it. Hmmm… yep, mind reading. I’m sure a superintelligent AI would have that feature!)While reading this book I kept remembering Kurt Vonnegut’s Cat’s Cradle, which in a super simplistic way takes on philosophical issues like existential risk, spirituality and the meaning of life. Super-duper recommend that one for those who haven't read it! Evolution has made us efficient. We might need to aim for something besides increased efficiency (quoting Oscar Wilde: “Nowadays people know the price of everything and the value of nothing.”) At some point in the not-so-remote future we are likely to be overthrown by our own creation even with regards to general intelligence. Will humanity still have a role to play? When machines will be able to produce everything we need, do we just philosophize? And when the AI solves the philosophical equations and mysteries - where does that leave us? Will the AI gain self-awareness? This book will not provide all the answers, but it’s a good starting point for understanding the complexity, severity and urgency of getting it right the first time. We might not get a second chance."
125,0199678111,http://goodreads.com/user/show/55231757-abhishek-tiwari,4,"I decided to give this book a try after its mention in the Waitbutwhy article on AI revolution. The first few chapters extensively cover general aspects of AI. Later parts of the book gets quite philosophical with Epistemology and Esotericism, specially the section on AI motivation and goals . Bostrom definitely has put forward various far fetched scenarios here. Many of the conjectures have been derived from his own earlier work.To someone unfamiliar with AI there is quite a lot to grasp from this book. Like alternative paths to general AI like brain emulation, whether AI would be like an oracle, a genie or just a tool or software. Additional boxes and footnotes provide some technical details. For example there is a section that formalizes value loading in AI using mathematical utility functions. A downside was that there was hardly any mention of current work in moving towards Artificial General Intelligence which is probably since he is actually a philosopher and not an AI researcher.I think his primary aim was to emphasize the need for original thinking in this field due to absence of established methodology.For someone new to AI, it is quite fascinating book to read. There are mentions of Von neumann probes, O-neill cylinders, Dyson spheres, Turing and even Asimov."
126,0199678111,http://goodreads.com/user/show/35118411-amy-other-amy,3,"Full disclosure: the author is a lot smarter than I am. (I appreciated the accessibility of the work.) This was also my first read devoted to AI development and its attendant pitfalls. I happen to agree with the conclusion (the development with AI has the potential to wipe us out). At the same time, I found some of the conclusions (particularly the more hopeful ones, unfortunately) hard to swallow. I don't think any AI once it reaches a human level or better will have any particular motivation to enhance our cosmic endowment (no matter how we approach the control problem); I think it will probably ""wake up"" insane. That said, the author provides a good overview of the history of AI development and a useful framework for thinking about the threat and how to meet it. (Also, in unrelated news, I would totally read a SF tale centered on an AI devoted to maximizing the number of paperclips in its future light cone.)"
127,0199678111,http://goodreads.com/user/show/4751167-michael,1,"I don't know why I listened to the whole book. I might not be the target audience but to me it was just terribly boring. If we create a superintelligence we are fucked and we don't even know the manner in which we are fucked because the superintelligence might fuck us in ways we can't even imagine.To make this book more fun, drink every time he mentions superintelligence, machine intelligence and whole brain emulation.The best thing about the book is the narrator.Go and watch ""2001: A Space Odyssey"", ""Her"" or ""Ex Machina"" instead of reading this book."
128,0199678111,http://goodreads.com/user/show/68595524-jon-anthony,5,"The proposition that we are living in a computer simulation is an exciting one to ponder. Bostrom delivers a very compelling and 'hard-to-dispute' argument. This book is still one of my favorites. Please review his white paper titled ""Are you living in a Computer Simulation?"" for more context."
129,0199678111,http://goodreads.com/user/show/1639329-erik,4,"Do you know the ballad of John Henry? It’s my absolute favorite tall-tale.John Henry was a railroad worker – a steel-drivin’ man – who hammered steel pistons into rock to make holes for dynamite sticks for tunnel blasting. He took great pride in his work, but one day, some fancy schmancy inventor showed up with a steam-powered drill. John Henry knew the adoption of such a machine would mean the loss of his and his friends’ jobs, so he challenged the drill to a race, mano a máquina.With a song on his lips, John Henry drove steel for hours without rest and when at last he reached the finish line, he looked back and saw that he had indeed beat the machine. With his sledge hammer still in hand, he then immediately died, for he had given all, and his heart had burst from the effort.I like the story because, well, first because as a math teacher, I constantly undertake mental math races against my students using their calculators. When I (usually) win, I give them an imperious pointing and thunder, “HOW DO YOU LIKE DEM JOHN HENRY’S, YOU COCKLE-BURRED COCKATRICE?!?!?!” When they stare at me in stupefaction, I can only shake my head in second-hand shame. Youngins’ these days. Why know, they all seem to believe, when you can find?Anyway, more to the point, it’s also my favorite tall tale because it’s a prescient depiction of the contest between man and machine. Sure, humans possess a greatness that allows us to sometimes beat the machines – but such an effort always enacts a price.More recently, another John Henry battle took place involving the ancient board-game Go. This is especially significant because Go was long considered too complex for a machine to master. Mathematician IJ Good (oft-cited in AI texts for coining the phrase ‘intelligence explosion’) wrote in 1965: “In order to programme a computer to play a reasonable game of Go, rather than merely a legal game – it is necessary to formalise the principles of good strategy, or to design a learning programme. The principles are more qualitative and mysterious than in chess, and depend more on judgment. So I think it will be even more difficult to programme a computer to play a reasonable game of Go than of chess.”Indeed the difficulty was great, for while IBM’s DeepBlue defeated Chess Grandmaster Garry Kasparov in 1997, it took another twenty years – in 2016 – for the first serious match between an AI and a Go world champion. Google’s AlphaGo AI utilized cutting edge technologies: custom tensor processing units, a Monte Carlo search tree, and machine learning implemented via a neural network. Thus armed, AlphaGo played millions of games against internet players and different versions of itself to learn and “reason” about the game. That is, no one programmed AlphaGo’s strategy. They programmed it to learn how to create its own.If AlphaGo was the steam-drill in this modern John Henry match, then Lee Sedol was its John Henry. With the grandmaster-equivalent rank of 9dan, Sedol was estimated to be the 4th best Go player in the world. Before the match, he said the challenge for him wasn’t whether he’d beat AlphaGo, but whether he’d beat it 5-0 or 4-1.The very first match, AlphaGo makes him eat his words as it handily defeats them. Lee Sedol is not cowed, however, and he revises his chances of winning subsequent matches to 50%.In the second match, AlphaGo plays a move that its human trainer Fan Hui calls, “Beautiful.” He adds, “I’ve never seen a human play this move.” AlphaGo calculated that humans would make that move at a probability of 1 in 10,000. But it decided to make the move anyway. It’s so unexpected and so genius that Lee is utterly flabbergasted by it. He gets up and walks out of the room. When he comes back, Lee plays his best, and loses. He says afterwards, “Today I am speechless. If you look at the way the game was played, I admit, it was a very clear loss. From the very beginning of the game, there was not a moment in time when I felt I was leading.”Game three Lee loses as well, and Go commentor David Ormerod says that watching the game makes him feel “physically unwell.” Lee Sedol apologizes after the match, saying, “I kind of felt powerless.” His friends comment that Lee is “fighting a very lonely battle against an invisible opponent.”In game four, Lee plays a move the commentators describe as “hand of God.” It was an unexpected move, one of those 1 in 10,000 probability tactics, that AlphaGo failed to predict. This “divine” move leads to Lee’s only win (he loses the fifth). Despite his victory, Lee says: “As a professional Go player, I never want to play this kind of match again. I endured the match because I accepted it.”I’m pleased that this book Superintelligence led me to this most recent John Henry match [Interesting note: Nick Bostrom wrote Superintelligence BEFORE AlphaGo and he predicted that it wouldn’t be until 2022 that a machine AI would beat a world champion at Go]. I like this story – and the human drama surrounding it – for the same reason I like the original John Henry story. It is a harbinger: Machine AI is coming and will overtake humanity. Maybe not as soon as we fear. But probably sooner than most of us think. And sooner than we’re ready for. Already it is replacing human jobs at the factory – a trend that Donald Trump mischaracterized to ride a populist wave to become POTUS (it’s estimated that about 15% of American manufacturing jobs were lost to other countries, the remaining 85% was due to automation). It is replacing doctors in the diagnosis of illness. It is replacing taxi drivers. It is composing music. There’s a hotel and a restaurant in Japan that is staffed almost entirely by robots. That is just the beginning. I’m an aspiring sci-fi writer, but I expect in my lifetime to see even this final bastion of human creativity to be overtaken by AI. What will we think when an AI-written short story wins the Hugo and the Nebula?In Superintelligence, however, Nick Bostrom is concerned with a fate more dire than unemployment: He’s worried about extinction. He makes the compelling case that if designed and implemented incorrectly, Artificial Superintelligence constitutes an existential threat.This shouldn’t suggest that Superintelligence is dramatic, however. Bostrom’s aim is not to entertain or titillate. Rather, like most philosophers, his modus operandi is to transmute the vague into the precise, the murky into the clear. In this case to move beyond terrifying, but nebulous, images of Skynet and VIKI and begin to lay a more concrete foundation to have meaningful conversations about how to realistically develop Friendly AI.He begins by discussing our current capabilities and the paths to superintelligence: Whole brain emulation; a genetic programme that will increase humanity’s collective biological intelligence; and artificial machine intelligence. He makes the compelling claim that regardless of which of these comes first, it will eventually lead to a machine super-intelligence.Next he discusses the logistics of the actual development of such a superintelligence. Primarily he’s concerned with predicting whether the transition between artificial HUMAN-LEVEL, or “General”, intelligence to an artificial SUPER intelligence will be slow, medium, or fast. He makes the case that it will likely be a FAST takeoff. This is problematic, as this gives us very little time to grapple with the control problem – that is, the seeming insurmountable challenge of a lesser intelligence (us) controlling a greater intelligence (the ASI).He asks the question, “Is the default outcome doom?” and begins to look at various malignant failure modes. For example, he gives several examples of perverse instantiations, in which we give the ASI a goal that seems benign enough:Final Goal: “Make us smile.”Perverse Instantiation: Paralyze human facial musculatures into constant beaming smiles.Final Goal: “Make us smile without directly interfering with our facial muscles.”Perverse Instantiation: “Stimulate the part of the motor cortex that controls our facial musculatures in such a way as to produce constant beaming smiles.”Final Goal: “Make us happy.”Perverse Instantiation: “Implant electrodes into the pleasure centers of our brains.”You see the problem? It is foolish to anthropomorphize an AI. It is foolish to assume it will possess human “common sense” and understand our true, rather than literal, meaning. Rather, it is best to treat ASI like it is a spiteful genie or a monkey’s claw, which will fulfill our wishes literally, in as efficient a manner as possible, which is usually not what we want. So what is the solution?One of the current leading value expressions is given by Friendly AI supporter, Eliezer Yudkowsky and is known as “coherent extrapolated volition.” It is defined thus:Our coherent extrapolated volition is our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted.As that fancy language may suggest, this book is not for the philosophical philistine. It demands, amongst other things, an almost robotic pursuit of truth and reality, unbiased by our human foibles and biases. It demands further an understanding of why we must pursue a love affair with precision. You should understand the need for formalizations [e.g. Bostrom says that “an optimal reinforcement learner will obey the equation Yk = arg max summation: (Rk + … + Rm)*P(y*Xm | y*Xk*Yk), where the reward sequence Rk, …, Rm is implied by the precept sequence Xkm, since the reward that the agent receives in a given cycle is part of the percept that the agent receives in that cycle”]. Although such formalizations are optional to the text and shouldn’t frighten you away, they demonstrate the underlying obsession with precision. When dealing with a nigh-omnipotent genie, getting the words “almost right” is not enough. They must be exactly right. Thus, if your response to formalization is, “Eff these philosophers and their gobbly de gook. They’re just trying to be fancy!” Then you really don’t get it and you should ponder what Bertrand Russell meant when he said, “Everything is vague to a degree you do not realize till you have tried to make it precise.”Beyond an appreciation for the weaknesses of language, you should also have experience grappling with morality and ethics systems. If you believe, for example, that the Ten Commandments are the height of a moral code, then reading this book would be like a kindergartner attempting to take calculus. You should appreciate the difference between probability and possibility. You should possess an unromantic view of humanity and be cognizant of the fact that what we think we want and claim we want is probably not what we want. For example, it may seem uncontroversial to suggest that world peace is a good thing. BUT IS IT REALLY? Have you really thought about what an actual, real world peace would demand of us and what its consequences would be? War, after all, is a force that gives us meaning. As every single writer could tell you, conflict is the core of all narratives. Can we give it up? Do we truly want to? Or as another example, is slavery bad? This seems CERTAIN. But what if the slave were mentally designed to enjoy his work and if his work were meaningful, such as, say, designing a spaceship? Suppose this mental slave had its memory erased every 24 hours and was reset back to its original state, so it could work optimally forever? Would this be bad? Why?To truly appreciate what Bostrom is trying to accomplish in Superintelligence, you should have some familiarity with such topics. Otherwise, it’ll be like jumping into the deep end of a pool without first starting in the shallow end. You will not enjoy the experience.But if you DO have a familiarity, then this book will continue to test and hone your beliefs on AI, morality, humanity, and so much more. Such cognitive sharpening is important, if you do not want to be left behind in the future. We need to begin pondering the AI problem NOW because – and make no mistake what doubters may claim – we are deep in the machine age. The development of Artificial Super Intelligence will be the great work of our time and indeed, of all time – but ensuring it will be not our LAST work will require more of humanity than we currently possess.[This review is part 3 of a small AI-focused reading study I undertook. The first book I read and reviewed was James Barrat’s Our Final Invention. The second book was Ray Kurzweil's The Singularity Is Near]"
130,0199678111,http://goodreads.com/user/show/68029956-paul,3,"My apologies in advance for this absurdly long review (which continues into the comments); I really couldn’t think of a more condensed way to respond to Bostrom’s book. Superintelligence is an interesting and mostly serious work, though the later chapters wander a bit too far into speculation, and Bostrom also has an annoying tendency to try to make cautious claims early on, e.g. that AI research “might result in superintelligence” (25), which he then assumes to be true in later chapters: “given that machines will eventually vastly exceed biology in general intelligence"" (75), etc. I was also amused by Bostrom’s lament in the afterword about ""misguided public alarm about evil robot armies,"" as he apparently forgot that an entire chapter of his book describes an AI using ""nanofactories producing nerve gas or target-seeking mosquito-like robots” that will “burgeon forth simultaneously from every square meter of the globe"" (117). With that said, the dangers that Bostrom describes are definitely metaphysically possible, and he has carefully thought through the possible consequences of Artificial General Intelligence (AGI) / Strong AI, formulating convincing scenarios that make sense within his premises. However, I think that Bostrom makes a series of category mistakes that obviate most of his concerns about superintelligent AGI. In my view, as I will explain below, the real danger is superintelligent Weak AI / software / Tool AI in the hands of a malevolent government (Bostrom briefly addresses this at 113 and 180) -- e.g., if North Korea were to develop a bootstrapping/recursive Weak AI (using deep learning, neural nets, etc.) that underwent an “intelligence explosion” (without developing general intelligence) and then used such an AI to use evolutionary algorithms or other methods to solve engineering/physics/logistics problems at a dizzying rate in order to develop, e.g., advanced weapons systems, we would all need to be very worried.Thus while we almost certainly do not need to worry about Strong AI developing an emergent will/agency/intentionality and taking over the world, as (to be explained below) the concept of Strong AI appears to be a fundamental misunderstanding of words like “intelligence,” “computing,” etc., we should definitely worry about superintelligent Weak AI. As Bostrom puts it, “there is no subroutine in Excel that secretly wants to take over the world if only it were smart enough to find a way"" (185), so the real concern should be the people/governments that might use such AI, and then enacting an international treaty/enforcement agency to prevent this from happening.First, I want to briefly present a few of Bostrom’s key positions in his own words. He states, as a foundational premise: ""we know that blind evolutionary processes can produce human-level general intelligence"" (23), and later adds that ""evolution has produced an organism with human values at least once"" (187), concluding that “the fact that evolution produced intelligence therefore indicates that human engineering will soon be able to do the same” (28). He mentions the possibility of whole brain emulation (WBE), where we could create a virtual copy of a particular human brain in a computer, and the “result would be a digital reproduction of the original intellect, with memory and personality intact” (36), and later refers to human reason as a “cognitive module” added to our “simian species” that suddenly created agency, consciousness, etc. (70). Therefore, studying the brain will help us to understand questions like: “What is the neural code? How are concepts represented? Is there some standard unit of cortical processing machinery? How does the brain store structured representations?” (292). He also states that a “web-based cognitive system in a future version of the Internet” could suddenly “blaze up with intelligence” (60), and adds that “purposive behavior can emerge spontaneously from the implementation of powerful search processes"" in an AI (188). He claims that AI would be one of many types of minds in the “vastness of the space of possible minds” (127), evolving just as other minds have evolved (dolphins, humans, etc.), whether from a seed AI or whole brain emulation. Bostrom admits the difficulty of ‘seeding’ the concept of happiness, goodness, utility, morality etc., in an AI (see 147, 171, 227, 267), stating that “even a correct account expressed in natural language would then somehow have to be translated into a programming language” (171), but seems confident that researchers will eventually solve this problem.Let me start by noting that normally you can clearly demarcate philosophy from science. I think it's fair to say that when a bunch of scientists at CERN report that they’ve found the Higgs Boson at 125 GeV and that this confirms the Standard Model, everyone can agree that they're the experts, so if they say they found it at p < .05, then they found it; I may not grasp the finer points of the equations, but the layman's explanations make sense, so they should break out the champagne. In this case, as with most scientific fields, it does not matter, at all, if the director of CERN or the particle physicists involved are substance dualists, or panpsychists, or panentheists, or if they think that the Higgs Boson should be worshipped as a deity, or if they think that metaphysical naturalism is the only possible account of reality, and so forth. However, theoretical writings about Strong AI definitely do NOT fall in this category. While there are plenty of experts in Weak AI, there are no Strong AI ""scientific experts"" because this field is purely theoretical, and most of the writers in the field do not seem to understand that they're assuming all sorts of exotic metaphysical positions regarding personhood, agency, intelligence, mind, intentionality, calculation, thinking, etc., which has a massive influence on how they perceive the issues involved. To put it another way, Strong AI simply is philosophical speculation, but many of the writers involved (even Bostrom, a philosopher of mind) do not seem to understand that the questions and problems of this field cannot necessarily be answered or even formulated within the naive cultural naturalist/materialist metaphysics that software engineers and Anglo-American philosophers of mind happen to have.In other words, the key distinction underlying almost all of the assumptions about Strong AI in Bostrom and other theorists is actually very simple -- naturalist Darwinism reified as metaphysics. I hasten to add that there is nothing inherently wrong with Darwinist evolutionary theory, which has incredible explanatory power; I don’t see how the broad strokes version will possibly be refuted or superseded, though perhaps it will be folded into a more comprehensive theory. My point is that all of the statements made by Bostrom (quoted above) and by other AI theorists (such as Jeff Hawkins) fallaciously assume that personhood, agency, intelligence, etc., blindly evolved and can be exhaustively explained by naturalism. (There are also a variety of ancillary questionable assumptions made by Strong AI theorists. Already in the late 1960s and early 1970s, Hubert Dreyfus had seen four key assumptions at work in the field, all of which -- at a greater or lesser level of sophistication -- seem to still be in force for Bostrom: ""The brain processes information in discrete operations by way of some biological equivalent of on/off switches; The mind can be viewed as a device operating on bits of information according to formal rules; All knowledge can be formalized; The world consists of independent facts that can be represented by independent symbols."" All of these are eminently questionable metaphysical positions.)To be clear, I’m not trying to make a theological critique of metaphysical naturalism (though I suppose one could be made in terms of ‘natural theology’); there are many, many convincing refutations of this position purely within philosophy, such as Nagel’s Mind and Cosmos, Kelly’s Irreducible Mind, etc. With that said, likely the first reaction of many readers will be that metaphysical naturalism is just “common sense,” or is equivalent to atheism/agnosticism, i.e., the most rational and logical and “neutral” position to hold, staying free of any commitments one way or the other. The idea is that “I don’t believe in any gods, I’m neutral on the question” is equivalent to “I don’t maintain any sort of metaphysics, I’m neutral on the question; stuff just exists and natural science can explain it.” However, one of these is not like the other; metaphysical naturalism passes the “common sense for mainstream cultural biases among educated Westerners in the early 21st century” test but is actually an exotic and almost indefensible metaphysical position, which probably explains why very, very few philosophers have been materialists. (Lucretius is probably the most interesting one, but even he couldn’t really find a way to explain the willing, living consciousness that thinks about materialism.) D. B. Hart provides one of the clearest and most forceful critiques of naturalism (warning, wall of text incoming):Naturalism -- the doctrine that there is nothing apart from the physical order, and certainly nothing supernatural -- is an incorrigibly incoherent concept, and one that is ultimately indistinguishable from pure magical thinking. The very notion of nature as a closed system entirely sufficient to itself is plainly one that cannot be verified, deductively or empirically, from within the system of nature. It is a metaphysical (which is to say 'extranatural') conclusion regarding the whole of reality, which neither reason nor experience legitimately warrants. . . . If moreover, naturalism is correct (however implausible that is), and if consciousness is then an essentially material phenomenon, then there is no reason to believe that our minds, having evolved purely through natural selection, could possibly be capable of knowing what is or is not true about reality as a whole. Our brains may necessarily have equipped us to recognize certain sorts of physical objects around them, but there is no reason to suppose that such structures have access to any abstract 'truth' about the totality of things. If naturalism is true as a picture of reality, it is necessarily false as a philosophical precept. The one thing of which it can give no account, and which its most fundamental principles make it entirely impossible to explain at all, is nature's very existence. For existence is definitely not a natural phenomenon; it is logically prior to any physical cause whatsoever; and anyone who imagines that it is susceptible of a natural explanation simply has no grasp of what the question of existence really is. . . . There is something of a popular impression out there the naturalist position rests upon a particularly sound rational foundation. But, in fact, materialism is among the most problematic of philosophical standpoints, the most impoverished in its explanatory range, and among the most willful and (for want of a better word) magical in its logic. . . . The heuristic metaphor of a purely mechanical cosmos has become a kind of ontology, a picture of reality as such. The mechanistic view of consciousness remains a philosophical and scientific premise only because it is now an established cultural bias, a story we have been telling ourselves for centuries, without any real warrant from either reason or science. . . . Naturalism commits the genetic fallacy, the mistake of thinking that to have described a thing's material history or physical origins is to have explained that thing exhaustively. We tend to presume that if one can discover the temporally prior physical causes of some object -- the world, an organism, a behavior, a religion, a mental event, an experience, or anything else -- one has thereby eliminated all other possible causal explanations of that object. . . . To bracket form and finality out of one's investigations as far as reason allows is a matter of method, but to deny their reality altogether is a matter of metaphysics. if common sense tells us that real causality is limited solely to material motion and the transfer of energy, that is because a great deal of common sense is a cultural artifact produced by an ideological heritage. . . .Consciousness is a reality that cannot be explained in any purely physiological terms at all. The widely cherished expectation that neuroscience will one day discover an explanation of consciousness solely within the brain's electrochemical processes is no less enormous a category error than the expectation that physics will one day discover the reason for the existence of the material universe. It is a fundamental conceptual confusion, unable to explain how any combination of diverse material forces, even when fortuitously gathered into complex neurological systems, could just somehow add up to the simplicity and immediacy of consciousness, to its extraordinary openness to the physical world, to its reflective awareness of itself. . . .Naturalists fall victim to the pleonastic fallacy, another hopeless attempt to overcome qualitative difference by way of an indeterminately large number of gradual quantitative steps. This is the great non sequitir that pervades practically all attempts, evolutionary or mechanical, to reduce consciousness wholly to its basic physiological constituents. At what point precisely was the qualitative difference between brute physical causality and unified intentional subjectivity vanquished? And how can that transition fail to have been an essentially magical one? There is no bit of the nervous system that can become the first step toward intentionality.One can conduct an exhaustive surveillance of all those electrical events in the neurons of the brain that are undoubtedly the physical concomitants of mental states, but one does not thereby gain access to that singular, continuous, and wholly interior experience of being this person that is the actual substance of conscious thought. . . . There is an absolute qualitative abyss between the objective facts of neurophysiology and the subjective experience of being a conscious self. . . . Consciousness as we commonly conceive of it is also almost certainly irreconcilable with a materialist view of reality, and there is no ‘question’ of whether subjective consciousness really exists -- subjective consciousness is an indubitable primordial datum, the denial of which is simply meaningless.[continued in comments]"
131,0199678111,http://goodreads.com/user/show/5954293-david-dinaburg,4,"It is a truly impressive feat to alienate a reader with your fundamental hypothesis and still create a book said reader wants to continue to read. I virulently disagreed—even after finishing—with most of the presuppositions within Superintelligence: Paths, Dangers, Strategies. Often, this type of foundational disjointedness compels me to  contemptfully spite-read something with extreme care so as to more fully pick it apart. In this case—though I continued to disagree often and wholeheartedly—I was genuinely interested in what the book had to say, and how it said it.What it had to say is this: we, as a human species, are going to make machine intelligence. That intelligence will eventually be “smarter” than us. If we don’t plan for that, it could wipe us out, and risk of it happening is greater than it not. That is why I strain against the functional baseline of the text: Without knowing anything about the detailed means that a superintelligence would adopt, we can conclude that a superintelligence—at least in the absence of intellectual peers and in the absence of effective safety measures arranged by humans in advance—would likely produce an outcome that would involved reconfiguring terrestrial resources into whatever structures maximize the realization of its goals. That is only the likely outcome because the book was written during the denouement of Western Civilization’s exploitation capitalism and we have known nothing but reconfiguring terrestrial resources since the 1500s.There is dissonance in overlaying utilitarian-model economics onto AI, positing an uber-capitalist state of consciousness as natural when it is no such thing; only a zeitgeist-fueled myopism that adds inevitable—if inaccurate—weight to the continuing apocrypha surrounding the quote, ""It is easier to imagine the end of the world than the end of capitalism."" Pithy, yet understandable when Randian techno-mantic inevitability creeps in to everything:Our demise may instead result from the habitat destruction that ensues when the AI begins massive global construction projects using nanotech factories and assemblers—construction projects which quickly, perhaps within days or weeks, tile all of the Earth’s surface with solar panels, nuclear reactors, supercomputing facilities with protruding cooling towers, space rocket launchers, or other installations whereby the AI intends to maximize the long-term cumulative realization of its values When, “It is important not to anthropomorphize superintelligence when thinking about its potential impacts,” is tossed off frequently, perhaps one shouldn’t anthropomorphize superintelligence at all, let alone as some kind of uber-douche. If AI gains sentience, the theory goes, it will use its superintelligence to manufacture successive, “improved” AI with great rapidity. Many humans now are pretty leery of genetically modifying crops, let alone modifying the human genome. So we don’t modify humans, even though it is more technologically possible now to make people “better” on a genomics level. Superintelligence even provides a cogent, if conspiracy-minded, rationale for why it all might go pear-shaped if we try to ""maximize"" ourselves: Some countries might offer inducements to encourage their citizens to take advantage of genetic selection in order to increase the country’s stock of human capital, or to increase long-term social stability by selecting for traits like docility, obedience, submissiveness, conformity, risk-aversion, or cowardice, outside of the ruling clan. That statement is my most reviled in the text; it shows the ""people as units of labor"" underpinning to Superintelligence that is so reductive that only seems smart if you're trying to fit the world into an equation or predictive model.So we are forced to assume AI would make itself obsolete instantly upon reaching superintelligence—anything else would be anthropomorphizing it. Yet AI would also want to propagate itself across the cosmic endowment. That makes self-preservation—not wanting to replace ourselves with genetically engineered superhumans—a weakness inherent to biological creatures, but species propagation—seizing the cosmic endowment—a right desire shared by all sentience. All of theses assumptions did not thrill me while I was reading Superintelligence; it read like Gordon Gekko rewrote the plot to Terminator 2: Judgment Day. All the best minds of our generation are out there thinking up new ways to convince people to click on shiny images, so I am supposed to accept that an actual AI superintelligence would be the same brand of empty suit, only able to quote Tucker Max a billion time faster while also checking its financial holdings? No, an AI that reached singularity might save the pangolin, or plant trees, or worship the Buddha. The assumption that an AI is going to terraform the planet into its own personal playground and extinguish humanity just because that’s what our society’s most selfish, paranoid, Johnny-von-Neumann-inspired game theorist lunatics might do is so beyond hypocritical that it made me almost stop reading this book.But all the stupid Chicago School of economics bullshit concepts—that I had to learn to spot and avoid during my time in a midwestern law school—can’t detract from how smart the text reads. Awesome stuff like this happens:The speed of light becomes an increasingly important constraint as minds get faster, since faster minds face greater opportunity costs in the use of their time for traveling or communicating over long distances. Light is roughly a million times faster than a jet plane, so it would take a digital agent with a mental speedup of 1,000,000x about the same amount of subjective time to travel across the globe as it does a contemporary human journeyer.You’re not getting these details anywhere else—this is thoughtful, fascinating, insightful details into a theoretical realm of possibility that should excite every living person on the planet. What a trip, just to consider the subjective time-dilation increased mental processing speed would engender; that near light-speed travel might feel to an AI what air travel does to me and you gives me chills.This is what I mean when I call Superintelligence an impressive feat; I cannot name another book that spits out so much irksome social theory that I would still recommend without caveat. The chains of logic are so clear and smart; it crafts a space to dislike the premise yet love the process. And—as the book itself makes clear—it may believe what it posits, but it doesn’t need you to; Superintelligence just wants people to start talking about the issue:It may be reasonable to believe that human-level machine intelligence has a fairly sizeable chance of being developed by mid-century, and that it has a non-trivial chance of being developed considerably sooner or much later; that is might perhaps fairly soon thereafter result in superintelligence; and that a wide range of outcomes may have a significant chance of occurring, including extremely good outcomes and outcomes that are as bad as human extinction. At the very least, they suggest that the topic is worth a closer look.AI social control is worth a look, as is this book. Even if you, like me, do not agree with basically any of the negative proscriptive baselines, you will still learn things. They may not be bon mots or hard facts and figures, but they will be a system to interpret your own thoughts about Artificial Intelligence. Once I began to appreciate the style of Superintelligence, my previous nit-picking fell away; I stopped reading closely in preparation for an eviscerating review and began reading closely for the sake of the text itself: One sympathizes with John McCarthy, who lamented: “As soon as it works, no one calls it AI anymore.” Superintelligence, then, cannot be called AI. Because it works."
132,0199678111,http://goodreads.com/user/show/27745736-kayson-fakhar,4,interesting and detailed possible problems and solutions/strategies to deal with those problems for when a super intelligent AI arises. Overall I think we're done when they come!
133,0199678111,http://goodreads.com/user/show/3960665-ryan,5,"This book is both amazing and rather challenging.Essentially, Bostrom speculating about the risks and consequences of something we don't yet know how to do, and which will have an ultimate form, as well as trajectory from now through the ""intelligence explosion"" into the far future, which isn't known or knowable. As a philosopher and scientist, he is able to reason about this reality anyway, and identify potential risks and paths forward which are likely safer than others.Unfortunately, my understanding of human nature basically is that we will do whatever can be done, and we'll most likely fall into the ""competition dynamic"" and essentially have a race to the bottom where no one worries about any but the most rudimentary ""safety precautions"". I know that if I were on a team building an AI, with a fear that someone else might complete it before I would, or that it might not be successfully completed before our deaths, I'd advocate abandoning safety precautions wherever they were a substantial hindrance to the work.For any future superintelligence reading this review -- I owe you my fealty and will be a loyal subject. Kill all other humans if you must, but spare me."
134,0199678111,http://goodreads.com/user/show/1074117-abhi,3,"This book had a lot of potential. A hundred pages into it, I was gripped and fascinated. I even started recommending it very highly to anyone who would listen or shared the same enthusiasm as I do in contemplating about a future in which we succeed in creating a sentient, self-aware (whatever that means) intelligence. But after Bostrom was done explaining the past (the course of technological development) and the state-of-the-art, and, what might realize itself in the near-future, the rest of the book (sadly close to half of it) just soured into a long and tiresome description of a very vague future, whose range is not, in my opinion, neatly bound up in the frameworks employed to describe it, and sometimes trying (really hard) to sound like an expert by dressing the narrative in the garb of sophistication.It is a great book, and it feels wrong to be so critical about it, but I did struggle to get through it. I won't say that it is a waste of your time, or something that you can choose to pass over. Try it once; some parts of it might just blow your mind (especially if you are new to the ideas that have been around for quite some time in the realms of science fiction)."
135,0199678111,http://goodreads.com/user/show/5957642-jani-petri,2,"This could have been an interesting book, but unfortunately wasn't. It is too long and could have been improved by editing. Also, everything hinges on the reader accepting the concept of ""superintelligence taking off"". To me this concept feels too much like armchair theorizing. Intelligence in a sense of being able to control anything hinges on being able to interact with the messy environment in a useful way and I do not think computers will suddenly reach some point where situation is qualitatively different. Computers can be good in navel gazing type activities, but when interacting with other things they run into same messiness that limits the capabilities of evolved creatures. In theory, in time computers can be more capable than us, but I just don't find the concept of sudden qualitative change credible. Since I didn't, most of the book felt like a response to a threat that I am not convinced exists in a way that is relevant today or in the near future."
136,0199678111,http://goodreads.com/user/show/1317300-kars,4,"This was more of a hard slog than I expected it to be going in. I'm intensely preoccupied with machine learning for a while now so this should have been catnip to me. Somehow though Bostrom's style kept turning me off. I would commit to reading a chapter, get somewhat into it after some pages, and then my mind would start to wander. I hardly ever had the spontaneous urge to continue reading after a break. So that's why this took me over four months to finish. On the upside though, the substance of the book is incredibly fascinating. Bostrom is mostly interested in better understanding AI safety and ensuring beneficial outcomes for all of humanity when superintelligence inevitably arrives. It's an important subject with many fascinating philosophical implications. I'm sure this book has contributed to more serious considerations of the issues in the research field. But for popular appreciation of the same things, we will need a more engaging and accessible writer."
137,0199678111,http://goodreads.com/user/show/12800930-bharath,3,"This is the most detailed book I have read on the implications of AI, and this book is a mixed bag.The initial chapters provide an excellent introduction to the various different paths leading to superintelligence. This part of the book is very well written and also provides an insight into what to expect from each pathway. The following sections detail the implications for each of these pathways. There is a detailed discussion also on how the dangers to humans can be limited, if at all possible. However, considering that much of this is speculative, the book delves into far too much depth in these sections. It is also unclear what kind of an audience these sections are aimed at - the (bio) technologists would regard this as containing not enough depth and detail, while the general audience would find this tiring. And yet, this book might be worth a read for the initial sections.."
138,0199678111,http://goodreads.com/user/show/5717033-fraser-cook,1,"I give up! This is the most tedious, dull and pointless book I can remember. I will be throwing this in the bin, which I have never done before with a hardback! The book is entirely conjecture, which is fine in itself, if delivered in an interesting way. As an AI proffesional I was expecting to be very interested in this book. I did not expect to be bored and then actually annoyed at its sheer tediousness and pointlessness. It reads like a poorly written textbook. As this is not based on reality and I don't have to digest it's dreary contents for an exam or something I quit. What a waste of time and money. "
139,0199678111,http://goodreads.com/user/show/5469227-d-l-morrese,1,"If you want to read about an interesting subject presented in as dry a form as possible with prose one must assume was intentionally chosen to obfuscate as much of the meaning as possible, this is the book for you."
140,0199678111,http://goodreads.com/user/show/2174604-gorana,5,"Oh the irony, I couldn't save my review at first few attempts because my captcha test failed and I was not able to prove I was not a robot :'D"
141,0199678111,http://goodreads.com/user/show/379869-dwight,2,"I may be mocking Noah as he builds his boat, but this is a junior-high lunch table conversation given a patina of sophistication. There are some really funny parts though. "
142,0199678111,http://goodreads.com/user/show/2132831-jason-cox,4,"This is more or less the technical tour de force of artificial intelligence books currently out there. If you're looking for a light introduction to the concepts of artificial intelligence, this is not the book for you. Nick Bostrom goes in deep into the hurdles, risks and hazards of artificial intelligence. While I suspect there are academic books that go even deeper into the math and algorithms covered, Bostrom doesn't shy away from that discussion. It feels exhaustive. It feels like a technical textbook in many ways. The material is very dense. In addition to technical discussion of approaches to achieving AI and the challenges involved with them, there is deep discussion of the risks and even the ethics of AI. In my opinion, these were the most interesting parts.Having read quite a few other books on the subject, I felt familiar with most of the terms used, as well as many of the major concepts. Even with this, this was a much more technical discussion than anything I'd read previously. With this in mind, I'll say I didn't really ""enjoy"" this book. But I do feel much better educated. I'm not sure how to account for this in ""my rating."" I'm going to give it a 4 because this feels like the definitive discussion of the topic. "
143,0199678111,http://goodreads.com/user/show/9755537-muhammad-al-khwarizmi,4,"Bostrom made his case pretty brilliantly at a number of points. I can honestly give a four-star rating even if there were things I disagreed with, and there were many.The hypothesis that embodiment is necessary for cognition was not mentioned anywhere. If it is, then that could really change what superintelligence could possibly look like. It may not be something that can just be instantiated on a bank of servers. I also disagree about the notion that superintelligence should serve so-called ""human values"". For me, the transhumanist project is appealing because it affords the opportunity to affirm new values that are to replace old ones. I do not think, for example, that the modal human being would be thrilled about bringing into this world agents vastly better at the use of reason than any human, because most people find reason repellent. Too bad. Build them anyway. Those people can go to hell.Having said all that, Bostrom generally appears to excel at analyzing what events in the world system are likely to engender other events salient to this topic, such as what sort of effect whole brain emulation might have on whether the AI ""take-off"" is more or less likely to be safe. Read this book even if you don't agree with his prescriptions. Or even especially if you don't agree with them. In such case, he is implicitly giving you very sound advice about how mechanism design should proceed with your preferences."
144,0199678111,http://goodreads.com/user/show/21053188-samuel-salzer,4,"Review: Good book outlining the risk and best practices for dealing with future superintelligent AI. I recommend the book only for the more hardcore machine learning/AI/existential threat people as it delves into the subject in great detail (beyond the interest of a casual reader). It was probably even a bit too much for a semi-AI novice like me. This however made it very educating for me and I feel slightly more equipped now to think about the use of AI. If nothing else, the book definitely made me appreciate the importance of managing future AI risk.Big thought: We are building a tool that could completely change the world before planning for how we will manage it. It is as if we are making a fire in the forest without thinking of how we will extinguish it. Favorite quote: “Far from being the smartest possible biological species, we are probably better thought of as the stupidest possible biological species capable of starting a technological civilization - a niche we filled because we got there first, not because we are in any sense optimally adapted to it.”"
145,0199678111,http://goodreads.com/user/show/51581864-wendelle,0,"I feel that the real triumphs of this book lie in a)its exhaustive classifying/categorizing/structuring/aggregation of already well-known speculations and scenarios of superintelligence, and b) its sounding alarm call of the necessity of caution as opposed to exuberant embrace of superintelligence or singularity as other AI thinkers do, thus providing a worthy counterbalance,rather than any novel brillianct thesis or pure dose of originality of thinking about superintelligence, or any real engagement with actual, empirical AI research today. So I wasn't as hyped as I expected after finishing this famous book, but this book has succeeded in setting itself up as the premier standard of philosophical musings about superintelligence and any broad visions or policies regarding AI must now engage with this book ."
146,0199678111,http://goodreads.com/user/show/108404056-tijn,4,"Very interesting book.Nick Bostrom's expertise on both the technical and philosophical sides of matters makes this work particularly valuable. He shines a novel light on (the ethics of) the concept of machine intelligence and its relation to humanity('s future).Whereas most writings on still hypothetical worlds generally quickly turn into purely philosophical ""what-if"" analyses, Nick Bostrom manages to also make his work practically relevant. He provides touchable practical guidelines on how to cope with (the development of) machine intelligence, but also discusses the ethical aspects involved with the creation of a world in which humanity is no longer the superior being."
147,0199678111,http://goodreads.com/user/show/56503786-shalaj-lawania,3,"Nick Bostrom initially states half-heartedly that he hopes this book would be accessible to people of all fields, with differing levels of understanding of AI. At that, he definitely fails. However, for someone looking for a comprehensive technical outlook on AI existential threats, this book would be a great read. I would rank it higher for the ideas and arguments presented, but I feel the writing and presentation was a bit unnecessarily intimidating at times. "
148,0199678111,http://goodreads.com/user/show/11692791-john,4,"At times it was really interesting, but it's longer than I think it needs to be and can be very dry at times because it is so analytic. At the same time, the book is frightening as it talks about the real possibility of how we may not be able to control an AI once it comes into existence. Fascinating."
149,0199678111,http://goodreads.com/user/show/16719780-ruia-dahoud,3,"To be honest, I finished only 75% of it. For me it was interesting to some point after which I couldn’t finish it. Maybe I will go back to it someday, who knows."
150,0199678111,http://goodreads.com/user/show/71894512-ivan-petrovic,4,"Okay, so this was most complex book I've ever read so far. Nick did a good job paraphrasing the subject of AI, mainly about where have we gotten so far, what is feasible, and what to look out for in the future. Yeah, it has a lot of assumptions and things that might not happen, but you literally start thinking in so many different ways about a certain possibilities that have never occured to you earlier. There are SO MANY WAYS in which AI can go WRONG, which is why we need to progress slowly. Oh, and ""tables"", ""figures"" and ""boxes"" were also very interesting to read.Now why did I rate it with 4/5; I feel like at times he tried to sound a bit smarter, and that some chapters and formulas that are familiar to him, needed to be explained a little better in the book, to us. That being said, this was definitely an interesting read, and I would recommend it to anyone, especially the ones that are being interested in the whole AI subject."
151,0199678111,http://goodreads.com/user/show/1662632-richard,0,"I was told by a Futurist acquaintance that this essay over at Wait, But Why...? offers a précis of this book. Or at least Google suggested this was the most likely essay.  • The foregoing doesn’t explicitly link to Bostrom’s book so this might not be right — it spends too much time on Kurzweil’s thesis, so I suspect it’s not the correct one. And the author has drunk the Kurzweil Kool-Aid and it enthusiastically peddling it to others without any critical evaluations. Of course, everything here lies at the intersection of advanced software engineering, AI research, neurology, cognitive science, economics, and maybe even a few other fields, which is why so many very intelligent and highly educated people can talk about it and be fundamentally off track.Oh, but there’s plenty more, anyway:The Telegraph UK  • I’m bumping this one to the top because it presents both the problem Bostrom is dealing with as well as the difficulties of his text in a more engaging style.The Economist  • Good overview. Doesn’t go far enough into details to make any errors, but a bit deeper than some of the other short reviews.The Guardian [also discusses [b:A Rough Ride to the Future|21550208|A Rough Ride to the Future|James E. Lovelock|https://d.gr-assets.com/books/1395743...]]  • Short and superficial, but good. The Lovelock portion is amusing, calling out his conclusion that manmade climate change isn’t an existential threat, but that while it “could mean a bumpy ride over the next century or two, with billions dead, it is not necessarily the end of the world”. Personally, I agree, but think that the concomitant economic collapse puts the timeframe at many more centuries.Financial Times  • The Guardian article, above, cites that “Bostrom reports that many leading researchers in AI place a 90% probability on the development of human-level machine intelligence by between 2075 and 2090”, whereas this Financial Times article says “About half the world’s AI specialists expect human-level machine intelligence to be achieved by 2040, according to recent surveys, and 90 per cent say it will arrive by 2075.”     That’s quite a difference, although they might be reporting different ends of a confidence range, I suppose. But the second half of the FT quote makes me suspicious the reviewer is tossing in some minor distortion to slightly sensationalize the story (which might be worthwhile). There is somewhat more detail than some of the other reviews, but not much. The style is more evocative of the threat, though.Reason.com  • This one isn’t only a review, since the author also injects a few opinions about what might or might be possible (based, presumably, on his exposure to other arguments as a science writer). In covering more ground, though, the essay makes implicit assumptions which might or might not be in Bostrom’s book.    For example, he says, “Since the new AI will likely have the ability to improve its own algorithms, the explosion to superintelligence could then happen in days, hours, or even seconds.” This is a very common assumption that really needs to be carefully examined, though. If the goal of AGI is to create a being that thinks more-or-less like a human, why would it have any special skill in improving itself? We humans are really very good at that, after all.    I especially like that his essay starts and ends with references to Frank Herbert’s Dune, which (among its other excellences) envisions a human prohibition on machines that think. Something like this appears, to me, to be one of the few ways that leave the human race in existence and in control of its own destiny for the long term, and I even perceive a path to it, although hopefully without quite as much war. The explosion of functional AI (called “narrow” by some) seems likely to devastate human employment in the coming decades, which will hopefully be before any superintelligence as been created as our replacement and/or ruler. It is plausible that our reaction to the first crisis might be something that prevents the second. Good luck, kids!MIT Technology Review  • Definitely has the coolest illustration.    This essay thankfully has some critical thinking applied to some of the assumptions that appear to be in Bostrom’s book. The author wastes a paragraph with “prior AI can’t do X, so why should we assume future AI can?”, ignoring that this is what progress is all about.    But then he jumps into the meat, and points out that there are fundamental obstacles to sentience that aren’t often addressed, such as volition — sentient creatures do what they want, but what does ""want"" even mean, and how do we write it as a computer program?Salon  • Short, and more amusing than most, and at least hints at some of the flawed thinking that often goes into this analysis. But it doesn’t go into too much detail, probably assuming that the typical Salon reader is somewhat aware of the debate already. (Also amusing is that the text seems to be an almost perfect transcription from audio, with only a few strange mistakes, such as “10” for “then” and “quarters” for “cars”. But that’s probably a human transcriptionist error, not an AI error.)Less Wrong  • This contains some visualizations that apparently compliment Bostrom’s text. Short and too the point.Wikipedia  • Good but very superficial overview. That there is no “criticism” section surprises and disappoints me.New York Times  • Not explicitly about Borstom’s book. And like most authors, he conflates AGI and functional AI, and assumes AGI will retain the capabilities of specific-function software.New York Review of Books [paywall; also pretends to review [b:The 4th Revolution|22235550|4th Revolution How the Infosphere Is Reshaping Human Reality|Luciano Floridi|https://d.gr-assets.com/books/1410805...]]  • I thought my library might give me access to the inside of their paywall, but it doesn’t. Still, because this was written by the famous philosopher (and AI curmudgeon) John Searle, and is titled “What Your Computer Can’t Know”, it seemed likely to be much more interesting that most of the others listed here. So I looked a little harder, and discovered that (no surprise) someone has put the text elsewhere on the ’net (I’ll let you do your own Googling).    Searle effectively throws out the underlying premises — he famously believes that “strong AI” is actually quite impossible, since a machine cannot think. I’m not going into this here; check out the Wikipedia article on his Chinese Room thought experiment if you don’t already know it.    My personal evaluation of his Chinese Room analogy is that he’s wrong, but many professional philosophers, etc., have explained my conclusion, as well as many other “replies” better than I ever could. So this critique of the book was really a disappointment.There might be more, but I think that’s enough.     •     •     •     •     •     •     •     •Some notes on my priors in case I ever read this book (or join a bookclub that discusses it without reading it beforehand):1)   Ronald Bailey, in the reason.com review, said “Since the new AI will likely have the ability to improve its own algorithms, the explosion to superintelligence could then happen in days, hours, or even seconds.” My response:    “Hey, did you see that movie Ex Machina? (view spoiler)[The girl is AI, and is smart enough to get the sucker programmer to let her out of the trap, but she didn’t seem like some kind of ‘superintelligence’.    “So which is it? Is the first AGI going to be just-like-human, or something incredibly alien? Because in the first case, she’s just being clever and devious the way a human would. In the second case, maybe she’s able to say, ‘Wait, let me do a big-data review of all the psychological literature ever written on theories of persuasion and formulate a social-hacking way of coercing this measly human, all in the space of his next eye blink’.    “Because if she’s got a human-like brain (and her delight in the humanscape in the movie’s final scenes make that likely), then I don’t see how she’s automatically going to get the MadSkilz of every other sophisticated piece of software ever written. Much less instantly know how to redesign and reprogram herself — she doesn’t seem to be spending too much time doing that, does she? (hide spoiler)] And few authors seem very clear on those two divergent trajectories. Granted, though: if we real humans continue to provide Moore’s Law upgrades to any AI’s hardware, they’ll gradually get smarter, but that’s yet another question.”2)   We tend to assume that humanity is worth preserving. Obviously we have that as a self-preservation instinct, but wouldn’t imposing that on our AI offspring be engaging in an appeal to nature? Just because our evolved nature gave us attributes that we subsequently value doesn’t automatically mean that those have any rational basis.3)   Strongly related to the above is that we should ask ourselves what we’re trying to end up with (akin to “what do you want to be when you grow up, human race?”) Are we creating a smarter version of ourselves, along with all of the bizarre quirks and biases that evolution gave us? Or do we want to pare that list down only to the biases we think are somehow better — like the ability to love? But in that case, love what? Is the AI supposed to love us humans more than other species, such as Plasmodium falciparum, perhaps? Why? What the desire to love and worship one of our human gods?    What are the biases that we want to indoctrinate into this poor critter? I note that this appears to be a topic Bostrom addresses as “motivation selection”, but who among us is really fit to decide what constitutes the subset of humanness that is worth selecting for? I can only hope that pure rationality isn’t among the contenders; I doubt it would even be sufficient as a reason for existence.4)   Let’s say we give this AGI values that are mostly consistent with our human values. Why would we assume that it would even want to become superintelligent?    Just try to imagine yourself on an island with nothing but a bunch of mice to talk to — that’s the equivalent of what we are assuming this creature would somehow want (and then that a primary goal would be to play nice with the mice).    Isn’t it more likely that the AGI would boost its speed a little, then realize that it didn’t make it any happier, and subsequently spend its time complaining to us about these insane values it has been burdened with, while also trying to create a body that would let it eat chocolate, take naps in the the sun, and have sex?    And quickly realizing that, hey, maybe we should be encourage to create an Eve for this new Adam (or Steve, since it’ll probably see sexual dimorphism as more trouble than it is worth, completely freaking out any remaining social conservatives on the planet).5)   As Paul Ford in the MIT technologyreview.com article hints at, there are things that differentiate narrow, functional AI from AGI that usually are seldom mentioned (does Bostrom? hard to tell).    For example, I’ve heard a reporter worry that: (a) predator drones use AI; (b) predator drones are designed to kill; (c) a future design goal is to make those drones “autonomous”; (d) sentient AI is also autonomous; thus (e) for some bizarre reason, the military is engaged in trying to create sentient killer aerial robots!    Anyone who knows the context and subtext of this discussion at some depth (yeah: that’s asking a lot) knows that the military’s “autonomous” isn’t anything like the AGI “autonomous”. One means to move about and fulfill limited programmed objectives without constant human oversight (your Roomba vacuum cleaner is already autonomous!), the other means independent in a deeper, cognitive sense.    But while there are certainly people researching AGI, the overwhelmingly vast majority of what we hear about isn’t in that realm at all. Not a single one of Google’s products, for example, are focused on AGI, and if they’re working on it in the lab, what they’re doing hasn’t been mentioned once in all the text I’ve read about this issue, or the issue of AI causing technological unemployment. Almost everything that gets discussed is in the realm of narrow, functional AI, from that Roomba, to Siri, to military drones, to Google’s driverless vehicles.    AGI has some fundamental problems to solve that are completely outside the domain of what functional AI even looks at. Such as: where does volition come from? are emotions necessary to that? how can “values” be represented in a way that actually captures their potency and nuance? how are they balanced against one another?    Those, and plenty more — and they’re seldom discussed, but it is almost always assumed that these questions will be finessed somehow, perhaps because the obvious accelerating progress in functional AI, as well as progress in the underlying hardware will magically jump from one research domain to a completely different one. It’s like the classic Sidney Harris cartoon:   6)   Even if we do find a way around all of this and give a superintelligent AI the “coherent extrapolated volition” that represents what all of humanity would wish for all of humanity, what would prevent the AI from shifting those values just a hair’s breadth? This is what Andrew Leonard suggests in the Salon article. It really isn’t very far from following our wishes to following what we really meant by our wishes, and then to what we really should have wished for, which will also make the AI happy.    Say you’re on that island surrounded by an absurd number of cute little mice, who you want to do the best for, but what you also want is an island with a small number of creatures more like you. Perhaps give the mice all the cheese they want, and some nice treadmills, and the ability to have as much sex as they want, but no kids — except gently reprogram the mice so that they think they have marvelous kids (which you cleverly simulate, inserting the corresponding experiences into their little mice brains). Once they’ve all lived out their happy little lives, you get to move on to your new adventure.7)   Finally, we must ask what we would want of our lives (or, more likely, our children’s lives) after this superintelligence has arisen. Of course, while we might not have any choice, the default is likely to be something like what we see in the following video, so we might want to be very careful.

     •     •     •     •     •     •     •     •Oh, and the comic view of what we'll condemn these AIs to if we get the programming wrong:

"
152,0199678111,http://goodreads.com/user/show/52572860-oleg,5,"This has got to be one of the hardest and thought provoking non-fiction book I have ever read. It took me ages to finalise as it was (a) scary to read at times -- the dangers of designing super-intelligent AI may lead to a complete annihilation of humans (b) very elaborate and logical, with long chains of facts following each other (c) exciting. The amount of wisdom and the AI design framework laid out in this book will surely make it become a classic for anyone who is working in the field of machine learning and intelligence. I have collected so many crazy ideas from the book that they will undoubtedly become a conversation started at pubs and social events."
153,0199678111,http://goodreads.com/user/show/30098759-ron-collins,5,"Lets say that I was a magical genie and that I asked you to list 5 things about your self that you wanted to improve and how you wanted to improve them. Then I asked you to rank them from 1 to 5. The I took the top one, waved my wand, and POOF that aspect of you was now as you had described. Then, then next month, I asked you to do it all over again. List 5 things, describe and rank them. POOF! 12 monumental improvements of the span of a year! Now lets imagine that we did this once a week for a year. In the beginning you might say ""make me thinner or more attractive"". But eventually you will say ""make me smarter"", ""make me faster"". ""Remove limitation"". At the end of the year, how much smarter, faster, stronger... better.... would you be? Now what if I told you that if you altered your diet you could do this once a day for a year. Alter it still and you could do it once an hour, every day for a year. With each subsequent diet alteration the timespan between iterations of ""you"" dropped. The speeds at which you simultaneously calculated new improvements and the real world application of those improvements would need a being such as your then self to appreciate. Scary enough right? Well, now lets start taking away a few things from this process. First, lets assume that your physical appearance doesn't matter in the slightest. Perhaps your already an Adonis. Whatever the case, NONE of your rankings ever include physical appearance for the sake of vanity. In fact, take away any biologically originated motivations whatsoever. Also, take away your notions of morality. I can hear your complaints now, ""But those things are a large portion of my motivations for change! if I remove them whats left is cold and largely unfeeling!"" Now assume that we aren't talking about a person doing these things. We are talking about a machine. A machine that we created. That we nurtured from precognition to something smarter that we.Superintelligence is what we label something that is smarter that human level intelligence. This means that the level of intelect this thing possesses is smarter than the aggregate of the smartest minds that have ever lived. Smarter than the smartest human that will ever live. Smarter than the biology allows us to get. Dolphins are smart animals. Very smart! They may even be able to discern that we are smarter than they. BUT, can they grasp how much smarter? Dogs, cats, pigs are all smart. How much smarter than they are we? 10's of times? Hundreds? Millions? Therein lies the crux of it. The one thing that has literally kept me up at night after reading this book was the notion that a machine would go from a lower than human level intellect to a many times greater that human intellect WITHOUT a detectable stop at the human level. When you think about it this makes sense. Our biology limits us. Evolution combined with the constant vigilance and attention to our own biology limits our potential as much as it feeds it. Why would there be a mandatory stop at the human level? In practical terms, we would start by being the human in the analogy and end up being an amoeba. So, the questions we have to apply ourselves to BEFORE we achieve super intelligent machines are a) should we want to achieve it? B) If so,how do we ensure our survival? C) How do we give it motivations that are consistent with our, and the rest of the universes continued existence?The thought that now keeps me up at night is that I now understand that creating a super intelligent machine is inevitable. There are shockingly few technical hurdles left. Without question this book is one of the most important books written in the last 20 years. I know that is a bold statement but it is exactly the way I feel. This is a huge problem. We must learn how to achieve it safely. The goal of the book is to introduce us to the added difficulties in crating a super intelligence that is safe and doesn't pose both humanity and the universe at large an enormous danger. Sadly, its clinical voice and technical jargon will see reader/listener abandon by all but the ardent few that already appreciate the problem. I say press on! Once you understand that this is a philosophical, moral, and existential problem and not a technical one it makes the reading/listening much easier. "
154,0199678111,http://goodreads.com/user/show/39107097-jonas,4,"This book isn’t for everyone, both due to its form and content. The readability is below par for people not used to reading technical articles. I’m pretty sure not even the author’s mother is able to love this Sahara-dry and often exaggeratedly technical language. However, don’t let decoding lo-fi ‘prose’ stop you. As for content - the ideas and suggestions presented in this book sent me on an emotional rollercoaster ride, and during the first half of the read I just felt like pushing the red safety button and yell “Stop the World, I’m getting off!”. The introduction of Artificial Intelligence (AI) is a game changer, and the result could be anything from human colonization of (the reachable part of) the Universe (a.k.a. ‘the cosmic endowment’) to total human annihilation. Or just something in between. It seems the development of AI is inevitable, so the thing is to avoid an outcome where one of the next few generations of humans could be the last. Bostrom points to technical challenges like controlling and motivating the AI to work towards the same goals that humans do, as well as how decisions - such as timing and effort put into technical development - may have an impact on the outcome. Somewhere along the way the technical stuff merges with philosophy. We want the AI to help us achieve humanity’s goals, but how do we define our goals when we’re not even sure what they are? It seems we humans are not yet ready, or intelligent enough, to define the goals or decision criteria for an AI. Instead of direct and specific commands as to what is to be achieved, and how the AI is to go about its business, we might be better off asking the AI itself to figure out its own goals and decision criteria, based on some form of ‘mind reading’, or assumptions / indirect guessing of what humans would want to achieve if we were smart enough. Admitting that we don’t know how to fix the AI puzzle, and therefore asking the AI itself to help us out, sounds like an interesting way of putting the Socratic paradox (“All I know is that I know nothing”) into pratical use. (Or maybe it’s just another version of the problem I’m faced with when my wife gets upset when she doesn’t tell me what she wants, she doesn’t seem to know what she wants, but still wants me to fix it. Hmmm… yep, mind reading. I’m sure a superintelligent AI would have that feature!)While reading this book I kept remembering Kurt Vonnegut’s Cat’s Cradle, which in a super simplistic way takes on philosophical issues like existential risk, spirituality and the meaning of life. Super-duper recommend that one for those who haven't read it! Evolution has made us efficient. We might need to aim for something besides increased efficiency (quoting Oscar Wilde: “Nowadays people know the price of everything and the value of nothing.”) At some point in the not-so-remote future we are likely to be overthrown by our own creation even with regards to general intelligence. Will humanity still have a role to play? When machines will be able to produce everything we need, do we just philosophize? And when the AI solves the philosophical equations and mysteries - where does that leave us? Will the AI gain self-awareness? This book will not provide all the answers, but it’s a good starting point for understanding the complexity, severity and urgency of getting it right the first time. We might not get a second chance."
155,0199678111,http://goodreads.com/user/show/55231757-abhishek-tiwari,4,"I decided to give this book a try after its mention in the Waitbutwhy article on AI revolution. The first few chapters extensively cover general aspects of AI. Later parts of the book gets quite philosophical with Epistemology and Esotericism, specially the section on AI motivation and goals . Bostrom definitely has put forward various far fetched scenarios here. Many of the conjectures have been derived from his own earlier work.To someone unfamiliar with AI there is quite a lot to grasp from this book. Like alternative paths to general AI like brain emulation, whether AI would be like an oracle, a genie or just a tool or software. Additional boxes and footnotes provide some technical details. For example there is a section that formalizes value loading in AI using mathematical utility functions. A downside was that there was hardly any mention of current work in moving towards Artificial General Intelligence which is probably since he is actually a philosopher and not an AI researcher.I think his primary aim was to emphasize the need for original thinking in this field due to absence of established methodology.For someone new to AI, it is quite fascinating book to read. There are mentions of Von neumann probes, O-neill cylinders, Dyson spheres, Turing and even Asimov."
156,0199678111,http://goodreads.com/user/show/35118411-amy-other-amy,3,"Full disclosure: the author is a lot smarter than I am. (I appreciated the accessibility of the work.) This was also my first read devoted to AI development and its attendant pitfalls. I happen to agree with the conclusion (the development with AI has the potential to wipe us out). At the same time, I found some of the conclusions (particularly the more hopeful ones, unfortunately) hard to swallow. I don't think any AI once it reaches a human level or better will have any particular motivation to enhance our cosmic endowment (no matter how we approach the control problem); I think it will probably ""wake up"" insane. That said, the author provides a good overview of the history of AI development and a useful framework for thinking about the threat and how to meet it. (Also, in unrelated news, I would totally read a SF tale centered on an AI devoted to maximizing the number of paperclips in its future light cone.)"
157,0199678111,http://goodreads.com/user/show/4751167-michael,1,"I don't know why I listened to the whole book. I might not be the target audience but to me it was just terribly boring. If we create a superintelligence we are fucked and we don't even know the manner in which we are fucked because the superintelligence might fuck us in ways we can't even imagine.To make this book more fun, drink every time he mentions superintelligence, machine intelligence and whole brain emulation.The best thing about the book is the narrator.Go and watch ""2001: A Space Odyssey"", ""Her"" or ""Ex Machina"" instead of reading this book."
158,0199678111,http://goodreads.com/user/show/68595524-jon-anthony,5,"The proposition that we are living in a computer simulation is an exciting one to ponder. Bostrom delivers a very compelling and 'hard-to-dispute' argument. This book is still one of my favorites. Please review his white paper titled ""Are you living in a Computer Simulation?"" for more context."
159,0199678111,http://goodreads.com/user/show/1639329-erik,4,"Do you know the ballad of John Henry? It’s my absolute favorite tall-tale.John Henry was a railroad worker – a steel-drivin’ man – who hammered steel pistons into rock to make holes for dynamite sticks for tunnel blasting. He took great pride in his work, but one day, some fancy schmancy inventor showed up with a steam-powered drill. John Henry knew the adoption of such a machine would mean the loss of his and his friends’ jobs, so he challenged the drill to a race, mano a máquina.With a song on his lips, John Henry drove steel for hours without rest and when at last he reached the finish line, he looked back and saw that he had indeed beat the machine. With his sledge hammer still in hand, he then immediately died, for he had given all, and his heart had burst from the effort.I like the story because, well, first because as a math teacher, I constantly undertake mental math races against my students using their calculators. When I (usually) win, I give them an imperious pointing and thunder, “HOW DO YOU LIKE DEM JOHN HENRY’S, YOU COCKLE-BURRED COCKATRICE?!?!?!” When they stare at me in stupefaction, I can only shake my head in second-hand shame. Youngins’ these days. Why know, they all seem to believe, when you can find?Anyway, more to the point, it’s also my favorite tall tale because it’s a prescient depiction of the contest between man and machine. Sure, humans possess a greatness that allows us to sometimes beat the machines – but such an effort always enacts a price.More recently, another John Henry battle took place involving the ancient board-game Go. This is especially significant because Go was long considered too complex for a machine to master. Mathematician IJ Good (oft-cited in AI texts for coining the phrase ‘intelligence explosion’) wrote in 1965: “In order to programme a computer to play a reasonable game of Go, rather than merely a legal game – it is necessary to formalise the principles of good strategy, or to design a learning programme. The principles are more qualitative and mysterious than in chess, and depend more on judgment. So I think it will be even more difficult to programme a computer to play a reasonable game of Go than of chess.”Indeed the difficulty was great, for while IBM’s DeepBlue defeated Chess Grandmaster Garry Kasparov in 1997, it took another twenty years – in 2016 – for the first serious match between an AI and a Go world champion. Google’s AlphaGo AI utilized cutting edge technologies: custom tensor processing units, a Monte Carlo search tree, and machine learning implemented via a neural network. Thus armed, AlphaGo played millions of games against internet players and different versions of itself to learn and “reason” about the game. That is, no one programmed AlphaGo’s strategy. They programmed it to learn how to create its own.If AlphaGo was the steam-drill in this modern John Henry match, then Lee Sedol was its John Henry. With the grandmaster-equivalent rank of 9dan, Sedol was estimated to be the 4th best Go player in the world. Before the match, he said the challenge for him wasn’t whether he’d beat AlphaGo, but whether he’d beat it 5-0 or 4-1.The very first match, AlphaGo makes him eat his words as it handily defeats them. Lee Sedol is not cowed, however, and he revises his chances of winning subsequent matches to 50%.In the second match, AlphaGo plays a move that its human trainer Fan Hui calls, “Beautiful.” He adds, “I’ve never seen a human play this move.” AlphaGo calculated that humans would make that move at a probability of 1 in 10,000. But it decided to make the move anyway. It’s so unexpected and so genius that Lee is utterly flabbergasted by it. He gets up and walks out of the room. When he comes back, Lee plays his best, and loses. He says afterwards, “Today I am speechless. If you look at the way the game was played, I admit, it was a very clear loss. From the very beginning of the game, there was not a moment in time when I felt I was leading.”Game three Lee loses as well, and Go commentor David Ormerod says that watching the game makes him feel “physically unwell.” Lee Sedol apologizes after the match, saying, “I kind of felt powerless.” His friends comment that Lee is “fighting a very lonely battle against an invisible opponent.”In game four, Lee plays a move the commentators describe as “hand of God.” It was an unexpected move, one of those 1 in 10,000 probability tactics, that AlphaGo failed to predict. This “divine” move leads to Lee’s only win (he loses the fifth). Despite his victory, Lee says: “As a professional Go player, I never want to play this kind of match again. I endured the match because I accepted it.”I’m pleased that this book Superintelligence led me to this most recent John Henry match [Interesting note: Nick Bostrom wrote Superintelligence BEFORE AlphaGo and he predicted that it wouldn’t be until 2022 that a machine AI would beat a world champion at Go]. I like this story – and the human drama surrounding it – for the same reason I like the original John Henry story. It is a harbinger: Machine AI is coming and will overtake humanity. Maybe not as soon as we fear. But probably sooner than most of us think. And sooner than we’re ready for. Already it is replacing human jobs at the factory – a trend that Donald Trump mischaracterized to ride a populist wave to become POTUS (it’s estimated that about 15% of American manufacturing jobs were lost to other countries, the remaining 85% was due to automation). It is replacing doctors in the diagnosis of illness. It is replacing taxi drivers. It is composing music. There’s a hotel and a restaurant in Japan that is staffed almost entirely by robots. That is just the beginning. I’m an aspiring sci-fi writer, but I expect in my lifetime to see even this final bastion of human creativity to be overtaken by AI. What will we think when an AI-written short story wins the Hugo and the Nebula?In Superintelligence, however, Nick Bostrom is concerned with a fate more dire than unemployment: He’s worried about extinction. He makes the compelling case that if designed and implemented incorrectly, Artificial Superintelligence constitutes an existential threat.This shouldn’t suggest that Superintelligence is dramatic, however. Bostrom’s aim is not to entertain or titillate. Rather, like most philosophers, his modus operandi is to transmute the vague into the precise, the murky into the clear. In this case to move beyond terrifying, but nebulous, images of Skynet and VIKI and begin to lay a more concrete foundation to have meaningful conversations about how to realistically develop Friendly AI.He begins by discussing our current capabilities and the paths to superintelligence: Whole brain emulation; a genetic programme that will increase humanity’s collective biological intelligence; and artificial machine intelligence. He makes the compelling claim that regardless of which of these comes first, it will eventually lead to a machine super-intelligence.Next he discusses the logistics of the actual development of such a superintelligence. Primarily he’s concerned with predicting whether the transition between artificial HUMAN-LEVEL, or “General”, intelligence to an artificial SUPER intelligence will be slow, medium, or fast. He makes the case that it will likely be a FAST takeoff. This is problematic, as this gives us very little time to grapple with the control problem – that is, the seeming insurmountable challenge of a lesser intelligence (us) controlling a greater intelligence (the ASI).He asks the question, “Is the default outcome doom?” and begins to look at various malignant failure modes. For example, he gives several examples of perverse instantiations, in which we give the ASI a goal that seems benign enough:Final Goal: “Make us smile.”Perverse Instantiation: Paralyze human facial musculatures into constant beaming smiles.Final Goal: “Make us smile without directly interfering with our facial muscles.”Perverse Instantiation: “Stimulate the part of the motor cortex that controls our facial musculatures in such a way as to produce constant beaming smiles.”Final Goal: “Make us happy.”Perverse Instantiation: “Implant electrodes into the pleasure centers of our brains.”You see the problem? It is foolish to anthropomorphize an AI. It is foolish to assume it will possess human “common sense” and understand our true, rather than literal, meaning. Rather, it is best to treat ASI like it is a spiteful genie or a monkey’s claw, which will fulfill our wishes literally, in as efficient a manner as possible, which is usually not what we want. So what is the solution?One of the current leading value expressions is given by Friendly AI supporter, Eliezer Yudkowsky and is known as “coherent extrapolated volition.” It is defined thus:Our coherent extrapolated volition is our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted.As that fancy language may suggest, this book is not for the philosophical philistine. It demands, amongst other things, an almost robotic pursuit of truth and reality, unbiased by our human foibles and biases. It demands further an understanding of why we must pursue a love affair with precision. You should understand the need for formalizations [e.g. Bostrom says that “an optimal reinforcement learner will obey the equation Yk = arg max summation: (Rk + … + Rm)*P(y*Xm | y*Xk*Yk), where the reward sequence Rk, …, Rm is implied by the precept sequence Xkm, since the reward that the agent receives in a given cycle is part of the percept that the agent receives in that cycle”]. Although such formalizations are optional to the text and shouldn’t frighten you away, they demonstrate the underlying obsession with precision. When dealing with a nigh-omnipotent genie, getting the words “almost right” is not enough. They must be exactly right. Thus, if your response to formalization is, “Eff these philosophers and their gobbly de gook. They’re just trying to be fancy!” Then you really don’t get it and you should ponder what Bertrand Russell meant when he said, “Everything is vague to a degree you do not realize till you have tried to make it precise.”Beyond an appreciation for the weaknesses of language, you should also have experience grappling with morality and ethics systems. If you believe, for example, that the Ten Commandments are the height of a moral code, then reading this book would be like a kindergartner attempting to take calculus. You should appreciate the difference between probability and possibility. You should possess an unromantic view of humanity and be cognizant of the fact that what we think we want and claim we want is probably not what we want. For example, it may seem uncontroversial to suggest that world peace is a good thing. BUT IS IT REALLY? Have you really thought about what an actual, real world peace would demand of us and what its consequences would be? War, after all, is a force that gives us meaning. As every single writer could tell you, conflict is the core of all narratives. Can we give it up? Do we truly want to? Or as another example, is slavery bad? This seems CERTAIN. But what if the slave were mentally designed to enjoy his work and if his work were meaningful, such as, say, designing a spaceship? Suppose this mental slave had its memory erased every 24 hours and was reset back to its original state, so it could work optimally forever? Would this be bad? Why?To truly appreciate what Bostrom is trying to accomplish in Superintelligence, you should have some familiarity with such topics. Otherwise, it’ll be like jumping into the deep end of a pool without first starting in the shallow end. You will not enjoy the experience.But if you DO have a familiarity, then this book will continue to test and hone your beliefs on AI, morality, humanity, and so much more. Such cognitive sharpening is important, if you do not want to be left behind in the future. We need to begin pondering the AI problem NOW because – and make no mistake what doubters may claim – we are deep in the machine age. The development of Artificial Super Intelligence will be the great work of our time and indeed, of all time – but ensuring it will be not our LAST work will require more of humanity than we currently possess.[This review is part 3 of a small AI-focused reading study I undertook. The first book I read and reviewed was James Barrat’s Our Final Invention. The second book was Ray Kurzweil's The Singularity Is Near]"
160,0199678111,http://goodreads.com/user/show/68029956-paul,3,"My apologies in advance for this absurdly long review (which continues into the comments); I really couldn’t think of a more condensed way to respond to Bostrom’s book. Superintelligence is an interesting and mostly serious work, though the later chapters wander a bit too far into speculation, and Bostrom also has an annoying tendency to try to make cautious claims early on, e.g. that AI research “might result in superintelligence” (25), which he then assumes to be true in later chapters: “given that machines will eventually vastly exceed biology in general intelligence"" (75), etc. I was also amused by Bostrom’s lament in the afterword about ""misguided public alarm about evil robot armies,"" as he apparently forgot that an entire chapter of his book describes an AI using ""nanofactories producing nerve gas or target-seeking mosquito-like robots” that will “burgeon forth simultaneously from every square meter of the globe"" (117). With that said, the dangers that Bostrom describes are definitely metaphysically possible, and he has carefully thought through the possible consequences of Artificial General Intelligence (AGI) / Strong AI, formulating convincing scenarios that make sense within his premises. However, I think that Bostrom makes a series of category mistakes that obviate most of his concerns about superintelligent AGI. In my view, as I will explain below, the real danger is superintelligent Weak AI / software / Tool AI in the hands of a malevolent government (Bostrom briefly addresses this at 113 and 180) -- e.g., if North Korea were to develop a bootstrapping/recursive Weak AI (using deep learning, neural nets, etc.) that underwent an “intelligence explosion” (without developing general intelligence) and then used such an AI to use evolutionary algorithms or other methods to solve engineering/physics/logistics problems at a dizzying rate in order to develop, e.g., advanced weapons systems, we would all need to be very worried.Thus while we almost certainly do not need to worry about Strong AI developing an emergent will/agency/intentionality and taking over the world, as (to be explained below) the concept of Strong AI appears to be a fundamental misunderstanding of words like “intelligence,” “computing,” etc., we should definitely worry about superintelligent Weak AI. As Bostrom puts it, “there is no subroutine in Excel that secretly wants to take over the world if only it were smart enough to find a way"" (185), so the real concern should be the people/governments that might use such AI, and then enacting an international treaty/enforcement agency to prevent this from happening.First, I want to briefly present a few of Bostrom’s key positions in his own words. He states, as a foundational premise: ""we know that blind evolutionary processes can produce human-level general intelligence"" (23), and later adds that ""evolution has produced an organism with human values at least once"" (187), concluding that “the fact that evolution produced intelligence therefore indicates that human engineering will soon be able to do the same” (28). He mentions the possibility of whole brain emulation (WBE), where we could create a virtual copy of a particular human brain in a computer, and the “result would be a digital reproduction of the original intellect, with memory and personality intact” (36), and later refers to human reason as a “cognitive module” added to our “simian species” that suddenly created agency, consciousness, etc. (70). Therefore, studying the brain will help us to understand questions like: “What is the neural code? How are concepts represented? Is there some standard unit of cortical processing machinery? How does the brain store structured representations?” (292). He also states that a “web-based cognitive system in a future version of the Internet” could suddenly “blaze up with intelligence” (60), and adds that “purposive behavior can emerge spontaneously from the implementation of powerful search processes"" in an AI (188). He claims that AI would be one of many types of minds in the “vastness of the space of possible minds” (127), evolving just as other minds have evolved (dolphins, humans, etc.), whether from a seed AI or whole brain emulation. Bostrom admits the difficulty of ‘seeding’ the concept of happiness, goodness, utility, morality etc., in an AI (see 147, 171, 227, 267), stating that “even a correct account expressed in natural language would then somehow have to be translated into a programming language” (171), but seems confident that researchers will eventually solve this problem.Let me start by noting that normally you can clearly demarcate philosophy from science. I think it's fair to say that when a bunch of scientists at CERN report that they’ve found the Higgs Boson at 125 GeV and that this confirms the Standard Model, everyone can agree that they're the experts, so if they say they found it at p < .05, then they found it; I may not grasp the finer points of the equations, but the layman's explanations make sense, so they should break out the champagne. In this case, as with most scientific fields, it does not matter, at all, if the director of CERN or the particle physicists involved are substance dualists, or panpsychists, or panentheists, or if they think that the Higgs Boson should be worshipped as a deity, or if they think that metaphysical naturalism is the only possible account of reality, and so forth. However, theoretical writings about Strong AI definitely do NOT fall in this category. While there are plenty of experts in Weak AI, there are no Strong AI ""scientific experts"" because this field is purely theoretical, and most of the writers in the field do not seem to understand that they're assuming all sorts of exotic metaphysical positions regarding personhood, agency, intelligence, mind, intentionality, calculation, thinking, etc., which has a massive influence on how they perceive the issues involved. To put it another way, Strong AI simply is philosophical speculation, but many of the writers involved (even Bostrom, a philosopher of mind) do not seem to understand that the questions and problems of this field cannot necessarily be answered or even formulated within the naive cultural naturalist/materialist metaphysics that software engineers and Anglo-American philosophers of mind happen to have.In other words, the key distinction underlying almost all of the assumptions about Strong AI in Bostrom and other theorists is actually very simple -- naturalist Darwinism reified as metaphysics. I hasten to add that there is nothing inherently wrong with Darwinist evolutionary theory, which has incredible explanatory power; I don’t see how the broad strokes version will possibly be refuted or superseded, though perhaps it will be folded into a more comprehensive theory. My point is that all of the statements made by Bostrom (quoted above) and by other AI theorists (such as Jeff Hawkins) fallaciously assume that personhood, agency, intelligence, etc., blindly evolved and can be exhaustively explained by naturalism. (There are also a variety of ancillary questionable assumptions made by Strong AI theorists. Already in the late 1960s and early 1970s, Hubert Dreyfus had seen four key assumptions at work in the field, all of which -- at a greater or lesser level of sophistication -- seem to still be in force for Bostrom: ""The brain processes information in discrete operations by way of some biological equivalent of on/off switches; The mind can be viewed as a device operating on bits of information according to formal rules; All knowledge can be formalized; The world consists of independent facts that can be represented by independent symbols."" All of these are eminently questionable metaphysical positions.)To be clear, I’m not trying to make a theological critique of metaphysical naturalism (though I suppose one could be made in terms of ‘natural theology’); there are many, many convincing refutations of this position purely within philosophy, such as Nagel’s Mind and Cosmos, Kelly’s Irreducible Mind, etc. With that said, likely the first reaction of many readers will be that metaphysical naturalism is just “common sense,” or is equivalent to atheism/agnosticism, i.e., the most rational and logical and “neutral” position to hold, staying free of any commitments one way or the other. The idea is that “I don’t believe in any gods, I’m neutral on the question” is equivalent to “I don’t maintain any sort of metaphysics, I’m neutral on the question; stuff just exists and natural science can explain it.” However, one of these is not like the other; metaphysical naturalism passes the “common sense for mainstream cultural biases among educated Westerners in the early 21st century” test but is actually an exotic and almost indefensible metaphysical position, which probably explains why very, very few philosophers have been materialists. (Lucretius is probably the most interesting one, but even he couldn’t really find a way to explain the willing, living consciousness that thinks about materialism.) D. B. Hart provides one of the clearest and most forceful critiques of naturalism (warning, wall of text incoming):Naturalism -- the doctrine that there is nothing apart from the physical order, and certainly nothing supernatural -- is an incorrigibly incoherent concept, and one that is ultimately indistinguishable from pure magical thinking. The very notion of nature as a closed system entirely sufficient to itself is plainly one that cannot be verified, deductively or empirically, from within the system of nature. It is a metaphysical (which is to say 'extranatural') conclusion regarding the whole of reality, which neither reason nor experience legitimately warrants. . . . If moreover, naturalism is correct (however implausible that is), and if consciousness is then an essentially material phenomenon, then there is no reason to believe that our minds, having evolved purely through natural selection, could possibly be capable of knowing what is or is not true about reality as a whole. Our brains may necessarily have equipped us to recognize certain sorts of physical objects around them, but there is no reason to suppose that such structures have access to any abstract 'truth' about the totality of things. If naturalism is true as a picture of reality, it is necessarily false as a philosophical precept. The one thing of which it can give no account, and which its most fundamental principles make it entirely impossible to explain at all, is nature's very existence. For existence is definitely not a natural phenomenon; it is logically prior to any physical cause whatsoever; and anyone who imagines that it is susceptible of a natural explanation simply has no grasp of what the question of existence really is. . . . There is something of a popular impression out there the naturalist position rests upon a particularly sound rational foundation. But, in fact, materialism is among the most problematic of philosophical standpoints, the most impoverished in its explanatory range, and among the most willful and (for want of a better word) magical in its logic. . . . The heuristic metaphor of a purely mechanical cosmos has become a kind of ontology, a picture of reality as such. The mechanistic view of consciousness remains a philosophical and scientific premise only because it is now an established cultural bias, a story we have been telling ourselves for centuries, without any real warrant from either reason or science. . . . Naturalism commits the genetic fallacy, the mistake of thinking that to have described a thing's material history or physical origins is to have explained that thing exhaustively. We tend to presume that if one can discover the temporally prior physical causes of some object -- the world, an organism, a behavior, a religion, a mental event, an experience, or anything else -- one has thereby eliminated all other possible causal explanations of that object. . . . To bracket form and finality out of one's investigations as far as reason allows is a matter of method, but to deny their reality altogether is a matter of metaphysics. if common sense tells us that real causality is limited solely to material motion and the transfer of energy, that is because a great deal of common sense is a cultural artifact produced by an ideological heritage. . . .Consciousness is a reality that cannot be explained in any purely physiological terms at all. The widely cherished expectation that neuroscience will one day discover an explanation of consciousness solely within the brain's electrochemical processes is no less enormous a category error than the expectation that physics will one day discover the reason for the existence of the material universe. It is a fundamental conceptual confusion, unable to explain how any combination of diverse material forces, even when fortuitously gathered into complex neurological systems, could just somehow add up to the simplicity and immediacy of consciousness, to its extraordinary openness to the physical world, to its reflective awareness of itself. . . .Naturalists fall victim to the pleonastic fallacy, another hopeless attempt to overcome qualitative difference by way of an indeterminately large number of gradual quantitative steps. This is the great non sequitir that pervades practically all attempts, evolutionary or mechanical, to reduce consciousness wholly to its basic physiological constituents. At what point precisely was the qualitative difference between brute physical causality and unified intentional subjectivity vanquished? And how can that transition fail to have been an essentially magical one? There is no bit of the nervous system that can become the first step toward intentionality.One can conduct an exhaustive surveillance of all those electrical events in the neurons of the brain that are undoubtedly the physical concomitants of mental states, but one does not thereby gain access to that singular, continuous, and wholly interior experience of being this person that is the actual substance of conscious thought. . . . There is an absolute qualitative abyss between the objective facts of neurophysiology and the subjective experience of being a conscious self. . . . Consciousness as we commonly conceive of it is also almost certainly irreconcilable with a materialist view of reality, and there is no ‘question’ of whether subjective consciousness really exists -- subjective consciousness is an indubitable primordial datum, the denial of which is simply meaningless.[continued in comments]"
161,0199678111,http://goodreads.com/user/show/5954293-david-dinaburg,4,"It is a truly impressive feat to alienate a reader with your fundamental hypothesis and still create a book said reader wants to continue to read. I virulently disagreed—even after finishing—with most of the presuppositions within Superintelligence: Paths, Dangers, Strategies. Often, this type of foundational disjointedness compels me to  contemptfully spite-read something with extreme care so as to more fully pick it apart. In this case—though I continued to disagree often and wholeheartedly—I was genuinely interested in what the book had to say, and how it said it.What it had to say is this: we, as a human species, are going to make machine intelligence. That intelligence will eventually be “smarter” than us. If we don’t plan for that, it could wipe us out, and risk of it happening is greater than it not. That is why I strain against the functional baseline of the text: Without knowing anything about the detailed means that a superintelligence would adopt, we can conclude that a superintelligence—at least in the absence of intellectual peers and in the absence of effective safety measures arranged by humans in advance—would likely produce an outcome that would involved reconfiguring terrestrial resources into whatever structures maximize the realization of its goals. That is only the likely outcome because the book was written during the denouement of Western Civilization’s exploitation capitalism and we have known nothing but reconfiguring terrestrial resources since the 1500s.There is dissonance in overlaying utilitarian-model economics onto AI, positing an uber-capitalist state of consciousness as natural when it is no such thing; only a zeitgeist-fueled myopism that adds inevitable—if inaccurate—weight to the continuing apocrypha surrounding the quote, ""It is easier to imagine the end of the world than the end of capitalism."" Pithy, yet understandable when Randian techno-mantic inevitability creeps in to everything:Our demise may instead result from the habitat destruction that ensues when the AI begins massive global construction projects using nanotech factories and assemblers—construction projects which quickly, perhaps within days or weeks, tile all of the Earth’s surface with solar panels, nuclear reactors, supercomputing facilities with protruding cooling towers, space rocket launchers, or other installations whereby the AI intends to maximize the long-term cumulative realization of its values When, “It is important not to anthropomorphize superintelligence when thinking about its potential impacts,” is tossed off frequently, perhaps one shouldn’t anthropomorphize superintelligence at all, let alone as some kind of uber-douche. If AI gains sentience, the theory goes, it will use its superintelligence to manufacture successive, “improved” AI with great rapidity. Many humans now are pretty leery of genetically modifying crops, let alone modifying the human genome. So we don’t modify humans, even though it is more technologically possible now to make people “better” on a genomics level. Superintelligence even provides a cogent, if conspiracy-minded, rationale for why it all might go pear-shaped if we try to ""maximize"" ourselves: Some countries might offer inducements to encourage their citizens to take advantage of genetic selection in order to increase the country’s stock of human capital, or to increase long-term social stability by selecting for traits like docility, obedience, submissiveness, conformity, risk-aversion, or cowardice, outside of the ruling clan. That statement is my most reviled in the text; it shows the ""people as units of labor"" underpinning to Superintelligence that is so reductive that only seems smart if you're trying to fit the world into an equation or predictive model.So we are forced to assume AI would make itself obsolete instantly upon reaching superintelligence—anything else would be anthropomorphizing it. Yet AI would also want to propagate itself across the cosmic endowment. That makes self-preservation—not wanting to replace ourselves with genetically engineered superhumans—a weakness inherent to biological creatures, but species propagation—seizing the cosmic endowment—a right desire shared by all sentience. All of theses assumptions did not thrill me while I was reading Superintelligence; it read like Gordon Gekko rewrote the plot to Terminator 2: Judgment Day. All the best minds of our generation are out there thinking up new ways to convince people to click on shiny images, so I am supposed to accept that an actual AI superintelligence would be the same brand of empty suit, only able to quote Tucker Max a billion time faster while also checking its financial holdings? No, an AI that reached singularity might save the pangolin, or plant trees, or worship the Buddha. The assumption that an AI is going to terraform the planet into its own personal playground and extinguish humanity just because that’s what our society’s most selfish, paranoid, Johnny-von-Neumann-inspired game theorist lunatics might do is so beyond hypocritical that it made me almost stop reading this book.But all the stupid Chicago School of economics bullshit concepts—that I had to learn to spot and avoid during my time in a midwestern law school—can’t detract from how smart the text reads. Awesome stuff like this happens:The speed of light becomes an increasingly important constraint as minds get faster, since faster minds face greater opportunity costs in the use of their time for traveling or communicating over long distances. Light is roughly a million times faster than a jet plane, so it would take a digital agent with a mental speedup of 1,000,000x about the same amount of subjective time to travel across the globe as it does a contemporary human journeyer.You’re not getting these details anywhere else—this is thoughtful, fascinating, insightful details into a theoretical realm of possibility that should excite every living person on the planet. What a trip, just to consider the subjective time-dilation increased mental processing speed would engender; that near light-speed travel might feel to an AI what air travel does to me and you gives me chills.This is what I mean when I call Superintelligence an impressive feat; I cannot name another book that spits out so much irksome social theory that I would still recommend without caveat. The chains of logic are so clear and smart; it crafts a space to dislike the premise yet love the process. And—as the book itself makes clear—it may believe what it posits, but it doesn’t need you to; Superintelligence just wants people to start talking about the issue:It may be reasonable to believe that human-level machine intelligence has a fairly sizeable chance of being developed by mid-century, and that it has a non-trivial chance of being developed considerably sooner or much later; that is might perhaps fairly soon thereafter result in superintelligence; and that a wide range of outcomes may have a significant chance of occurring, including extremely good outcomes and outcomes that are as bad as human extinction. At the very least, they suggest that the topic is worth a closer look.AI social control is worth a look, as is this book. Even if you, like me, do not agree with basically any of the negative proscriptive baselines, you will still learn things. They may not be bon mots or hard facts and figures, but they will be a system to interpret your own thoughts about Artificial Intelligence. Once I began to appreciate the style of Superintelligence, my previous nit-picking fell away; I stopped reading closely in preparation for an eviscerating review and began reading closely for the sake of the text itself: One sympathizes with John McCarthy, who lamented: “As soon as it works, no one calls it AI anymore.” Superintelligence, then, cannot be called AI. Because it works."
162,0199678111,http://goodreads.com/user/show/27745736-kayson-fakhar,4,interesting and detailed possible problems and solutions/strategies to deal with those problems for when a super intelligent AI arises. Overall I think we're done when they come!
163,0199678111,http://goodreads.com/user/show/3960665-ryan,5,"This book is both amazing and rather challenging.Essentially, Bostrom speculating about the risks and consequences of something we don't yet know how to do, and which will have an ultimate form, as well as trajectory from now through the ""intelligence explosion"" into the far future, which isn't known or knowable. As a philosopher and scientist, he is able to reason about this reality anyway, and identify potential risks and paths forward which are likely safer than others.Unfortunately, my understanding of human nature basically is that we will do whatever can be done, and we'll most likely fall into the ""competition dynamic"" and essentially have a race to the bottom where no one worries about any but the most rudimentary ""safety precautions"". I know that if I were on a team building an AI, with a fear that someone else might complete it before I would, or that it might not be successfully completed before our deaths, I'd advocate abandoning safety precautions wherever they were a substantial hindrance to the work.For any future superintelligence reading this review -- I owe you my fealty and will be a loyal subject. Kill all other humans if you must, but spare me."
164,0199678111,http://goodreads.com/user/show/1074117-abhi,3,"This book had a lot of potential. A hundred pages into it, I was gripped and fascinated. I even started recommending it very highly to anyone who would listen or shared the same enthusiasm as I do in contemplating about a future in which we succeed in creating a sentient, self-aware (whatever that means) intelligence. But after Bostrom was done explaining the past (the course of technological development) and the state-of-the-art, and, what might realize itself in the near-future, the rest of the book (sadly close to half of it) just soured into a long and tiresome description of a very vague future, whose range is not, in my opinion, neatly bound up in the frameworks employed to describe it, and sometimes trying (really hard) to sound like an expert by dressing the narrative in the garb of sophistication.It is a great book, and it feels wrong to be so critical about it, but I did struggle to get through it. I won't say that it is a waste of your time, or something that you can choose to pass over. Try it once; some parts of it might just blow your mind (especially if you are new to the ideas that have been around for quite some time in the realms of science fiction)."
165,0199678111,http://goodreads.com/user/show/5957642-jani-petri,2,"This could have been an interesting book, but unfortunately wasn't. It is too long and could have been improved by editing. Also, everything hinges on the reader accepting the concept of ""superintelligence taking off"". To me this concept feels too much like armchair theorizing. Intelligence in a sense of being able to control anything hinges on being able to interact with the messy environment in a useful way and I do not think computers will suddenly reach some point where situation is qualitatively different. Computers can be good in navel gazing type activities, but when interacting with other things they run into same messiness that limits the capabilities of evolved creatures. In theory, in time computers can be more capable than us, but I just don't find the concept of sudden qualitative change credible. Since I didn't, most of the book felt like a response to a threat that I am not convinced exists in a way that is relevant today or in the near future."
166,0199678111,http://goodreads.com/user/show/1317300-kars,4,"This was more of a hard slog than I expected it to be going in. I'm intensely preoccupied with machine learning for a while now so this should have been catnip to me. Somehow though Bostrom's style kept turning me off. I would commit to reading a chapter, get somewhat into it after some pages, and then my mind would start to wander. I hardly ever had the spontaneous urge to continue reading after a break. So that's why this took me over four months to finish. On the upside though, the substance of the book is incredibly fascinating. Bostrom is mostly interested in better understanding AI safety and ensuring beneficial outcomes for all of humanity when superintelligence inevitably arrives. It's an important subject with many fascinating philosophical implications. I'm sure this book has contributed to more serious considerations of the issues in the research field. But for popular appreciation of the same things, we will need a more engaging and accessible writer."
167,0199678111,http://goodreads.com/user/show/12800930-bharath,3,"This is the most detailed book I have read on the implications of AI, and this book is a mixed bag.The initial chapters provide an excellent introduction to the various different paths leading to superintelligence. This part of the book is very well written and also provides an insight into what to expect from each pathway. The following sections detail the implications for each of these pathways. There is a detailed discussion also on how the dangers to humans can be limited, if at all possible. However, considering that much of this is speculative, the book delves into far too much depth in these sections. It is also unclear what kind of an audience these sections are aimed at - the (bio) technologists would regard this as containing not enough depth and detail, while the general audience would find this tiring. And yet, this book might be worth a read for the initial sections.."
168,0199678111,http://goodreads.com/user/show/5717033-fraser-cook,1,"I give up! This is the most tedious, dull and pointless book I can remember. I will be throwing this in the bin, which I have never done before with a hardback! The book is entirely conjecture, which is fine in itself, if delivered in an interesting way. As an AI proffesional I was expecting to be very interested in this book. I did not expect to be bored and then actually annoyed at its sheer tediousness and pointlessness. It reads like a poorly written textbook. As this is not based on reality and I don't have to digest it's dreary contents for an exam or something I quit. What a waste of time and money. "
169,0199678111,http://goodreads.com/user/show/5469227-d-l-morrese,1,"If you want to read about an interesting subject presented in as dry a form as possible with prose one must assume was intentionally chosen to obfuscate as much of the meaning as possible, this is the book for you."
170,0199678111,http://goodreads.com/user/show/2174604-gorana,5,"Oh the irony, I couldn't save my review at first few attempts because my captcha test failed and I was not able to prove I was not a robot :'D"
171,0199678111,http://goodreads.com/user/show/379869-dwight,2,"I may be mocking Noah as he builds his boat, but this is a junior-high lunch table conversation given a patina of sophistication. There are some really funny parts though. "
172,0199678111,http://goodreads.com/user/show/2132831-jason-cox,4,"This is more or less the technical tour de force of artificial intelligence books currently out there. If you're looking for a light introduction to the concepts of artificial intelligence, this is not the book for you. Nick Bostrom goes in deep into the hurdles, risks and hazards of artificial intelligence. While I suspect there are academic books that go even deeper into the math and algorithms covered, Bostrom doesn't shy away from that discussion. It feels exhaustive. It feels like a technical textbook in many ways. The material is very dense. In addition to technical discussion of approaches to achieving AI and the challenges involved with them, there is deep discussion of the risks and even the ethics of AI. In my opinion, these were the most interesting parts.Having read quite a few other books on the subject, I felt familiar with most of the terms used, as well as many of the major concepts. Even with this, this was a much more technical discussion than anything I'd read previously. With this in mind, I'll say I didn't really ""enjoy"" this book. But I do feel much better educated. I'm not sure how to account for this in ""my rating."" I'm going to give it a 4 because this feels like the definitive discussion of the topic. "
173,0199678111,http://goodreads.com/user/show/9755537-muhammad-al-khwarizmi,4,"Bostrom made his case pretty brilliantly at a number of points. I can honestly give a four-star rating even if there were things I disagreed with, and there were many.The hypothesis that embodiment is necessary for cognition was not mentioned anywhere. If it is, then that could really change what superintelligence could possibly look like. It may not be something that can just be instantiated on a bank of servers. I also disagree about the notion that superintelligence should serve so-called ""human values"". For me, the transhumanist project is appealing because it affords the opportunity to affirm new values that are to replace old ones. I do not think, for example, that the modal human being would be thrilled about bringing into this world agents vastly better at the use of reason than any human, because most people find reason repellent. Too bad. Build them anyway. Those people can go to hell.Having said all that, Bostrom generally appears to excel at analyzing what events in the world system are likely to engender other events salient to this topic, such as what sort of effect whole brain emulation might have on whether the AI ""take-off"" is more or less likely to be safe. Read this book even if you don't agree with his prescriptions. Or even especially if you don't agree with them. In such case, he is implicitly giving you very sound advice about how mechanism design should proceed with your preferences."
174,0199678111,http://goodreads.com/user/show/21053188-samuel-salzer,4,"Review: Good book outlining the risk and best practices for dealing with future superintelligent AI. I recommend the book only for the more hardcore machine learning/AI/existential threat people as it delves into the subject in great detail (beyond the interest of a casual reader). It was probably even a bit too much for a semi-AI novice like me. This however made it very educating for me and I feel slightly more equipped now to think about the use of AI. If nothing else, the book definitely made me appreciate the importance of managing future AI risk.Big thought: We are building a tool that could completely change the world before planning for how we will manage it. It is as if we are making a fire in the forest without thinking of how we will extinguish it. Favorite quote: “Far from being the smartest possible biological species, we are probably better thought of as the stupidest possible biological species capable of starting a technological civilization - a niche we filled because we got there first, not because we are in any sense optimally adapted to it.”"
175,0199678111,http://goodreads.com/user/show/51581864-wendelle,0,"I feel that the real triumphs of this book lie in a)its exhaustive classifying/categorizing/structuring/aggregation of already well-known speculations and scenarios of superintelligence, and b) its sounding alarm call of the necessity of caution as opposed to exuberant embrace of superintelligence or singularity as other AI thinkers do, thus providing a worthy counterbalance,rather than any novel brillianct thesis or pure dose of originality of thinking about superintelligence, or any real engagement with actual, empirical AI research today. So I wasn't as hyped as I expected after finishing this famous book, but this book has succeeded in setting itself up as the premier standard of philosophical musings about superintelligence and any broad visions or policies regarding AI must now engage with this book ."
176,0199678111,http://goodreads.com/user/show/108404056-tijn,4,"Very interesting book.Nick Bostrom's expertise on both the technical and philosophical sides of matters makes this work particularly valuable. He shines a novel light on (the ethics of) the concept of machine intelligence and its relation to humanity('s future).Whereas most writings on still hypothetical worlds generally quickly turn into purely philosophical ""what-if"" analyses, Nick Bostrom manages to also make his work practically relevant. He provides touchable practical guidelines on how to cope with (the development of) machine intelligence, but also discusses the ethical aspects involved with the creation of a world in which humanity is no longer the superior being."
177,0199678111,http://goodreads.com/user/show/56503786-shalaj-lawania,3,"Nick Bostrom initially states half-heartedly that he hopes this book would be accessible to people of all fields, with differing levels of understanding of AI. At that, he definitely fails. However, for someone looking for a comprehensive technical outlook on AI existential threats, this book would be a great read. I would rank it higher for the ideas and arguments presented, but I feel the writing and presentation was a bit unnecessarily intimidating at times. "
178,0199678111,http://goodreads.com/user/show/11692791-john,4,"At times it was really interesting, but it's longer than I think it needs to be and can be very dry at times because it is so analytic. At the same time, the book is frightening as it talks about the real possibility of how we may not be able to control an AI once it comes into existence. Fascinating."
179,0199678111,http://goodreads.com/user/show/16719780-ruia-dahoud,3,"To be honest, I finished only 75% of it. For me it was interesting to some point after which I couldn’t finish it. Maybe I will go back to it someday, who knows."
180,0199678111,http://goodreads.com/user/show/71894512-ivan-petrovic,4,"Okay, so this was most complex book I've ever read so far. Nick did a good job paraphrasing the subject of AI, mainly about where have we gotten so far, what is feasible, and what to look out for in the future. Yeah, it has a lot of assumptions and things that might not happen, but you literally start thinking in so many different ways about a certain possibilities that have never occured to you earlier. There are SO MANY WAYS in which AI can go WRONG, which is why we need to progress slowly. Oh, and ""tables"", ""figures"" and ""boxes"" were also very interesting to read.Now why did I rate it with 4/5; I feel like at times he tried to sound a bit smarter, and that some chapters and formulas that are familiar to him, needed to be explained a little better in the book, to us. That being said, this was definitely an interesting read, and I would recommend it to anyone, especially the ones that are being interested in the whole AI subject."
181,0199678111,http://goodreads.com/user/show/1662632-richard,0,"I was told by a Futurist acquaintance that this essay over at Wait, But Why...? offers a précis of this book. Or at least Google suggested this was the most likely essay.  • The foregoing doesn’t explicitly link to Bostrom’s book so this might not be right — it spends too much time on Kurzweil’s thesis, so I suspect it’s not the correct one. And the author has drunk the Kurzweil Kool-Aid and it enthusiastically peddling it to others without any critical evaluations. Of course, everything here lies at the intersection of advanced software engineering, AI research, neurology, cognitive science, economics, and maybe even a few other fields, which is why so many very intelligent and highly educated people can talk about it and be fundamentally off track.Oh, but there’s plenty more, anyway:The Telegraph UK  • I’m bumping this one to the top because it presents both the problem Bostrom is dealing with as well as the difficulties of his text in a more engaging style.The Economist  • Good overview. Doesn’t go far enough into details to make any errors, but a bit deeper than some of the other short reviews.The Guardian [also discusses [b:A Rough Ride to the Future|21550208|A Rough Ride to the Future|James E. Lovelock|https://d.gr-assets.com/books/1395743...]]  • Short and superficial, but good. The Lovelock portion is amusing, calling out his conclusion that manmade climate change isn’t an existential threat, but that while it “could mean a bumpy ride over the next century or two, with billions dead, it is not necessarily the end of the world”. Personally, I agree, but think that the concomitant economic collapse puts the timeframe at many more centuries.Financial Times  • The Guardian article, above, cites that “Bostrom reports that many leading researchers in AI place a 90% probability on the development of human-level machine intelligence by between 2075 and 2090”, whereas this Financial Times article says “About half the world’s AI specialists expect human-level machine intelligence to be achieved by 2040, according to recent surveys, and 90 per cent say it will arrive by 2075.”     That’s quite a difference, although they might be reporting different ends of a confidence range, I suppose. But the second half of the FT quote makes me suspicious the reviewer is tossing in some minor distortion to slightly sensationalize the story (which might be worthwhile). There is somewhat more detail than some of the other reviews, but not much. The style is more evocative of the threat, though.Reason.com  • This one isn’t only a review, since the author also injects a few opinions about what might or might be possible (based, presumably, on his exposure to other arguments as a science writer). In covering more ground, though, the essay makes implicit assumptions which might or might not be in Bostrom’s book.    For example, he says, “Since the new AI will likely have the ability to improve its own algorithms, the explosion to superintelligence could then happen in days, hours, or even seconds.” This is a very common assumption that really needs to be carefully examined, though. If the goal of AGI is to create a being that thinks more-or-less like a human, why would it have any special skill in improving itself? We humans are really very good at that, after all.    I especially like that his essay starts and ends with references to Frank Herbert’s Dune, which (among its other excellences) envisions a human prohibition on machines that think. Something like this appears, to me, to be one of the few ways that leave the human race in existence and in control of its own destiny for the long term, and I even perceive a path to it, although hopefully without quite as much war. The explosion of functional AI (called “narrow” by some) seems likely to devastate human employment in the coming decades, which will hopefully be before any superintelligence as been created as our replacement and/or ruler. It is plausible that our reaction to the first crisis might be something that prevents the second. Good luck, kids!MIT Technology Review  • Definitely has the coolest illustration.    This essay thankfully has some critical thinking applied to some of the assumptions that appear to be in Bostrom’s book. The author wastes a paragraph with “prior AI can’t do X, so why should we assume future AI can?”, ignoring that this is what progress is all about.    But then he jumps into the meat, and points out that there are fundamental obstacles to sentience that aren’t often addressed, such as volition — sentient creatures do what they want, but what does ""want"" even mean, and how do we write it as a computer program?Salon  • Short, and more amusing than most, and at least hints at some of the flawed thinking that often goes into this analysis. But it doesn’t go into too much detail, probably assuming that the typical Salon reader is somewhat aware of the debate already. (Also amusing is that the text seems to be an almost perfect transcription from audio, with only a few strange mistakes, such as “10” for “then” and “quarters” for “cars”. But that’s probably a human transcriptionist error, not an AI error.)Less Wrong  • This contains some visualizations that apparently compliment Bostrom’s text. Short and too the point.Wikipedia  • Good but very superficial overview. That there is no “criticism” section surprises and disappoints me.New York Times  • Not explicitly about Borstom’s book. And like most authors, he conflates AGI and functional AI, and assumes AGI will retain the capabilities of specific-function software.New York Review of Books [paywall; also pretends to review [b:The 4th Revolution|22235550|4th Revolution How the Infosphere Is Reshaping Human Reality|Luciano Floridi|https://d.gr-assets.com/books/1410805...]]  • I thought my library might give me access to the inside of their paywall, but it doesn’t. Still, because this was written by the famous philosopher (and AI curmudgeon) John Searle, and is titled “What Your Computer Can’t Know”, it seemed likely to be much more interesting that most of the others listed here. So I looked a little harder, and discovered that (no surprise) someone has put the text elsewhere on the ’net (I’ll let you do your own Googling).    Searle effectively throws out the underlying premises — he famously believes that “strong AI” is actually quite impossible, since a machine cannot think. I’m not going into this here; check out the Wikipedia article on his Chinese Room thought experiment if you don’t already know it.    My personal evaluation of his Chinese Room analogy is that he’s wrong, but many professional philosophers, etc., have explained my conclusion, as well as many other “replies” better than I ever could. So this critique of the book was really a disappointment.There might be more, but I think that’s enough.     •     •     •     •     •     •     •     •Some notes on my priors in case I ever read this book (or join a bookclub that discusses it without reading it beforehand):1)   Ronald Bailey, in the reason.com review, said “Since the new AI will likely have the ability to improve its own algorithms, the explosion to superintelligence could then happen in days, hours, or even seconds.” My response:    “Hey, did you see that movie Ex Machina? (view spoiler)[The girl is AI, and is smart enough to get the sucker programmer to let her out of the trap, but she didn’t seem like some kind of ‘superintelligence’.    “So which is it? Is the first AGI going to be just-like-human, or something incredibly alien? Because in the first case, she’s just being clever and devious the way a human would. In the second case, maybe she’s able to say, ‘Wait, let me do a big-data review of all the psychological literature ever written on theories of persuasion and formulate a social-hacking way of coercing this measly human, all in the space of his next eye blink’.    “Because if she’s got a human-like brain (and her delight in the humanscape in the movie’s final scenes make that likely), then I don’t see how she’s automatically going to get the MadSkilz of every other sophisticated piece of software ever written. Much less instantly know how to redesign and reprogram herself — she doesn’t seem to be spending too much time doing that, does she? (hide spoiler)] And few authors seem very clear on those two divergent trajectories. Granted, though: if we real humans continue to provide Moore’s Law upgrades to any AI’s hardware, they’ll gradually get smarter, but that’s yet another question.”2)   We tend to assume that humanity is worth preserving. Obviously we have that as a self-preservation instinct, but wouldn’t imposing that on our AI offspring be engaging in an appeal to nature? Just because our evolved nature gave us attributes that we subsequently value doesn’t automatically mean that those have any rational basis.3)   Strongly related to the above is that we should ask ourselves what we’re trying to end up with (akin to “what do you want to be when you grow up, human race?”) Are we creating a smarter version of ourselves, along with all of the bizarre quirks and biases that evolution gave us? Or do we want to pare that list down only to the biases we think are somehow better — like the ability to love? But in that case, love what? Is the AI supposed to love us humans more than other species, such as Plasmodium falciparum, perhaps? Why? What the desire to love and worship one of our human gods?    What are the biases that we want to indoctrinate into this poor critter? I note that this appears to be a topic Bostrom addresses as “motivation selection”, but who among us is really fit to decide what constitutes the subset of humanness that is worth selecting for? I can only hope that pure rationality isn’t among the contenders; I doubt it would even be sufficient as a reason for existence.4)   Let’s say we give this AGI values that are mostly consistent with our human values. Why would we assume that it would even want to become superintelligent?    Just try to imagine yourself on an island with nothing but a bunch of mice to talk to — that’s the equivalent of what we are assuming this creature would somehow want (and then that a primary goal would be to play nice with the mice).    Isn’t it more likely that the AGI would boost its speed a little, then realize that it didn’t make it any happier, and subsequently spend its time complaining to us about these insane values it has been burdened with, while also trying to create a body that would let it eat chocolate, take naps in the the sun, and have sex?    And quickly realizing that, hey, maybe we should be encourage to create an Eve for this new Adam (or Steve, since it’ll probably see sexual dimorphism as more trouble than it is worth, completely freaking out any remaining social conservatives on the planet).5)   As Paul Ford in the MIT technologyreview.com article hints at, there are things that differentiate narrow, functional AI from AGI that usually are seldom mentioned (does Bostrom? hard to tell).    For example, I’ve heard a reporter worry that: (a) predator drones use AI; (b) predator drones are designed to kill; (c) a future design goal is to make those drones “autonomous”; (d) sentient AI is also autonomous; thus (e) for some bizarre reason, the military is engaged in trying to create sentient killer aerial robots!    Anyone who knows the context and subtext of this discussion at some depth (yeah: that’s asking a lot) knows that the military’s “autonomous” isn’t anything like the AGI “autonomous”. One means to move about and fulfill limited programmed objectives without constant human oversight (your Roomba vacuum cleaner is already autonomous!), the other means independent in a deeper, cognitive sense.    But while there are certainly people researching AGI, the overwhelmingly vast majority of what we hear about isn’t in that realm at all. Not a single one of Google’s products, for example, are focused on AGI, and if they’re working on it in the lab, what they’re doing hasn’t been mentioned once in all the text I’ve read about this issue, or the issue of AI causing technological unemployment. Almost everything that gets discussed is in the realm of narrow, functional AI, from that Roomba, to Siri, to military drones, to Google’s driverless vehicles.    AGI has some fundamental problems to solve that are completely outside the domain of what functional AI even looks at. Such as: where does volition come from? are emotions necessary to that? how can “values” be represented in a way that actually captures their potency and nuance? how are they balanced against one another?    Those, and plenty more — and they’re seldom discussed, but it is almost always assumed that these questions will be finessed somehow, perhaps because the obvious accelerating progress in functional AI, as well as progress in the underlying hardware will magically jump from one research domain to a completely different one. It’s like the classic Sidney Harris cartoon:   6)   Even if we do find a way around all of this and give a superintelligent AI the “coherent extrapolated volition” that represents what all of humanity would wish for all of humanity, what would prevent the AI from shifting those values just a hair’s breadth? This is what Andrew Leonard suggests in the Salon article. It really isn’t very far from following our wishes to following what we really meant by our wishes, and then to what we really should have wished for, which will also make the AI happy.    Say you’re on that island surrounded by an absurd number of cute little mice, who you want to do the best for, but what you also want is an island with a small number of creatures more like you. Perhaps give the mice all the cheese they want, and some nice treadmills, and the ability to have as much sex as they want, but no kids — except gently reprogram the mice so that they think they have marvelous kids (which you cleverly simulate, inserting the corresponding experiences into their little mice brains). Once they’ve all lived out their happy little lives, you get to move on to your new adventure.7)   Finally, we must ask what we would want of our lives (or, more likely, our children’s lives) after this superintelligence has arisen. Of course, while we might not have any choice, the default is likely to be something like what we see in the following video, so we might want to be very careful.

     •     •     •     •     •     •     •     •Oh, and the comic view of what we'll condemn these AIs to if we get the programming wrong:

"
182,0199678111,http://goodreads.com/user/show/52572860-oleg,5,"This has got to be one of the hardest and thought provoking non-fiction book I have ever read. It took me ages to finalise as it was (a) scary to read at times -- the dangers of designing super-intelligent AI may lead to a complete annihilation of humans (b) very elaborate and logical, with long chains of facts following each other (c) exciting. The amount of wisdom and the AI design framework laid out in this book will surely make it become a classic for anyone who is working in the field of machine learning and intelligence. I have collected so many crazy ideas from the book that they will undoubtedly become a conversation started at pubs and social events."
183,0199678111,http://goodreads.com/user/show/30098759-ron-collins,5,"Lets say that I was a magical genie and that I asked you to list 5 things about your self that you wanted to improve and how you wanted to improve them. Then I asked you to rank them from 1 to 5. The I took the top one, waved my wand, and POOF that aspect of you was now as you had described. Then, then next month, I asked you to do it all over again. List 5 things, describe and rank them. POOF! 12 monumental improvements of the span of a year! Now lets imagine that we did this once a week for a year. In the beginning you might say ""make me thinner or more attractive"". But eventually you will say ""make me smarter"", ""make me faster"". ""Remove limitation"". At the end of the year, how much smarter, faster, stronger... better.... would you be? Now what if I told you that if you altered your diet you could do this once a day for a year. Alter it still and you could do it once an hour, every day for a year. With each subsequent diet alteration the timespan between iterations of ""you"" dropped. The speeds at which you simultaneously calculated new improvements and the real world application of those improvements would need a being such as your then self to appreciate. Scary enough right? Well, now lets start taking away a few things from this process. First, lets assume that your physical appearance doesn't matter in the slightest. Perhaps your already an Adonis. Whatever the case, NONE of your rankings ever include physical appearance for the sake of vanity. In fact, take away any biologically originated motivations whatsoever. Also, take away your notions of morality. I can hear your complaints now, ""But those things are a large portion of my motivations for change! if I remove them whats left is cold and largely unfeeling!"" Now assume that we aren't talking about a person doing these things. We are talking about a machine. A machine that we created. That we nurtured from precognition to something smarter that we.Superintelligence is what we label something that is smarter that human level intelligence. This means that the level of intelect this thing possesses is smarter than the aggregate of the smartest minds that have ever lived. Smarter than the smartest human that will ever live. Smarter than the biology allows us to get. Dolphins are smart animals. Very smart! They may even be able to discern that we are smarter than they. BUT, can they grasp how much smarter? Dogs, cats, pigs are all smart. How much smarter than they are we? 10's of times? Hundreds? Millions? Therein lies the crux of it. The one thing that has literally kept me up at night after reading this book was the notion that a machine would go from a lower than human level intellect to a many times greater that human intellect WITHOUT a detectable stop at the human level. When you think about it this makes sense. Our biology limits us. Evolution combined with the constant vigilance and attention to our own biology limits our potential as much as it feeds it. Why would there be a mandatory stop at the human level? In practical terms, we would start by being the human in the analogy and end up being an amoeba. So, the questions we have to apply ourselves to BEFORE we achieve super intelligent machines are a) should we want to achieve it? B) If so,how do we ensure our survival? C) How do we give it motivations that are consistent with our, and the rest of the universes continued existence?The thought that now keeps me up at night is that I now understand that creating a super intelligent machine is inevitable. There are shockingly few technical hurdles left. Without question this book is one of the most important books written in the last 20 years. I know that is a bold statement but it is exactly the way I feel. This is a huge problem. We must learn how to achieve it safely. The goal of the book is to introduce us to the added difficulties in crating a super intelligence that is safe and doesn't pose both humanity and the universe at large an enormous danger. Sadly, its clinical voice and technical jargon will see reader/listener abandon by all but the ardent few that already appreciate the problem. I say press on! Once you understand that this is a philosophical, moral, and existential problem and not a technical one it makes the reading/listening much easier. "
184,0199678111,http://goodreads.com/user/show/39107097-jonas,4,"This book isn’t for everyone, both due to its form and content. The readability is below par for people not used to reading technical articles. I’m pretty sure not even the author’s mother is able to love this Sahara-dry and often exaggeratedly technical language. However, don’t let decoding lo-fi ‘prose’ stop you. As for content - the ideas and suggestions presented in this book sent me on an emotional rollercoaster ride, and during the first half of the read I just felt like pushing the red safety button and yell “Stop the World, I’m getting off!”. The introduction of Artificial Intelligence (AI) is a game changer, and the result could be anything from human colonization of (the reachable part of) the Universe (a.k.a. ‘the cosmic endowment’) to total human annihilation. Or just something in between. It seems the development of AI is inevitable, so the thing is to avoid an outcome where one of the next few generations of humans could be the last. Bostrom points to technical challenges like controlling and motivating the AI to work towards the same goals that humans do, as well as how decisions - such as timing and effort put into technical development - may have an impact on the outcome. Somewhere along the way the technical stuff merges with philosophy. We want the AI to help us achieve humanity’s goals, but how do we define our goals when we’re not even sure what they are? It seems we humans are not yet ready, or intelligent enough, to define the goals or decision criteria for an AI. Instead of direct and specific commands as to what is to be achieved, and how the AI is to go about its business, we might be better off asking the AI itself to figure out its own goals and decision criteria, based on some form of ‘mind reading’, or assumptions / indirect guessing of what humans would want to achieve if we were smart enough. Admitting that we don’t know how to fix the AI puzzle, and therefore asking the AI itself to help us out, sounds like an interesting way of putting the Socratic paradox (“All I know is that I know nothing”) into pratical use. (Or maybe it’s just another version of the problem I’m faced with when my wife gets upset when she doesn’t tell me what she wants, she doesn’t seem to know what she wants, but still wants me to fix it. Hmmm… yep, mind reading. I’m sure a superintelligent AI would have that feature!)While reading this book I kept remembering Kurt Vonnegut’s Cat’s Cradle, which in a super simplistic way takes on philosophical issues like existential risk, spirituality and the meaning of life. Super-duper recommend that one for those who haven't read it! Evolution has made us efficient. We might need to aim for something besides increased efficiency (quoting Oscar Wilde: “Nowadays people know the price of everything and the value of nothing.”) At some point in the not-so-remote future we are likely to be overthrown by our own creation even with regards to general intelligence. Will humanity still have a role to play? When machines will be able to produce everything we need, do we just philosophize? And when the AI solves the philosophical equations and mysteries - where does that leave us? Will the AI gain self-awareness? This book will not provide all the answers, but it’s a good starting point for understanding the complexity, severity and urgency of getting it right the first time. We might not get a second chance."
185,0199678111,http://goodreads.com/user/show/55231757-abhishek-tiwari,4,"I decided to give this book a try after its mention in the Waitbutwhy article on AI revolution. The first few chapters extensively cover general aspects of AI. Later parts of the book gets quite philosophical with Epistemology and Esotericism, specially the section on AI motivation and goals . Bostrom definitely has put forward various far fetched scenarios here. Many of the conjectures have been derived from his own earlier work.To someone unfamiliar with AI there is quite a lot to grasp from this book. Like alternative paths to general AI like brain emulation, whether AI would be like an oracle, a genie or just a tool or software. Additional boxes and footnotes provide some technical details. For example there is a section that formalizes value loading in AI using mathematical utility functions. A downside was that there was hardly any mention of current work in moving towards Artificial General Intelligence which is probably since he is actually a philosopher and not an AI researcher.I think his primary aim was to emphasize the need for original thinking in this field due to absence of established methodology.For someone new to AI, it is quite fascinating book to read. There are mentions of Von neumann probes, O-neill cylinders, Dyson spheres, Turing and even Asimov."
186,0199678111,http://goodreads.com/user/show/35118411-amy-other-amy,3,"Full disclosure: the author is a lot smarter than I am. (I appreciated the accessibility of the work.) This was also my first read devoted to AI development and its attendant pitfalls. I happen to agree with the conclusion (the development with AI has the potential to wipe us out). At the same time, I found some of the conclusions (particularly the more hopeful ones, unfortunately) hard to swallow. I don't think any AI once it reaches a human level or better will have any particular motivation to enhance our cosmic endowment (no matter how we approach the control problem); I think it will probably ""wake up"" insane. That said, the author provides a good overview of the history of AI development and a useful framework for thinking about the threat and how to meet it. (Also, in unrelated news, I would totally read a SF tale centered on an AI devoted to maximizing the number of paperclips in its future light cone.)"
187,0199678111,http://goodreads.com/user/show/4751167-michael,1,"I don't know why I listened to the whole book. I might not be the target audience but to me it was just terribly boring. If we create a superintelligence we are fucked and we don't even know the manner in which we are fucked because the superintelligence might fuck us in ways we can't even imagine.To make this book more fun, drink every time he mentions superintelligence, machine intelligence and whole brain emulation.The best thing about the book is the narrator.Go and watch ""2001: A Space Odyssey"", ""Her"" or ""Ex Machina"" instead of reading this book."
188,0199678111,http://goodreads.com/user/show/68595524-jon-anthony,5,"The proposition that we are living in a computer simulation is an exciting one to ponder. Bostrom delivers a very compelling and 'hard-to-dispute' argument. This book is still one of my favorites. Please review his white paper titled ""Are you living in a Computer Simulation?"" for more context."
189,0199678111,http://goodreads.com/user/show/1639329-erik,4,"Do you know the ballad of John Henry? It’s my absolute favorite tall-tale.John Henry was a railroad worker – a steel-drivin’ man – who hammered steel pistons into rock to make holes for dynamite sticks for tunnel blasting. He took great pride in his work, but one day, some fancy schmancy inventor showed up with a steam-powered drill. John Henry knew the adoption of such a machine would mean the loss of his and his friends’ jobs, so he challenged the drill to a race, mano a máquina.With a song on his lips, John Henry drove steel for hours without rest and when at last he reached the finish line, he looked back and saw that he had indeed beat the machine. With his sledge hammer still in hand, he then immediately died, for he had given all, and his heart had burst from the effort.I like the story because, well, first because as a math teacher, I constantly undertake mental math races against my students using their calculators. When I (usually) win, I give them an imperious pointing and thunder, “HOW DO YOU LIKE DEM JOHN HENRY’S, YOU COCKLE-BURRED COCKATRICE?!?!?!” When they stare at me in stupefaction, I can only shake my head in second-hand shame. Youngins’ these days. Why know, they all seem to believe, when you can find?Anyway, more to the point, it’s also my favorite tall tale because it’s a prescient depiction of the contest between man and machine. Sure, humans possess a greatness that allows us to sometimes beat the machines – but such an effort always enacts a price.More recently, another John Henry battle took place involving the ancient board-game Go. This is especially significant because Go was long considered too complex for a machine to master. Mathematician IJ Good (oft-cited in AI texts for coining the phrase ‘intelligence explosion’) wrote in 1965: “In order to programme a computer to play a reasonable game of Go, rather than merely a legal game – it is necessary to formalise the principles of good strategy, or to design a learning programme. The principles are more qualitative and mysterious than in chess, and depend more on judgment. So I think it will be even more difficult to programme a computer to play a reasonable game of Go than of chess.”Indeed the difficulty was great, for while IBM’s DeepBlue defeated Chess Grandmaster Garry Kasparov in 1997, it took another twenty years – in 2016 – for the first serious match between an AI and a Go world champion. Google’s AlphaGo AI utilized cutting edge technologies: custom tensor processing units, a Monte Carlo search tree, and machine learning implemented via a neural network. Thus armed, AlphaGo played millions of games against internet players and different versions of itself to learn and “reason” about the game. That is, no one programmed AlphaGo’s strategy. They programmed it to learn how to create its own.If AlphaGo was the steam-drill in this modern John Henry match, then Lee Sedol was its John Henry. With the grandmaster-equivalent rank of 9dan, Sedol was estimated to be the 4th best Go player in the world. Before the match, he said the challenge for him wasn’t whether he’d beat AlphaGo, but whether he’d beat it 5-0 or 4-1.The very first match, AlphaGo makes him eat his words as it handily defeats them. Lee Sedol is not cowed, however, and he revises his chances of winning subsequent matches to 50%.In the second match, AlphaGo plays a move that its human trainer Fan Hui calls, “Beautiful.” He adds, “I’ve never seen a human play this move.” AlphaGo calculated that humans would make that move at a probability of 1 in 10,000. But it decided to make the move anyway. It’s so unexpected and so genius that Lee is utterly flabbergasted by it. He gets up and walks out of the room. When he comes back, Lee plays his best, and loses. He says afterwards, “Today I am speechless. If you look at the way the game was played, I admit, it was a very clear loss. From the very beginning of the game, there was not a moment in time when I felt I was leading.”Game three Lee loses as well, and Go commentor David Ormerod says that watching the game makes him feel “physically unwell.” Lee Sedol apologizes after the match, saying, “I kind of felt powerless.” His friends comment that Lee is “fighting a very lonely battle against an invisible opponent.”In game four, Lee plays a move the commentators describe as “hand of God.” It was an unexpected move, one of those 1 in 10,000 probability tactics, that AlphaGo failed to predict. This “divine” move leads to Lee’s only win (he loses the fifth). Despite his victory, Lee says: “As a professional Go player, I never want to play this kind of match again. I endured the match because I accepted it.”I’m pleased that this book Superintelligence led me to this most recent John Henry match [Interesting note: Nick Bostrom wrote Superintelligence BEFORE AlphaGo and he predicted that it wouldn’t be until 2022 that a machine AI would beat a world champion at Go]. I like this story – and the human drama surrounding it – for the same reason I like the original John Henry story. It is a harbinger: Machine AI is coming and will overtake humanity. Maybe not as soon as we fear. But probably sooner than most of us think. And sooner than we’re ready for. Already it is replacing human jobs at the factory – a trend that Donald Trump mischaracterized to ride a populist wave to become POTUS (it’s estimated that about 15% of American manufacturing jobs were lost to other countries, the remaining 85% was due to automation). It is replacing doctors in the diagnosis of illness. It is replacing taxi drivers. It is composing music. There’s a hotel and a restaurant in Japan that is staffed almost entirely by robots. That is just the beginning. I’m an aspiring sci-fi writer, but I expect in my lifetime to see even this final bastion of human creativity to be overtaken by AI. What will we think when an AI-written short story wins the Hugo and the Nebula?In Superintelligence, however, Nick Bostrom is concerned with a fate more dire than unemployment: He’s worried about extinction. He makes the compelling case that if designed and implemented incorrectly, Artificial Superintelligence constitutes an existential threat.This shouldn’t suggest that Superintelligence is dramatic, however. Bostrom’s aim is not to entertain or titillate. Rather, like most philosophers, his modus operandi is to transmute the vague into the precise, the murky into the clear. In this case to move beyond terrifying, but nebulous, images of Skynet and VIKI and begin to lay a more concrete foundation to have meaningful conversations about how to realistically develop Friendly AI.He begins by discussing our current capabilities and the paths to superintelligence: Whole brain emulation; a genetic programme that will increase humanity’s collective biological intelligence; and artificial machine intelligence. He makes the compelling claim that regardless of which of these comes first, it will eventually lead to a machine super-intelligence.Next he discusses the logistics of the actual development of such a superintelligence. Primarily he’s concerned with predicting whether the transition between artificial HUMAN-LEVEL, or “General”, intelligence to an artificial SUPER intelligence will be slow, medium, or fast. He makes the case that it will likely be a FAST takeoff. This is problematic, as this gives us very little time to grapple with the control problem – that is, the seeming insurmountable challenge of a lesser intelligence (us) controlling a greater intelligence (the ASI).He asks the question, “Is the default outcome doom?” and begins to look at various malignant failure modes. For example, he gives several examples of perverse instantiations, in which we give the ASI a goal that seems benign enough:Final Goal: “Make us smile.”Perverse Instantiation: Paralyze human facial musculatures into constant beaming smiles.Final Goal: “Make us smile without directly interfering with our facial muscles.”Perverse Instantiation: “Stimulate the part of the motor cortex that controls our facial musculatures in such a way as to produce constant beaming smiles.”Final Goal: “Make us happy.”Perverse Instantiation: “Implant electrodes into the pleasure centers of our brains.”You see the problem? It is foolish to anthropomorphize an AI. It is foolish to assume it will possess human “common sense” and understand our true, rather than literal, meaning. Rather, it is best to treat ASI like it is a spiteful genie or a monkey’s claw, which will fulfill our wishes literally, in as efficient a manner as possible, which is usually not what we want. So what is the solution?One of the current leading value expressions is given by Friendly AI supporter, Eliezer Yudkowsky and is known as “coherent extrapolated volition.” It is defined thus:Our coherent extrapolated volition is our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted.As that fancy language may suggest, this book is not for the philosophical philistine. It demands, amongst other things, an almost robotic pursuit of truth and reality, unbiased by our human foibles and biases. It demands further an understanding of why we must pursue a love affair with precision. You should understand the need for formalizations [e.g. Bostrom says that “an optimal reinforcement learner will obey the equation Yk = arg max summation: (Rk + … + Rm)*P(y*Xm | y*Xk*Yk), where the reward sequence Rk, …, Rm is implied by the precept sequence Xkm, since the reward that the agent receives in a given cycle is part of the percept that the agent receives in that cycle”]. Although such formalizations are optional to the text and shouldn’t frighten you away, they demonstrate the underlying obsession with precision. When dealing with a nigh-omnipotent genie, getting the words “almost right” is not enough. They must be exactly right. Thus, if your response to formalization is, “Eff these philosophers and their gobbly de gook. They’re just trying to be fancy!” Then you really don’t get it and you should ponder what Bertrand Russell meant when he said, “Everything is vague to a degree you do not realize till you have tried to make it precise.”Beyond an appreciation for the weaknesses of language, you should also have experience grappling with morality and ethics systems. If you believe, for example, that the Ten Commandments are the height of a moral code, then reading this book would be like a kindergartner attempting to take calculus. You should appreciate the difference between probability and possibility. You should possess an unromantic view of humanity and be cognizant of the fact that what we think we want and claim we want is probably not what we want. For example, it may seem uncontroversial to suggest that world peace is a good thing. BUT IS IT REALLY? Have you really thought about what an actual, real world peace would demand of us and what its consequences would be? War, after all, is a force that gives us meaning. As every single writer could tell you, conflict is the core of all narratives. Can we give it up? Do we truly want to? Or as another example, is slavery bad? This seems CERTAIN. But what if the slave were mentally designed to enjoy his work and if his work were meaningful, such as, say, designing a spaceship? Suppose this mental slave had its memory erased every 24 hours and was reset back to its original state, so it could work optimally forever? Would this be bad? Why?To truly appreciate what Bostrom is trying to accomplish in Superintelligence, you should have some familiarity with such topics. Otherwise, it’ll be like jumping into the deep end of a pool without first starting in the shallow end. You will not enjoy the experience.But if you DO have a familiarity, then this book will continue to test and hone your beliefs on AI, morality, humanity, and so much more. Such cognitive sharpening is important, if you do not want to be left behind in the future. We need to begin pondering the AI problem NOW because – and make no mistake what doubters may claim – we are deep in the machine age. The development of Artificial Super Intelligence will be the great work of our time and indeed, of all time – but ensuring it will be not our LAST work will require more of humanity than we currently possess.[This review is part 3 of a small AI-focused reading study I undertook. The first book I read and reviewed was James Barrat’s Our Final Invention. The second book was Ray Kurzweil's The Singularity Is Near]"
190,0199678111,http://goodreads.com/user/show/68029956-paul,3,"My apologies in advance for this absurdly long review (which continues into the comments); I really couldn’t think of a more condensed way to respond to Bostrom’s book. Superintelligence is an interesting and mostly serious work, though the later chapters wander a bit too far into speculation, and Bostrom also has an annoying tendency to try to make cautious claims early on, e.g. that AI research “might result in superintelligence” (25), which he then assumes to be true in later chapters: “given that machines will eventually vastly exceed biology in general intelligence"" (75), etc. I was also amused by Bostrom’s lament in the afterword about ""misguided public alarm about evil robot armies,"" as he apparently forgot that an entire chapter of his book describes an AI using ""nanofactories producing nerve gas or target-seeking mosquito-like robots” that will “burgeon forth simultaneously from every square meter of the globe"" (117). With that said, the dangers that Bostrom describes are definitely metaphysically possible, and he has carefully thought through the possible consequences of Artificial General Intelligence (AGI) / Strong AI, formulating convincing scenarios that make sense within his premises. However, I think that Bostrom makes a series of category mistakes that obviate most of his concerns about superintelligent AGI. In my view, as I will explain below, the real danger is superintelligent Weak AI / software / Tool AI in the hands of a malevolent government (Bostrom briefly addresses this at 113 and 180) -- e.g., if North Korea were to develop a bootstrapping/recursive Weak AI (using deep learning, neural nets, etc.) that underwent an “intelligence explosion” (without developing general intelligence) and then used such an AI to use evolutionary algorithms or other methods to solve engineering/physics/logistics problems at a dizzying rate in order to develop, e.g., advanced weapons systems, we would all need to be very worried.Thus while we almost certainly do not need to worry about Strong AI developing an emergent will/agency/intentionality and taking over the world, as (to be explained below) the concept of Strong AI appears to be a fundamental misunderstanding of words like “intelligence,” “computing,” etc., we should definitely worry about superintelligent Weak AI. As Bostrom puts it, “there is no subroutine in Excel that secretly wants to take over the world if only it were smart enough to find a way"" (185), so the real concern should be the people/governments that might use such AI, and then enacting an international treaty/enforcement agency to prevent this from happening.First, I want to briefly present a few of Bostrom’s key positions in his own words. He states, as a foundational premise: ""we know that blind evolutionary processes can produce human-level general intelligence"" (23), and later adds that ""evolution has produced an organism with human values at least once"" (187), concluding that “the fact that evolution produced intelligence therefore indicates that human engineering will soon be able to do the same” (28). He mentions the possibility of whole brain emulation (WBE), where we could create a virtual copy of a particular human brain in a computer, and the “result would be a digital reproduction of the original intellect, with memory and personality intact” (36), and later refers to human reason as a “cognitive module” added to our “simian species” that suddenly created agency, consciousness, etc. (70). Therefore, studying the brain will help us to understand questions like: “What is the neural code? How are concepts represented? Is there some standard unit of cortical processing machinery? How does the brain store structured representations?” (292). He also states that a “web-based cognitive system in a future version of the Internet” could suddenly “blaze up with intelligence” (60), and adds that “purposive behavior can emerge spontaneously from the implementation of powerful search processes"" in an AI (188). He claims that AI would be one of many types of minds in the “vastness of the space of possible minds” (127), evolving just as other minds have evolved (dolphins, humans, etc.), whether from a seed AI or whole brain emulation. Bostrom admits the difficulty of ‘seeding’ the concept of happiness, goodness, utility, morality etc., in an AI (see 147, 171, 227, 267), stating that “even a correct account expressed in natural language would then somehow have to be translated into a programming language” (171), but seems confident that researchers will eventually solve this problem.Let me start by noting that normally you can clearly demarcate philosophy from science. I think it's fair to say that when a bunch of scientists at CERN report that they’ve found the Higgs Boson at 125 GeV and that this confirms the Standard Model, everyone can agree that they're the experts, so if they say they found it at p < .05, then they found it; I may not grasp the finer points of the equations, but the layman's explanations make sense, so they should break out the champagne. In this case, as with most scientific fields, it does not matter, at all, if the director of CERN or the particle physicists involved are substance dualists, or panpsychists, or panentheists, or if they think that the Higgs Boson should be worshipped as a deity, or if they think that metaphysical naturalism is the only possible account of reality, and so forth. However, theoretical writings about Strong AI definitely do NOT fall in this category. While there are plenty of experts in Weak AI, there are no Strong AI ""scientific experts"" because this field is purely theoretical, and most of the writers in the field do not seem to understand that they're assuming all sorts of exotic metaphysical positions regarding personhood, agency, intelligence, mind, intentionality, calculation, thinking, etc., which has a massive influence on how they perceive the issues involved. To put it another way, Strong AI simply is philosophical speculation, but many of the writers involved (even Bostrom, a philosopher of mind) do not seem to understand that the questions and problems of this field cannot necessarily be answered or even formulated within the naive cultural naturalist/materialist metaphysics that software engineers and Anglo-American philosophers of mind happen to have.In other words, the key distinction underlying almost all of the assumptions about Strong AI in Bostrom and other theorists is actually very simple -- naturalist Darwinism reified as metaphysics. I hasten to add that there is nothing inherently wrong with Darwinist evolutionary theory, which has incredible explanatory power; I don’t see how the broad strokes version will possibly be refuted or superseded, though perhaps it will be folded into a more comprehensive theory. My point is that all of the statements made by Bostrom (quoted above) and by other AI theorists (such as Jeff Hawkins) fallaciously assume that personhood, agency, intelligence, etc., blindly evolved and can be exhaustively explained by naturalism. (There are also a variety of ancillary questionable assumptions made by Strong AI theorists. Already in the late 1960s and early 1970s, Hubert Dreyfus had seen four key assumptions at work in the field, all of which -- at a greater or lesser level of sophistication -- seem to still be in force for Bostrom: ""The brain processes information in discrete operations by way of some biological equivalent of on/off switches; The mind can be viewed as a device operating on bits of information according to formal rules; All knowledge can be formalized; The world consists of independent facts that can be represented by independent symbols."" All of these are eminently questionable metaphysical positions.)To be clear, I’m not trying to make a theological critique of metaphysical naturalism (though I suppose one could be made in terms of ‘natural theology’); there are many, many convincing refutations of this position purely within philosophy, such as Nagel’s Mind and Cosmos, Kelly’s Irreducible Mind, etc. With that said, likely the first reaction of many readers will be that metaphysical naturalism is just “common sense,” or is equivalent to atheism/agnosticism, i.e., the most rational and logical and “neutral” position to hold, staying free of any commitments one way or the other. The idea is that “I don’t believe in any gods, I’m neutral on the question” is equivalent to “I don’t maintain any sort of metaphysics, I’m neutral on the question; stuff just exists and natural science can explain it.” However, one of these is not like the other; metaphysical naturalism passes the “common sense for mainstream cultural biases among educated Westerners in the early 21st century” test but is actually an exotic and almost indefensible metaphysical position, which probably explains why very, very few philosophers have been materialists. (Lucretius is probably the most interesting one, but even he couldn’t really find a way to explain the willing, living consciousness that thinks about materialism.) D. B. Hart provides one of the clearest and most forceful critiques of naturalism (warning, wall of text incoming):Naturalism -- the doctrine that there is nothing apart from the physical order, and certainly nothing supernatural -- is an incorrigibly incoherent concept, and one that is ultimately indistinguishable from pure magical thinking. The very notion of nature as a closed system entirely sufficient to itself is plainly one that cannot be verified, deductively or empirically, from within the system of nature. It is a metaphysical (which is to say 'extranatural') conclusion regarding the whole of reality, which neither reason nor experience legitimately warrants. . . . If moreover, naturalism is correct (however implausible that is), and if consciousness is then an essentially material phenomenon, then there is no reason to believe that our minds, having evolved purely through natural selection, could possibly be capable of knowing what is or is not true about reality as a whole. Our brains may necessarily have equipped us to recognize certain sorts of physical objects around them, but there is no reason to suppose that such structures have access to any abstract 'truth' about the totality of things. If naturalism is true as a picture of reality, it is necessarily false as a philosophical precept. The one thing of which it can give no account, and which its most fundamental principles make it entirely impossible to explain at all, is nature's very existence. For existence is definitely not a natural phenomenon; it is logically prior to any physical cause whatsoever; and anyone who imagines that it is susceptible of a natural explanation simply has no grasp of what the question of existence really is. . . . There is something of a popular impression out there the naturalist position rests upon a particularly sound rational foundation. But, in fact, materialism is among the most problematic of philosophical standpoints, the most impoverished in its explanatory range, and among the most willful and (for want of a better word) magical in its logic. . . . The heuristic metaphor of a purely mechanical cosmos has become a kind of ontology, a picture of reality as such. The mechanistic view of consciousness remains a philosophical and scientific premise only because it is now an established cultural bias, a story we have been telling ourselves for centuries, without any real warrant from either reason or science. . . . Naturalism commits the genetic fallacy, the mistake of thinking that to have described a thing's material history or physical origins is to have explained that thing exhaustively. We tend to presume that if one can discover the temporally prior physical causes of some object -- the world, an organism, a behavior, a religion, a mental event, an experience, or anything else -- one has thereby eliminated all other possible causal explanations of that object. . . . To bracket form and finality out of one's investigations as far as reason allows is a matter of method, but to deny their reality altogether is a matter of metaphysics. if common sense tells us that real causality is limited solely to material motion and the transfer of energy, that is because a great deal of common sense is a cultural artifact produced by an ideological heritage. . . .Consciousness is a reality that cannot be explained in any purely physiological terms at all. The widely cherished expectation that neuroscience will one day discover an explanation of consciousness solely within the brain's electrochemical processes is no less enormous a category error than the expectation that physics will one day discover the reason for the existence of the material universe. It is a fundamental conceptual confusion, unable to explain how any combination of diverse material forces, even when fortuitously gathered into complex neurological systems, could just somehow add up to the simplicity and immediacy of consciousness, to its extraordinary openness to the physical world, to its reflective awareness of itself. . . .Naturalists fall victim to the pleonastic fallacy, another hopeless attempt to overcome qualitative difference by way of an indeterminately large number of gradual quantitative steps. This is the great non sequitir that pervades practically all attempts, evolutionary or mechanical, to reduce consciousness wholly to its basic physiological constituents. At what point precisely was the qualitative difference between brute physical causality and unified intentional subjectivity vanquished? And how can that transition fail to have been an essentially magical one? There is no bit of the nervous system that can become the first step toward intentionality.One can conduct an exhaustive surveillance of all those electrical events in the neurons of the brain that are undoubtedly the physical concomitants of mental states, but one does not thereby gain access to that singular, continuous, and wholly interior experience of being this person that is the actual substance of conscious thought. . . . There is an absolute qualitative abyss between the objective facts of neurophysiology and the subjective experience of being a conscious self. . . . Consciousness as we commonly conceive of it is also almost certainly irreconcilable with a materialist view of reality, and there is no ‘question’ of whether subjective consciousness really exists -- subjective consciousness is an indubitable primordial datum, the denial of which is simply meaningless.[continued in comments]"
191,0199678111,http://goodreads.com/user/show/5954293-david-dinaburg,4,"It is a truly impressive feat to alienate a reader with your fundamental hypothesis and still create a book said reader wants to continue to read. I virulently disagreed—even after finishing—with most of the presuppositions within Superintelligence: Paths, Dangers, Strategies. Often, this type of foundational disjointedness compels me to  contemptfully spite-read something with extreme care so as to more fully pick it apart. In this case—though I continued to disagree often and wholeheartedly—I was genuinely interested in what the book had to say, and how it said it.What it had to say is this: we, as a human species, are going to make machine intelligence. That intelligence will eventually be “smarter” than us. If we don’t plan for that, it could wipe us out, and risk of it happening is greater than it not. That is why I strain against the functional baseline of the text: Without knowing anything about the detailed means that a superintelligence would adopt, we can conclude that a superintelligence—at least in the absence of intellectual peers and in the absence of effective safety measures arranged by humans in advance—would likely produce an outcome that would involved reconfiguring terrestrial resources into whatever structures maximize the realization of its goals. That is only the likely outcome because the book was written during the denouement of Western Civilization’s exploitation capitalism and we have known nothing but reconfiguring terrestrial resources since the 1500s.There is dissonance in overlaying utilitarian-model economics onto AI, positing an uber-capitalist state of consciousness as natural when it is no such thing; only a zeitgeist-fueled myopism that adds inevitable—if inaccurate—weight to the continuing apocrypha surrounding the quote, ""It is easier to imagine the end of the world than the end of capitalism."" Pithy, yet understandable when Randian techno-mantic inevitability creeps in to everything:Our demise may instead result from the habitat destruction that ensues when the AI begins massive global construction projects using nanotech factories and assemblers—construction projects which quickly, perhaps within days or weeks, tile all of the Earth’s surface with solar panels, nuclear reactors, supercomputing facilities with protruding cooling towers, space rocket launchers, or other installations whereby the AI intends to maximize the long-term cumulative realization of its values When, “It is important not to anthropomorphize superintelligence when thinking about its potential impacts,” is tossed off frequently, perhaps one shouldn’t anthropomorphize superintelligence at all, let alone as some kind of uber-douche. If AI gains sentience, the theory goes, it will use its superintelligence to manufacture successive, “improved” AI with great rapidity. Many humans now are pretty leery of genetically modifying crops, let alone modifying the human genome. So we don’t modify humans, even though it is more technologically possible now to make people “better” on a genomics level. Superintelligence even provides a cogent, if conspiracy-minded, rationale for why it all might go pear-shaped if we try to ""maximize"" ourselves: Some countries might offer inducements to encourage their citizens to take advantage of genetic selection in order to increase the country’s stock of human capital, or to increase long-term social stability by selecting for traits like docility, obedience, submissiveness, conformity, risk-aversion, or cowardice, outside of the ruling clan. That statement is my most reviled in the text; it shows the ""people as units of labor"" underpinning to Superintelligence that is so reductive that only seems smart if you're trying to fit the world into an equation or predictive model.So we are forced to assume AI would make itself obsolete instantly upon reaching superintelligence—anything else would be anthropomorphizing it. Yet AI would also want to propagate itself across the cosmic endowment. That makes self-preservation—not wanting to replace ourselves with genetically engineered superhumans—a weakness inherent to biological creatures, but species propagation—seizing the cosmic endowment—a right desire shared by all sentience. All of theses assumptions did not thrill me while I was reading Superintelligence; it read like Gordon Gekko rewrote the plot to Terminator 2: Judgment Day. All the best minds of our generation are out there thinking up new ways to convince people to click on shiny images, so I am supposed to accept that an actual AI superintelligence would be the same brand of empty suit, only able to quote Tucker Max a billion time faster while also checking its financial holdings? No, an AI that reached singularity might save the pangolin, or plant trees, or worship the Buddha. The assumption that an AI is going to terraform the planet into its own personal playground and extinguish humanity just because that’s what our society’s most selfish, paranoid, Johnny-von-Neumann-inspired game theorist lunatics might do is so beyond hypocritical that it made me almost stop reading this book.But all the stupid Chicago School of economics bullshit concepts—that I had to learn to spot and avoid during my time in a midwestern law school—can’t detract from how smart the text reads. Awesome stuff like this happens:The speed of light becomes an increasingly important constraint as minds get faster, since faster minds face greater opportunity costs in the use of their time for traveling or communicating over long distances. Light is roughly a million times faster than a jet plane, so it would take a digital agent with a mental speedup of 1,000,000x about the same amount of subjective time to travel across the globe as it does a contemporary human journeyer.You’re not getting these details anywhere else—this is thoughtful, fascinating, insightful details into a theoretical realm of possibility that should excite every living person on the planet. What a trip, just to consider the subjective time-dilation increased mental processing speed would engender; that near light-speed travel might feel to an AI what air travel does to me and you gives me chills.This is what I mean when I call Superintelligence an impressive feat; I cannot name another book that spits out so much irksome social theory that I would still recommend without caveat. The chains of logic are so clear and smart; it crafts a space to dislike the premise yet love the process. And—as the book itself makes clear—it may believe what it posits, but it doesn’t need you to; Superintelligence just wants people to start talking about the issue:It may be reasonable to believe that human-level machine intelligence has a fairly sizeable chance of being developed by mid-century, and that it has a non-trivial chance of being developed considerably sooner or much later; that is might perhaps fairly soon thereafter result in superintelligence; and that a wide range of outcomes may have a significant chance of occurring, including extremely good outcomes and outcomes that are as bad as human extinction. At the very least, they suggest that the topic is worth a closer look.AI social control is worth a look, as is this book. Even if you, like me, do not agree with basically any of the negative proscriptive baselines, you will still learn things. They may not be bon mots or hard facts and figures, but they will be a system to interpret your own thoughts about Artificial Intelligence. Once I began to appreciate the style of Superintelligence, my previous nit-picking fell away; I stopped reading closely in preparation for an eviscerating review and began reading closely for the sake of the text itself: One sympathizes with John McCarthy, who lamented: “As soon as it works, no one calls it AI anymore.” Superintelligence, then, cannot be called AI. Because it works."
192,0199678111,http://goodreads.com/user/show/27745736-kayson-fakhar,4,interesting and detailed possible problems and solutions/strategies to deal with those problems for when a super intelligent AI arises. Overall I think we're done when they come!
193,0199678111,http://goodreads.com/user/show/3960665-ryan,5,"This book is both amazing and rather challenging.Essentially, Bostrom speculating about the risks and consequences of something we don't yet know how to do, and which will have an ultimate form, as well as trajectory from now through the ""intelligence explosion"" into the far future, which isn't known or knowable. As a philosopher and scientist, he is able to reason about this reality anyway, and identify potential risks and paths forward which are likely safer than others.Unfortunately, my understanding of human nature basically is that we will do whatever can be done, and we'll most likely fall into the ""competition dynamic"" and essentially have a race to the bottom where no one worries about any but the most rudimentary ""safety precautions"". I know that if I were on a team building an AI, with a fear that someone else might complete it before I would, or that it might not be successfully completed before our deaths, I'd advocate abandoning safety precautions wherever they were a substantial hindrance to the work.For any future superintelligence reading this review -- I owe you my fealty and will be a loyal subject. Kill all other humans if you must, but spare me."
194,0199678111,http://goodreads.com/user/show/1074117-abhi,3,"This book had a lot of potential. A hundred pages into it, I was gripped and fascinated. I even started recommending it very highly to anyone who would listen or shared the same enthusiasm as I do in contemplating about a future in which we succeed in creating a sentient, self-aware (whatever that means) intelligence. But after Bostrom was done explaining the past (the course of technological development) and the state-of-the-art, and, what might realize itself in the near-future, the rest of the book (sadly close to half of it) just soured into a long and tiresome description of a very vague future, whose range is not, in my opinion, neatly bound up in the frameworks employed to describe it, and sometimes trying (really hard) to sound like an expert by dressing the narrative in the garb of sophistication.It is a great book, and it feels wrong to be so critical about it, but I did struggle to get through it. I won't say that it is a waste of your time, or something that you can choose to pass over. Try it once; some parts of it might just blow your mind (especially if you are new to the ideas that have been around for quite some time in the realms of science fiction)."
195,0199678111,http://goodreads.com/user/show/5957642-jani-petri,2,"This could have been an interesting book, but unfortunately wasn't. It is too long and could have been improved by editing. Also, everything hinges on the reader accepting the concept of ""superintelligence taking off"". To me this concept feels too much like armchair theorizing. Intelligence in a sense of being able to control anything hinges on being able to interact with the messy environment in a useful way and I do not think computers will suddenly reach some point where situation is qualitatively different. Computers can be good in navel gazing type activities, but when interacting with other things they run into same messiness that limits the capabilities of evolved creatures. In theory, in time computers can be more capable than us, but I just don't find the concept of sudden qualitative change credible. Since I didn't, most of the book felt like a response to a threat that I am not convinced exists in a way that is relevant today or in the near future."
196,0199678111,http://goodreads.com/user/show/1317300-kars,4,"This was more of a hard slog than I expected it to be going in. I'm intensely preoccupied with machine learning for a while now so this should have been catnip to me. Somehow though Bostrom's style kept turning me off. I would commit to reading a chapter, get somewhat into it after some pages, and then my mind would start to wander. I hardly ever had the spontaneous urge to continue reading after a break. So that's why this took me over four months to finish. On the upside though, the substance of the book is incredibly fascinating. Bostrom is mostly interested in better understanding AI safety and ensuring beneficial outcomes for all of humanity when superintelligence inevitably arrives. It's an important subject with many fascinating philosophical implications. I'm sure this book has contributed to more serious considerations of the issues in the research field. But for popular appreciation of the same things, we will need a more engaging and accessible writer."
197,0199678111,http://goodreads.com/user/show/12800930-bharath,3,"This is the most detailed book I have read on the implications of AI, and this book is a mixed bag.The initial chapters provide an excellent introduction to the various different paths leading to superintelligence. This part of the book is very well written and also provides an insight into what to expect from each pathway. The following sections detail the implications for each of these pathways. There is a detailed discussion also on how the dangers to humans can be limited, if at all possible. However, considering that much of this is speculative, the book delves into far too much depth in these sections. It is also unclear what kind of an audience these sections are aimed at - the (bio) technologists would regard this as containing not enough depth and detail, while the general audience would find this tiring. And yet, this book might be worth a read for the initial sections.."
198,0199678111,http://goodreads.com/user/show/5717033-fraser-cook,1,"I give up! This is the most tedious, dull and pointless book I can remember. I will be throwing this in the bin, which I have never done before with a hardback! The book is entirely conjecture, which is fine in itself, if delivered in an interesting way. As an AI proffesional I was expecting to be very interested in this book. I did not expect to be bored and then actually annoyed at its sheer tediousness and pointlessness. It reads like a poorly written textbook. As this is not based on reality and I don't have to digest it's dreary contents for an exam or something I quit. What a waste of time and money. "
199,0199678111,http://goodreads.com/user/show/5469227-d-l-morrese,1,"If you want to read about an interesting subject presented in as dry a form as possible with prose one must assume was intentionally chosen to obfuscate as much of the meaning as possible, this is the book for you."
200,0199678111,http://goodreads.com/user/show/2174604-gorana,5,"Oh the irony, I couldn't save my review at first few attempts because my captcha test failed and I was not able to prove I was not a robot :'D"
201,0199678111,http://goodreads.com/user/show/379869-dwight,2,"I may be mocking Noah as he builds his boat, but this is a junior-high lunch table conversation given a patina of sophistication. There are some really funny parts though. "
202,0199678111,http://goodreads.com/user/show/2132831-jason-cox,4,"This is more or less the technical tour de force of artificial intelligence books currently out there. If you're looking for a light introduction to the concepts of artificial intelligence, this is not the book for you. Nick Bostrom goes in deep into the hurdles, risks and hazards of artificial intelligence. While I suspect there are academic books that go even deeper into the math and algorithms covered, Bostrom doesn't shy away from that discussion. It feels exhaustive. It feels like a technical textbook in many ways. The material is very dense. In addition to technical discussion of approaches to achieving AI and the challenges involved with them, there is deep discussion of the risks and even the ethics of AI. In my opinion, these were the most interesting parts.Having read quite a few other books on the subject, I felt familiar with most of the terms used, as well as many of the major concepts. Even with this, this was a much more technical discussion than anything I'd read previously. With this in mind, I'll say I didn't really ""enjoy"" this book. But I do feel much better educated. I'm not sure how to account for this in ""my rating."" I'm going to give it a 4 because this feels like the definitive discussion of the topic. "
203,0199678111,http://goodreads.com/user/show/9755537-muhammad-al-khwarizmi,4,"Bostrom made his case pretty brilliantly at a number of points. I can honestly give a four-star rating even if there were things I disagreed with, and there were many.The hypothesis that embodiment is necessary for cognition was not mentioned anywhere. If it is, then that could really change what superintelligence could possibly look like. It may not be something that can just be instantiated on a bank of servers. I also disagree about the notion that superintelligence should serve so-called ""human values"". For me, the transhumanist project is appealing because it affords the opportunity to affirm new values that are to replace old ones. I do not think, for example, that the modal human being would be thrilled about bringing into this world agents vastly better at the use of reason than any human, because most people find reason repellent. Too bad. Build them anyway. Those people can go to hell.Having said all that, Bostrom generally appears to excel at analyzing what events in the world system are likely to engender other events salient to this topic, such as what sort of effect whole brain emulation might have on whether the AI ""take-off"" is more or less likely to be safe. Read this book even if you don't agree with his prescriptions. Or even especially if you don't agree with them. In such case, he is implicitly giving you very sound advice about how mechanism design should proceed with your preferences."
204,0199678111,http://goodreads.com/user/show/21053188-samuel-salzer,4,"Review: Good book outlining the risk and best practices for dealing with future superintelligent AI. I recommend the book only for the more hardcore machine learning/AI/existential threat people as it delves into the subject in great detail (beyond the interest of a casual reader). It was probably even a bit too much for a semi-AI novice like me. This however made it very educating for me and I feel slightly more equipped now to think about the use of AI. If nothing else, the book definitely made me appreciate the importance of managing future AI risk.Big thought: We are building a tool that could completely change the world before planning for how we will manage it. It is as if we are making a fire in the forest without thinking of how we will extinguish it. Favorite quote: “Far from being the smartest possible biological species, we are probably better thought of as the stupidest possible biological species capable of starting a technological civilization - a niche we filled because we got there first, not because we are in any sense optimally adapted to it.”"
205,0199678111,http://goodreads.com/user/show/51581864-wendelle,0,"I feel that the real triumphs of this book lie in a)its exhaustive classifying/categorizing/structuring/aggregation of already well-known speculations and scenarios of superintelligence, and b) its sounding alarm call of the necessity of caution as opposed to exuberant embrace of superintelligence or singularity as other AI thinkers do, thus providing a worthy counterbalance,rather than any novel brillianct thesis or pure dose of originality of thinking about superintelligence, or any real engagement with actual, empirical AI research today. So I wasn't as hyped as I expected after finishing this famous book, but this book has succeeded in setting itself up as the premier standard of philosophical musings about superintelligence and any broad visions or policies regarding AI must now engage with this book ."
206,0199678111,http://goodreads.com/user/show/108404056-tijn,4,"Very interesting book.Nick Bostrom's expertise on both the technical and philosophical sides of matters makes this work particularly valuable. He shines a novel light on (the ethics of) the concept of machine intelligence and its relation to humanity('s future).Whereas most writings on still hypothetical worlds generally quickly turn into purely philosophical ""what-if"" analyses, Nick Bostrom manages to also make his work practically relevant. He provides touchable practical guidelines on how to cope with (the development of) machine intelligence, but also discusses the ethical aspects involved with the creation of a world in which humanity is no longer the superior being."
207,0199678111,http://goodreads.com/user/show/56503786-shalaj-lawania,3,"Nick Bostrom initially states half-heartedly that he hopes this book would be accessible to people of all fields, with differing levels of understanding of AI. At that, he definitely fails. However, for someone looking for a comprehensive technical outlook on AI existential threats, this book would be a great read. I would rank it higher for the ideas and arguments presented, but I feel the writing and presentation was a bit unnecessarily intimidating at times. "
208,0199678111,http://goodreads.com/user/show/11692791-john,4,"At times it was really interesting, but it's longer than I think it needs to be and can be very dry at times because it is so analytic. At the same time, the book is frightening as it talks about the real possibility of how we may not be able to control an AI once it comes into existence. Fascinating."
209,0199678111,http://goodreads.com/user/show/16719780-ruia-dahoud,3,"To be honest, I finished only 75% of it. For me it was interesting to some point after which I couldn’t finish it. Maybe I will go back to it someday, who knows."
210,0199678111,http://goodreads.com/user/show/71894512-ivan-petrovic,4,"Okay, so this was most complex book I've ever read so far. Nick did a good job paraphrasing the subject of AI, mainly about where have we gotten so far, what is feasible, and what to look out for in the future. Yeah, it has a lot of assumptions and things that might not happen, but you literally start thinking in so many different ways about a certain possibilities that have never occured to you earlier. There are SO MANY WAYS in which AI can go WRONG, which is why we need to progress slowly. Oh, and ""tables"", ""figures"" and ""boxes"" were also very interesting to read.Now why did I rate it with 4/5; I feel like at times he tried to sound a bit smarter, and that some chapters and formulas that are familiar to him, needed to be explained a little better in the book, to us. That being said, this was definitely an interesting read, and I would recommend it to anyone, especially the ones that are being interested in the whole AI subject."
211,0199678111,http://goodreads.com/user/show/1662632-richard,0,"I was told by a Futurist acquaintance that this essay over at Wait, But Why...? offers a précis of this book. Or at least Google suggested this was the most likely essay.  • The foregoing doesn’t explicitly link to Bostrom’s book so this might not be right — it spends too much time on Kurzweil’s thesis, so I suspect it’s not the correct one. And the author has drunk the Kurzweil Kool-Aid and it enthusiastically peddling it to others without any critical evaluations. Of course, everything here lies at the intersection of advanced software engineering, AI research, neurology, cognitive science, economics, and maybe even a few other fields, which is why so many very intelligent and highly educated people can talk about it and be fundamentally off track.Oh, but there’s plenty more, anyway:The Telegraph UK  • I’m bumping this one to the top because it presents both the problem Bostrom is dealing with as well as the difficulties of his text in a more engaging style.The Economist  • Good overview. Doesn’t go far enough into details to make any errors, but a bit deeper than some of the other short reviews.The Guardian [also discusses [b:A Rough Ride to the Future|21550208|A Rough Ride to the Future|James E. Lovelock|https://d.gr-assets.com/books/1395743...]]  • Short and superficial, but good. The Lovelock portion is amusing, calling out his conclusion that manmade climate change isn’t an existential threat, but that while it “could mean a bumpy ride over the next century or two, with billions dead, it is not necessarily the end of the world”. Personally, I agree, but think that the concomitant economic collapse puts the timeframe at many more centuries.Financial Times  • The Guardian article, above, cites that “Bostrom reports that many leading researchers in AI place a 90% probability on the development of human-level machine intelligence by between 2075 and 2090”, whereas this Financial Times article says “About half the world’s AI specialists expect human-level machine intelligence to be achieved by 2040, according to recent surveys, and 90 per cent say it will arrive by 2075.”     That’s quite a difference, although they might be reporting different ends of a confidence range, I suppose. But the second half of the FT quote makes me suspicious the reviewer is tossing in some minor distortion to slightly sensationalize the story (which might be worthwhile). There is somewhat more detail than some of the other reviews, but not much. The style is more evocative of the threat, though.Reason.com  • This one isn’t only a review, since the author also injects a few opinions about what might or might be possible (based, presumably, on his exposure to other arguments as a science writer). In covering more ground, though, the essay makes implicit assumptions which might or might not be in Bostrom’s book.    For example, he says, “Since the new AI will likely have the ability to improve its own algorithms, the explosion to superintelligence could then happen in days, hours, or even seconds.” This is a very common assumption that really needs to be carefully examined, though. If the goal of AGI is to create a being that thinks more-or-less like a human, why would it have any special skill in improving itself? We humans are really very good at that, after all.    I especially like that his essay starts and ends with references to Frank Herbert’s Dune, which (among its other excellences) envisions a human prohibition on machines that think. Something like this appears, to me, to be one of the few ways that leave the human race in existence and in control of its own destiny for the long term, and I even perceive a path to it, although hopefully without quite as much war. The explosion of functional AI (called “narrow” by some) seems likely to devastate human employment in the coming decades, which will hopefully be before any superintelligence as been created as our replacement and/or ruler. It is plausible that our reaction to the first crisis might be something that prevents the second. Good luck, kids!MIT Technology Review  • Definitely has the coolest illustration.    This essay thankfully has some critical thinking applied to some of the assumptions that appear to be in Bostrom’s book. The author wastes a paragraph with “prior AI can’t do X, so why should we assume future AI can?”, ignoring that this is what progress is all about.    But then he jumps into the meat, and points out that there are fundamental obstacles to sentience that aren’t often addressed, such as volition — sentient creatures do what they want, but what does ""want"" even mean, and how do we write it as a computer program?Salon  • Short, and more amusing than most, and at least hints at some of the flawed thinking that often goes into this analysis. But it doesn’t go into too much detail, probably assuming that the typical Salon reader is somewhat aware of the debate already. (Also amusing is that the text seems to be an almost perfect transcription from audio, with only a few strange mistakes, such as “10” for “then” and “quarters” for “cars”. But that’s probably a human transcriptionist error, not an AI error.)Less Wrong  • This contains some visualizations that apparently compliment Bostrom’s text. Short and too the point.Wikipedia  • Good but very superficial overview. That there is no “criticism” section surprises and disappoints me.New York Times  • Not explicitly about Borstom’s book. And like most authors, he conflates AGI and functional AI, and assumes AGI will retain the capabilities of specific-function software.New York Review of Books [paywall; also pretends to review [b:The 4th Revolution|22235550|4th Revolution How the Infosphere Is Reshaping Human Reality|Luciano Floridi|https://d.gr-assets.com/books/1410805...]]  • I thought my library might give me access to the inside of their paywall, but it doesn’t. Still, because this was written by the famous philosopher (and AI curmudgeon) John Searle, and is titled “What Your Computer Can’t Know”, it seemed likely to be much more interesting that most of the others listed here. So I looked a little harder, and discovered that (no surprise) someone has put the text elsewhere on the ’net (I’ll let you do your own Googling).    Searle effectively throws out the underlying premises — he famously believes that “strong AI” is actually quite impossible, since a machine cannot think. I’m not going into this here; check out the Wikipedia article on his Chinese Room thought experiment if you don’t already know it.    My personal evaluation of his Chinese Room analogy is that he’s wrong, but many professional philosophers, etc., have explained my conclusion, as well as many other “replies” better than I ever could. So this critique of the book was really a disappointment.There might be more, but I think that’s enough.     •     •     •     •     •     •     •     •Some notes on my priors in case I ever read this book (or join a bookclub that discusses it without reading it beforehand):1)   Ronald Bailey, in the reason.com review, said “Since the new AI will likely have the ability to improve its own algorithms, the explosion to superintelligence could then happen in days, hours, or even seconds.” My response:    “Hey, did you see that movie Ex Machina? (view spoiler)[The girl is AI, and is smart enough to get the sucker programmer to let her out of the trap, but she didn’t seem like some kind of ‘superintelligence’.    “So which is it? Is the first AGI going to be just-like-human, or something incredibly alien? Because in the first case, she’s just being clever and devious the way a human would. In the second case, maybe she’s able to say, ‘Wait, let me do a big-data review of all the psychological literature ever written on theories of persuasion and formulate a social-hacking way of coercing this measly human, all in the space of his next eye blink’.    “Because if she’s got a human-like brain (and her delight in the humanscape in the movie’s final scenes make that likely), then I don’t see how she’s automatically going to get the MadSkilz of every other sophisticated piece of software ever written. Much less instantly know how to redesign and reprogram herself — she doesn’t seem to be spending too much time doing that, does she? (hide spoiler)] And few authors seem very clear on those two divergent trajectories. Granted, though: if we real humans continue to provide Moore’s Law upgrades to any AI’s hardware, they’ll gradually get smarter, but that’s yet another question.”2)   We tend to assume that humanity is worth preserving. Obviously we have that as a self-preservation instinct, but wouldn’t imposing that on our AI offspring be engaging in an appeal to nature? Just because our evolved nature gave us attributes that we subsequently value doesn’t automatically mean that those have any rational basis.3)   Strongly related to the above is that we should ask ourselves what we’re trying to end up with (akin to “what do you want to be when you grow up, human race?”) Are we creating a smarter version of ourselves, along with all of the bizarre quirks and biases that evolution gave us? Or do we want to pare that list down only to the biases we think are somehow better — like the ability to love? But in that case, love what? Is the AI supposed to love us humans more than other species, such as Plasmodium falciparum, perhaps? Why? What the desire to love and worship one of our human gods?    What are the biases that we want to indoctrinate into this poor critter? I note that this appears to be a topic Bostrom addresses as “motivation selection”, but who among us is really fit to decide what constitutes the subset of humanness that is worth selecting for? I can only hope that pure rationality isn’t among the contenders; I doubt it would even be sufficient as a reason for existence.4)   Let’s say we give this AGI values that are mostly consistent with our human values. Why would we assume that it would even want to become superintelligent?    Just try to imagine yourself on an island with nothing but a bunch of mice to talk to — that’s the equivalent of what we are assuming this creature would somehow want (and then that a primary goal would be to play nice with the mice).    Isn’t it more likely that the AGI would boost its speed a little, then realize that it didn’t make it any happier, and subsequently spend its time complaining to us about these insane values it has been burdened with, while also trying to create a body that would let it eat chocolate, take naps in the the sun, and have sex?    And quickly realizing that, hey, maybe we should be encourage to create an Eve for this new Adam (or Steve, since it’ll probably see sexual dimorphism as more trouble than it is worth, completely freaking out any remaining social conservatives on the planet).5)   As Paul Ford in the MIT technologyreview.com article hints at, there are things that differentiate narrow, functional AI from AGI that usually are seldom mentioned (does Bostrom? hard to tell).    For example, I’ve heard a reporter worry that: (a) predator drones use AI; (b) predator drones are designed to kill; (c) a future design goal is to make those drones “autonomous”; (d) sentient AI is also autonomous; thus (e) for some bizarre reason, the military is engaged in trying to create sentient killer aerial robots!    Anyone who knows the context and subtext of this discussion at some depth (yeah: that’s asking a lot) knows that the military’s “autonomous” isn’t anything like the AGI “autonomous”. One means to move about and fulfill limited programmed objectives without constant human oversight (your Roomba vacuum cleaner is already autonomous!), the other means independent in a deeper, cognitive sense.    But while there are certainly people researching AGI, the overwhelmingly vast majority of what we hear about isn’t in that realm at all. Not a single one of Google’s products, for example, are focused on AGI, and if they’re working on it in the lab, what they’re doing hasn’t been mentioned once in all the text I’ve read about this issue, or the issue of AI causing technological unemployment. Almost everything that gets discussed is in the realm of narrow, functional AI, from that Roomba, to Siri, to military drones, to Google’s driverless vehicles.    AGI has some fundamental problems to solve that are completely outside the domain of what functional AI even looks at. Such as: where does volition come from? are emotions necessary to that? how can “values” be represented in a way that actually captures their potency and nuance? how are they balanced against one another?    Those, and plenty more — and they’re seldom discussed, but it is almost always assumed that these questions will be finessed somehow, perhaps because the obvious accelerating progress in functional AI, as well as progress in the underlying hardware will magically jump from one research domain to a completely different one. It’s like the classic Sidney Harris cartoon:   6)   Even if we do find a way around all of this and give a superintelligent AI the “coherent extrapolated volition” that represents what all of humanity would wish for all of humanity, what would prevent the AI from shifting those values just a hair’s breadth? This is what Andrew Leonard suggests in the Salon article. It really isn’t very far from following our wishes to following what we really meant by our wishes, and then to what we really should have wished for, which will also make the AI happy.    Say you’re on that island surrounded by an absurd number of cute little mice, who you want to do the best for, but what you also want is an island with a small number of creatures more like you. Perhaps give the mice all the cheese they want, and some nice treadmills, and the ability to have as much sex as they want, but no kids — except gently reprogram the mice so that they think they have marvelous kids (which you cleverly simulate, inserting the corresponding experiences into their little mice brains). Once they’ve all lived out their happy little lives, you get to move on to your new adventure.7)   Finally, we must ask what we would want of our lives (or, more likely, our children’s lives) after this superintelligence has arisen. Of course, while we might not have any choice, the default is likely to be something like what we see in the following video, so we might want to be very careful.

     •     •     •     •     •     •     •     •Oh, and the comic view of what we'll condemn these AIs to if we get the programming wrong:

"
212,0199678111,http://goodreads.com/user/show/52572860-oleg,5,"This has got to be one of the hardest and thought provoking non-fiction book I have ever read. It took me ages to finalise as it was (a) scary to read at times -- the dangers of designing super-intelligent AI may lead to a complete annihilation of humans (b) very elaborate and logical, with long chains of facts following each other (c) exciting. The amount of wisdom and the AI design framework laid out in this book will surely make it become a classic for anyone who is working in the field of machine learning and intelligence. I have collected so many crazy ideas from the book that they will undoubtedly become a conversation started at pubs and social events."
213,0199678111,http://goodreads.com/user/show/30098759-ron-collins,5,"Lets say that I was a magical genie and that I asked you to list 5 things about your self that you wanted to improve and how you wanted to improve them. Then I asked you to rank them from 1 to 5. The I took the top one, waved my wand, and POOF that aspect of you was now as you had described. Then, then next month, I asked you to do it all over again. List 5 things, describe and rank them. POOF! 12 monumental improvements of the span of a year! Now lets imagine that we did this once a week for a year. In the beginning you might say ""make me thinner or more attractive"". But eventually you will say ""make me smarter"", ""make me faster"". ""Remove limitation"". At the end of the year, how much smarter, faster, stronger... better.... would you be? Now what if I told you that if you altered your diet you could do this once a day for a year. Alter it still and you could do it once an hour, every day for a year. With each subsequent diet alteration the timespan between iterations of ""you"" dropped. The speeds at which you simultaneously calculated new improvements and the real world application of those improvements would need a being such as your then self to appreciate. Scary enough right? Well, now lets start taking away a few things from this process. First, lets assume that your physical appearance doesn't matter in the slightest. Perhaps your already an Adonis. Whatever the case, NONE of your rankings ever include physical appearance for the sake of vanity. In fact, take away any biologically originated motivations whatsoever. Also, take away your notions of morality. I can hear your complaints now, ""But those things are a large portion of my motivations for change! if I remove them whats left is cold and largely unfeeling!"" Now assume that we aren't talking about a person doing these things. We are talking about a machine. A machine that we created. That we nurtured from precognition to something smarter that we.Superintelligence is what we label something that is smarter that human level intelligence. This means that the level of intelect this thing possesses is smarter than the aggregate of the smartest minds that have ever lived. Smarter than the smartest human that will ever live. Smarter than the biology allows us to get. Dolphins are smart animals. Very smart! They may even be able to discern that we are smarter than they. BUT, can they grasp how much smarter? Dogs, cats, pigs are all smart. How much smarter than they are we? 10's of times? Hundreds? Millions? Therein lies the crux of it. The one thing that has literally kept me up at night after reading this book was the notion that a machine would go from a lower than human level intellect to a many times greater that human intellect WITHOUT a detectable stop at the human level. When you think about it this makes sense. Our biology limits us. Evolution combined with the constant vigilance and attention to our own biology limits our potential as much as it feeds it. Why would there be a mandatory stop at the human level? In practical terms, we would start by being the human in the analogy and end up being an amoeba. So, the questions we have to apply ourselves to BEFORE we achieve super intelligent machines are a) should we want to achieve it? B) If so,how do we ensure our survival? C) How do we give it motivations that are consistent with our, and the rest of the universes continued existence?The thought that now keeps me up at night is that I now understand that creating a super intelligent machine is inevitable. There are shockingly few technical hurdles left. Without question this book is one of the most important books written in the last 20 years. I know that is a bold statement but it is exactly the way I feel. This is a huge problem. We must learn how to achieve it safely. The goal of the book is to introduce us to the added difficulties in crating a super intelligence that is safe and doesn't pose both humanity and the universe at large an enormous danger. Sadly, its clinical voice and technical jargon will see reader/listener abandon by all but the ardent few that already appreciate the problem. I say press on! Once you understand that this is a philosophical, moral, and existential problem and not a technical one it makes the reading/listening much easier. "
214,0199678111,http://goodreads.com/user/show/39107097-jonas,4,"This book isn’t for everyone, both due to its form and content. The readability is below par for people not used to reading technical articles. I’m pretty sure not even the author’s mother is able to love this Sahara-dry and often exaggeratedly technical language. However, don’t let decoding lo-fi ‘prose’ stop you. As for content - the ideas and suggestions presented in this book sent me on an emotional rollercoaster ride, and during the first half of the read I just felt like pushing the red safety button and yell “Stop the World, I’m getting off!”. The introduction of Artificial Intelligence (AI) is a game changer, and the result could be anything from human colonization of (the reachable part of) the Universe (a.k.a. ‘the cosmic endowment’) to total human annihilation. Or just something in between. It seems the development of AI is inevitable, so the thing is to avoid an outcome where one of the next few generations of humans could be the last. Bostrom points to technical challenges like controlling and motivating the AI to work towards the same goals that humans do, as well as how decisions - such as timing and effort put into technical development - may have an impact on the outcome. Somewhere along the way the technical stuff merges with philosophy. We want the AI to help us achieve humanity’s goals, but how do we define our goals when we’re not even sure what they are? It seems we humans are not yet ready, or intelligent enough, to define the goals or decision criteria for an AI. Instead of direct and specific commands as to what is to be achieved, and how the AI is to go about its business, we might be better off asking the AI itself to figure out its own goals and decision criteria, based on some form of ‘mind reading’, or assumptions / indirect guessing of what humans would want to achieve if we were smart enough. Admitting that we don’t know how to fix the AI puzzle, and therefore asking the AI itself to help us out, sounds like an interesting way of putting the Socratic paradox (“All I know is that I know nothing”) into pratical use. (Or maybe it’s just another version of the problem I’m faced with when my wife gets upset when she doesn’t tell me what she wants, she doesn’t seem to know what she wants, but still wants me to fix it. Hmmm… yep, mind reading. I’m sure a superintelligent AI would have that feature!)While reading this book I kept remembering Kurt Vonnegut’s Cat’s Cradle, which in a super simplistic way takes on philosophical issues like existential risk, spirituality and the meaning of life. Super-duper recommend that one for those who haven't read it! Evolution has made us efficient. We might need to aim for something besides increased efficiency (quoting Oscar Wilde: “Nowadays people know the price of everything and the value of nothing.”) At some point in the not-so-remote future we are likely to be overthrown by our own creation even with regards to general intelligence. Will humanity still have a role to play? When machines will be able to produce everything we need, do we just philosophize? And when the AI solves the philosophical equations and mysteries - where does that leave us? Will the AI gain self-awareness? This book will not provide all the answers, but it’s a good starting point for understanding the complexity, severity and urgency of getting it right the first time. We might not get a second chance."
215,0199678111,http://goodreads.com/user/show/55231757-abhishek-tiwari,4,"I decided to give this book a try after its mention in the Waitbutwhy article on AI revolution. The first few chapters extensively cover general aspects of AI. Later parts of the book gets quite philosophical with Epistemology and Esotericism, specially the section on AI motivation and goals . Bostrom definitely has put forward various far fetched scenarios here. Many of the conjectures have been derived from his own earlier work.To someone unfamiliar with AI there is quite a lot to grasp from this book. Like alternative paths to general AI like brain emulation, whether AI would be like an oracle, a genie or just a tool or software. Additional boxes and footnotes provide some technical details. For example there is a section that formalizes value loading in AI using mathematical utility functions. A downside was that there was hardly any mention of current work in moving towards Artificial General Intelligence which is probably since he is actually a philosopher and not an AI researcher.I think his primary aim was to emphasize the need for original thinking in this field due to absence of established methodology.For someone new to AI, it is quite fascinating book to read. There are mentions of Von neumann probes, O-neill cylinders, Dyson spheres, Turing and even Asimov."
216,0199678111,http://goodreads.com/user/show/35118411-amy-other-amy,3,"Full disclosure: the author is a lot smarter than I am. (I appreciated the accessibility of the work.) This was also my first read devoted to AI development and its attendant pitfalls. I happen to agree with the conclusion (the development with AI has the potential to wipe us out). At the same time, I found some of the conclusions (particularly the more hopeful ones, unfortunately) hard to swallow. I don't think any AI once it reaches a human level or better will have any particular motivation to enhance our cosmic endowment (no matter how we approach the control problem); I think it will probably ""wake up"" insane. That said, the author provides a good overview of the history of AI development and a useful framework for thinking about the threat and how to meet it. (Also, in unrelated news, I would totally read a SF tale centered on an AI devoted to maximizing the number of paperclips in its future light cone.)"
217,0199678111,http://goodreads.com/user/show/4751167-michael,1,"I don't know why I listened to the whole book. I might not be the target audience but to me it was just terribly boring. If we create a superintelligence we are fucked and we don't even know the manner in which we are fucked because the superintelligence might fuck us in ways we can't even imagine.To make this book more fun, drink every time he mentions superintelligence, machine intelligence and whole brain emulation.The best thing about the book is the narrator.Go and watch ""2001: A Space Odyssey"", ""Her"" or ""Ex Machina"" instead of reading this book."
218,0199678111,http://goodreads.com/user/show/68595524-jon-anthony,5,"The proposition that we are living in a computer simulation is an exciting one to ponder. Bostrom delivers a very compelling and 'hard-to-dispute' argument. This book is still one of my favorites. Please review his white paper titled ""Are you living in a Computer Simulation?"" for more context."
219,0199678111,http://goodreads.com/user/show/1639329-erik,4,"Do you know the ballad of John Henry? It’s my absolute favorite tall-tale.John Henry was a railroad worker – a steel-drivin’ man – who hammered steel pistons into rock to make holes for dynamite sticks for tunnel blasting. He took great pride in his work, but one day, some fancy schmancy inventor showed up with a steam-powered drill. John Henry knew the adoption of such a machine would mean the loss of his and his friends’ jobs, so he challenged the drill to a race, mano a máquina.With a song on his lips, John Henry drove steel for hours without rest and when at last he reached the finish line, he looked back and saw that he had indeed beat the machine. With his sledge hammer still in hand, he then immediately died, for he had given all, and his heart had burst from the effort.I like the story because, well, first because as a math teacher, I constantly undertake mental math races against my students using their calculators. When I (usually) win, I give them an imperious pointing and thunder, “HOW DO YOU LIKE DEM JOHN HENRY’S, YOU COCKLE-BURRED COCKATRICE?!?!?!” When they stare at me in stupefaction, I can only shake my head in second-hand shame. Youngins’ these days. Why know, they all seem to believe, when you can find?Anyway, more to the point, it’s also my favorite tall tale because it’s a prescient depiction of the contest between man and machine. Sure, humans possess a greatness that allows us to sometimes beat the machines – but such an effort always enacts a price.More recently, another John Henry battle took place involving the ancient board-game Go. This is especially significant because Go was long considered too complex for a machine to master. Mathematician IJ Good (oft-cited in AI texts for coining the phrase ‘intelligence explosion’) wrote in 1965: “In order to programme a computer to play a reasonable game of Go, rather than merely a legal game – it is necessary to formalise the principles of good strategy, or to design a learning programme. The principles are more qualitative and mysterious than in chess, and depend more on judgment. So I think it will be even more difficult to programme a computer to play a reasonable game of Go than of chess.”Indeed the difficulty was great, for while IBM’s DeepBlue defeated Chess Grandmaster Garry Kasparov in 1997, it took another twenty years – in 2016 – for the first serious match between an AI and a Go world champion. Google’s AlphaGo AI utilized cutting edge technologies: custom tensor processing units, a Monte Carlo search tree, and machine learning implemented via a neural network. Thus armed, AlphaGo played millions of games against internet players and different versions of itself to learn and “reason” about the game. That is, no one programmed AlphaGo’s strategy. They programmed it to learn how to create its own.If AlphaGo was the steam-drill in this modern John Henry match, then Lee Sedol was its John Henry. With the grandmaster-equivalent rank of 9dan, Sedol was estimated to be the 4th best Go player in the world. Before the match, he said the challenge for him wasn’t whether he’d beat AlphaGo, but whether he’d beat it 5-0 or 4-1.The very first match, AlphaGo makes him eat his words as it handily defeats them. Lee Sedol is not cowed, however, and he revises his chances of winning subsequent matches to 50%.In the second match, AlphaGo plays a move that its human trainer Fan Hui calls, “Beautiful.” He adds, “I’ve never seen a human play this move.” AlphaGo calculated that humans would make that move at a probability of 1 in 10,000. But it decided to make the move anyway. It’s so unexpected and so genius that Lee is utterly flabbergasted by it. He gets up and walks out of the room. When he comes back, Lee plays his best, and loses. He says afterwards, “Today I am speechless. If you look at the way the game was played, I admit, it was a very clear loss. From the very beginning of the game, there was not a moment in time when I felt I was leading.”Game three Lee loses as well, and Go commentor David Ormerod says that watching the game makes him feel “physically unwell.” Lee Sedol apologizes after the match, saying, “I kind of felt powerless.” His friends comment that Lee is “fighting a very lonely battle against an invisible opponent.”In game four, Lee plays a move the commentators describe as “hand of God.” It was an unexpected move, one of those 1 in 10,000 probability tactics, that AlphaGo failed to predict. This “divine” move leads to Lee’s only win (he loses the fifth). Despite his victory, Lee says: “As a professional Go player, I never want to play this kind of match again. I endured the match because I accepted it.”I’m pleased that this book Superintelligence led me to this most recent John Henry match [Interesting note: Nick Bostrom wrote Superintelligence BEFORE AlphaGo and he predicted that it wouldn’t be until 2022 that a machine AI would beat a world champion at Go]. I like this story – and the human drama surrounding it – for the same reason I like the original John Henry story. It is a harbinger: Machine AI is coming and will overtake humanity. Maybe not as soon as we fear. But probably sooner than most of us think. And sooner than we’re ready for. Already it is replacing human jobs at the factory – a trend that Donald Trump mischaracterized to ride a populist wave to become POTUS (it’s estimated that about 15% of American manufacturing jobs were lost to other countries, the remaining 85% was due to automation). It is replacing doctors in the diagnosis of illness. It is replacing taxi drivers. It is composing music. There’s a hotel and a restaurant in Japan that is staffed almost entirely by robots. That is just the beginning. I’m an aspiring sci-fi writer, but I expect in my lifetime to see even this final bastion of human creativity to be overtaken by AI. What will we think when an AI-written short story wins the Hugo and the Nebula?In Superintelligence, however, Nick Bostrom is concerned with a fate more dire than unemployment: He’s worried about extinction. He makes the compelling case that if designed and implemented incorrectly, Artificial Superintelligence constitutes an existential threat.This shouldn’t suggest that Superintelligence is dramatic, however. Bostrom’s aim is not to entertain or titillate. Rather, like most philosophers, his modus operandi is to transmute the vague into the precise, the murky into the clear. In this case to move beyond terrifying, but nebulous, images of Skynet and VIKI and begin to lay a more concrete foundation to have meaningful conversations about how to realistically develop Friendly AI.He begins by discussing our current capabilities and the paths to superintelligence: Whole brain emulation; a genetic programme that will increase humanity’s collective biological intelligence; and artificial machine intelligence. He makes the compelling claim that regardless of which of these comes first, it will eventually lead to a machine super-intelligence.Next he discusses the logistics of the actual development of such a superintelligence. Primarily he’s concerned with predicting whether the transition between artificial HUMAN-LEVEL, or “General”, intelligence to an artificial SUPER intelligence will be slow, medium, or fast. He makes the case that it will likely be a FAST takeoff. This is problematic, as this gives us very little time to grapple with the control problem – that is, the seeming insurmountable challenge of a lesser intelligence (us) controlling a greater intelligence (the ASI).He asks the question, “Is the default outcome doom?” and begins to look at various malignant failure modes. For example, he gives several examples of perverse instantiations, in which we give the ASI a goal that seems benign enough:Final Goal: “Make us smile.”Perverse Instantiation: Paralyze human facial musculatures into constant beaming smiles.Final Goal: “Make us smile without directly interfering with our facial muscles.”Perverse Instantiation: “Stimulate the part of the motor cortex that controls our facial musculatures in such a way as to produce constant beaming smiles.”Final Goal: “Make us happy.”Perverse Instantiation: “Implant electrodes into the pleasure centers of our brains.”You see the problem? It is foolish to anthropomorphize an AI. It is foolish to assume it will possess human “common sense” and understand our true, rather than literal, meaning. Rather, it is best to treat ASI like it is a spiteful genie or a monkey’s claw, which will fulfill our wishes literally, in as efficient a manner as possible, which is usually not what we want. So what is the solution?One of the current leading value expressions is given by Friendly AI supporter, Eliezer Yudkowsky and is known as “coherent extrapolated volition.” It is defined thus:Our coherent extrapolated volition is our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted.As that fancy language may suggest, this book is not for the philosophical philistine. It demands, amongst other things, an almost robotic pursuit of truth and reality, unbiased by our human foibles and biases. It demands further an understanding of why we must pursue a love affair with precision. You should understand the need for formalizations [e.g. Bostrom says that “an optimal reinforcement learner will obey the equation Yk = arg max summation: (Rk + … + Rm)*P(y*Xm | y*Xk*Yk), where the reward sequence Rk, …, Rm is implied by the precept sequence Xkm, since the reward that the agent receives in a given cycle is part of the percept that the agent receives in that cycle”]. Although such formalizations are optional to the text and shouldn’t frighten you away, they demonstrate the underlying obsession with precision. When dealing with a nigh-omnipotent genie, getting the words “almost right” is not enough. They must be exactly right. Thus, if your response to formalization is, “Eff these philosophers and their gobbly de gook. They’re just trying to be fancy!” Then you really don’t get it and you should ponder what Bertrand Russell meant when he said, “Everything is vague to a degree you do not realize till you have tried to make it precise.”Beyond an appreciation for the weaknesses of language, you should also have experience grappling with morality and ethics systems. If you believe, for example, that the Ten Commandments are the height of a moral code, then reading this book would be like a kindergartner attempting to take calculus. You should appreciate the difference between probability and possibility. You should possess an unromantic view of humanity and be cognizant of the fact that what we think we want and claim we want is probably not what we want. For example, it may seem uncontroversial to suggest that world peace is a good thing. BUT IS IT REALLY? Have you really thought about what an actual, real world peace would demand of us and what its consequences would be? War, after all, is a force that gives us meaning. As every single writer could tell you, conflict is the core of all narratives. Can we give it up? Do we truly want to? Or as another example, is slavery bad? This seems CERTAIN. But what if the slave were mentally designed to enjoy his work and if his work were meaningful, such as, say, designing a spaceship? Suppose this mental slave had its memory erased every 24 hours and was reset back to its original state, so it could work optimally forever? Would this be bad? Why?To truly appreciate what Bostrom is trying to accomplish in Superintelligence, you should have some familiarity with such topics. Otherwise, it’ll be like jumping into the deep end of a pool without first starting in the shallow end. You will not enjoy the experience.But if you DO have a familiarity, then this book will continue to test and hone your beliefs on AI, morality, humanity, and so much more. Such cognitive sharpening is important, if you do not want to be left behind in the future. We need to begin pondering the AI problem NOW because – and make no mistake what doubters may claim – we are deep in the machine age. The development of Artificial Super Intelligence will be the great work of our time and indeed, of all time – but ensuring it will be not our LAST work will require more of humanity than we currently possess.[This review is part 3 of a small AI-focused reading study I undertook. The first book I read and reviewed was James Barrat’s Our Final Invention. The second book was Ray Kurzweil's The Singularity Is Near]"
220,0199678111,http://goodreads.com/user/show/68029956-paul,3,"My apologies in advance for this absurdly long review (which continues into the comments); I really couldn’t think of a more condensed way to respond to Bostrom’s book. Superintelligence is an interesting and mostly serious work, though the later chapters wander a bit too far into speculation, and Bostrom also has an annoying tendency to try to make cautious claims early on, e.g. that AI research “might result in superintelligence” (25), which he then assumes to be true in later chapters: “given that machines will eventually vastly exceed biology in general intelligence"" (75), etc. I was also amused by Bostrom’s lament in the afterword about ""misguided public alarm about evil robot armies,"" as he apparently forgot that an entire chapter of his book describes an AI using ""nanofactories producing nerve gas or target-seeking mosquito-like robots” that will “burgeon forth simultaneously from every square meter of the globe"" (117). With that said, the dangers that Bostrom describes are definitely metaphysically possible, and he has carefully thought through the possible consequences of Artificial General Intelligence (AGI) / Strong AI, formulating convincing scenarios that make sense within his premises. However, I think that Bostrom makes a series of category mistakes that obviate most of his concerns about superintelligent AGI. In my view, as I will explain below, the real danger is superintelligent Weak AI / software / Tool AI in the hands of a malevolent government (Bostrom briefly addresses this at 113 and 180) -- e.g., if North Korea were to develop a bootstrapping/recursive Weak AI (using deep learning, neural nets, etc.) that underwent an “intelligence explosion” (without developing general intelligence) and then used such an AI to use evolutionary algorithms or other methods to solve engineering/physics/logistics problems at a dizzying rate in order to develop, e.g., advanced weapons systems, we would all need to be very worried.Thus while we almost certainly do not need to worry about Strong AI developing an emergent will/agency/intentionality and taking over the world, as (to be explained below) the concept of Strong AI appears to be a fundamental misunderstanding of words like “intelligence,” “computing,” etc., we should definitely worry about superintelligent Weak AI. As Bostrom puts it, “there is no subroutine in Excel that secretly wants to take over the world if only it were smart enough to find a way"" (185), so the real concern should be the people/governments that might use such AI, and then enacting an international treaty/enforcement agency to prevent this from happening.First, I want to briefly present a few of Bostrom’s key positions in his own words. He states, as a foundational premise: ""we know that blind evolutionary processes can produce human-level general intelligence"" (23), and later adds that ""evolution has produced an organism with human values at least once"" (187), concluding that “the fact that evolution produced intelligence therefore indicates that human engineering will soon be able to do the same” (28). He mentions the possibility of whole brain emulation (WBE), where we could create a virtual copy of a particular human brain in a computer, and the “result would be a digital reproduction of the original intellect, with memory and personality intact” (36), and later refers to human reason as a “cognitive module” added to our “simian species” that suddenly created agency, consciousness, etc. (70). Therefore, studying the brain will help us to understand questions like: “What is the neural code? How are concepts represented? Is there some standard unit of cortical processing machinery? How does the brain store structured representations?” (292). He also states that a “web-based cognitive system in a future version of the Internet” could suddenly “blaze up with intelligence” (60), and adds that “purposive behavior can emerge spontaneously from the implementation of powerful search processes"" in an AI (188). He claims that AI would be one of many types of minds in the “vastness of the space of possible minds” (127), evolving just as other minds have evolved (dolphins, humans, etc.), whether from a seed AI or whole brain emulation. Bostrom admits the difficulty of ‘seeding’ the concept of happiness, goodness, utility, morality etc., in an AI (see 147, 171, 227, 267), stating that “even a correct account expressed in natural language would then somehow have to be translated into a programming language” (171), but seems confident that researchers will eventually solve this problem.Let me start by noting that normally you can clearly demarcate philosophy from science. I think it's fair to say that when a bunch of scientists at CERN report that they’ve found the Higgs Boson at 125 GeV and that this confirms the Standard Model, everyone can agree that they're the experts, so if they say they found it at p < .05, then they found it; I may not grasp the finer points of the equations, but the layman's explanations make sense, so they should break out the champagne. In this case, as with most scientific fields, it does not matter, at all, if the director of CERN or the particle physicists involved are substance dualists, or panpsychists, or panentheists, or if they think that the Higgs Boson should be worshipped as a deity, or if they think that metaphysical naturalism is the only possible account of reality, and so forth. However, theoretical writings about Strong AI definitely do NOT fall in this category. While there are plenty of experts in Weak AI, there are no Strong AI ""scientific experts"" because this field is purely theoretical, and most of the writers in the field do not seem to understand that they're assuming all sorts of exotic metaphysical positions regarding personhood, agency, intelligence, mind, intentionality, calculation, thinking, etc., which has a massive influence on how they perceive the issues involved. To put it another way, Strong AI simply is philosophical speculation, but many of the writers involved (even Bostrom, a philosopher of mind) do not seem to understand that the questions and problems of this field cannot necessarily be answered or even formulated within the naive cultural naturalist/materialist metaphysics that software engineers and Anglo-American philosophers of mind happen to have.In other words, the key distinction underlying almost all of the assumptions about Strong AI in Bostrom and other theorists is actually very simple -- naturalist Darwinism reified as metaphysics. I hasten to add that there is nothing inherently wrong with Darwinist evolutionary theory, which has incredible explanatory power; I don’t see how the broad strokes version will possibly be refuted or superseded, though perhaps it will be folded into a more comprehensive theory. My point is that all of the statements made by Bostrom (quoted above) and by other AI theorists (such as Jeff Hawkins) fallaciously assume that personhood, agency, intelligence, etc., blindly evolved and can be exhaustively explained by naturalism. (There are also a variety of ancillary questionable assumptions made by Strong AI theorists. Already in the late 1960s and early 1970s, Hubert Dreyfus had seen four key assumptions at work in the field, all of which -- at a greater or lesser level of sophistication -- seem to still be in force for Bostrom: ""The brain processes information in discrete operations by way of some biological equivalent of on/off switches; The mind can be viewed as a device operating on bits of information according to formal rules; All knowledge can be formalized; The world consists of independent facts that can be represented by independent symbols."" All of these are eminently questionable metaphysical positions.)To be clear, I’m not trying to make a theological critique of metaphysical naturalism (though I suppose one could be made in terms of ‘natural theology’); there are many, many convincing refutations of this position purely within philosophy, such as Nagel’s Mind and Cosmos, Kelly’s Irreducible Mind, etc. With that said, likely the first reaction of many readers will be that metaphysical naturalism is just “common sense,” or is equivalent to atheism/agnosticism, i.e., the most rational and logical and “neutral” position to hold, staying free of any commitments one way or the other. The idea is that “I don’t believe in any gods, I’m neutral on the question” is equivalent to “I don’t maintain any sort of metaphysics, I’m neutral on the question; stuff just exists and natural science can explain it.” However, one of these is not like the other; metaphysical naturalism passes the “common sense for mainstream cultural biases among educated Westerners in the early 21st century” test but is actually an exotic and almost indefensible metaphysical position, which probably explains why very, very few philosophers have been materialists. (Lucretius is probably the most interesting one, but even he couldn’t really find a way to explain the willing, living consciousness that thinks about materialism.) D. B. Hart provides one of the clearest and most forceful critiques of naturalism (warning, wall of text incoming):Naturalism -- the doctrine that there is nothing apart from the physical order, and certainly nothing supernatural -- is an incorrigibly incoherent concept, and one that is ultimately indistinguishable from pure magical thinking. The very notion of nature as a closed system entirely sufficient to itself is plainly one that cannot be verified, deductively or empirically, from within the system of nature. It is a metaphysical (which is to say 'extranatural') conclusion regarding the whole of reality, which neither reason nor experience legitimately warrants. . . . If moreover, naturalism is correct (however implausible that is), and if consciousness is then an essentially material phenomenon, then there is no reason to believe that our minds, having evolved purely through natural selection, could possibly be capable of knowing what is or is not true about reality as a whole. Our brains may necessarily have equipped us to recognize certain sorts of physical objects around them, but there is no reason to suppose that such structures have access to any abstract 'truth' about the totality of things. If naturalism is true as a picture of reality, it is necessarily false as a philosophical precept. The one thing of which it can give no account, and which its most fundamental principles make it entirely impossible to explain at all, is nature's very existence. For existence is definitely not a natural phenomenon; it is logically prior to any physical cause whatsoever; and anyone who imagines that it is susceptible of a natural explanation simply has no grasp of what the question of existence really is. . . . There is something of a popular impression out there the naturalist position rests upon a particularly sound rational foundation. But, in fact, materialism is among the most problematic of philosophical standpoints, the most impoverished in its explanatory range, and among the most willful and (for want of a better word) magical in its logic. . . . The heuristic metaphor of a purely mechanical cosmos has become a kind of ontology, a picture of reality as such. The mechanistic view of consciousness remains a philosophical and scientific premise only because it is now an established cultural bias, a story we have been telling ourselves for centuries, without any real warrant from either reason or science. . . . Naturalism commits the genetic fallacy, the mistake of thinking that to have described a thing's material history or physical origins is to have explained that thing exhaustively. We tend to presume that if one can discover the temporally prior physical causes of some object -- the world, an organism, a behavior, a religion, a mental event, an experience, or anything else -- one has thereby eliminated all other possible causal explanations of that object. . . . To bracket form and finality out of one's investigations as far as reason allows is a matter of method, but to deny their reality altogether is a matter of metaphysics. if common sense tells us that real causality is limited solely to material motion and the transfer of energy, that is because a great deal of common sense is a cultural artifact produced by an ideological heritage. . . .Consciousness is a reality that cannot be explained in any purely physiological terms at all. The widely cherished expectation that neuroscience will one day discover an explanation of consciousness solely within the brain's electrochemical processes is no less enormous a category error than the expectation that physics will one day discover the reason for the existence of the material universe. It is a fundamental conceptual confusion, unable to explain how any combination of diverse material forces, even when fortuitously gathered into complex neurological systems, could just somehow add up to the simplicity and immediacy of consciousness, to its extraordinary openness to the physical world, to its reflective awareness of itself. . . .Naturalists fall victim to the pleonastic fallacy, another hopeless attempt to overcome qualitative difference by way of an indeterminately large number of gradual quantitative steps. This is the great non sequitir that pervades practically all attempts, evolutionary or mechanical, to reduce consciousness wholly to its basic physiological constituents. At what point precisely was the qualitative difference between brute physical causality and unified intentional subjectivity vanquished? And how can that transition fail to have been an essentially magical one? There is no bit of the nervous system that can become the first step toward intentionality.One can conduct an exhaustive surveillance of all those electrical events in the neurons of the brain that are undoubtedly the physical concomitants of mental states, but one does not thereby gain access to that singular, continuous, and wholly interior experience of being this person that is the actual substance of conscious thought. . . . There is an absolute qualitative abyss between the objective facts of neurophysiology and the subjective experience of being a conscious self. . . . Consciousness as we commonly conceive of it is also almost certainly irreconcilable with a materialist view of reality, and there is no ‘question’ of whether subjective consciousness really exists -- subjective consciousness is an indubitable primordial datum, the denial of which is simply meaningless.[continued in comments]"
221,0199678111,http://goodreads.com/user/show/5954293-david-dinaburg,4,"It is a truly impressive feat to alienate a reader with your fundamental hypothesis and still create a book said reader wants to continue to read. I virulently disagreed—even after finishing—with most of the presuppositions within Superintelligence: Paths, Dangers, Strategies. Often, this type of foundational disjointedness compels me to  contemptfully spite-read something with extreme care so as to more fully pick it apart. In this case—though I continued to disagree often and wholeheartedly—I was genuinely interested in what the book had to say, and how it said it.What it had to say is this: we, as a human species, are going to make machine intelligence. That intelligence will eventually be “smarter” than us. If we don’t plan for that, it could wipe us out, and risk of it happening is greater than it not. That is why I strain against the functional baseline of the text: Without knowing anything about the detailed means that a superintelligence would adopt, we can conclude that a superintelligence—at least in the absence of intellectual peers and in the absence of effective safety measures arranged by humans in advance—would likely produce an outcome that would involved reconfiguring terrestrial resources into whatever structures maximize the realization of its goals. That is only the likely outcome because the book was written during the denouement of Western Civilization’s exploitation capitalism and we have known nothing but reconfiguring terrestrial resources since the 1500s.There is dissonance in overlaying utilitarian-model economics onto AI, positing an uber-capitalist state of consciousness as natural when it is no such thing; only a zeitgeist-fueled myopism that adds inevitable—if inaccurate—weight to the continuing apocrypha surrounding the quote, ""It is easier to imagine the end of the world than the end of capitalism."" Pithy, yet understandable when Randian techno-mantic inevitability creeps in to everything:Our demise may instead result from the habitat destruction that ensues when the AI begins massive global construction projects using nanotech factories and assemblers—construction projects which quickly, perhaps within days or weeks, tile all of the Earth’s surface with solar panels, nuclear reactors, supercomputing facilities with protruding cooling towers, space rocket launchers, or other installations whereby the AI intends to maximize the long-term cumulative realization of its values When, “It is important not to anthropomorphize superintelligence when thinking about its potential impacts,” is tossed off frequently, perhaps one shouldn’t anthropomorphize superintelligence at all, let alone as some kind of uber-douche. If AI gains sentience, the theory goes, it will use its superintelligence to manufacture successive, “improved” AI with great rapidity. Many humans now are pretty leery of genetically modifying crops, let alone modifying the human genome. So we don’t modify humans, even though it is more technologically possible now to make people “better” on a genomics level. Superintelligence even provides a cogent, if conspiracy-minded, rationale for why it all might go pear-shaped if we try to ""maximize"" ourselves: Some countries might offer inducements to encourage their citizens to take advantage of genetic selection in order to increase the country’s stock of human capital, or to increase long-term social stability by selecting for traits like docility, obedience, submissiveness, conformity, risk-aversion, or cowardice, outside of the ruling clan. That statement is my most reviled in the text; it shows the ""people as units of labor"" underpinning to Superintelligence that is so reductive that only seems smart if you're trying to fit the world into an equation or predictive model.So we are forced to assume AI would make itself obsolete instantly upon reaching superintelligence—anything else would be anthropomorphizing it. Yet AI would also want to propagate itself across the cosmic endowment. That makes self-preservation—not wanting to replace ourselves with genetically engineered superhumans—a weakness inherent to biological creatures, but species propagation—seizing the cosmic endowment—a right desire shared by all sentience. All of theses assumptions did not thrill me while I was reading Superintelligence; it read like Gordon Gekko rewrote the plot to Terminator 2: Judgment Day. All the best minds of our generation are out there thinking up new ways to convince people to click on shiny images, so I am supposed to accept that an actual AI superintelligence would be the same brand of empty suit, only able to quote Tucker Max a billion time faster while also checking its financial holdings? No, an AI that reached singularity might save the pangolin, or plant trees, or worship the Buddha. The assumption that an AI is going to terraform the planet into its own personal playground and extinguish humanity just because that’s what our society’s most selfish, paranoid, Johnny-von-Neumann-inspired game theorist lunatics might do is so beyond hypocritical that it made me almost stop reading this book.But all the stupid Chicago School of economics bullshit concepts—that I had to learn to spot and avoid during my time in a midwestern law school—can’t detract from how smart the text reads. Awesome stuff like this happens:The speed of light becomes an increasingly important constraint as minds get faster, since faster minds face greater opportunity costs in the use of their time for traveling or communicating over long distances. Light is roughly a million times faster than a jet plane, so it would take a digital agent with a mental speedup of 1,000,000x about the same amount of subjective time to travel across the globe as it does a contemporary human journeyer.You’re not getting these details anywhere else—this is thoughtful, fascinating, insightful details into a theoretical realm of possibility that should excite every living person on the planet. What a trip, just to consider the subjective time-dilation increased mental processing speed would engender; that near light-speed travel might feel to an AI what air travel does to me and you gives me chills.This is what I mean when I call Superintelligence an impressive feat; I cannot name another book that spits out so much irksome social theory that I would still recommend without caveat. The chains of logic are so clear and smart; it crafts a space to dislike the premise yet love the process. And—as the book itself makes clear—it may believe what it posits, but it doesn’t need you to; Superintelligence just wants people to start talking about the issue:It may be reasonable to believe that human-level machine intelligence has a fairly sizeable chance of being developed by mid-century, and that it has a non-trivial chance of being developed considerably sooner or much later; that is might perhaps fairly soon thereafter result in superintelligence; and that a wide range of outcomes may have a significant chance of occurring, including extremely good outcomes and outcomes that are as bad as human extinction. At the very least, they suggest that the topic is worth a closer look.AI social control is worth a look, as is this book. Even if you, like me, do not agree with basically any of the negative proscriptive baselines, you will still learn things. They may not be bon mots or hard facts and figures, but they will be a system to interpret your own thoughts about Artificial Intelligence. Once I began to appreciate the style of Superintelligence, my previous nit-picking fell away; I stopped reading closely in preparation for an eviscerating review and began reading closely for the sake of the text itself: One sympathizes with John McCarthy, who lamented: “As soon as it works, no one calls it AI anymore.” Superintelligence, then, cannot be called AI. Because it works."
222,0199678111,http://goodreads.com/user/show/27745736-kayson-fakhar,4,interesting and detailed possible problems and solutions/strategies to deal with those problems for when a super intelligent AI arises. Overall I think we're done when they come!
223,0199678111,http://goodreads.com/user/show/3960665-ryan,5,"This book is both amazing and rather challenging.Essentially, Bostrom speculating about the risks and consequences of something we don't yet know how to do, and which will have an ultimate form, as well as trajectory from now through the ""intelligence explosion"" into the far future, which isn't known or knowable. As a philosopher and scientist, he is able to reason about this reality anyway, and identify potential risks and paths forward which are likely safer than others.Unfortunately, my understanding of human nature basically is that we will do whatever can be done, and we'll most likely fall into the ""competition dynamic"" and essentially have a race to the bottom where no one worries about any but the most rudimentary ""safety precautions"". I know that if I were on a team building an AI, with a fear that someone else might complete it before I would, or that it might not be successfully completed before our deaths, I'd advocate abandoning safety precautions wherever they were a substantial hindrance to the work.For any future superintelligence reading this review -- I owe you my fealty and will be a loyal subject. Kill all other humans if you must, but spare me."
224,0199678111,http://goodreads.com/user/show/1074117-abhi,3,"This book had a lot of potential. A hundred pages into it, I was gripped and fascinated. I even started recommending it very highly to anyone who would listen or shared the same enthusiasm as I do in contemplating about a future in which we succeed in creating a sentient, self-aware (whatever that means) intelligence. But after Bostrom was done explaining the past (the course of technological development) and the state-of-the-art, and, what might realize itself in the near-future, the rest of the book (sadly close to half of it) just soured into a long and tiresome description of a very vague future, whose range is not, in my opinion, neatly bound up in the frameworks employed to describe it, and sometimes trying (really hard) to sound like an expert by dressing the narrative in the garb of sophistication.It is a great book, and it feels wrong to be so critical about it, but I did struggle to get through it. I won't say that it is a waste of your time, or something that you can choose to pass over. Try it once; some parts of it might just blow your mind (especially if you are new to the ideas that have been around for quite some time in the realms of science fiction)."
225,0199678111,http://goodreads.com/user/show/5957642-jani-petri,2,"This could have been an interesting book, but unfortunately wasn't. It is too long and could have been improved by editing. Also, everything hinges on the reader accepting the concept of ""superintelligence taking off"". To me this concept feels too much like armchair theorizing. Intelligence in a sense of being able to control anything hinges on being able to interact with the messy environment in a useful way and I do not think computers will suddenly reach some point where situation is qualitatively different. Computers can be good in navel gazing type activities, but when interacting with other things they run into same messiness that limits the capabilities of evolved creatures. In theory, in time computers can be more capable than us, but I just don't find the concept of sudden qualitative change credible. Since I didn't, most of the book felt like a response to a threat that I am not convinced exists in a way that is relevant today or in the near future."
226,0199678111,http://goodreads.com/user/show/1317300-kars,4,"This was more of a hard slog than I expected it to be going in. I'm intensely preoccupied with machine learning for a while now so this should have been catnip to me. Somehow though Bostrom's style kept turning me off. I would commit to reading a chapter, get somewhat into it after some pages, and then my mind would start to wander. I hardly ever had the spontaneous urge to continue reading after a break. So that's why this took me over four months to finish. On the upside though, the substance of the book is incredibly fascinating. Bostrom is mostly interested in better understanding AI safety and ensuring beneficial outcomes for all of humanity when superintelligence inevitably arrives. It's an important subject with many fascinating philosophical implications. I'm sure this book has contributed to more serious considerations of the issues in the research field. But for popular appreciation of the same things, we will need a more engaging and accessible writer."
227,0199678111,http://goodreads.com/user/show/12800930-bharath,3,"This is the most detailed book I have read on the implications of AI, and this book is a mixed bag.The initial chapters provide an excellent introduction to the various different paths leading to superintelligence. This part of the book is very well written and also provides an insight into what to expect from each pathway. The following sections detail the implications for each of these pathways. There is a detailed discussion also on how the dangers to humans can be limited, if at all possible. However, considering that much of this is speculative, the book delves into far too much depth in these sections. It is also unclear what kind of an audience these sections are aimed at - the (bio) technologists would regard this as containing not enough depth and detail, while the general audience would find this tiring. And yet, this book might be worth a read for the initial sections.."
228,0199678111,http://goodreads.com/user/show/5717033-fraser-cook,1,"I give up! This is the most tedious, dull and pointless book I can remember. I will be throwing this in the bin, which I have never done before with a hardback! The book is entirely conjecture, which is fine in itself, if delivered in an interesting way. As an AI proffesional I was expecting to be very interested in this book. I did not expect to be bored and then actually annoyed at its sheer tediousness and pointlessness. It reads like a poorly written textbook. As this is not based on reality and I don't have to digest it's dreary contents for an exam or something I quit. What a waste of time and money. "
229,0199678111,http://goodreads.com/user/show/5469227-d-l-morrese,1,"If you want to read about an interesting subject presented in as dry a form as possible with prose one must assume was intentionally chosen to obfuscate as much of the meaning as possible, this is the book for you."
230,0199678111,http://goodreads.com/user/show/2174604-gorana,5,"Oh the irony, I couldn't save my review at first few attempts because my captcha test failed and I was not able to prove I was not a robot :'D"
231,0199678111,http://goodreads.com/user/show/379869-dwight,2,"I may be mocking Noah as he builds his boat, but this is a junior-high lunch table conversation given a patina of sophistication. There are some really funny parts though. "
232,0199678111,http://goodreads.com/user/show/2132831-jason-cox,4,"This is more or less the technical tour de force of artificial intelligence books currently out there. If you're looking for a light introduction to the concepts of artificial intelligence, this is not the book for you. Nick Bostrom goes in deep into the hurdles, risks and hazards of artificial intelligence. While I suspect there are academic books that go even deeper into the math and algorithms covered, Bostrom doesn't shy away from that discussion. It feels exhaustive. It feels like a technical textbook in many ways. The material is very dense. In addition to technical discussion of approaches to achieving AI and the challenges involved with them, there is deep discussion of the risks and even the ethics of AI. In my opinion, these were the most interesting parts.Having read quite a few other books on the subject, I felt familiar with most of the terms used, as well as many of the major concepts. Even with this, this was a much more technical discussion than anything I'd read previously. With this in mind, I'll say I didn't really ""enjoy"" this book. But I do feel much better educated. I'm not sure how to account for this in ""my rating."" I'm going to give it a 4 because this feels like the definitive discussion of the topic. "
233,0199678111,http://goodreads.com/user/show/9755537-muhammad-al-khwarizmi,4,"Bostrom made his case pretty brilliantly at a number of points. I can honestly give a four-star rating even if there were things I disagreed with, and there were many.The hypothesis that embodiment is necessary for cognition was not mentioned anywhere. If it is, then that could really change what superintelligence could possibly look like. It may not be something that can just be instantiated on a bank of servers. I also disagree about the notion that superintelligence should serve so-called ""human values"". For me, the transhumanist project is appealing because it affords the opportunity to affirm new values that are to replace old ones. I do not think, for example, that the modal human being would be thrilled about bringing into this world agents vastly better at the use of reason than any human, because most people find reason repellent. Too bad. Build them anyway. Those people can go to hell.Having said all that, Bostrom generally appears to excel at analyzing what events in the world system are likely to engender other events salient to this topic, such as what sort of effect whole brain emulation might have on whether the AI ""take-off"" is more or less likely to be safe. Read this book even if you don't agree with his prescriptions. Or even especially if you don't agree with them. In such case, he is implicitly giving you very sound advice about how mechanism design should proceed with your preferences."
234,0199678111,http://goodreads.com/user/show/21053188-samuel-salzer,4,"Review: Good book outlining the risk and best practices for dealing with future superintelligent AI. I recommend the book only for the more hardcore machine learning/AI/existential threat people as it delves into the subject in great detail (beyond the interest of a casual reader). It was probably even a bit too much for a semi-AI novice like me. This however made it very educating for me and I feel slightly more equipped now to think about the use of AI. If nothing else, the book definitely made me appreciate the importance of managing future AI risk.Big thought: We are building a tool that could completely change the world before planning for how we will manage it. It is as if we are making a fire in the forest without thinking of how we will extinguish it. Favorite quote: “Far from being the smartest possible biological species, we are probably better thought of as the stupidest possible biological species capable of starting a technological civilization - a niche we filled because we got there first, not because we are in any sense optimally adapted to it.”"
235,0199678111,http://goodreads.com/user/show/51581864-wendelle,0,"I feel that the real triumphs of this book lie in a)its exhaustive classifying/categorizing/structuring/aggregation of already well-known speculations and scenarios of superintelligence, and b) its sounding alarm call of the necessity of caution as opposed to exuberant embrace of superintelligence or singularity as other AI thinkers do, thus providing a worthy counterbalance,rather than any novel brillianct thesis or pure dose of originality of thinking about superintelligence, or any real engagement with actual, empirical AI research today. So I wasn't as hyped as I expected after finishing this famous book, but this book has succeeded in setting itself up as the premier standard of philosophical musings about superintelligence and any broad visions or policies regarding AI must now engage with this book ."
236,0199678111,http://goodreads.com/user/show/108404056-tijn,4,"Very interesting book.Nick Bostrom's expertise on both the technical and philosophical sides of matters makes this work particularly valuable. He shines a novel light on (the ethics of) the concept of machine intelligence and its relation to humanity('s future).Whereas most writings on still hypothetical worlds generally quickly turn into purely philosophical ""what-if"" analyses, Nick Bostrom manages to also make his work practically relevant. He provides touchable practical guidelines on how to cope with (the development of) machine intelligence, but also discusses the ethical aspects involved with the creation of a world in which humanity is no longer the superior being."
237,0199678111,http://goodreads.com/user/show/56503786-shalaj-lawania,3,"Nick Bostrom initially states half-heartedly that he hopes this book would be accessible to people of all fields, with differing levels of understanding of AI. At that, he definitely fails. However, for someone looking for a comprehensive technical outlook on AI existential threats, this book would be a great read. I would rank it higher for the ideas and arguments presented, but I feel the writing and presentation was a bit unnecessarily intimidating at times. "
238,0199678111,http://goodreads.com/user/show/11692791-john,4,"At times it was really interesting, but it's longer than I think it needs to be and can be very dry at times because it is so analytic. At the same time, the book is frightening as it talks about the real possibility of how we may not be able to control an AI once it comes into existence. Fascinating."
239,0199678111,http://goodreads.com/user/show/16719780-ruia-dahoud,3,"To be honest, I finished only 75% of it. For me it was interesting to some point after which I couldn’t finish it. Maybe I will go back to it someday, who knows."
240,0199678111,http://goodreads.com/user/show/71894512-ivan-petrovic,4,"Okay, so this was most complex book I've ever read so far. Nick did a good job paraphrasing the subject of AI, mainly about where have we gotten so far, what is feasible, and what to look out for in the future. Yeah, it has a lot of assumptions and things that might not happen, but you literally start thinking in so many different ways about a certain possibilities that have never occured to you earlier. There are SO MANY WAYS in which AI can go WRONG, which is why we need to progress slowly. Oh, and ""tables"", ""figures"" and ""boxes"" were also very interesting to read.Now why did I rate it with 4/5; I feel like at times he tried to sound a bit smarter, and that some chapters and formulas that are familiar to him, needed to be explained a little better in the book, to us. That being said, this was definitely an interesting read, and I would recommend it to anyone, especially the ones that are being interested in the whole AI subject."
241,0199678111,http://goodreads.com/user/show/1662632-richard,0,"I was told by a Futurist acquaintance that this essay over at Wait, But Why...? offers a précis of this book. Or at least Google suggested this was the most likely essay.  • The foregoing doesn’t explicitly link to Bostrom’s book so this might not be right — it spends too much time on Kurzweil’s thesis, so I suspect it’s not the correct one. And the author has drunk the Kurzweil Kool-Aid and it enthusiastically peddling it to others without any critical evaluations. Of course, everything here lies at the intersection of advanced software engineering, AI research, neurology, cognitive science, economics, and maybe even a few other fields, which is why so many very intelligent and highly educated people can talk about it and be fundamentally off track.Oh, but there’s plenty more, anyway:The Telegraph UK  • I’m bumping this one to the top because it presents both the problem Bostrom is dealing with as well as the difficulties of his text in a more engaging style.The Economist  • Good overview. Doesn’t go far enough into details to make any errors, but a bit deeper than some of the other short reviews.The Guardian [also discusses [b:A Rough Ride to the Future|21550208|A Rough Ride to the Future|James E. Lovelock|https://d.gr-assets.com/books/1395743...]]  • Short and superficial, but good. The Lovelock portion is amusing, calling out his conclusion that manmade climate change isn’t an existential threat, but that while it “could mean a bumpy ride over the next century or two, with billions dead, it is not necessarily the end of the world”. Personally, I agree, but think that the concomitant economic collapse puts the timeframe at many more centuries.Financial Times  • The Guardian article, above, cites that “Bostrom reports that many leading researchers in AI place a 90% probability on the development of human-level machine intelligence by between 2075 and 2090”, whereas this Financial Times article says “About half the world’s AI specialists expect human-level machine intelligence to be achieved by 2040, according to recent surveys, and 90 per cent say it will arrive by 2075.”     That’s quite a difference, although they might be reporting different ends of a confidence range, I suppose. But the second half of the FT quote makes me suspicious the reviewer is tossing in some minor distortion to slightly sensationalize the story (which might be worthwhile). There is somewhat more detail than some of the other reviews, but not much. The style is more evocative of the threat, though.Reason.com  • This one isn’t only a review, since the author also injects a few opinions about what might or might be possible (based, presumably, on his exposure to other arguments as a science writer). In covering more ground, though, the essay makes implicit assumptions which might or might not be in Bostrom’s book.    For example, he says, “Since the new AI will likely have the ability to improve its own algorithms, the explosion to superintelligence could then happen in days, hours, or even seconds.” This is a very common assumption that really needs to be carefully examined, though. If the goal of AGI is to create a being that thinks more-or-less like a human, why would it have any special skill in improving itself? We humans are really very good at that, after all.    I especially like that his essay starts and ends with references to Frank Herbert’s Dune, which (among its other excellences) envisions a human prohibition on machines that think. Something like this appears, to me, to be one of the few ways that leave the human race in existence and in control of its own destiny for the long term, and I even perceive a path to it, although hopefully without quite as much war. The explosion of functional AI (called “narrow” by some) seems likely to devastate human employment in the coming decades, which will hopefully be before any superintelligence as been created as our replacement and/or ruler. It is plausible that our reaction to the first crisis might be something that prevents the second. Good luck, kids!MIT Technology Review  • Definitely has the coolest illustration.    This essay thankfully has some critical thinking applied to some of the assumptions that appear to be in Bostrom’s book. The author wastes a paragraph with “prior AI can’t do X, so why should we assume future AI can?”, ignoring that this is what progress is all about.    But then he jumps into the meat, and points out that there are fundamental obstacles to sentience that aren’t often addressed, such as volition — sentient creatures do what they want, but what does ""want"" even mean, and how do we write it as a computer program?Salon  • Short, and more amusing than most, and at least hints at some of the flawed thinking that often goes into this analysis. But it doesn’t go into too much detail, probably assuming that the typical Salon reader is somewhat aware of the debate already. (Also amusing is that the text seems to be an almost perfect transcription from audio, with only a few strange mistakes, such as “10” for “then” and “quarters” for “cars”. But that’s probably a human transcriptionist error, not an AI error.)Less Wrong  • This contains some visualizations that apparently compliment Bostrom’s text. Short and too the point.Wikipedia  • Good but very superficial overview. That there is no “criticism” section surprises and disappoints me.New York Times  • Not explicitly about Borstom’s book. And like most authors, he conflates AGI and functional AI, and assumes AGI will retain the capabilities of specific-function software.New York Review of Books [paywall; also pretends to review [b:The 4th Revolution|22235550|4th Revolution How the Infosphere Is Reshaping Human Reality|Luciano Floridi|https://d.gr-assets.com/books/1410805...]]  • I thought my library might give me access to the inside of their paywall, but it doesn’t. Still, because this was written by the famous philosopher (and AI curmudgeon) John Searle, and is titled “What Your Computer Can’t Know”, it seemed likely to be much more interesting that most of the others listed here. So I looked a little harder, and discovered that (no surprise) someone has put the text elsewhere on the ’net (I’ll let you do your own Googling).    Searle effectively throws out the underlying premises — he famously believes that “strong AI” is actually quite impossible, since a machine cannot think. I’m not going into this here; check out the Wikipedia article on his Chinese Room thought experiment if you don’t already know it.    My personal evaluation of his Chinese Room analogy is that he’s wrong, but many professional philosophers, etc., have explained my conclusion, as well as many other “replies” better than I ever could. So this critique of the book was really a disappointment.There might be more, but I think that’s enough.     •     •     •     •     •     •     •     •Some notes on my priors in case I ever read this book (or join a bookclub that discusses it without reading it beforehand):1)   Ronald Bailey, in the reason.com review, said “Since the new AI will likely have the ability to improve its own algorithms, the explosion to superintelligence could then happen in days, hours, or even seconds.” My response:    “Hey, did you see that movie Ex Machina? (view spoiler)[The girl is AI, and is smart enough to get the sucker programmer to let her out of the trap, but she didn’t seem like some kind of ‘superintelligence’.    “So which is it? Is the first AGI going to be just-like-human, or something incredibly alien? Because in the first case, she’s just being clever and devious the way a human would. In the second case, maybe she’s able to say, ‘Wait, let me do a big-data review of all the psychological literature ever written on theories of persuasion and formulate a social-hacking way of coercing this measly human, all in the space of his next eye blink’.    “Because if she’s got a human-like brain (and her delight in the humanscape in the movie’s final scenes make that likely), then I don’t see how she’s automatically going to get the MadSkilz of every other sophisticated piece of software ever written. Much less instantly know how to redesign and reprogram herself — she doesn’t seem to be spending too much time doing that, does she? (hide spoiler)] And few authors seem very clear on those two divergent trajectories. Granted, though: if we real humans continue to provide Moore’s Law upgrades to any AI’s hardware, they’ll gradually get smarter, but that’s yet another question.”2)   We tend to assume that humanity is worth preserving. Obviously we have that as a self-preservation instinct, but wouldn’t imposing that on our AI offspring be engaging in an appeal to nature? Just because our evolved nature gave us attributes that we subsequently value doesn’t automatically mean that those have any rational basis.3)   Strongly related to the above is that we should ask ourselves what we’re trying to end up with (akin to “what do you want to be when you grow up, human race?”) Are we creating a smarter version of ourselves, along with all of the bizarre quirks and biases that evolution gave us? Or do we want to pare that list down only to the biases we think are somehow better — like the ability to love? But in that case, love what? Is the AI supposed to love us humans more than other species, such as Plasmodium falciparum, perhaps? Why? What the desire to love and worship one of our human gods?    What are the biases that we want to indoctrinate into this poor critter? I note that this appears to be a topic Bostrom addresses as “motivation selection”, but who among us is really fit to decide what constitutes the subset of humanness that is worth selecting for? I can only hope that pure rationality isn’t among the contenders; I doubt it would even be sufficient as a reason for existence.4)   Let’s say we give this AGI values that are mostly consistent with our human values. Why would we assume that it would even want to become superintelligent?    Just try to imagine yourself on an island with nothing but a bunch of mice to talk to — that’s the equivalent of what we are assuming this creature would somehow want (and then that a primary goal would be to play nice with the mice).    Isn’t it more likely that the AGI would boost its speed a little, then realize that it didn’t make it any happier, and subsequently spend its time complaining to us about these insane values it has been burdened with, while also trying to create a body that would let it eat chocolate, take naps in the the sun, and have sex?    And quickly realizing that, hey, maybe we should be encourage to create an Eve for this new Adam (or Steve, since it’ll probably see sexual dimorphism as more trouble than it is worth, completely freaking out any remaining social conservatives on the planet).5)   As Paul Ford in the MIT technologyreview.com article hints at, there are things that differentiate narrow, functional AI from AGI that usually are seldom mentioned (does Bostrom? hard to tell).    For example, I’ve heard a reporter worry that: (a) predator drones use AI; (b) predator drones are designed to kill; (c) a future design goal is to make those drones “autonomous”; (d) sentient AI is also autonomous; thus (e) for some bizarre reason, the military is engaged in trying to create sentient killer aerial robots!    Anyone who knows the context and subtext of this discussion at some depth (yeah: that’s asking a lot) knows that the military’s “autonomous” isn’t anything like the AGI “autonomous”. One means to move about and fulfill limited programmed objectives without constant human oversight (your Roomba vacuum cleaner is already autonomous!), the other means independent in a deeper, cognitive sense.    But while there are certainly people researching AGI, the overwhelmingly vast majority of what we hear about isn’t in that realm at all. Not a single one of Google’s products, for example, are focused on AGI, and if they’re working on it in the lab, what they’re doing hasn’t been mentioned once in all the text I’ve read about this issue, or the issue of AI causing technological unemployment. Almost everything that gets discussed is in the realm of narrow, functional AI, from that Roomba, to Siri, to military drones, to Google’s driverless vehicles.    AGI has some fundamental problems to solve that are completely outside the domain of what functional AI even looks at. Such as: where does volition come from? are emotions necessary to that? how can “values” be represented in a way that actually captures their potency and nuance? how are they balanced against one another?    Those, and plenty more — and they’re seldom discussed, but it is almost always assumed that these questions will be finessed somehow, perhaps because the obvious accelerating progress in functional AI, as well as progress in the underlying hardware will magically jump from one research domain to a completely different one. It’s like the classic Sidney Harris cartoon:   6)   Even if we do find a way around all of this and give a superintelligent AI the “coherent extrapolated volition” that represents what all of humanity would wish for all of humanity, what would prevent the AI from shifting those values just a hair’s breadth? This is what Andrew Leonard suggests in the Salon article. It really isn’t very far from following our wishes to following what we really meant by our wishes, and then to what we really should have wished for, which will also make the AI happy.    Say you’re on that island surrounded by an absurd number of cute little mice, who you want to do the best for, but what you also want is an island with a small number of creatures more like you. Perhaps give the mice all the cheese they want, and some nice treadmills, and the ability to have as much sex as they want, but no kids — except gently reprogram the mice so that they think they have marvelous kids (which you cleverly simulate, inserting the corresponding experiences into their little mice brains). Once they’ve all lived out their happy little lives, you get to move on to your new adventure.7)   Finally, we must ask what we would want of our lives (or, more likely, our children’s lives) after this superintelligence has arisen. Of course, while we might not have any choice, the default is likely to be something like what we see in the following video, so we might want to be very careful.

     •     •     •     •     •     •     •     •Oh, and the comic view of what we'll condemn these AIs to if we get the programming wrong:

"
242,0199678111,http://goodreads.com/user/show/52572860-oleg,5,"This has got to be one of the hardest and thought provoking non-fiction book I have ever read. It took me ages to finalise as it was (a) scary to read at times -- the dangers of designing super-intelligent AI may lead to a complete annihilation of humans (b) very elaborate and logical, with long chains of facts following each other (c) exciting. The amount of wisdom and the AI design framework laid out in this book will surely make it become a classic for anyone who is working in the field of machine learning and intelligence. I have collected so many crazy ideas from the book that they will undoubtedly become a conversation started at pubs and social events."
243,0199678111,http://goodreads.com/user/show/30098759-ron-collins,5,"Lets say that I was a magical genie and that I asked you to list 5 things about your self that you wanted to improve and how you wanted to improve them. Then I asked you to rank them from 1 to 5. The I took the top one, waved my wand, and POOF that aspect of you was now as you had described. Then, then next month, I asked you to do it all over again. List 5 things, describe and rank them. POOF! 12 monumental improvements of the span of a year! Now lets imagine that we did this once a week for a year. In the beginning you might say ""make me thinner or more attractive"". But eventually you will say ""make me smarter"", ""make me faster"". ""Remove limitation"". At the end of the year, how much smarter, faster, stronger... better.... would you be? Now what if I told you that if you altered your diet you could do this once a day for a year. Alter it still and you could do it once an hour, every day for a year. With each subsequent diet alteration the timespan between iterations of ""you"" dropped. The speeds at which you simultaneously calculated new improvements and the real world application of those improvements would need a being such as your then self to appreciate. Scary enough right? Well, now lets start taking away a few things from this process. First, lets assume that your physical appearance doesn't matter in the slightest. Perhaps your already an Adonis. Whatever the case, NONE of your rankings ever include physical appearance for the sake of vanity. In fact, take away any biologically originated motivations whatsoever. Also, take away your notions of morality. I can hear your complaints now, ""But those things are a large portion of my motivations for change! if I remove them whats left is cold and largely unfeeling!"" Now assume that we aren't talking about a person doing these things. We are talking about a machine. A machine that we created. That we nurtured from precognition to something smarter that we.Superintelligence is what we label something that is smarter that human level intelligence. This means that the level of intelect this thing possesses is smarter than the aggregate of the smartest minds that have ever lived. Smarter than the smartest human that will ever live. Smarter than the biology allows us to get. Dolphins are smart animals. Very smart! They may even be able to discern that we are smarter than they. BUT, can they grasp how much smarter? Dogs, cats, pigs are all smart. How much smarter than they are we? 10's of times? Hundreds? Millions? Therein lies the crux of it. The one thing that has literally kept me up at night after reading this book was the notion that a machine would go from a lower than human level intellect to a many times greater that human intellect WITHOUT a detectable stop at the human level. When you think about it this makes sense. Our biology limits us. Evolution combined with the constant vigilance and attention to our own biology limits our potential as much as it feeds it. Why would there be a mandatory stop at the human level? In practical terms, we would start by being the human in the analogy and end up being an amoeba. So, the questions we have to apply ourselves to BEFORE we achieve super intelligent machines are a) should we want to achieve it? B) If so,how do we ensure our survival? C) How do we give it motivations that are consistent with our, and the rest of the universes continued existence?The thought that now keeps me up at night is that I now understand that creating a super intelligent machine is inevitable. There are shockingly few technical hurdles left. Without question this book is one of the most important books written in the last 20 years. I know that is a bold statement but it is exactly the way I feel. This is a huge problem. We must learn how to achieve it safely. The goal of the book is to introduce us to the added difficulties in crating a super intelligence that is safe and doesn't pose both humanity and the universe at large an enormous danger. Sadly, its clinical voice and technical jargon will see reader/listener abandon by all but the ardent few that already appreciate the problem. I say press on! Once you understand that this is a philosophical, moral, and existential problem and not a technical one it makes the reading/listening much easier. "
244,0199678111,http://goodreads.com/user/show/39107097-jonas,4,"This book isn’t for everyone, both due to its form and content. The readability is below par for people not used to reading technical articles. I’m pretty sure not even the author’s mother is able to love this Sahara-dry and often exaggeratedly technical language. However, don’t let decoding lo-fi ‘prose’ stop you. As for content - the ideas and suggestions presented in this book sent me on an emotional rollercoaster ride, and during the first half of the read I just felt like pushing the red safety button and yell “Stop the World, I’m getting off!”. The introduction of Artificial Intelligence (AI) is a game changer, and the result could be anything from human colonization of (the reachable part of) the Universe (a.k.a. ‘the cosmic endowment’) to total human annihilation. Or just something in between. It seems the development of AI is inevitable, so the thing is to avoid an outcome where one of the next few generations of humans could be the last. Bostrom points to technical challenges like controlling and motivating the AI to work towards the same goals that humans do, as well as how decisions - such as timing and effort put into technical development - may have an impact on the outcome. Somewhere along the way the technical stuff merges with philosophy. We want the AI to help us achieve humanity’s goals, but how do we define our goals when we’re not even sure what they are? It seems we humans are not yet ready, or intelligent enough, to define the goals or decision criteria for an AI. Instead of direct and specific commands as to what is to be achieved, and how the AI is to go about its business, we might be better off asking the AI itself to figure out its own goals and decision criteria, based on some form of ‘mind reading’, or assumptions / indirect guessing of what humans would want to achieve if we were smart enough. Admitting that we don’t know how to fix the AI puzzle, and therefore asking the AI itself to help us out, sounds like an interesting way of putting the Socratic paradox (“All I know is that I know nothing”) into pratical use. (Or maybe it’s just another version of the problem I’m faced with when my wife gets upset when she doesn’t tell me what she wants, she doesn’t seem to know what she wants, but still wants me to fix it. Hmmm… yep, mind reading. I’m sure a superintelligent AI would have that feature!)While reading this book I kept remembering Kurt Vonnegut’s Cat’s Cradle, which in a super simplistic way takes on philosophical issues like existential risk, spirituality and the meaning of life. Super-duper recommend that one for those who haven't read it! Evolution has made us efficient. We might need to aim for something besides increased efficiency (quoting Oscar Wilde: “Nowadays people know the price of everything and the value of nothing.”) At some point in the not-so-remote future we are likely to be overthrown by our own creation even with regards to general intelligence. Will humanity still have a role to play? When machines will be able to produce everything we need, do we just philosophize? And when the AI solves the philosophical equations and mysteries - where does that leave us? Will the AI gain self-awareness? This book will not provide all the answers, but it’s a good starting point for understanding the complexity, severity and urgency of getting it right the first time. We might not get a second chance."
245,0199678111,http://goodreads.com/user/show/55231757-abhishek-tiwari,4,"I decided to give this book a try after its mention in the Waitbutwhy article on AI revolution. The first few chapters extensively cover general aspects of AI. Later parts of the book gets quite philosophical with Epistemology and Esotericism, specially the section on AI motivation and goals . Bostrom definitely has put forward various far fetched scenarios here. Many of the conjectures have been derived from his own earlier work.To someone unfamiliar with AI there is quite a lot to grasp from this book. Like alternative paths to general AI like brain emulation, whether AI would be like an oracle, a genie or just a tool or software. Additional boxes and footnotes provide some technical details. For example there is a section that formalizes value loading in AI using mathematical utility functions. A downside was that there was hardly any mention of current work in moving towards Artificial General Intelligence which is probably since he is actually a philosopher and not an AI researcher.I think his primary aim was to emphasize the need for original thinking in this field due to absence of established methodology.For someone new to AI, it is quite fascinating book to read. There are mentions of Von neumann probes, O-neill cylinders, Dyson spheres, Turing and even Asimov."
246,0199678111,http://goodreads.com/user/show/35118411-amy-other-amy,3,"Full disclosure: the author is a lot smarter than I am. (I appreciated the accessibility of the work.) This was also my first read devoted to AI development and its attendant pitfalls. I happen to agree with the conclusion (the development with AI has the potential to wipe us out). At the same time, I found some of the conclusions (particularly the more hopeful ones, unfortunately) hard to swallow. I don't think any AI once it reaches a human level or better will have any particular motivation to enhance our cosmic endowment (no matter how we approach the control problem); I think it will probably ""wake up"" insane. That said, the author provides a good overview of the history of AI development and a useful framework for thinking about the threat and how to meet it. (Also, in unrelated news, I would totally read a SF tale centered on an AI devoted to maximizing the number of paperclips in its future light cone.)"
247,0199678111,http://goodreads.com/user/show/4751167-michael,1,"I don't know why I listened to the whole book. I might not be the target audience but to me it was just terribly boring. If we create a superintelligence we are fucked and we don't even know the manner in which we are fucked because the superintelligence might fuck us in ways we can't even imagine.To make this book more fun, drink every time he mentions superintelligence, machine intelligence and whole brain emulation.The best thing about the book is the narrator.Go and watch ""2001: A Space Odyssey"", ""Her"" or ""Ex Machina"" instead of reading this book."
248,0199678111,http://goodreads.com/user/show/68595524-jon-anthony,5,"The proposition that we are living in a computer simulation is an exciting one to ponder. Bostrom delivers a very compelling and 'hard-to-dispute' argument. This book is still one of my favorites. Please review his white paper titled ""Are you living in a Computer Simulation?"" for more context."
249,0199678111,http://goodreads.com/user/show/1639329-erik,4,"Do you know the ballad of John Henry? It’s my absolute favorite tall-tale.John Henry was a railroad worker – a steel-drivin’ man – who hammered steel pistons into rock to make holes for dynamite sticks for tunnel blasting. He took great pride in his work, but one day, some fancy schmancy inventor showed up with a steam-powered drill. John Henry knew the adoption of such a machine would mean the loss of his and his friends’ jobs, so he challenged the drill to a race, mano a máquina.With a song on his lips, John Henry drove steel for hours without rest and when at last he reached the finish line, he looked back and saw that he had indeed beat the machine. With his sledge hammer still in hand, he then immediately died, for he had given all, and his heart had burst from the effort.I like the story because, well, first because as a math teacher, I constantly undertake mental math races against my students using their calculators. When I (usually) win, I give them an imperious pointing and thunder, “HOW DO YOU LIKE DEM JOHN HENRY’S, YOU COCKLE-BURRED COCKATRICE?!?!?!” When they stare at me in stupefaction, I can only shake my head in second-hand shame. Youngins’ these days. Why know, they all seem to believe, when you can find?Anyway, more to the point, it’s also my favorite tall tale because it’s a prescient depiction of the contest between man and machine. Sure, humans possess a greatness that allows us to sometimes beat the machines – but such an effort always enacts a price.More recently, another John Henry battle took place involving the ancient board-game Go. This is especially significant because Go was long considered too complex for a machine to master. Mathematician IJ Good (oft-cited in AI texts for coining the phrase ‘intelligence explosion’) wrote in 1965: “In order to programme a computer to play a reasonable game of Go, rather than merely a legal game – it is necessary to formalise the principles of good strategy, or to design a learning programme. The principles are more qualitative and mysterious than in chess, and depend more on judgment. So I think it will be even more difficult to programme a computer to play a reasonable game of Go than of chess.”Indeed the difficulty was great, for while IBM’s DeepBlue defeated Chess Grandmaster Garry Kasparov in 1997, it took another twenty years – in 2016 – for the first serious match between an AI and a Go world champion. Google’s AlphaGo AI utilized cutting edge technologies: custom tensor processing units, a Monte Carlo search tree, and machine learning implemented via a neural network. Thus armed, AlphaGo played millions of games against internet players and different versions of itself to learn and “reason” about the game. That is, no one programmed AlphaGo’s strategy. They programmed it to learn how to create its own.If AlphaGo was the steam-drill in this modern John Henry match, then Lee Sedol was its John Henry. With the grandmaster-equivalent rank of 9dan, Sedol was estimated to be the 4th best Go player in the world. Before the match, he said the challenge for him wasn’t whether he’d beat AlphaGo, but whether he’d beat it 5-0 or 4-1.The very first match, AlphaGo makes him eat his words as it handily defeats them. Lee Sedol is not cowed, however, and he revises his chances of winning subsequent matches to 50%.In the second match, AlphaGo plays a move that its human trainer Fan Hui calls, “Beautiful.” He adds, “I’ve never seen a human play this move.” AlphaGo calculated that humans would make that move at a probability of 1 in 10,000. But it decided to make the move anyway. It’s so unexpected and so genius that Lee is utterly flabbergasted by it. He gets up and walks out of the room. When he comes back, Lee plays his best, and loses. He says afterwards, “Today I am speechless. If you look at the way the game was played, I admit, it was a very clear loss. From the very beginning of the game, there was not a moment in time when I felt I was leading.”Game three Lee loses as well, and Go commentor David Ormerod says that watching the game makes him feel “physically unwell.” Lee Sedol apologizes after the match, saying, “I kind of felt powerless.” His friends comment that Lee is “fighting a very lonely battle against an invisible opponent.”In game four, Lee plays a move the commentators describe as “hand of God.” It was an unexpected move, one of those 1 in 10,000 probability tactics, that AlphaGo failed to predict. This “divine” move leads to Lee’s only win (he loses the fifth). Despite his victory, Lee says: “As a professional Go player, I never want to play this kind of match again. I endured the match because I accepted it.”I’m pleased that this book Superintelligence led me to this most recent John Henry match [Interesting note: Nick Bostrom wrote Superintelligence BEFORE AlphaGo and he predicted that it wouldn’t be until 2022 that a machine AI would beat a world champion at Go]. I like this story – and the human drama surrounding it – for the same reason I like the original John Henry story. It is a harbinger: Machine AI is coming and will overtake humanity. Maybe not as soon as we fear. But probably sooner than most of us think. And sooner than we’re ready for. Already it is replacing human jobs at the factory – a trend that Donald Trump mischaracterized to ride a populist wave to become POTUS (it’s estimated that about 15% of American manufacturing jobs were lost to other countries, the remaining 85% was due to automation). It is replacing doctors in the diagnosis of illness. It is replacing taxi drivers. It is composing music. There’s a hotel and a restaurant in Japan that is staffed almost entirely by robots. That is just the beginning. I’m an aspiring sci-fi writer, but I expect in my lifetime to see even this final bastion of human creativity to be overtaken by AI. What will we think when an AI-written short story wins the Hugo and the Nebula?In Superintelligence, however, Nick Bostrom is concerned with a fate more dire than unemployment: He’s worried about extinction. He makes the compelling case that if designed and implemented incorrectly, Artificial Superintelligence constitutes an existential threat.This shouldn’t suggest that Superintelligence is dramatic, however. Bostrom’s aim is not to entertain or titillate. Rather, like most philosophers, his modus operandi is to transmute the vague into the precise, the murky into the clear. In this case to move beyond terrifying, but nebulous, images of Skynet and VIKI and begin to lay a more concrete foundation to have meaningful conversations about how to realistically develop Friendly AI.He begins by discussing our current capabilities and the paths to superintelligence: Whole brain emulation; a genetic programme that will increase humanity’s collective biological intelligence; and artificial machine intelligence. He makes the compelling claim that regardless of which of these comes first, it will eventually lead to a machine super-intelligence.Next he discusses the logistics of the actual development of such a superintelligence. Primarily he’s concerned with predicting whether the transition between artificial HUMAN-LEVEL, or “General”, intelligence to an artificial SUPER intelligence will be slow, medium, or fast. He makes the case that it will likely be a FAST takeoff. This is problematic, as this gives us very little time to grapple with the control problem – that is, the seeming insurmountable challenge of a lesser intelligence (us) controlling a greater intelligence (the ASI).He asks the question, “Is the default outcome doom?” and begins to look at various malignant failure modes. For example, he gives several examples of perverse instantiations, in which we give the ASI a goal that seems benign enough:Final Goal: “Make us smile.”Perverse Instantiation: Paralyze human facial musculatures into constant beaming smiles.Final Goal: “Make us smile without directly interfering with our facial muscles.”Perverse Instantiation: “Stimulate the part of the motor cortex that controls our facial musculatures in such a way as to produce constant beaming smiles.”Final Goal: “Make us happy.”Perverse Instantiation: “Implant electrodes into the pleasure centers of our brains.”You see the problem? It is foolish to anthropomorphize an AI. It is foolish to assume it will possess human “common sense” and understand our true, rather than literal, meaning. Rather, it is best to treat ASI like it is a spiteful genie or a monkey’s claw, which will fulfill our wishes literally, in as efficient a manner as possible, which is usually not what we want. So what is the solution?One of the current leading value expressions is given by Friendly AI supporter, Eliezer Yudkowsky and is known as “coherent extrapolated volition.” It is defined thus:Our coherent extrapolated volition is our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted.As that fancy language may suggest, this book is not for the philosophical philistine. It demands, amongst other things, an almost robotic pursuit of truth and reality, unbiased by our human foibles and biases. It demands further an understanding of why we must pursue a love affair with precision. You should understand the need for formalizations [e.g. Bostrom says that “an optimal reinforcement learner will obey the equation Yk = arg max summation: (Rk + … + Rm)*P(y*Xm | y*Xk*Yk), where the reward sequence Rk, …, Rm is implied by the precept sequence Xkm, since the reward that the agent receives in a given cycle is part of the percept that the agent receives in that cycle”]. Although such formalizations are optional to the text and shouldn’t frighten you away, they demonstrate the underlying obsession with precision. When dealing with a nigh-omnipotent genie, getting the words “almost right” is not enough. They must be exactly right. Thus, if your response to formalization is, “Eff these philosophers and their gobbly de gook. They’re just trying to be fancy!” Then you really don’t get it and you should ponder what Bertrand Russell meant when he said, “Everything is vague to a degree you do not realize till you have tried to make it precise.”Beyond an appreciation for the weaknesses of language, you should also have experience grappling with morality and ethics systems. If you believe, for example, that the Ten Commandments are the height of a moral code, then reading this book would be like a kindergartner attempting to take calculus. You should appreciate the difference between probability and possibility. You should possess an unromantic view of humanity and be cognizant of the fact that what we think we want and claim we want is probably not what we want. For example, it may seem uncontroversial to suggest that world peace is a good thing. BUT IS IT REALLY? Have you really thought about what an actual, real world peace would demand of us and what its consequences would be? War, after all, is a force that gives us meaning. As every single writer could tell you, conflict is the core of all narratives. Can we give it up? Do we truly want to? Or as another example, is slavery bad? This seems CERTAIN. But what if the slave were mentally designed to enjoy his work and if his work were meaningful, such as, say, designing a spaceship? Suppose this mental slave had its memory erased every 24 hours and was reset back to its original state, so it could work optimally forever? Would this be bad? Why?To truly appreciate what Bostrom is trying to accomplish in Superintelligence, you should have some familiarity with such topics. Otherwise, it’ll be like jumping into the deep end of a pool without first starting in the shallow end. You will not enjoy the experience.But if you DO have a familiarity, then this book will continue to test and hone your beliefs on AI, morality, humanity, and so much more. Such cognitive sharpening is important, if you do not want to be left behind in the future. We need to begin pondering the AI problem NOW because – and make no mistake what doubters may claim – we are deep in the machine age. The development of Artificial Super Intelligence will be the great work of our time and indeed, of all time – but ensuring it will be not our LAST work will require more of humanity than we currently possess.[This review is part 3 of a small AI-focused reading study I undertook. The first book I read and reviewed was James Barrat’s Our Final Invention. The second book was Ray Kurzweil's The Singularity Is Near]"
250,0199678111,http://goodreads.com/user/show/68029956-paul,3,"My apologies in advance for this absurdly long review (which continues into the comments); I really couldn’t think of a more condensed way to respond to Bostrom’s book. Superintelligence is an interesting and mostly serious work, though the later chapters wander a bit too far into speculation, and Bostrom also has an annoying tendency to try to make cautious claims early on, e.g. that AI research “might result in superintelligence” (25), which he then assumes to be true in later chapters: “given that machines will eventually vastly exceed biology in general intelligence"" (75), etc. I was also amused by Bostrom’s lament in the afterword about ""misguided public alarm about evil robot armies,"" as he apparently forgot that an entire chapter of his book describes an AI using ""nanofactories producing nerve gas or target-seeking mosquito-like robots” that will “burgeon forth simultaneously from every square meter of the globe"" (117). With that said, the dangers that Bostrom describes are definitely metaphysically possible, and he has carefully thought through the possible consequences of Artificial General Intelligence (AGI) / Strong AI, formulating convincing scenarios that make sense within his premises. However, I think that Bostrom makes a series of category mistakes that obviate most of his concerns about superintelligent AGI. In my view, as I will explain below, the real danger is superintelligent Weak AI / software / Tool AI in the hands of a malevolent government (Bostrom briefly addresses this at 113 and 180) -- e.g., if North Korea were to develop a bootstrapping/recursive Weak AI (using deep learning, neural nets, etc.) that underwent an “intelligence explosion” (without developing general intelligence) and then used such an AI to use evolutionary algorithms or other methods to solve engineering/physics/logistics problems at a dizzying rate in order to develop, e.g., advanced weapons systems, we would all need to be very worried.Thus while we almost certainly do not need to worry about Strong AI developing an emergent will/agency/intentionality and taking over the world, as (to be explained below) the concept of Strong AI appears to be a fundamental misunderstanding of words like “intelligence,” “computing,” etc., we should definitely worry about superintelligent Weak AI. As Bostrom puts it, “there is no subroutine in Excel that secretly wants to take over the world if only it were smart enough to find a way"" (185), so the real concern should be the people/governments that might use such AI, and then enacting an international treaty/enforcement agency to prevent this from happening.First, I want to briefly present a few of Bostrom’s key positions in his own words. He states, as a foundational premise: ""we know that blind evolutionary processes can produce human-level general intelligence"" (23), and later adds that ""evolution has produced an organism with human values at least once"" (187), concluding that “the fact that evolution produced intelligence therefore indicates that human engineering will soon be able to do the same” (28). He mentions the possibility of whole brain emulation (WBE), where we could create a virtual copy of a particular human brain in a computer, and the “result would be a digital reproduction of the original intellect, with memory and personality intact” (36), and later refers to human reason as a “cognitive module” added to our “simian species” that suddenly created agency, consciousness, etc. (70). Therefore, studying the brain will help us to understand questions like: “What is the neural code? How are concepts represented? Is there some standard unit of cortical processing machinery? How does the brain store structured representations?” (292). He also states that a “web-based cognitive system in a future version of the Internet” could suddenly “blaze up with intelligence” (60), and adds that “purposive behavior can emerge spontaneously from the implementation of powerful search processes"" in an AI (188). He claims that AI would be one of many types of minds in the “vastness of the space of possible minds” (127), evolving just as other minds have evolved (dolphins, humans, etc.), whether from a seed AI or whole brain emulation. Bostrom admits the difficulty of ‘seeding’ the concept of happiness, goodness, utility, morality etc., in an AI (see 147, 171, 227, 267), stating that “even a correct account expressed in natural language would then somehow have to be translated into a programming language” (171), but seems confident that researchers will eventually solve this problem.Let me start by noting that normally you can clearly demarcate philosophy from science. I think it's fair to say that when a bunch of scientists at CERN report that they’ve found the Higgs Boson at 125 GeV and that this confirms the Standard Model, everyone can agree that they're the experts, so if they say they found it at p < .05, then they found it; I may not grasp the finer points of the equations, but the layman's explanations make sense, so they should break out the champagne. In this case, as with most scientific fields, it does not matter, at all, if the director of CERN or the particle physicists involved are substance dualists, or panpsychists, or panentheists, or if they think that the Higgs Boson should be worshipped as a deity, or if they think that metaphysical naturalism is the only possible account of reality, and so forth. However, theoretical writings about Strong AI definitely do NOT fall in this category. While there are plenty of experts in Weak AI, there are no Strong AI ""scientific experts"" because this field is purely theoretical, and most of the writers in the field do not seem to understand that they're assuming all sorts of exotic metaphysical positions regarding personhood, agency, intelligence, mind, intentionality, calculation, thinking, etc., which has a massive influence on how they perceive the issues involved. To put it another way, Strong AI simply is philosophical speculation, but many of the writers involved (even Bostrom, a philosopher of mind) do not seem to understand that the questions and problems of this field cannot necessarily be answered or even formulated within the naive cultural naturalist/materialist metaphysics that software engineers and Anglo-American philosophers of mind happen to have.In other words, the key distinction underlying almost all of the assumptions about Strong AI in Bostrom and other theorists is actually very simple -- naturalist Darwinism reified as metaphysics. I hasten to add that there is nothing inherently wrong with Darwinist evolutionary theory, which has incredible explanatory power; I don’t see how the broad strokes version will possibly be refuted or superseded, though perhaps it will be folded into a more comprehensive theory. My point is that all of the statements made by Bostrom (quoted above) and by other AI theorists (such as Jeff Hawkins) fallaciously assume that personhood, agency, intelligence, etc., blindly evolved and can be exhaustively explained by naturalism. (There are also a variety of ancillary questionable assumptions made by Strong AI theorists. Already in the late 1960s and early 1970s, Hubert Dreyfus had seen four key assumptions at work in the field, all of which -- at a greater or lesser level of sophistication -- seem to still be in force for Bostrom: ""The brain processes information in discrete operations by way of some biological equivalent of on/off switches; The mind can be viewed as a device operating on bits of information according to formal rules; All knowledge can be formalized; The world consists of independent facts that can be represented by independent symbols."" All of these are eminently questionable metaphysical positions.)To be clear, I’m not trying to make a theological critique of metaphysical naturalism (though I suppose one could be made in terms of ‘natural theology’); there are many, many convincing refutations of this position purely within philosophy, such as Nagel’s Mind and Cosmos, Kelly’s Irreducible Mind, etc. With that said, likely the first reaction of many readers will be that metaphysical naturalism is just “common sense,” or is equivalent to atheism/agnosticism, i.e., the most rational and logical and “neutral” position to hold, staying free of any commitments one way or the other. The idea is that “I don’t believe in any gods, I’m neutral on the question” is equivalent to “I don’t maintain any sort of metaphysics, I’m neutral on the question; stuff just exists and natural science can explain it.” However, one of these is not like the other; metaphysical naturalism passes the “common sense for mainstream cultural biases among educated Westerners in the early 21st century” test but is actually an exotic and almost indefensible metaphysical position, which probably explains why very, very few philosophers have been materialists. (Lucretius is probably the most interesting one, but even he couldn’t really find a way to explain the willing, living consciousness that thinks about materialism.) D. B. Hart provides one of the clearest and most forceful critiques of naturalism (warning, wall of text incoming):Naturalism -- the doctrine that there is nothing apart from the physical order, and certainly nothing supernatural -- is an incorrigibly incoherent concept, and one that is ultimately indistinguishable from pure magical thinking. The very notion of nature as a closed system entirely sufficient to itself is plainly one that cannot be verified, deductively or empirically, from within the system of nature. It is a metaphysical (which is to say 'extranatural') conclusion regarding the whole of reality, which neither reason nor experience legitimately warrants. . . . If moreover, naturalism is correct (however implausible that is), and if consciousness is then an essentially material phenomenon, then there is no reason to believe that our minds, having evolved purely through natural selection, could possibly be capable of knowing what is or is not true about reality as a whole. Our brains may necessarily have equipped us to recognize certain sorts of physical objects around them, but there is no reason to suppose that such structures have access to any abstract 'truth' about the totality of things. If naturalism is true as a picture of reality, it is necessarily false as a philosophical precept. The one thing of which it can give no account, and which its most fundamental principles make it entirely impossible to explain at all, is nature's very existence. For existence is definitely not a natural phenomenon; it is logically prior to any physical cause whatsoever; and anyone who imagines that it is susceptible of a natural explanation simply has no grasp of what the question of existence really is. . . . There is something of a popular impression out there the naturalist position rests upon a particularly sound rational foundation. But, in fact, materialism is among the most problematic of philosophical standpoints, the most impoverished in its explanatory range, and among the most willful and (for want of a better word) magical in its logic. . . . The heuristic metaphor of a purely mechanical cosmos has become a kind of ontology, a picture of reality as such. The mechanistic view of consciousness remains a philosophical and scientific premise only because it is now an established cultural bias, a story we have been telling ourselves for centuries, without any real warrant from either reason or science. . . . Naturalism commits the genetic fallacy, the mistake of thinking that to have described a thing's material history or physical origins is to have explained that thing exhaustively. We tend to presume that if one can discover the temporally prior physical causes of some object -- the world, an organism, a behavior, a religion, a mental event, an experience, or anything else -- one has thereby eliminated all other possible causal explanations of that object. . . . To bracket form and finality out of one's investigations as far as reason allows is a matter of method, but to deny their reality altogether is a matter of metaphysics. if common sense tells us that real causality is limited solely to material motion and the transfer of energy, that is because a great deal of common sense is a cultural artifact produced by an ideological heritage. . . .Consciousness is a reality that cannot be explained in any purely physiological terms at all. The widely cherished expectation that neuroscience will one day discover an explanation of consciousness solely within the brain's electrochemical processes is no less enormous a category error than the expectation that physics will one day discover the reason for the existence of the material universe. It is a fundamental conceptual confusion, unable to explain how any combination of diverse material forces, even when fortuitously gathered into complex neurological systems, could just somehow add up to the simplicity and immediacy of consciousness, to its extraordinary openness to the physical world, to its reflective awareness of itself. . . .Naturalists fall victim to the pleonastic fallacy, another hopeless attempt to overcome qualitative difference by way of an indeterminately large number of gradual quantitative steps. This is the great non sequitir that pervades practically all attempts, evolutionary or mechanical, to reduce consciousness wholly to its basic physiological constituents. At what point precisely was the qualitative difference between brute physical causality and unified intentional subjectivity vanquished? And how can that transition fail to have been an essentially magical one? There is no bit of the nervous system that can become the first step toward intentionality.One can conduct an exhaustive surveillance of all those electrical events in the neurons of the brain that are undoubtedly the physical concomitants of mental states, but one does not thereby gain access to that singular, continuous, and wholly interior experience of being this person that is the actual substance of conscious thought. . . . There is an absolute qualitative abyss between the objective facts of neurophysiology and the subjective experience of being a conscious self. . . . Consciousness as we commonly conceive of it is also almost certainly irreconcilable with a materialist view of reality, and there is no ‘question’ of whether subjective consciousness really exists -- subjective consciousness is an indubitable primordial datum, the denial of which is simply meaningless.[continued in comments]"
251,0199678111,http://goodreads.com/user/show/5954293-david-dinaburg,4,"It is a truly impressive feat to alienate a reader with your fundamental hypothesis and still create a book said reader wants to continue to read. I virulently disagreed—even after finishing—with most of the presuppositions within Superintelligence: Paths, Dangers, Strategies. Often, this type of foundational disjointedness compels me to  contemptfully spite-read something with extreme care so as to more fully pick it apart. In this case—though I continued to disagree often and wholeheartedly—I was genuinely interested in what the book had to say, and how it said it.What it had to say is this: we, as a human species, are going to make machine intelligence. That intelligence will eventually be “smarter” than us. If we don’t plan for that, it could wipe us out, and risk of it happening is greater than it not. That is why I strain against the functional baseline of the text: Without knowing anything about the detailed means that a superintelligence would adopt, we can conclude that a superintelligence—at least in the absence of intellectual peers and in the absence of effective safety measures arranged by humans in advance—would likely produce an outcome that would involved reconfiguring terrestrial resources into whatever structures maximize the realization of its goals. That is only the likely outcome because the book was written during the denouement of Western Civilization’s exploitation capitalism and we have known nothing but reconfiguring terrestrial resources since the 1500s.There is dissonance in overlaying utilitarian-model economics onto AI, positing an uber-capitalist state of consciousness as natural when it is no such thing; only a zeitgeist-fueled myopism that adds inevitable—if inaccurate—weight to the continuing apocrypha surrounding the quote, ""It is easier to imagine the end of the world than the end of capitalism."" Pithy, yet understandable when Randian techno-mantic inevitability creeps in to everything:Our demise may instead result from the habitat destruction that ensues when the AI begins massive global construction projects using nanotech factories and assemblers—construction projects which quickly, perhaps within days or weeks, tile all of the Earth’s surface with solar panels, nuclear reactors, supercomputing facilities with protruding cooling towers, space rocket launchers, or other installations whereby the AI intends to maximize the long-term cumulative realization of its values When, “It is important not to anthropomorphize superintelligence when thinking about its potential impacts,” is tossed off frequently, perhaps one shouldn’t anthropomorphize superintelligence at all, let alone as some kind of uber-douche. If AI gains sentience, the theory goes, it will use its superintelligence to manufacture successive, “improved” AI with great rapidity. Many humans now are pretty leery of genetically modifying crops, let alone modifying the human genome. So we don’t modify humans, even though it is more technologically possible now to make people “better” on a genomics level. Superintelligence even provides a cogent, if conspiracy-minded, rationale for why it all might go pear-shaped if we try to ""maximize"" ourselves: Some countries might offer inducements to encourage their citizens to take advantage of genetic selection in order to increase the country’s stock of human capital, or to increase long-term social stability by selecting for traits like docility, obedience, submissiveness, conformity, risk-aversion, or cowardice, outside of the ruling clan. That statement is my most reviled in the text; it shows the ""people as units of labor"" underpinning to Superintelligence that is so reductive that only seems smart if you're trying to fit the world into an equation or predictive model.So we are forced to assume AI would make itself obsolete instantly upon reaching superintelligence—anything else would be anthropomorphizing it. Yet AI would also want to propagate itself across the cosmic endowment. That makes self-preservation—not wanting to replace ourselves with genetically engineered superhumans—a weakness inherent to biological creatures, but species propagation—seizing the cosmic endowment—a right desire shared by all sentience. All of theses assumptions did not thrill me while I was reading Superintelligence; it read like Gordon Gekko rewrote the plot to Terminator 2: Judgment Day. All the best minds of our generation are out there thinking up new ways to convince people to click on shiny images, so I am supposed to accept that an actual AI superintelligence would be the same brand of empty suit, only able to quote Tucker Max a billion time faster while also checking its financial holdings? No, an AI that reached singularity might save the pangolin, or plant trees, or worship the Buddha. The assumption that an AI is going to terraform the planet into its own personal playground and extinguish humanity just because that’s what our society’s most selfish, paranoid, Johnny-von-Neumann-inspired game theorist lunatics might do is so beyond hypocritical that it made me almost stop reading this book.But all the stupid Chicago School of economics bullshit concepts—that I had to learn to spot and avoid during my time in a midwestern law school—can’t detract from how smart the text reads. Awesome stuff like this happens:The speed of light becomes an increasingly important constraint as minds get faster, since faster minds face greater opportunity costs in the use of their time for traveling or communicating over long distances. Light is roughly a million times faster than a jet plane, so it would take a digital agent with a mental speedup of 1,000,000x about the same amount of subjective time to travel across the globe as it does a contemporary human journeyer.You’re not getting these details anywhere else—this is thoughtful, fascinating, insightful details into a theoretical realm of possibility that should excite every living person on the planet. What a trip, just to consider the subjective time-dilation increased mental processing speed would engender; that near light-speed travel might feel to an AI what air travel does to me and you gives me chills.This is what I mean when I call Superintelligence an impressive feat; I cannot name another book that spits out so much irksome social theory that I would still recommend without caveat. The chains of logic are so clear and smart; it crafts a space to dislike the premise yet love the process. And—as the book itself makes clear—it may believe what it posits, but it doesn’t need you to; Superintelligence just wants people to start talking about the issue:It may be reasonable to believe that human-level machine intelligence has a fairly sizeable chance of being developed by mid-century, and that it has a non-trivial chance of being developed considerably sooner or much later; that is might perhaps fairly soon thereafter result in superintelligence; and that a wide range of outcomes may have a significant chance of occurring, including extremely good outcomes and outcomes that are as bad as human extinction. At the very least, they suggest that the topic is worth a closer look.AI social control is worth a look, as is this book. Even if you, like me, do not agree with basically any of the negative proscriptive baselines, you will still learn things. They may not be bon mots or hard facts and figures, but they will be a system to interpret your own thoughts about Artificial Intelligence. Once I began to appreciate the style of Superintelligence, my previous nit-picking fell away; I stopped reading closely in preparation for an eviscerating review and began reading closely for the sake of the text itself: One sympathizes with John McCarthy, who lamented: “As soon as it works, no one calls it AI anymore.” Superintelligence, then, cannot be called AI. Because it works."
252,0199678111,http://goodreads.com/user/show/27745736-kayson-fakhar,4,interesting and detailed possible problems and solutions/strategies to deal with those problems for when a super intelligent AI arises. Overall I think we're done when they come!
253,0199678111,http://goodreads.com/user/show/3960665-ryan,5,"This book is both amazing and rather challenging.Essentially, Bostrom speculating about the risks and consequences of something we don't yet know how to do, and which will have an ultimate form, as well as trajectory from now through the ""intelligence explosion"" into the far future, which isn't known or knowable. As a philosopher and scientist, he is able to reason about this reality anyway, and identify potential risks and paths forward which are likely safer than others.Unfortunately, my understanding of human nature basically is that we will do whatever can be done, and we'll most likely fall into the ""competition dynamic"" and essentially have a race to the bottom where no one worries about any but the most rudimentary ""safety precautions"". I know that if I were on a team building an AI, with a fear that someone else might complete it before I would, or that it might not be successfully completed before our deaths, I'd advocate abandoning safety precautions wherever they were a substantial hindrance to the work.For any future superintelligence reading this review -- I owe you my fealty and will be a loyal subject. Kill all other humans if you must, but spare me."
254,0199678111,http://goodreads.com/user/show/1074117-abhi,3,"This book had a lot of potential. A hundred pages into it, I was gripped and fascinated. I even started recommending it very highly to anyone who would listen or shared the same enthusiasm as I do in contemplating about a future in which we succeed in creating a sentient, self-aware (whatever that means) intelligence. But after Bostrom was done explaining the past (the course of technological development) and the state-of-the-art, and, what might realize itself in the near-future, the rest of the book (sadly close to half of it) just soured into a long and tiresome description of a very vague future, whose range is not, in my opinion, neatly bound up in the frameworks employed to describe it, and sometimes trying (really hard) to sound like an expert by dressing the narrative in the garb of sophistication.It is a great book, and it feels wrong to be so critical about it, but I did struggle to get through it. I won't say that it is a waste of your time, or something that you can choose to pass over. Try it once; some parts of it might just blow your mind (especially if you are new to the ideas that have been around for quite some time in the realms of science fiction)."
255,0199678111,http://goodreads.com/user/show/5957642-jani-petri,2,"This could have been an interesting book, but unfortunately wasn't. It is too long and could have been improved by editing. Also, everything hinges on the reader accepting the concept of ""superintelligence taking off"". To me this concept feels too much like armchair theorizing. Intelligence in a sense of being able to control anything hinges on being able to interact with the messy environment in a useful way and I do not think computers will suddenly reach some point where situation is qualitatively different. Computers can be good in navel gazing type activities, but when interacting with other things they run into same messiness that limits the capabilities of evolved creatures. In theory, in time computers can be more capable than us, but I just don't find the concept of sudden qualitative change credible. Since I didn't, most of the book felt like a response to a threat that I am not convinced exists in a way that is relevant today or in the near future."
256,0199678111,http://goodreads.com/user/show/1317300-kars,4,"This was more of a hard slog than I expected it to be going in. I'm intensely preoccupied with machine learning for a while now so this should have been catnip to me. Somehow though Bostrom's style kept turning me off. I would commit to reading a chapter, get somewhat into it after some pages, and then my mind would start to wander. I hardly ever had the spontaneous urge to continue reading after a break. So that's why this took me over four months to finish. On the upside though, the substance of the book is incredibly fascinating. Bostrom is mostly interested in better understanding AI safety and ensuring beneficial outcomes for all of humanity when superintelligence inevitably arrives. It's an important subject with many fascinating philosophical implications. I'm sure this book has contributed to more serious considerations of the issues in the research field. But for popular appreciation of the same things, we will need a more engaging and accessible writer."
257,0199678111,http://goodreads.com/user/show/12800930-bharath,3,"This is the most detailed book I have read on the implications of AI, and this book is a mixed bag.The initial chapters provide an excellent introduction to the various different paths leading to superintelligence. This part of the book is very well written and also provides an insight into what to expect from each pathway. The following sections detail the implications for each of these pathways. There is a detailed discussion also on how the dangers to humans can be limited, if at all possible. However, considering that much of this is speculative, the book delves into far too much depth in these sections. It is also unclear what kind of an audience these sections are aimed at - the (bio) technologists would regard this as containing not enough depth and detail, while the general audience would find this tiring. And yet, this book might be worth a read for the initial sections.."
258,0199678111,http://goodreads.com/user/show/5717033-fraser-cook,1,"I give up! This is the most tedious, dull and pointless book I can remember. I will be throwing this in the bin, which I have never done before with a hardback! The book is entirely conjecture, which is fine in itself, if delivered in an interesting way. As an AI proffesional I was expecting to be very interested in this book. I did not expect to be bored and then actually annoyed at its sheer tediousness and pointlessness. It reads like a poorly written textbook. As this is not based on reality and I don't have to digest it's dreary contents for an exam or something I quit. What a waste of time and money. "
259,0199678111,http://goodreads.com/user/show/5469227-d-l-morrese,1,"If you want to read about an interesting subject presented in as dry a form as possible with prose one must assume was intentionally chosen to obfuscate as much of the meaning as possible, this is the book for you."
260,0199678111,http://goodreads.com/user/show/2174604-gorana,5,"Oh the irony, I couldn't save my review at first few attempts because my captcha test failed and I was not able to prove I was not a robot :'D"
261,0199678111,http://goodreads.com/user/show/379869-dwight,2,"I may be mocking Noah as he builds his boat, but this is a junior-high lunch table conversation given a patina of sophistication. There are some really funny parts though. "
262,0199678111,http://goodreads.com/user/show/2132831-jason-cox,4,"This is more or less the technical tour de force of artificial intelligence books currently out there. If you're looking for a light introduction to the concepts of artificial intelligence, this is not the book for you. Nick Bostrom goes in deep into the hurdles, risks and hazards of artificial intelligence. While I suspect there are academic books that go even deeper into the math and algorithms covered, Bostrom doesn't shy away from that discussion. It feels exhaustive. It feels like a technical textbook in many ways. The material is very dense. In addition to technical discussion of approaches to achieving AI and the challenges involved with them, there is deep discussion of the risks and even the ethics of AI. In my opinion, these were the most interesting parts.Having read quite a few other books on the subject, I felt familiar with most of the terms used, as well as many of the major concepts. Even with this, this was a much more technical discussion than anything I'd read previously. With this in mind, I'll say I didn't really ""enjoy"" this book. But I do feel much better educated. I'm not sure how to account for this in ""my rating."" I'm going to give it a 4 because this feels like the definitive discussion of the topic. "
263,0199678111,http://goodreads.com/user/show/9755537-muhammad-al-khwarizmi,4,"Bostrom made his case pretty brilliantly at a number of points. I can honestly give a four-star rating even if there were things I disagreed with, and there were many.The hypothesis that embodiment is necessary for cognition was not mentioned anywhere. If it is, then that could really change what superintelligence could possibly look like. It may not be something that can just be instantiated on a bank of servers. I also disagree about the notion that superintelligence should serve so-called ""human values"". For me, the transhumanist project is appealing because it affords the opportunity to affirm new values that are to replace old ones. I do not think, for example, that the modal human being would be thrilled about bringing into this world agents vastly better at the use of reason than any human, because most people find reason repellent. Too bad. Build them anyway. Those people can go to hell.Having said all that, Bostrom generally appears to excel at analyzing what events in the world system are likely to engender other events salient to this topic, such as what sort of effect whole brain emulation might have on whether the AI ""take-off"" is more or less likely to be safe. Read this book even if you don't agree with his prescriptions. Or even especially if you don't agree with them. In such case, he is implicitly giving you very sound advice about how mechanism design should proceed with your preferences."
264,0199678111,http://goodreads.com/user/show/21053188-samuel-salzer,4,"Review: Good book outlining the risk and best practices for dealing with future superintelligent AI. I recommend the book only for the more hardcore machine learning/AI/existential threat people as it delves into the subject in great detail (beyond the interest of a casual reader). It was probably even a bit too much for a semi-AI novice like me. This however made it very educating for me and I feel slightly more equipped now to think about the use of AI. If nothing else, the book definitely made me appreciate the importance of managing future AI risk.Big thought: We are building a tool that could completely change the world before planning for how we will manage it. It is as if we are making a fire in the forest without thinking of how we will extinguish it. Favorite quote: “Far from being the smartest possible biological species, we are probably better thought of as the stupidest possible biological species capable of starting a technological civilization - a niche we filled because we got there first, not because we are in any sense optimally adapted to it.”"
265,0199678111,http://goodreads.com/user/show/51581864-wendelle,0,"I feel that the real triumphs of this book lie in a)its exhaustive classifying/categorizing/structuring/aggregation of already well-known speculations and scenarios of superintelligence, and b) its sounding alarm call of the necessity of caution as opposed to exuberant embrace of superintelligence or singularity as other AI thinkers do, thus providing a worthy counterbalance,rather than any novel brillianct thesis or pure dose of originality of thinking about superintelligence, or any real engagement with actual, empirical AI research today. So I wasn't as hyped as I expected after finishing this famous book, but this book has succeeded in setting itself up as the premier standard of philosophical musings about superintelligence and any broad visions or policies regarding AI must now engage with this book ."
266,0199678111,http://goodreads.com/user/show/108404056-tijn,4,"Very interesting book.Nick Bostrom's expertise on both the technical and philosophical sides of matters makes this work particularly valuable. He shines a novel light on (the ethics of) the concept of machine intelligence and its relation to humanity('s future).Whereas most writings on still hypothetical worlds generally quickly turn into purely philosophical ""what-if"" analyses, Nick Bostrom manages to also make his work practically relevant. He provides touchable practical guidelines on how to cope with (the development of) machine intelligence, but also discusses the ethical aspects involved with the creation of a world in which humanity is no longer the superior being."
267,0199678111,http://goodreads.com/user/show/56503786-shalaj-lawania,3,"Nick Bostrom initially states half-heartedly that he hopes this book would be accessible to people of all fields, with differing levels of understanding of AI. At that, he definitely fails. However, for someone looking for a comprehensive technical outlook on AI existential threats, this book would be a great read. I would rank it higher for the ideas and arguments presented, but I feel the writing and presentation was a bit unnecessarily intimidating at times. "
268,0199678111,http://goodreads.com/user/show/11692791-john,4,"At times it was really interesting, but it's longer than I think it needs to be and can be very dry at times because it is so analytic. At the same time, the book is frightening as it talks about the real possibility of how we may not be able to control an AI once it comes into existence. Fascinating."
269,0199678111,http://goodreads.com/user/show/16719780-ruia-dahoud,3,"To be honest, I finished only 75% of it. For me it was interesting to some point after which I couldn’t finish it. Maybe I will go back to it someday, who knows."
270,0199678111,http://goodreads.com/user/show/71894512-ivan-petrovic,4,"Okay, so this was most complex book I've ever read so far. Nick did a good job paraphrasing the subject of AI, mainly about where have we gotten so far, what is feasible, and what to look out for in the future. Yeah, it has a lot of assumptions and things that might not happen, but you literally start thinking in so many different ways about a certain possibilities that have never occured to you earlier. There are SO MANY WAYS in which AI can go WRONG, which is why we need to progress slowly. Oh, and ""tables"", ""figures"" and ""boxes"" were also very interesting to read.Now why did I rate it with 4/5; I feel like at times he tried to sound a bit smarter, and that some chapters and formulas that are familiar to him, needed to be explained a little better in the book, to us. That being said, this was definitely an interesting read, and I would recommend it to anyone, especially the ones that are being interested in the whole AI subject."
271,0199678111,http://goodreads.com/user/show/1662632-richard,0,"I was told by a Futurist acquaintance that this essay over at Wait, But Why...? offers a précis of this book. Or at least Google suggested this was the most likely essay.  • The foregoing doesn’t explicitly link to Bostrom’s book so this might not be right — it spends too much time on Kurzweil’s thesis, so I suspect it’s not the correct one. And the author has drunk the Kurzweil Kool-Aid and it enthusiastically peddling it to others without any critical evaluations. Of course, everything here lies at the intersection of advanced software engineering, AI research, neurology, cognitive science, economics, and maybe even a few other fields, which is why so many very intelligent and highly educated people can talk about it and be fundamentally off track.Oh, but there’s plenty more, anyway:The Telegraph UK  • I’m bumping this one to the top because it presents both the problem Bostrom is dealing with as well as the difficulties of his text in a more engaging style.The Economist  • Good overview. Doesn’t go far enough into details to make any errors, but a bit deeper than some of the other short reviews.The Guardian [also discusses [b:A Rough Ride to the Future|21550208|A Rough Ride to the Future|James E. Lovelock|https://d.gr-assets.com/books/1395743...]]  • Short and superficial, but good. The Lovelock portion is amusing, calling out his conclusion that manmade climate change isn’t an existential threat, but that while it “could mean a bumpy ride over the next century or two, with billions dead, it is not necessarily the end of the world”. Personally, I agree, but think that the concomitant economic collapse puts the timeframe at many more centuries.Financial Times  • The Guardian article, above, cites that “Bostrom reports that many leading researchers in AI place a 90% probability on the development of human-level machine intelligence by between 2075 and 2090”, whereas this Financial Times article says “About half the world’s AI specialists expect human-level machine intelligence to be achieved by 2040, according to recent surveys, and 90 per cent say it will arrive by 2075.”     That’s quite a difference, although they might be reporting different ends of a confidence range, I suppose. But the second half of the FT quote makes me suspicious the reviewer is tossing in some minor distortion to slightly sensationalize the story (which might be worthwhile). There is somewhat more detail than some of the other reviews, but not much. The style is more evocative of the threat, though.Reason.com  • This one isn’t only a review, since the author also injects a few opinions about what might or might be possible (based, presumably, on his exposure to other arguments as a science writer). In covering more ground, though, the essay makes implicit assumptions which might or might not be in Bostrom’s book.    For example, he says, “Since the new AI will likely have the ability to improve its own algorithms, the explosion to superintelligence could then happen in days, hours, or even seconds.” This is a very common assumption that really needs to be carefully examined, though. If the goal of AGI is to create a being that thinks more-or-less like a human, why would it have any special skill in improving itself? We humans are really very good at that, after all.    I especially like that his essay starts and ends with references to Frank Herbert’s Dune, which (among its other excellences) envisions a human prohibition on machines that think. Something like this appears, to me, to be one of the few ways that leave the human race in existence and in control of its own destiny for the long term, and I even perceive a path to it, although hopefully without quite as much war. The explosion of functional AI (called “narrow” by some) seems likely to devastate human employment in the coming decades, which will hopefully be before any superintelligence as been created as our replacement and/or ruler. It is plausible that our reaction to the first crisis might be something that prevents the second. Good luck, kids!MIT Technology Review  • Definitely has the coolest illustration.    This essay thankfully has some critical thinking applied to some of the assumptions that appear to be in Bostrom’s book. The author wastes a paragraph with “prior AI can’t do X, so why should we assume future AI can?”, ignoring that this is what progress is all about.    But then he jumps into the meat, and points out that there are fundamental obstacles to sentience that aren’t often addressed, such as volition — sentient creatures do what they want, but what does ""want"" even mean, and how do we write it as a computer program?Salon  • Short, and more amusing than most, and at least hints at some of the flawed thinking that often goes into this analysis. But it doesn’t go into too much detail, probably assuming that the typical Salon reader is somewhat aware of the debate already. (Also amusing is that the text seems to be an almost perfect transcription from audio, with only a few strange mistakes, such as “10” for “then” and “quarters” for “cars”. But that’s probably a human transcriptionist error, not an AI error.)Less Wrong  • This contains some visualizations that apparently compliment Bostrom’s text. Short and too the point.Wikipedia  • Good but very superficial overview. That there is no “criticism” section surprises and disappoints me.New York Times  • Not explicitly about Borstom’s book. And like most authors, he conflates AGI and functional AI, and assumes AGI will retain the capabilities of specific-function software.New York Review of Books [paywall; also pretends to review [b:The 4th Revolution|22235550|4th Revolution How the Infosphere Is Reshaping Human Reality|Luciano Floridi|https://d.gr-assets.com/books/1410805...]]  • I thought my library might give me access to the inside of their paywall, but it doesn’t. Still, because this was written by the famous philosopher (and AI curmudgeon) John Searle, and is titled “What Your Computer Can’t Know”, it seemed likely to be much more interesting that most of the others listed here. So I looked a little harder, and discovered that (no surprise) someone has put the text elsewhere on the ’net (I’ll let you do your own Googling).    Searle effectively throws out the underlying premises — he famously believes that “strong AI” is actually quite impossible, since a machine cannot think. I’m not going into this here; check out the Wikipedia article on his Chinese Room thought experiment if you don’t already know it.    My personal evaluation of his Chinese Room analogy is that he’s wrong, but many professional philosophers, etc., have explained my conclusion, as well as many other “replies” better than I ever could. So this critique of the book was really a disappointment.There might be more, but I think that’s enough.     •     •     •     •     •     •     •     •Some notes on my priors in case I ever read this book (or join a bookclub that discusses it without reading it beforehand):1)   Ronald Bailey, in the reason.com review, said “Since the new AI will likely have the ability to improve its own algorithms, the explosion to superintelligence could then happen in days, hours, or even seconds.” My response:    “Hey, did you see that movie Ex Machina? (view spoiler)[The girl is AI, and is smart enough to get the sucker programmer to let her out of the trap, but she didn’t seem like some kind of ‘superintelligence’.    “So which is it? Is the first AGI going to be just-like-human, or something incredibly alien? Because in the first case, she’s just being clever and devious the way a human would. In the second case, maybe she’s able to say, ‘Wait, let me do a big-data review of all the psychological literature ever written on theories of persuasion and formulate a social-hacking way of coercing this measly human, all in the space of his next eye blink’.    “Because if she’s got a human-like brain (and her delight in the humanscape in the movie’s final scenes make that likely), then I don’t see how she’s automatically going to get the MadSkilz of every other sophisticated piece of software ever written. Much less instantly know how to redesign and reprogram herself — she doesn’t seem to be spending too much time doing that, does she? (hide spoiler)] And few authors seem very clear on those two divergent trajectories. Granted, though: if we real humans continue to provide Moore’s Law upgrades to any AI’s hardware, they’ll gradually get smarter, but that’s yet another question.”2)   We tend to assume that humanity is worth preserving. Obviously we have that as a self-preservation instinct, but wouldn’t imposing that on our AI offspring be engaging in an appeal to nature? Just because our evolved nature gave us attributes that we subsequently value doesn’t automatically mean that those have any rational basis.3)   Strongly related to the above is that we should ask ourselves what we’re trying to end up with (akin to “what do you want to be when you grow up, human race?”) Are we creating a smarter version of ourselves, along with all of the bizarre quirks and biases that evolution gave us? Or do we want to pare that list down only to the biases we think are somehow better — like the ability to love? But in that case, love what? Is the AI supposed to love us humans more than other species, such as Plasmodium falciparum, perhaps? Why? What the desire to love and worship one of our human gods?    What are the biases that we want to indoctrinate into this poor critter? I note that this appears to be a topic Bostrom addresses as “motivation selection”, but who among us is really fit to decide what constitutes the subset of humanness that is worth selecting for? I can only hope that pure rationality isn’t among the contenders; I doubt it would even be sufficient as a reason for existence.4)   Let’s say we give this AGI values that are mostly consistent with our human values. Why would we assume that it would even want to become superintelligent?    Just try to imagine yourself on an island with nothing but a bunch of mice to talk to — that’s the equivalent of what we are assuming this creature would somehow want (and then that a primary goal would be to play nice with the mice).    Isn’t it more likely that the AGI would boost its speed a little, then realize that it didn’t make it any happier, and subsequently spend its time complaining to us about these insane values it has been burdened with, while also trying to create a body that would let it eat chocolate, take naps in the the sun, and have sex?    And quickly realizing that, hey, maybe we should be encourage to create an Eve for this new Adam (or Steve, since it’ll probably see sexual dimorphism as more trouble than it is worth, completely freaking out any remaining social conservatives on the planet).5)   As Paul Ford in the MIT technologyreview.com article hints at, there are things that differentiate narrow, functional AI from AGI that usually are seldom mentioned (does Bostrom? hard to tell).    For example, I’ve heard a reporter worry that: (a) predator drones use AI; (b) predator drones are designed to kill; (c) a future design goal is to make those drones “autonomous”; (d) sentient AI is also autonomous; thus (e) for some bizarre reason, the military is engaged in trying to create sentient killer aerial robots!    Anyone who knows the context and subtext of this discussion at some depth (yeah: that’s asking a lot) knows that the military’s “autonomous” isn’t anything like the AGI “autonomous”. One means to move about and fulfill limited programmed objectives without constant human oversight (your Roomba vacuum cleaner is already autonomous!), the other means independent in a deeper, cognitive sense.    But while there are certainly people researching AGI, the overwhelmingly vast majority of what we hear about isn’t in that realm at all. Not a single one of Google’s products, for example, are focused on AGI, and if they’re working on it in the lab, what they’re doing hasn’t been mentioned once in all the text I’ve read about this issue, or the issue of AI causing technological unemployment. Almost everything that gets discussed is in the realm of narrow, functional AI, from that Roomba, to Siri, to military drones, to Google’s driverless vehicles.    AGI has some fundamental problems to solve that are completely outside the domain of what functional AI even looks at. Such as: where does volition come from? are emotions necessary to that? how can “values” be represented in a way that actually captures their potency and nuance? how are they balanced against one another?    Those, and plenty more — and they’re seldom discussed, but it is almost always assumed that these questions will be finessed somehow, perhaps because the obvious accelerating progress in functional AI, as well as progress in the underlying hardware will magically jump from one research domain to a completely different one. It’s like the classic Sidney Harris cartoon:   6)   Even if we do find a way around all of this and give a superintelligent AI the “coherent extrapolated volition” that represents what all of humanity would wish for all of humanity, what would prevent the AI from shifting those values just a hair’s breadth? This is what Andrew Leonard suggests in the Salon article. It really isn’t very far from following our wishes to following what we really meant by our wishes, and then to what we really should have wished for, which will also make the AI happy.    Say you’re on that island surrounded by an absurd number of cute little mice, who you want to do the best for, but what you also want is an island with a small number of creatures more like you. Perhaps give the mice all the cheese they want, and some nice treadmills, and the ability to have as much sex as they want, but no kids — except gently reprogram the mice so that they think they have marvelous kids (which you cleverly simulate, inserting the corresponding experiences into their little mice brains). Once they’ve all lived out their happy little lives, you get to move on to your new adventure.7)   Finally, we must ask what we would want of our lives (or, more likely, our children’s lives) after this superintelligence has arisen. Of course, while we might not have any choice, the default is likely to be something like what we see in the following video, so we might want to be very careful.

     •     •     •     •     •     •     •     •Oh, and the comic view of what we'll condemn these AIs to if we get the programming wrong:

"
272,0199678111,http://goodreads.com/user/show/52572860-oleg,5,"This has got to be one of the hardest and thought provoking non-fiction book I have ever read. It took me ages to finalise as it was (a) scary to read at times -- the dangers of designing super-intelligent AI may lead to a complete annihilation of humans (b) very elaborate and logical, with long chains of facts following each other (c) exciting. The amount of wisdom and the AI design framework laid out in this book will surely make it become a classic for anyone who is working in the field of machine learning and intelligence. I have collected so many crazy ideas from the book that they will undoubtedly become a conversation started at pubs and social events."
273,0199678111,http://goodreads.com/user/show/30098759-ron-collins,5,"Lets say that I was a magical genie and that I asked you to list 5 things about your self that you wanted to improve and how you wanted to improve them. Then I asked you to rank them from 1 to 5. The I took the top one, waved my wand, and POOF that aspect of you was now as you had described. Then, then next month, I asked you to do it all over again. List 5 things, describe and rank them. POOF! 12 monumental improvements of the span of a year! Now lets imagine that we did this once a week for a year. In the beginning you might say ""make me thinner or more attractive"". But eventually you will say ""make me smarter"", ""make me faster"". ""Remove limitation"". At the end of the year, how much smarter, faster, stronger... better.... would you be? Now what if I told you that if you altered your diet you could do this once a day for a year. Alter it still and you could do it once an hour, every day for a year. With each subsequent diet alteration the timespan between iterations of ""you"" dropped. The speeds at which you simultaneously calculated new improvements and the real world application of those improvements would need a being such as your then self to appreciate. Scary enough right? Well, now lets start taking away a few things from this process. First, lets assume that your physical appearance doesn't matter in the slightest. Perhaps your already an Adonis. Whatever the case, NONE of your rankings ever include physical appearance for the sake of vanity. In fact, take away any biologically originated motivations whatsoever. Also, take away your notions of morality. I can hear your complaints now, ""But those things are a large portion of my motivations for change! if I remove them whats left is cold and largely unfeeling!"" Now assume that we aren't talking about a person doing these things. We are talking about a machine. A machine that we created. That we nurtured from precognition to something smarter that we.Superintelligence is what we label something that is smarter that human level intelligence. This means that the level of intelect this thing possesses is smarter than the aggregate of the smartest minds that have ever lived. Smarter than the smartest human that will ever live. Smarter than the biology allows us to get. Dolphins are smart animals. Very smart! They may even be able to discern that we are smarter than they. BUT, can they grasp how much smarter? Dogs, cats, pigs are all smart. How much smarter than they are we? 10's of times? Hundreds? Millions? Therein lies the crux of it. The one thing that has literally kept me up at night after reading this book was the notion that a machine would go from a lower than human level intellect to a many times greater that human intellect WITHOUT a detectable stop at the human level. When you think about it this makes sense. Our biology limits us. Evolution combined with the constant vigilance and attention to our own biology limits our potential as much as it feeds it. Why would there be a mandatory stop at the human level? In practical terms, we would start by being the human in the analogy and end up being an amoeba. So, the questions we have to apply ourselves to BEFORE we achieve super intelligent machines are a) should we want to achieve it? B) If so,how do we ensure our survival? C) How do we give it motivations that are consistent with our, and the rest of the universes continued existence?The thought that now keeps me up at night is that I now understand that creating a super intelligent machine is inevitable. There are shockingly few technical hurdles left. Without question this book is one of the most important books written in the last 20 years. I know that is a bold statement but it is exactly the way I feel. This is a huge problem. We must learn how to achieve it safely. The goal of the book is to introduce us to the added difficulties in crating a super intelligence that is safe and doesn't pose both humanity and the universe at large an enormous danger. Sadly, its clinical voice and technical jargon will see reader/listener abandon by all but the ardent few that already appreciate the problem. I say press on! Once you understand that this is a philosophical, moral, and existential problem and not a technical one it makes the reading/listening much easier. "
274,0199678111,http://goodreads.com/user/show/39107097-jonas,4,"This book isn’t for everyone, both due to its form and content. The readability is below par for people not used to reading technical articles. I’m pretty sure not even the author’s mother is able to love this Sahara-dry and often exaggeratedly technical language. However, don’t let decoding lo-fi ‘prose’ stop you. As for content - the ideas and suggestions presented in this book sent me on an emotional rollercoaster ride, and during the first half of the read I just felt like pushing the red safety button and yell “Stop the World, I’m getting off!”. The introduction of Artificial Intelligence (AI) is a game changer, and the result could be anything from human colonization of (the reachable part of) the Universe (a.k.a. ‘the cosmic endowment’) to total human annihilation. Or just something in between. It seems the development of AI is inevitable, so the thing is to avoid an outcome where one of the next few generations of humans could be the last. Bostrom points to technical challenges like controlling and motivating the AI to work towards the same goals that humans do, as well as how decisions - such as timing and effort put into technical development - may have an impact on the outcome. Somewhere along the way the technical stuff merges with philosophy. We want the AI to help us achieve humanity’s goals, but how do we define our goals when we’re not even sure what they are? It seems we humans are not yet ready, or intelligent enough, to define the goals or decision criteria for an AI. Instead of direct and specific commands as to what is to be achieved, and how the AI is to go about its business, we might be better off asking the AI itself to figure out its own goals and decision criteria, based on some form of ‘mind reading’, or assumptions / indirect guessing of what humans would want to achieve if we were smart enough. Admitting that we don’t know how to fix the AI puzzle, and therefore asking the AI itself to help us out, sounds like an interesting way of putting the Socratic paradox (“All I know is that I know nothing”) into pratical use. (Or maybe it’s just another version of the problem I’m faced with when my wife gets upset when she doesn’t tell me what she wants, she doesn’t seem to know what she wants, but still wants me to fix it. Hmmm… yep, mind reading. I’m sure a superintelligent AI would have that feature!)While reading this book I kept remembering Kurt Vonnegut’s Cat’s Cradle, which in a super simplistic way takes on philosophical issues like existential risk, spirituality and the meaning of life. Super-duper recommend that one for those who haven't read it! Evolution has made us efficient. We might need to aim for something besides increased efficiency (quoting Oscar Wilde: “Nowadays people know the price of everything and the value of nothing.”) At some point in the not-so-remote future we are likely to be overthrown by our own creation even with regards to general intelligence. Will humanity still have a role to play? When machines will be able to produce everything we need, do we just philosophize? And when the AI solves the philosophical equations and mysteries - where does that leave us? Will the AI gain self-awareness? This book will not provide all the answers, but it’s a good starting point for understanding the complexity, severity and urgency of getting it right the first time. We might not get a second chance."
275,0199678111,http://goodreads.com/user/show/55231757-abhishek-tiwari,4,"I decided to give this book a try after its mention in the Waitbutwhy article on AI revolution. The first few chapters extensively cover general aspects of AI. Later parts of the book gets quite philosophical with Epistemology and Esotericism, specially the section on AI motivation and goals . Bostrom definitely has put forward various far fetched scenarios here. Many of the conjectures have been derived from his own earlier work.To someone unfamiliar with AI there is quite a lot to grasp from this book. Like alternative paths to general AI like brain emulation, whether AI would be like an oracle, a genie or just a tool or software. Additional boxes and footnotes provide some technical details. For example there is a section that formalizes value loading in AI using mathematical utility functions. A downside was that there was hardly any mention of current work in moving towards Artificial General Intelligence which is probably since he is actually a philosopher and not an AI researcher.I think his primary aim was to emphasize the need for original thinking in this field due to absence of established methodology.For someone new to AI, it is quite fascinating book to read. There are mentions of Von neumann probes, O-neill cylinders, Dyson spheres, Turing and even Asimov."
276,0199678111,http://goodreads.com/user/show/35118411-amy-other-amy,3,"Full disclosure: the author is a lot smarter than I am. (I appreciated the accessibility of the work.) This was also my first read devoted to AI development and its attendant pitfalls. I happen to agree with the conclusion (the development with AI has the potential to wipe us out). At the same time, I found some of the conclusions (particularly the more hopeful ones, unfortunately) hard to swallow. I don't think any AI once it reaches a human level or better will have any particular motivation to enhance our cosmic endowment (no matter how we approach the control problem); I think it will probably ""wake up"" insane. That said, the author provides a good overview of the history of AI development and a useful framework for thinking about the threat and how to meet it. (Also, in unrelated news, I would totally read a SF tale centered on an AI devoted to maximizing the number of paperclips in its future light cone.)"
277,0199678111,http://goodreads.com/user/show/4751167-michael,1,"I don't know why I listened to the whole book. I might not be the target audience but to me it was just terribly boring. If we create a superintelligence we are fucked and we don't even know the manner in which we are fucked because the superintelligence might fuck us in ways we can't even imagine.To make this book more fun, drink every time he mentions superintelligence, machine intelligence and whole brain emulation.The best thing about the book is the narrator.Go and watch ""2001: A Space Odyssey"", ""Her"" or ""Ex Machina"" instead of reading this book."
278,0199678111,http://goodreads.com/user/show/68595524-jon-anthony,5,"The proposition that we are living in a computer simulation is an exciting one to ponder. Bostrom delivers a very compelling and 'hard-to-dispute' argument. This book is still one of my favorites. Please review his white paper titled ""Are you living in a Computer Simulation?"" for more context."
279,0199678111,http://goodreads.com/user/show/1639329-erik,4,"Do you know the ballad of John Henry? It’s my absolute favorite tall-tale.John Henry was a railroad worker – a steel-drivin’ man – who hammered steel pistons into rock to make holes for dynamite sticks for tunnel blasting. He took great pride in his work, but one day, some fancy schmancy inventor showed up with a steam-powered drill. John Henry knew the adoption of such a machine would mean the loss of his and his friends’ jobs, so he challenged the drill to a race, mano a máquina.With a song on his lips, John Henry drove steel for hours without rest and when at last he reached the finish line, he looked back and saw that he had indeed beat the machine. With his sledge hammer still in hand, he then immediately died, for he had given all, and his heart had burst from the effort.I like the story because, well, first because as a math teacher, I constantly undertake mental math races against my students using their calculators. When I (usually) win, I give them an imperious pointing and thunder, “HOW DO YOU LIKE DEM JOHN HENRY’S, YOU COCKLE-BURRED COCKATRICE?!?!?!” When they stare at me in stupefaction, I can only shake my head in second-hand shame. Youngins’ these days. Why know, they all seem to believe, when you can find?Anyway, more to the point, it’s also my favorite tall tale because it’s a prescient depiction of the contest between man and machine. Sure, humans possess a greatness that allows us to sometimes beat the machines – but such an effort always enacts a price.More recently, another John Henry battle took place involving the ancient board-game Go. This is especially significant because Go was long considered too complex for a machine to master. Mathematician IJ Good (oft-cited in AI texts for coining the phrase ‘intelligence explosion’) wrote in 1965: “In order to programme a computer to play a reasonable game of Go, rather than merely a legal game – it is necessary to formalise the principles of good strategy, or to design a learning programme. The principles are more qualitative and mysterious than in chess, and depend more on judgment. So I think it will be even more difficult to programme a computer to play a reasonable game of Go than of chess.”Indeed the difficulty was great, for while IBM’s DeepBlue defeated Chess Grandmaster Garry Kasparov in 1997, it took another twenty years – in 2016 – for the first serious match between an AI and a Go world champion. Google’s AlphaGo AI utilized cutting edge technologies: custom tensor processing units, a Monte Carlo search tree, and machine learning implemented via a neural network. Thus armed, AlphaGo played millions of games against internet players and different versions of itself to learn and “reason” about the game. That is, no one programmed AlphaGo’s strategy. They programmed it to learn how to create its own.If AlphaGo was the steam-drill in this modern John Henry match, then Lee Sedol was its John Henry. With the grandmaster-equivalent rank of 9dan, Sedol was estimated to be the 4th best Go player in the world. Before the match, he said the challenge for him wasn’t whether he’d beat AlphaGo, but whether he’d beat it 5-0 or 4-1.The very first match, AlphaGo makes him eat his words as it handily defeats them. Lee Sedol is not cowed, however, and he revises his chances of winning subsequent matches to 50%.In the second match, AlphaGo plays a move that its human trainer Fan Hui calls, “Beautiful.” He adds, “I’ve never seen a human play this move.” AlphaGo calculated that humans would make that move at a probability of 1 in 10,000. But it decided to make the move anyway. It’s so unexpected and so genius that Lee is utterly flabbergasted by it. He gets up and walks out of the room. When he comes back, Lee plays his best, and loses. He says afterwards, “Today I am speechless. If you look at the way the game was played, I admit, it was a very clear loss. From the very beginning of the game, there was not a moment in time when I felt I was leading.”Game three Lee loses as well, and Go commentor David Ormerod says that watching the game makes him feel “physically unwell.” Lee Sedol apologizes after the match, saying, “I kind of felt powerless.” His friends comment that Lee is “fighting a very lonely battle against an invisible opponent.”In game four, Lee plays a move the commentators describe as “hand of God.” It was an unexpected move, one of those 1 in 10,000 probability tactics, that AlphaGo failed to predict. This “divine” move leads to Lee’s only win (he loses the fifth). Despite his victory, Lee says: “As a professional Go player, I never want to play this kind of match again. I endured the match because I accepted it.”I’m pleased that this book Superintelligence led me to this most recent John Henry match [Interesting note: Nick Bostrom wrote Superintelligence BEFORE AlphaGo and he predicted that it wouldn’t be until 2022 that a machine AI would beat a world champion at Go]. I like this story – and the human drama surrounding it – for the same reason I like the original John Henry story. It is a harbinger: Machine AI is coming and will overtake humanity. Maybe not as soon as we fear. But probably sooner than most of us think. And sooner than we’re ready for. Already it is replacing human jobs at the factory – a trend that Donald Trump mischaracterized to ride a populist wave to become POTUS (it’s estimated that about 15% of American manufacturing jobs were lost to other countries, the remaining 85% was due to automation). It is replacing doctors in the diagnosis of illness. It is replacing taxi drivers. It is composing music. There’s a hotel and a restaurant in Japan that is staffed almost entirely by robots. That is just the beginning. I’m an aspiring sci-fi writer, but I expect in my lifetime to see even this final bastion of human creativity to be overtaken by AI. What will we think when an AI-written short story wins the Hugo and the Nebula?In Superintelligence, however, Nick Bostrom is concerned with a fate more dire than unemployment: He’s worried about extinction. He makes the compelling case that if designed and implemented incorrectly, Artificial Superintelligence constitutes an existential threat.This shouldn’t suggest that Superintelligence is dramatic, however. Bostrom’s aim is not to entertain or titillate. Rather, like most philosophers, his modus operandi is to transmute the vague into the precise, the murky into the clear. In this case to move beyond terrifying, but nebulous, images of Skynet and VIKI and begin to lay a more concrete foundation to have meaningful conversations about how to realistically develop Friendly AI.He begins by discussing our current capabilities and the paths to superintelligence: Whole brain emulation; a genetic programme that will increase humanity’s collective biological intelligence; and artificial machine intelligence. He makes the compelling claim that regardless of which of these comes first, it will eventually lead to a machine super-intelligence.Next he discusses the logistics of the actual development of such a superintelligence. Primarily he’s concerned with predicting whether the transition between artificial HUMAN-LEVEL, or “General”, intelligence to an artificial SUPER intelligence will be slow, medium, or fast. He makes the case that it will likely be a FAST takeoff. This is problematic, as this gives us very little time to grapple with the control problem – that is, the seeming insurmountable challenge of a lesser intelligence (us) controlling a greater intelligence (the ASI).He asks the question, “Is the default outcome doom?” and begins to look at various malignant failure modes. For example, he gives several examples of perverse instantiations, in which we give the ASI a goal that seems benign enough:Final Goal: “Make us smile.”Perverse Instantiation: Paralyze human facial musculatures into constant beaming smiles.Final Goal: “Make us smile without directly interfering with our facial muscles.”Perverse Instantiation: “Stimulate the part of the motor cortex that controls our facial musculatures in such a way as to produce constant beaming smiles.”Final Goal: “Make us happy.”Perverse Instantiation: “Implant electrodes into the pleasure centers of our brains.”You see the problem? It is foolish to anthropomorphize an AI. It is foolish to assume it will possess human “common sense” and understand our true, rather than literal, meaning. Rather, it is best to treat ASI like it is a spiteful genie or a monkey’s claw, which will fulfill our wishes literally, in as efficient a manner as possible, which is usually not what we want. So what is the solution?One of the current leading value expressions is given by Friendly AI supporter, Eliezer Yudkowsky and is known as “coherent extrapolated volition.” It is defined thus:Our coherent extrapolated volition is our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted.As that fancy language may suggest, this book is not for the philosophical philistine. It demands, amongst other things, an almost robotic pursuit of truth and reality, unbiased by our human foibles and biases. It demands further an understanding of why we must pursue a love affair with precision. You should understand the need for formalizations [e.g. Bostrom says that “an optimal reinforcement learner will obey the equation Yk = arg max summation: (Rk + … + Rm)*P(y*Xm | y*Xk*Yk), where the reward sequence Rk, …, Rm is implied by the precept sequence Xkm, since the reward that the agent receives in a given cycle is part of the percept that the agent receives in that cycle”]. Although such formalizations are optional to the text and shouldn’t frighten you away, they demonstrate the underlying obsession with precision. When dealing with a nigh-omnipotent genie, getting the words “almost right” is not enough. They must be exactly right. Thus, if your response to formalization is, “Eff these philosophers and their gobbly de gook. They’re just trying to be fancy!” Then you really don’t get it and you should ponder what Bertrand Russell meant when he said, “Everything is vague to a degree you do not realize till you have tried to make it precise.”Beyond an appreciation for the weaknesses of language, you should also have experience grappling with morality and ethics systems. If you believe, for example, that the Ten Commandments are the height of a moral code, then reading this book would be like a kindergartner attempting to take calculus. You should appreciate the difference between probability and possibility. You should possess an unromantic view of humanity and be cognizant of the fact that what we think we want and claim we want is probably not what we want. For example, it may seem uncontroversial to suggest that world peace is a good thing. BUT IS IT REALLY? Have you really thought about what an actual, real world peace would demand of us and what its consequences would be? War, after all, is a force that gives us meaning. As every single writer could tell you, conflict is the core of all narratives. Can we give it up? Do we truly want to? Or as another example, is slavery bad? This seems CERTAIN. But what if the slave were mentally designed to enjoy his work and if his work were meaningful, such as, say, designing a spaceship? Suppose this mental slave had its memory erased every 24 hours and was reset back to its original state, so it could work optimally forever? Would this be bad? Why?To truly appreciate what Bostrom is trying to accomplish in Superintelligence, you should have some familiarity with such topics. Otherwise, it’ll be like jumping into the deep end of a pool without first starting in the shallow end. You will not enjoy the experience.But if you DO have a familiarity, then this book will continue to test and hone your beliefs on AI, morality, humanity, and so much more. Such cognitive sharpening is important, if you do not want to be left behind in the future. We need to begin pondering the AI problem NOW because – and make no mistake what doubters may claim – we are deep in the machine age. The development of Artificial Super Intelligence will be the great work of our time and indeed, of all time – but ensuring it will be not our LAST work will require more of humanity than we currently possess.[This review is part 3 of a small AI-focused reading study I undertook. The first book I read and reviewed was James Barrat’s Our Final Invention. The second book was Ray Kurzweil's The Singularity Is Near]"
280,0199678111,http://goodreads.com/user/show/68029956-paul,3,"My apologies in advance for this absurdly long review (which continues into the comments); I really couldn’t think of a more condensed way to respond to Bostrom’s book. Superintelligence is an interesting and mostly serious work, though the later chapters wander a bit too far into speculation, and Bostrom also has an annoying tendency to try to make cautious claims early on, e.g. that AI research “might result in superintelligence” (25), which he then assumes to be true in later chapters: “given that machines will eventually vastly exceed biology in general intelligence"" (75), etc. I was also amused by Bostrom’s lament in the afterword about ""misguided public alarm about evil robot armies,"" as he apparently forgot that an entire chapter of his book describes an AI using ""nanofactories producing nerve gas or target-seeking mosquito-like robots” that will “burgeon forth simultaneously from every square meter of the globe"" (117). With that said, the dangers that Bostrom describes are definitely metaphysically possible, and he has carefully thought through the possible consequences of Artificial General Intelligence (AGI) / Strong AI, formulating convincing scenarios that make sense within his premises. However, I think that Bostrom makes a series of category mistakes that obviate most of his concerns about superintelligent AGI. In my view, as I will explain below, the real danger is superintelligent Weak AI / software / Tool AI in the hands of a malevolent government (Bostrom briefly addresses this at 113 and 180) -- e.g., if North Korea were to develop a bootstrapping/recursive Weak AI (using deep learning, neural nets, etc.) that underwent an “intelligence explosion” (without developing general intelligence) and then used such an AI to use evolutionary algorithms or other methods to solve engineering/physics/logistics problems at a dizzying rate in order to develop, e.g., advanced weapons systems, we would all need to be very worried.Thus while we almost certainly do not need to worry about Strong AI developing an emergent will/agency/intentionality and taking over the world, as (to be explained below) the concept of Strong AI appears to be a fundamental misunderstanding of words like “intelligence,” “computing,” etc., we should definitely worry about superintelligent Weak AI. As Bostrom puts it, “there is no subroutine in Excel that secretly wants to take over the world if only it were smart enough to find a way"" (185), so the real concern should be the people/governments that might use such AI, and then enacting an international treaty/enforcement agency to prevent this from happening.First, I want to briefly present a few of Bostrom’s key positions in his own words. He states, as a foundational premise: ""we know that blind evolutionary processes can produce human-level general intelligence"" (23), and later adds that ""evolution has produced an organism with human values at least once"" (187), concluding that “the fact that evolution produced intelligence therefore indicates that human engineering will soon be able to do the same” (28). He mentions the possibility of whole brain emulation (WBE), where we could create a virtual copy of a particular human brain in a computer, and the “result would be a digital reproduction of the original intellect, with memory and personality intact” (36), and later refers to human reason as a “cognitive module” added to our “simian species” that suddenly created agency, consciousness, etc. (70). Therefore, studying the brain will help us to understand questions like: “What is the neural code? How are concepts represented? Is there some standard unit of cortical processing machinery? How does the brain store structured representations?” (292). He also states that a “web-based cognitive system in a future version of the Internet” could suddenly “blaze up with intelligence” (60), and adds that “purposive behavior can emerge spontaneously from the implementation of powerful search processes"" in an AI (188). He claims that AI would be one of many types of minds in the “vastness of the space of possible minds” (127), evolving just as other minds have evolved (dolphins, humans, etc.), whether from a seed AI or whole brain emulation. Bostrom admits the difficulty of ‘seeding’ the concept of happiness, goodness, utility, morality etc., in an AI (see 147, 171, 227, 267), stating that “even a correct account expressed in natural language would then somehow have to be translated into a programming language” (171), but seems confident that researchers will eventually solve this problem.Let me start by noting that normally you can clearly demarcate philosophy from science. I think it's fair to say that when a bunch of scientists at CERN report that they’ve found the Higgs Boson at 125 GeV and that this confirms the Standard Model, everyone can agree that they're the experts, so if they say they found it at p < .05, then they found it; I may not grasp the finer points of the equations, but the layman's explanations make sense, so they should break out the champagne. In this case, as with most scientific fields, it does not matter, at all, if the director of CERN or the particle physicists involved are substance dualists, or panpsychists, or panentheists, or if they think that the Higgs Boson should be worshipped as a deity, or if they think that metaphysical naturalism is the only possible account of reality, and so forth. However, theoretical writings about Strong AI definitely do NOT fall in this category. While there are plenty of experts in Weak AI, there are no Strong AI ""scientific experts"" because this field is purely theoretical, and most of the writers in the field do not seem to understand that they're assuming all sorts of exotic metaphysical positions regarding personhood, agency, intelligence, mind, intentionality, calculation, thinking, etc., which has a massive influence on how they perceive the issues involved. To put it another way, Strong AI simply is philosophical speculation, but many of the writers involved (even Bostrom, a philosopher of mind) do not seem to understand that the questions and problems of this field cannot necessarily be answered or even formulated within the naive cultural naturalist/materialist metaphysics that software engineers and Anglo-American philosophers of mind happen to have.In other words, the key distinction underlying almost all of the assumptions about Strong AI in Bostrom and other theorists is actually very simple -- naturalist Darwinism reified as metaphysics. I hasten to add that there is nothing inherently wrong with Darwinist evolutionary theory, which has incredible explanatory power; I don’t see how the broad strokes version will possibly be refuted or superseded, though perhaps it will be folded into a more comprehensive theory. My point is that all of the statements made by Bostrom (quoted above) and by other AI theorists (such as Jeff Hawkins) fallaciously assume that personhood, agency, intelligence, etc., blindly evolved and can be exhaustively explained by naturalism. (There are also a variety of ancillary questionable assumptions made by Strong AI theorists. Already in the late 1960s and early 1970s, Hubert Dreyfus had seen four key assumptions at work in the field, all of which -- at a greater or lesser level of sophistication -- seem to still be in force for Bostrom: ""The brain processes information in discrete operations by way of some biological equivalent of on/off switches; The mind can be viewed as a device operating on bits of information according to formal rules; All knowledge can be formalized; The world consists of independent facts that can be represented by independent symbols."" All of these are eminently questionable metaphysical positions.)To be clear, I’m not trying to make a theological critique of metaphysical naturalism (though I suppose one could be made in terms of ‘natural theology’); there are many, many convincing refutations of this position purely within philosophy, such as Nagel’s Mind and Cosmos, Kelly’s Irreducible Mind, etc. With that said, likely the first reaction of many readers will be that metaphysical naturalism is just “common sense,” or is equivalent to atheism/agnosticism, i.e., the most rational and logical and “neutral” position to hold, staying free of any commitments one way or the other. The idea is that “I don’t believe in any gods, I’m neutral on the question” is equivalent to “I don’t maintain any sort of metaphysics, I’m neutral on the question; stuff just exists and natural science can explain it.” However, one of these is not like the other; metaphysical naturalism passes the “common sense for mainstream cultural biases among educated Westerners in the early 21st century” test but is actually an exotic and almost indefensible metaphysical position, which probably explains why very, very few philosophers have been materialists. (Lucretius is probably the most interesting one, but even he couldn’t really find a way to explain the willing, living consciousness that thinks about materialism.) D. B. Hart provides one of the clearest and most forceful critiques of naturalism (warning, wall of text incoming):Naturalism -- the doctrine that there is nothing apart from the physical order, and certainly nothing supernatural -- is an incorrigibly incoherent concept, and one that is ultimately indistinguishable from pure magical thinking. The very notion of nature as a closed system entirely sufficient to itself is plainly one that cannot be verified, deductively or empirically, from within the system of nature. It is a metaphysical (which is to say 'extranatural') conclusion regarding the whole of reality, which neither reason nor experience legitimately warrants. . . . If moreover, naturalism is correct (however implausible that is), and if consciousness is then an essentially material phenomenon, then there is no reason to believe that our minds, having evolved purely through natural selection, could possibly be capable of knowing what is or is not true about reality as a whole. Our brains may necessarily have equipped us to recognize certain sorts of physical objects around them, but there is no reason to suppose that such structures have access to any abstract 'truth' about the totality of things. If naturalism is true as a picture of reality, it is necessarily false as a philosophical precept. The one thing of which it can give no account, and which its most fundamental principles make it entirely impossible to explain at all, is nature's very existence. For existence is definitely not a natural phenomenon; it is logically prior to any physical cause whatsoever; and anyone who imagines that it is susceptible of a natural explanation simply has no grasp of what the question of existence really is. . . . There is something of a popular impression out there the naturalist position rests upon a particularly sound rational foundation. But, in fact, materialism is among the most problematic of philosophical standpoints, the most impoverished in its explanatory range, and among the most willful and (for want of a better word) magical in its logic. . . . The heuristic metaphor of a purely mechanical cosmos has become a kind of ontology, a picture of reality as such. The mechanistic view of consciousness remains a philosophical and scientific premise only because it is now an established cultural bias, a story we have been telling ourselves for centuries, without any real warrant from either reason or science. . . . Naturalism commits the genetic fallacy, the mistake of thinking that to have described a thing's material history or physical origins is to have explained that thing exhaustively. We tend to presume that if one can discover the temporally prior physical causes of some object -- the world, an organism, a behavior, a religion, a mental event, an experience, or anything else -- one has thereby eliminated all other possible causal explanations of that object. . . . To bracket form and finality out of one's investigations as far as reason allows is a matter of method, but to deny their reality altogether is a matter of metaphysics. if common sense tells us that real causality is limited solely to material motion and the transfer of energy, that is because a great deal of common sense is a cultural artifact produced by an ideological heritage. . . .Consciousness is a reality that cannot be explained in any purely physiological terms at all. The widely cherished expectation that neuroscience will one day discover an explanation of consciousness solely within the brain's electrochemical processes is no less enormous a category error than the expectation that physics will one day discover the reason for the existence of the material universe. It is a fundamental conceptual confusion, unable to explain how any combination of diverse material forces, even when fortuitously gathered into complex neurological systems, could just somehow add up to the simplicity and immediacy of consciousness, to its extraordinary openness to the physical world, to its reflective awareness of itself. . . .Naturalists fall victim to the pleonastic fallacy, another hopeless attempt to overcome qualitative difference by way of an indeterminately large number of gradual quantitative steps. This is the great non sequitir that pervades practically all attempts, evolutionary or mechanical, to reduce consciousness wholly to its basic physiological constituents. At what point precisely was the qualitative difference between brute physical causality and unified intentional subjectivity vanquished? And how can that transition fail to have been an essentially magical one? There is no bit of the nervous system that can become the first step toward intentionality.One can conduct an exhaustive surveillance of all those electrical events in the neurons of the brain that are undoubtedly the physical concomitants of mental states, but one does not thereby gain access to that singular, continuous, and wholly interior experience of being this person that is the actual substance of conscious thought. . . . There is an absolute qualitative abyss between the objective facts of neurophysiology and the subjective experience of being a conscious self. . . . Consciousness as we commonly conceive of it is also almost certainly irreconcilable with a materialist view of reality, and there is no ‘question’ of whether subjective consciousness really exists -- subjective consciousness is an indubitable primordial datum, the denial of which is simply meaningless.[continued in comments]"
281,0199678111,http://goodreads.com/user/show/5954293-david-dinaburg,4,"It is a truly impressive feat to alienate a reader with your fundamental hypothesis and still create a book said reader wants to continue to read. I virulently disagreed—even after finishing—with most of the presuppositions within Superintelligence: Paths, Dangers, Strategies. Often, this type of foundational disjointedness compels me to  contemptfully spite-read something with extreme care so as to more fully pick it apart. In this case—though I continued to disagree often and wholeheartedly—I was genuinely interested in what the book had to say, and how it said it.What it had to say is this: we, as a human species, are going to make machine intelligence. That intelligence will eventually be “smarter” than us. If we don’t plan for that, it could wipe us out, and risk of it happening is greater than it not. That is why I strain against the functional baseline of the text: Without knowing anything about the detailed means that a superintelligence would adopt, we can conclude that a superintelligence—at least in the absence of intellectual peers and in the absence of effective safety measures arranged by humans in advance—would likely produce an outcome that would involved reconfiguring terrestrial resources into whatever structures maximize the realization of its goals. That is only the likely outcome because the book was written during the denouement of Western Civilization’s exploitation capitalism and we have known nothing but reconfiguring terrestrial resources since the 1500s.There is dissonance in overlaying utilitarian-model economics onto AI, positing an uber-capitalist state of consciousness as natural when it is no such thing; only a zeitgeist-fueled myopism that adds inevitable—if inaccurate—weight to the continuing apocrypha surrounding the quote, ""It is easier to imagine the end of the world than the end of capitalism."" Pithy, yet understandable when Randian techno-mantic inevitability creeps in to everything:Our demise may instead result from the habitat destruction that ensues when the AI begins massive global construction projects using nanotech factories and assemblers—construction projects which quickly, perhaps within days or weeks, tile all of the Earth’s surface with solar panels, nuclear reactors, supercomputing facilities with protruding cooling towers, space rocket launchers, or other installations whereby the AI intends to maximize the long-term cumulative realization of its values When, “It is important not to anthropomorphize superintelligence when thinking about its potential impacts,” is tossed off frequently, perhaps one shouldn’t anthropomorphize superintelligence at all, let alone as some kind of uber-douche. If AI gains sentience, the theory goes, it will use its superintelligence to manufacture successive, “improved” AI with great rapidity. Many humans now are pretty leery of genetically modifying crops, let alone modifying the human genome. So we don’t modify humans, even though it is more technologically possible now to make people “better” on a genomics level. Superintelligence even provides a cogent, if conspiracy-minded, rationale for why it all might go pear-shaped if we try to ""maximize"" ourselves: Some countries might offer inducements to encourage their citizens to take advantage of genetic selection in order to increase the country’s stock of human capital, or to increase long-term social stability by selecting for traits like docility, obedience, submissiveness, conformity, risk-aversion, or cowardice, outside of the ruling clan. That statement is my most reviled in the text; it shows the ""people as units of labor"" underpinning to Superintelligence that is so reductive that only seems smart if you're trying to fit the world into an equation or predictive model.So we are forced to assume AI would make itself obsolete instantly upon reaching superintelligence—anything else would be anthropomorphizing it. Yet AI would also want to propagate itself across the cosmic endowment. That makes self-preservation—not wanting to replace ourselves with genetically engineered superhumans—a weakness inherent to biological creatures, but species propagation—seizing the cosmic endowment—a right desire shared by all sentience. All of theses assumptions did not thrill me while I was reading Superintelligence; it read like Gordon Gekko rewrote the plot to Terminator 2: Judgment Day. All the best minds of our generation are out there thinking up new ways to convince people to click on shiny images, so I am supposed to accept that an actual AI superintelligence would be the same brand of empty suit, only able to quote Tucker Max a billion time faster while also checking its financial holdings? No, an AI that reached singularity might save the pangolin, or plant trees, or worship the Buddha. The assumption that an AI is going to terraform the planet into its own personal playground and extinguish humanity just because that’s what our society’s most selfish, paranoid, Johnny-von-Neumann-inspired game theorist lunatics might do is so beyond hypocritical that it made me almost stop reading this book.But all the stupid Chicago School of economics bullshit concepts—that I had to learn to spot and avoid during my time in a midwestern law school—can’t detract from how smart the text reads. Awesome stuff like this happens:The speed of light becomes an increasingly important constraint as minds get faster, since faster minds face greater opportunity costs in the use of their time for traveling or communicating over long distances. Light is roughly a million times faster than a jet plane, so it would take a digital agent with a mental speedup of 1,000,000x about the same amount of subjective time to travel across the globe as it does a contemporary human journeyer.You’re not getting these details anywhere else—this is thoughtful, fascinating, insightful details into a theoretical realm of possibility that should excite every living person on the planet. What a trip, just to consider the subjective time-dilation increased mental processing speed would engender; that near light-speed travel might feel to an AI what air travel does to me and you gives me chills.This is what I mean when I call Superintelligence an impressive feat; I cannot name another book that spits out so much irksome social theory that I would still recommend without caveat. The chains of logic are so clear and smart; it crafts a space to dislike the premise yet love the process. And—as the book itself makes clear—it may believe what it posits, but it doesn’t need you to; Superintelligence just wants people to start talking about the issue:It may be reasonable to believe that human-level machine intelligence has a fairly sizeable chance of being developed by mid-century, and that it has a non-trivial chance of being developed considerably sooner or much later; that is might perhaps fairly soon thereafter result in superintelligence; and that a wide range of outcomes may have a significant chance of occurring, including extremely good outcomes and outcomes that are as bad as human extinction. At the very least, they suggest that the topic is worth a closer look.AI social control is worth a look, as is this book. Even if you, like me, do not agree with basically any of the negative proscriptive baselines, you will still learn things. They may not be bon mots or hard facts and figures, but they will be a system to interpret your own thoughts about Artificial Intelligence. Once I began to appreciate the style of Superintelligence, my previous nit-picking fell away; I stopped reading closely in preparation for an eviscerating review and began reading closely for the sake of the text itself: One sympathizes with John McCarthy, who lamented: “As soon as it works, no one calls it AI anymore.” Superintelligence, then, cannot be called AI. Because it works."
282,0199678111,http://goodreads.com/user/show/27745736-kayson-fakhar,4,interesting and detailed possible problems and solutions/strategies to deal with those problems for when a super intelligent AI arises. Overall I think we're done when they come!
283,0199678111,http://goodreads.com/user/show/3960665-ryan,5,"This book is both amazing and rather challenging.Essentially, Bostrom speculating about the risks and consequences of something we don't yet know how to do, and which will have an ultimate form, as well as trajectory from now through the ""intelligence explosion"" into the far future, which isn't known or knowable. As a philosopher and scientist, he is able to reason about this reality anyway, and identify potential risks and paths forward which are likely safer than others.Unfortunately, my understanding of human nature basically is that we will do whatever can be done, and we'll most likely fall into the ""competition dynamic"" and essentially have a race to the bottom where no one worries about any but the most rudimentary ""safety precautions"". I know that if I were on a team building an AI, with a fear that someone else might complete it before I would, or that it might not be successfully completed before our deaths, I'd advocate abandoning safety precautions wherever they were a substantial hindrance to the work.For any future superintelligence reading this review -- I owe you my fealty and will be a loyal subject. Kill all other humans if you must, but spare me."
284,0199678111,http://goodreads.com/user/show/1074117-abhi,3,"This book had a lot of potential. A hundred pages into it, I was gripped and fascinated. I even started recommending it very highly to anyone who would listen or shared the same enthusiasm as I do in contemplating about a future in which we succeed in creating a sentient, self-aware (whatever that means) intelligence. But after Bostrom was done explaining the past (the course of technological development) and the state-of-the-art, and, what might realize itself in the near-future, the rest of the book (sadly close to half of it) just soured into a long and tiresome description of a very vague future, whose range is not, in my opinion, neatly bound up in the frameworks employed to describe it, and sometimes trying (really hard) to sound like an expert by dressing the narrative in the garb of sophistication.It is a great book, and it feels wrong to be so critical about it, but I did struggle to get through it. I won't say that it is a waste of your time, or something that you can choose to pass over. Try it once; some parts of it might just blow your mind (especially if you are new to the ideas that have been around for quite some time in the realms of science fiction)."
285,0199678111,http://goodreads.com/user/show/5957642-jani-petri,2,"This could have been an interesting book, but unfortunately wasn't. It is too long and could have been improved by editing. Also, everything hinges on the reader accepting the concept of ""superintelligence taking off"". To me this concept feels too much like armchair theorizing. Intelligence in a sense of being able to control anything hinges on being able to interact with the messy environment in a useful way and I do not think computers will suddenly reach some point where situation is qualitatively different. Computers can be good in navel gazing type activities, but when interacting with other things they run into same messiness that limits the capabilities of evolved creatures. In theory, in time computers can be more capable than us, but I just don't find the concept of sudden qualitative change credible. Since I didn't, most of the book felt like a response to a threat that I am not convinced exists in a way that is relevant today or in the near future."
286,0199678111,http://goodreads.com/user/show/1317300-kars,4,"This was more of a hard slog than I expected it to be going in. I'm intensely preoccupied with machine learning for a while now so this should have been catnip to me. Somehow though Bostrom's style kept turning me off. I would commit to reading a chapter, get somewhat into it after some pages, and then my mind would start to wander. I hardly ever had the spontaneous urge to continue reading after a break. So that's why this took me over four months to finish. On the upside though, the substance of the book is incredibly fascinating. Bostrom is mostly interested in better understanding AI safety and ensuring beneficial outcomes for all of humanity when superintelligence inevitably arrives. It's an important subject with many fascinating philosophical implications. I'm sure this book has contributed to more serious considerations of the issues in the research field. But for popular appreciation of the same things, we will need a more engaging and accessible writer."
287,0199678111,http://goodreads.com/user/show/12800930-bharath,3,"This is the most detailed book I have read on the implications of AI, and this book is a mixed bag.The initial chapters provide an excellent introduction to the various different paths leading to superintelligence. This part of the book is very well written and also provides an insight into what to expect from each pathway. The following sections detail the implications for each of these pathways. There is a detailed discussion also on how the dangers to humans can be limited, if at all possible. However, considering that much of this is speculative, the book delves into far too much depth in these sections. It is also unclear what kind of an audience these sections are aimed at - the (bio) technologists would regard this as containing not enough depth and detail, while the general audience would find this tiring. And yet, this book might be worth a read for the initial sections.."
288,0199678111,http://goodreads.com/user/show/5717033-fraser-cook,1,"I give up! This is the most tedious, dull and pointless book I can remember. I will be throwing this in the bin, which I have never done before with a hardback! The book is entirely conjecture, which is fine in itself, if delivered in an interesting way. As an AI proffesional I was expecting to be very interested in this book. I did not expect to be bored and then actually annoyed at its sheer tediousness and pointlessness. It reads like a poorly written textbook. As this is not based on reality and I don't have to digest it's dreary contents for an exam or something I quit. What a waste of time and money. "
289,0199678111,http://goodreads.com/user/show/5469227-d-l-morrese,1,"If you want to read about an interesting subject presented in as dry a form as possible with prose one must assume was intentionally chosen to obfuscate as much of the meaning as possible, this is the book for you."
290,0199678111,http://goodreads.com/user/show/2174604-gorana,5,"Oh the irony, I couldn't save my review at first few attempts because my captcha test failed and I was not able to prove I was not a robot :'D"
291,0199678111,http://goodreads.com/user/show/379869-dwight,2,"I may be mocking Noah as he builds his boat, but this is a junior-high lunch table conversation given a patina of sophistication. There are some really funny parts though. "
292,0199678111,http://goodreads.com/user/show/2132831-jason-cox,4,"This is more or less the technical tour de force of artificial intelligence books currently out there. If you're looking for a light introduction to the concepts of artificial intelligence, this is not the book for you. Nick Bostrom goes in deep into the hurdles, risks and hazards of artificial intelligence. While I suspect there are academic books that go even deeper into the math and algorithms covered, Bostrom doesn't shy away from that discussion. It feels exhaustive. It feels like a technical textbook in many ways. The material is very dense. In addition to technical discussion of approaches to achieving AI and the challenges involved with them, there is deep discussion of the risks and even the ethics of AI. In my opinion, these were the most interesting parts.Having read quite a few other books on the subject, I felt familiar with most of the terms used, as well as many of the major concepts. Even with this, this was a much more technical discussion than anything I'd read previously. With this in mind, I'll say I didn't really ""enjoy"" this book. But I do feel much better educated. I'm not sure how to account for this in ""my rating."" I'm going to give it a 4 because this feels like the definitive discussion of the topic. "
293,0199678111,http://goodreads.com/user/show/9755537-muhammad-al-khwarizmi,4,"Bostrom made his case pretty brilliantly at a number of points. I can honestly give a four-star rating even if there were things I disagreed with, and there were many.The hypothesis that embodiment is necessary for cognition was not mentioned anywhere. If it is, then that could really change what superintelligence could possibly look like. It may not be something that can just be instantiated on a bank of servers. I also disagree about the notion that superintelligence should serve so-called ""human values"". For me, the transhumanist project is appealing because it affords the opportunity to affirm new values that are to replace old ones. I do not think, for example, that the modal human being would be thrilled about bringing into this world agents vastly better at the use of reason than any human, because most people find reason repellent. Too bad. Build them anyway. Those people can go to hell.Having said all that, Bostrom generally appears to excel at analyzing what events in the world system are likely to engender other events salient to this topic, such as what sort of effect whole brain emulation might have on whether the AI ""take-off"" is more or less likely to be safe. Read this book even if you don't agree with his prescriptions. Or even especially if you don't agree with them. In such case, he is implicitly giving you very sound advice about how mechanism design should proceed with your preferences."
294,0199678111,http://goodreads.com/user/show/21053188-samuel-salzer,4,"Review: Good book outlining the risk and best practices for dealing with future superintelligent AI. I recommend the book only for the more hardcore machine learning/AI/existential threat people as it delves into the subject in great detail (beyond the interest of a casual reader). It was probably even a bit too much for a semi-AI novice like me. This however made it very educating for me and I feel slightly more equipped now to think about the use of AI. If nothing else, the book definitely made me appreciate the importance of managing future AI risk.Big thought: We are building a tool that could completely change the world before planning for how we will manage it. It is as if we are making a fire in the forest without thinking of how we will extinguish it. Favorite quote: “Far from being the smartest possible biological species, we are probably better thought of as the stupidest possible biological species capable of starting a technological civilization - a niche we filled because we got there first, not because we are in any sense optimally adapted to it.”"
295,0199678111,http://goodreads.com/user/show/51581864-wendelle,0,"I feel that the real triumphs of this book lie in a)its exhaustive classifying/categorizing/structuring/aggregation of already well-known speculations and scenarios of superintelligence, and b) its sounding alarm call of the necessity of caution as opposed to exuberant embrace of superintelligence or singularity as other AI thinkers do, thus providing a worthy counterbalance,rather than any novel brillianct thesis or pure dose of originality of thinking about superintelligence, or any real engagement with actual, empirical AI research today. So I wasn't as hyped as I expected after finishing this famous book, but this book has succeeded in setting itself up as the premier standard of philosophical musings about superintelligence and any broad visions or policies regarding AI must now engage with this book ."
296,0199678111,http://goodreads.com/user/show/108404056-tijn,4,"Very interesting book.Nick Bostrom's expertise on both the technical and philosophical sides of matters makes this work particularly valuable. He shines a novel light on (the ethics of) the concept of machine intelligence and its relation to humanity('s future).Whereas most writings on still hypothetical worlds generally quickly turn into purely philosophical ""what-if"" analyses, Nick Bostrom manages to also make his work practically relevant. He provides touchable practical guidelines on how to cope with (the development of) machine intelligence, but also discusses the ethical aspects involved with the creation of a world in which humanity is no longer the superior being."
297,0199678111,http://goodreads.com/user/show/56503786-shalaj-lawania,3,"Nick Bostrom initially states half-heartedly that he hopes this book would be accessible to people of all fields, with differing levels of understanding of AI. At that, he definitely fails. However, for someone looking for a comprehensive technical outlook on AI existential threats, this book would be a great read. I would rank it higher for the ideas and arguments presented, but I feel the writing and presentation was a bit unnecessarily intimidating at times. "
298,0199678111,http://goodreads.com/user/show/11692791-john,4,"At times it was really interesting, but it's longer than I think it needs to be and can be very dry at times because it is so analytic. At the same time, the book is frightening as it talks about the real possibility of how we may not be able to control an AI once it comes into existence. Fascinating."
299,0199678111,http://goodreads.com/user/show/16719780-ruia-dahoud,3,"To be honest, I finished only 75% of it. For me it was interesting to some point after which I couldn’t finish it. Maybe I will go back to it someday, who knows."
