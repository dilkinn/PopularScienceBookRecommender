,isbn,user_link,ranking,review
0,0804136696,http://goodreads.com/user/show/16546933-yannick-serres,4,"During the first hundred pages, I was sure to give the book a perfect score. It totally caught my attention and made me want more and more. The book made me feel like it had been written for me, someone that don't know much about predictions and forecasts, but feels like he could be good at it.Then, after the half of the book, you get a little bored because it always come back to the same thing: Use number to make your predictions in a well established timeframe, always question your predictions till the time runs out, learn from the past and see beyond your conic vision.This book is very interesting and worth giving a shot. It's a good mix of science and history, but you still feel like you're reading a novel. I was expecting nothing from this book and got quite a fun at reading it. I've been positively surprised and hope you'll be too.I got to thank Philip E. Tetlock and Random House of Canada for this book I received through Goodreads giveaways."
1,0804136696,http://goodreads.com/user/show/155663-david,5,"Philip Tetlock is a professor at the University of Pennsylvania. He is a co-leader of the Good Judgment Project, a long-term forecasting study. It is a fascinating project whose purpose is to improve the accuracy of forecasts. You can learn more about the project on theGood Judgment website. In this book you can learn the basics of how to make accurate forecasts in the face of uncertainty and incomplete facts.An amazing tournament was held, which pitted amateur volunteers in the Good Judgment Project with the best analysts at IARPA (Intelligence Advanced Research Projects Agency). The amateurs with the best records for accuracy are termed ""superforecasters"". They performed 30% better than the professional analysts, who had access to classified information. This was not a simple tournament. It was held over a long period of time, enough time to allow a good amount of research and thinking and discussions among team members. It involved hundreds of questions. These questions were asked in a precise, quantitative way, with definite time frames. And besides giving predictions, players in the tournament estimated their confidence levels in each of their predictions. Their forecasts, along with their estimated confidence levels went into the final scores.So, what are the qualities of a good superforecaster? Perhaps the dominant trait is active open-mindedness. They do not hold onto beliefs when evidence is brought against them. They all have an intellectual humility; they realize that reality is very complex. Superforecasters are almost all highly numerate people. They do not use sophisticated mathematical models, but they understand probability and confidence levels. Superforecasters intuitively apply Bayes theorem, without explicitly using the formula quantitatively. They care about their reputations, but their self esteem stakes are less than those of career CIA analysts and reputable pundits. So, when new evidence develops, they are more likely to update their forecasts. Superforecasters update their forecasts often, in small increments of probability.The book discusses the movie, Zero Dark Thirty, about the military assault on the compound in Pakistan, where Osama bin Laden was hiding. The character playing Leon Panetta railed against all the different opinions of the intelligence analysts. But the real Leon Panetta understood the differences in opinions, and welcomed them. He understood that analysts do not all think alike, they have diverse perspectives, and this helps to make the ""wisdom of the crowd"" more accurate overall. It was found that teams score 23% better than individuals.The book dispells the myth that during World War II, German soldiers unquestioningly followed orders, while Americans took the initiative and improvised. The truth, especially in the early phases of the war, was often exactly the opposite. The Germans followed a philosophy that military orders should tell leaders what to do, but not how to do it. American leaders were given very detailed orders that removed initiative, creativity, and improvisation. The author deliberately chose this example to make us squirm. One should always keep in mind, that even an evil, vicious, immoral enemy can be competent. Never underestimate your adversary. This is difficult in practice; even superforecasters can conflate facts and values. Nowadays, the military has radically changed. The military encourages initiative and improvisation. However, corporations are much more focused on command and control. Their hierarchical structure tends to micro-manage. In fact, some corporations have hired ex-military officers to advise company executives to worry less about status, and instead to empower their employees.An appendix at the end of the book is a list of the Ten Commandments for superforecasting. These are useful generalities for successful forecasting. But even here, the authors are intellectually humble; their last commandment is not always to treat all the commandments as commandments!This is a fascinating, engaging book, about a subject I had never thought much about. The book is easy reading, filled with lots of anecdotes and interesting examples. The authors rely quite a bit on the wisdom of behavioral economists, Daniel Kahneman and Amos Twersky. They have given a lot of thought to the subject of forecasting, and it really shows."
2,0804136696,http://goodreads.com/user/show/20923295-anton,5,"5⭐️ - What a great book! It will definitely appeal to the fans of Thinking, Fast and Slow, Predictably Irrational: The Hidden Forces That Shape Our Decisions and The Black Swan: The Impact of the Highly Improbable.Thought-provoking and full of very perceptive observations. But I particularly would like to commend authors for how well this book is written. This is an example of non-fiction at its best. There is definitely research and background science overview but each chapter is a proper story as well. Philip E. Tetlock and/or his co-author (not sure who should take the credit) are superb storytellers! It was not only insightful but genuinely enjoyable to read this book.I usually read several books simultaneously one or two non-fiction titles and a bunch of fiction stories. But last week 'Superforecasting' monopolised my reading time. And it is particularly telling how well it managed to trample competition from its fiction 'rivals'.It goes straight to my absolute best non-fiction shelf. I recommend it strongly to all curious about the psychology of decision making and an ability of our mind to cope the uncertainty."
3,0804136696,http://goodreads.com/user/show/7958239-elizabeth-theiss,5,"When it comes to forecasting, most pundits and professionals do little better than chimps with dartboards, according to Phillip Tetlock, who ought to know because he has spent a good deal of his life keeping track. Tetlock has partnered with Dan Gardner, an excellent science journalist, to write this engaging book about the 2 percent of forecasters who manage to consistently outperform their peers. Oddly, consumers of forecasts generally do not require evidence of accuracy. Few television networks or web sites score the accuracy of forecasts. Years ago, as a stockbroker I gave very little weight to the forecasts of my firm's experts; the stocks they recommended were as likely to go down as they were to go up. Today, as an occasional television pundit, I'm often asked to forecast electoral outcomes, so I was very curious about Tetlock's 2 percent that managed ""superforecasting.""""How predictable something is depends on what we are trying to predict, how far into the future, and under what circumstances,"" according to Tetlock and Gardner. It makes no sense to try to predict the economy ten years from now, for example. But he wanted to understand how the best forecasters manage to maintain accuracy over the course of many predictions. In order to find out, he launched the Good Judgement Project, which involved 2800 volunteer forecasters who worked on a series of prediction problems over several years. After the first year, he identified the best forecasters and put them on teams to answer questions like whether Arafat was poisoned by polonium, whether WMDs were in Iraq and whether Osama bin Laden was in Abbottabad. His findings shed light on the kind of evidence-based, probabilistic, logical thought processes that go into the best predictions. A section on group think is nicely illustrated by the Bay of Pigs disaster; the ability of JFK's team to learn from their mistakes is demonstrated by the same group's more skillful response to the Cuban missile crisis. Written in an engaging and accessible style, Superforecasting illustrates every concept with a good story, often featuring national surprises like 9/11 and the lack of WMDs in Iraq with explanations of why forecasters missed what looks obvious in hindsight. Ultimately, this is a book about critical thinking that challenges the reader to bring more rigor to his or her own thought processes. Tetlock and Gardner have made a valuable contribution to a world of internet factoids and snap judgments. "
4,0804136696,http://goodreads.com/user/show/4751167-michael,3,"Philip E. Tetlock feels a bit too polite. Sometimes it seems he is excusing wrong predictions by finding weasel words in them or interpreting them kindly instead of using the intended assertion.Just say, Thomas Friedman is a bad forecaster.Instead of reading this book I recommend reading the books he references:Thinking, Fast and Slow The Black Swan: The Impact of the Highly Improbable and The Signal and the Noise: Why So Many Predictions Fail - But Some Don'tThis books feels like a (superficial) summary of the afore mentioned books and an attempt to combine them.""The central lessons of “Superforecasting” can be distilled into a handful of directives. Base predictions on data and logic, and try to eliminate personal bias. Keep track of records so that you know how accurate you (and others) are. Think in terms of probabilities and recognize that everything is uncertain. Unpack a question into its component parts, distinguishing between what is known and unknown, and scrutinizing your assumptions."" (New York Times review)"
5,0804136696,http://goodreads.com/user/show/23428788-john-kaufmann,3,"This book was solid, though perhaps not quite as good as I hoped/expected. It was interesting reading, full of interesting stories and examples. The author doesn't prescribe a particular method - superforecasting, it appears, is more about a toolbox or set of guidelines that must be used and adapted based on the particular circumstances. As a result, at times I felt the author's thread was being lost or scattered; however, upon reflection I realized it was part of the nature of making predictions. On reflection, his guidelines are clear and should be helpful, even if they cannot provide a method for correct predictions 100% of the time.One critique I had was that the author didn't provide any statistical evidence of why the people he identified as superforecasters were good as opposed to lucky. I continued to think some of the examples he gave were based on luck, not necessarily skill - the author distilled a lesson that contributed to the success, but I would have had more confidence that his conclusion represented the reason for the superforecasters' success if he had provided more statistical evidence to support that conclusion. Nonetheless, his conclusions/guidelines appear sound, and I plan on using them."
6,0804136696,http://goodreads.com/user/show/2199404-andy,2,"This book features some interesting trivia about ""Super-forecasters"" but when it comes to explaining evidence-based practice, it was Super-disappointing. It starts off well with a discussion of Archie Cochrane and evidence-based medicine (EBM), but then it bizarrely ignores the core concepts of EBM. -In EBM, you look up what works and then use that info to help people instead of killing them. But when Tetlock talks about social philanthropy he implies that it's evidence-based as long as you rigorously evaluate what you're doing. NO! If your doctor gives you arsenic instead of antibiotics for your bacterial infection, that's not OK even if he does lots of lab tests afterwards to see how you're progressing. -In EBM, you focus on the best available evidence. There's a difference between what some drug rep told you vs. the conclusions of a randomized clinical trial. But when Tetlock reviews the Iraq War fiasco, he argues there was a really big pile of evidence so it made sense to go to war. He doesn't seem to get that a really big pile of crap is still just crap. Some elements of the pro-war narrative were known to be bogus before the war. Others turned out to be bogus after (Curveball, etc.), so those were not investigated and confirmed as solid evidence beforehand either. -In EBM, the point of a diagnostic test is to get a predictive value. This number tells you how likely a test result is to be true, based on its track record. Instead, Tetlock praises his forecasters for making up percentages that reflect their subjective degree of certitude. And he calls those ""probabilities"" but that is very misleading, because in science a probability is something like the chance of drawing a royal flush in poker, i.e. it's an objectively calculated number based on reality. -In EBM, the big issue is whether the treatment works for the main relevant outcome. So for the Iraq War example, the question for the CIA was whether an invasion would A) spread democracy in the Middle East after preventing an imminent nuclear attack on the USA, or B) not prevent anything (because there was no secret new nukes program) and increase regional chaos as well as global terrorism (think ISIS). This decision tree is absent from the book, and that omission violates Tetlock's own rule about asking the meaningful hard question. This book has good content on cognitive biases. But I would recommend going directly to the source on that topic. 

"
7,0804136696,http://goodreads.com/user/show/17460503-pavlo-illiashenko,5,"Harry Truman famously said: Give me a one-handed economist! All my economics say, ''On the one hand? on the other.''Philip Tetlock combines three major findings from different areas of research:1) People don't like experts who are context specific and could not provide us with clear simple answers regarding complex phenomena in a probabilistic world. People don't like if an expert sounds not 100% confident. They reason, that confidence represents skills.2) Experts who employ publicly acceptable role of hedgehogs (ideologically narrow-minded) and/or express ideas with 100% certainty are wrong on most things most of the time. General public is fooled by hindsight bias (on the part of experts) and lack of accountability.3) We live in the nonlinear complex probabilistic world, thus, we need to shape our thinking accordingly. Those who do it (""foxes"" comparing to ""hedgehogs"" can think non-simplistically) become much better experts in their own field and better forecasters in general.I guess, nobody with sufficient IQ or relevant experience will find any new and surprising ideas in this book. However, the story is interesting in itself and many Tetlock's arguments and examples can be borrowed for further discussions with real people in the real life settings. "
8,0804136696,http://goodreads.com/user/show/66855950-tony,4,I'm giving this a 4 even though I didn't complete it. It's very well written and structured but I just decided half way through that the subject wasn't for me.Some exceptional real-world examples though!
9,0804136696,http://goodreads.com/user/show/48074717-michal-mironov,5,"I usually rank my favorite books on a line between „extremely readable“ and „ very useful“. This one is probably among my Top 3 most useful books ever. The other two are Kahneman's “Thinking, Fast and Slow”, and Taleb's “Black Swan”. You don't have to agree on everything with the author, but you still will get dozens of truly important facts that can fundamentally affect your life. Don't be misguided by the title – you really have to read this book even if you don't have the ambition to predict stock prices or revolutions in the Arab world. Whether we like it or not, we are all forecasters - making important life decisions such as changing career path or choosing a partner - based on dubious, personal forecasts. This book will show you how to dramatically improve those forecasts based on data and experience of the most successful forecasters. You’ll be surprised that those experts usually aren’t CIA analysts or skilled journalists, but ordinary intelligent people, who knows how to avoid most common biases, suppress their ego and systematically assess things from different angles. We will never be able to make perfect predictions but at least we can learn from the very best."
10,0804136696,http://goodreads.com/user/show/6816078-maru-kun,0,"Troubled to find I own, as yet unread, a book recommended by Dominic Cummings.Now I’ve lost all desire to read it and have to put it on the “interesting-but-dubious shelf” and wonder whether or not I’m a weirdo who can’t tell the difference between real science and pseudoscience. The book gets a mention from Cummings in this video: Tory backlash as Boris Johnson's rogue No10 chief Dominic Cummings refuses to condemn ousted 'superforecaster' Andrew Sabisky who 'posted vile Reddit comments defending rape and incest fantasies'"
11,0804136696,http://goodreads.com/user/show/48010064-paul-phillips,5,"Really good and well thought out ideas, particularly relevant to anyone who has any sort of forecasting responsibilities in their work. I think this is a must read for economists. My only quarrel is that the beginning is a lot more punchy and the end kind of drags. "
12,0804136696,http://goodreads.com/user/show/15527240-leland-beaumont,5,"Summarizing 20 years of research on forecasting accuracy conducted from 1984 through 2004, Philip Tetlock concluded “the average expert was roughly as accurate as a dart-throwing chimpanzee.” More worrisome is the inverse correlation between fame and accuracy—the more famous a forecasting expert was, the less accurate he was. This book describes what was learned as Tetlock set out to improve forecasting accuracy with the Good Judgement Project.Largely in response to colossal US intelligence errors, the Intelligence Advanced Research Projects Activity (IARPA) was created in 2006. The goal was to fund cutting-edge research with the potential to make the intelligence community smarter and more effective. Acting on recommendations of a research report the IARPA sponsored a massive tournament to see who could invent the best methods of making the sorts of forecasts that intelligence analysis make every day. This tournament provided the experimental basis for rigorously testing the effectiveness of many diverse approaches to forecasting. And learn they did! Thousands of ordinary citizen volunteers applied, approximately 3,200 were invited to participate, and 2,800 eventually joined the project. “Over four years, nearly five hundred questions about international affairs were asked of thousands of Good Judgment Project’s forecasters, generating well over one million judgments about the future.” Because fuzzy thinking can never be proven wrong, questions and forecasts were specific enough that the correctness of each forecast could be clearly judged. These results were used to compute a Brier score—a quantitative assessment of the accuracy of each forecast— for each forecaster. In the first year 58 forecasters scored extraordinary well; they outperformed regular forecasters in the tournament by 60%. Remarkably these amateur superforecasters “performed about 30 percent better than the average for intelligence community analysts who could read intercepts and other secret data.” This is not just luck; the superforecasters as a whole increased their lead over all other forecasters in subsequent years.Superforecasters share several traits that set them apart, but more importantly they use many techniques that we can all learn. Superforecasters have above average intelligence, are numerically literate, pay attention to emerging world events, and continually learn from their successes and failures. But perhaps more importantly, they approach forecasting problems using a particular philosophic outlook, thinking style, and methods, combined with a growth mindset and grit. The specific skills they apply can be taught and learned by anyone who wants to improve their forecasting accuracy.This is an important book. Forecasting accuracy matters and the track record has been miserable. Public policy, diplomacy, military action, and financial decisions often depend on forecast accuracy. Getting it wrong, as so often happens, is very costly. The detailed results presented in this book can improve intelligence forecasts, economic forecasts, and other consequential forecasts if we are willing to learn from them. This is as close to a page-turner as a nonfiction book can get. The book is well-written and clearly presented. The many rigorous arguments presented throughout the book are remarkably accessible. Sophisticated quantitative reasoning is well presented using examples, diagrams, and only a bare minimum of elementary mathematical formulas. Representative evidence from the tournament results support the clearly-argued conclusions presented. Personal accounts of individual superforecasters add interest and help create an entertaining narrative. An appendix summarizes “Ten Commandments for Aspiring Superforecasters”. Extensive notes allow further investigation, however the advanced reader edition lacks an index. Applying the insights presented in this book can help anyone evaluate and improve forecast accuracy. “Evidence-based policy is a movement modeled on evidence-based medicine.” The book ends with simple advice and a call to action: “All we have to do is get serious about keeping score.” "
13,0804136696,http://goodreads.com/user/show/2458054-frank,2,"PT's Superforecasting correctly remarks upon the notable failure to track the performance of people who engaged in predicting the outcome of political events. This lack of accountability has led to a situation where punditry amounts little more than entertainment; extreme positions offered with superficial, one-sided reasoning; aimed mainly at flattering the listeners' visceral prejudices. One problem is expressed positions are deliberately vague. This makes it easy for the pundit to later requalify his position conform with the eventual outcome. For example: a pundit claims quantitative easing will lead to inflation. When consumer inflation doesn't appear, he can claim that 1) it will, given enough time, 2) in fact, there is inflation in stock prices 3) He never said how much inflation. Thus, the first task in assessing performance is to require statements of clearly defined, easily measurable criteria. Once this is done, PT began a series of experiments, testing which personality characteristics and process variables led to good prediction outcomes, both for individuals and groups. Key attributes include independence from ideology, an openness to consider a variety of sources and points of view and a willingness to change one's mind. Native intelligence, numeracy and practical experience with logical thinking all correlate positively with prediction accuracy; at least to a point. But moderately intelligent and diligent individuals can often surpass the super bright, who sometimes show a tendency to be blinded by their own rhetorik. And some ""superforecasters"" consistently outperform professionals with access to privileged information. The chapter on how to get a group to function well together is especially applicable for business management. PT wrote his book at a mid brow style, and anyone already familiar with basic psychology writing, e.g. from D Kahneman, will often feel annoyed by his long and overly folksy explanations. Indeed, while it has good things to say about applied epistemology, it isn't necessary read all all 200 pages. A good alternative starting point would be to consult Evan Gaensbauer's review at the Less Wrong website: https://www.lesswrong.com/posts/dvYeS...."
14,0804136696,http://goodreads.com/user/show/12798350-andrew,2,"Superforcasting: The Art of Science and Prediction, by Philip E. Tetlock, is a book about the art and science of statistical prediction, and its everyday uses. Except it isn't really, that is just what they are selling it as. The book starts off really strong, analyzing skepticism, illusions of statistical knowledge, and various types of bias. However, the majority of the book focuses on a US government intelligence project called IARPA, designed to use everyday citizens to make statistical predictions on real life events. This book really threw me off, after such a strong start. I was expecting a book on forecasting; its design, uses and various techniques. And in some ways Tetlock delivers. However, I did not expect a book on US foreign policy, or a comparison of the fictional version of CIA director Leon Panetta from Zero Dark Thirty with the real one. It was all a little bit mind-boggling, and not in a statistical way. The book just doesn't hold my interest. Frankly, I could continue to criticize, suffice to say that a book on a US intelligence program should probably be labeled a bit better than this. The title suggests a cut and dry analysis of forecasting, but the book delivers in propaganda, US political criticism and so on, as opposed to interesting information on a form of statistical prediction. I would recommend a pass on this one, if you are not interested in the addition of fairly watered down US political theory. If you are, however, the book may be of interest to you. I was more disappointed than anything, and hope to read a book actually focusing on forecasting in the near future. "
15,0804136696,http://goodreads.com/user/show/10968716-asif,5,"Possibly the best book I read in 2015. Couldn't have read at a better time as the year nears an end. I could relate with a lot of things as I work as an equity analyst trying to do the seemingly impossible thing of forecasting stock prices. In particular, the examples of how superforecasters go about doing their jobs were pretty inspiring. Examples of taking the outside view and creating a tree of various outcomes and breaking down that tree into branches are something I could benefit from. As an analyst I am certain of only one thing. My estimates for earnings and target price for stocks will be wrong 99% of the time. However, I try to make sure that I am not missing the big picture and when there is a screaming buy or a sell caused by changing fundamentals or market overreactions I do not want to miss that. My earnings estimates and target prices are actually much less important. What is true however that small bit and pieces of information, including quarterly earnings do play a role in getting to a high conviction big picture story. "
16,0804136696,http://goodreads.com/user/show/29028496-frank-ruscica,5,Just finished reading an advance copy. The signal-to-noise ratio of this book: maximum. 
17,0804136696,http://goodreads.com/user/show/7309374-romanas,5,"As Hume noted, there’s no rational basis for believing that the sun will rise tomorrow. Yet our brain wants to have it simple – we believe that the future will be like the past. The problem here is that the truth of that belief is not self-evident, and there are always numerous possibilities that the future will change. It means that our causal reasoning can’t be justified rationally, and thus there’s no rational basis for believing that the sun will rise tomorrow. However, virtually nobody would claim there will be no sunrise tomorrow (at least in Stockholm we haven’t seen it during the last three weeks). This is human nature – to believe that certain things will happen. To predict and forecast is in human nature too. Some people claim they are very good at it. The author takes on it and gives and excellent overview and analysis of the state of affairs in the prediction and forecasting world, and introduces some big achievers in this area – superforecasters.Regarding superforecasters, obviously, they exist. Exist just like Warren Buffet, like the guys winning the lottery, like some personas becoming presidents of the US, etc., i.e. the rare ones who reaches extremely improbable heights. Is it a pure skill or is it a product of the skill and the result of the randomness filter, aka as luck? Forecasting of the events emerging from the social formations and human made systems is virtually impossible. Assuming free will exists, it makes our world extremely complex, and complicated. There’s a possibility that supercomputers and AI will eventually greatly improve forecasting of natural phenomena, like weather, earthquakes, flooding and the like. But then again, humans are in the loop, and most likely have the capacity to significantly alter the natural processes. This makes the whole human altered natural system practically unpredictable.To become a superforecaster one needs to constantly calibrate and verify the underlying models. As author points out, it is relatively easy in some areas, because some events forecasted have a high frequency, like weather forecasts, where one can verify the forecast on daily basis and improve the models. Rare processes are virtually impossible to calibrate, just because they are rare and there is not enough data to calibrate the prediction model, would it be a mental or a computer model. That’s why the forecasting of the events residing in fat tails, the extremes, Black Swans, as Taleb calls them, is epistemically impossible.I took on this book with the premise that listening to anyone who thinks they can predict the future is a big waste of time. The author mapped out what abilities a superforecaster usually possess to become such a good one. Among the strengths of a good forecaster is the open-mindedness and the ability to update one’s believes based on the new evidence. I think, I have those dispositions and this book actually shifted me some degrees toward the optimistic sceptic. My curiosity on the aspect of how the groups can do some valuable forecasting has increased.Sometimes, it can be very dangerous to base ones activities on predictive grounds. Good reactive capabilities, like risk management is essential to be able to cope with whatever the future will bring.Someone said that planning is useless, but it is indispensable. And many say we should live in a moment. Still, it’s hard and maybe even not so wise to not put down some ideas on how the future would unfold. Maybe planning is an illusion, but like many illusions it might give some existential comfort.Very soon we will close the books, raise a glass and try to look into the new year with the hope it will be at least a better year than the previous one. We will certainly forecast many things we believe will happen next year. No one could have said it better about this kind game as Lin Wells (just change 2010 to 2019) -- “All of which is to say that I’m not sure what 2010 will look like, but I’m sure that it will be very little like we expect, so we should plan accordingly”.This was my 100th book this year. I have forecasted this to happen, and it did! On this forecast my Brier score is 0 – meaning: perfection! On that note, to my followers and friends:Warmest thoughts and best wishes for a wonderful holiday and a very happy new year!!!"
18,0804136696,http://goodreads.com/user/show/7410827-allen-adams,5,"http://www.themaineedge.com/style/fut...Ever since mankind has grasped the concept of time, we have been trying to predict the future. Whole cottage industries have sprung up around the process of prediction. Knowing what is coming next is a need that borders on the obsessive within our culture.But is it even possible to predict what has yet to happen?According to “Superforecasters: The Art and Science of Prediction”, the answer is yes…sort of. Social scientist Philip Tetlock and journalist Dan Gardner have teamed up to offer a treatise on the nature of prognostication. Not only do they discuss the many pitfalls of prediction, but they also offer up some thoughts on that small percentage of the population who, for a variety of reasons, are very, VERY good at it.Tetlock has spent decades researching the power of prediction. Basically, people are pretty terrible at it. He himself uses the oft-offered analogy of a chimpanzee throwing darts – the implication is that random chance is at least as good at predicting future outcomes as the average forecaster. Even the well-known pundits, the newspaper columnists and talking heads – even they struggle to outperform the proverbial dart-tossing simian.But over the course of Tetlock’s years of study by way of his ongoing Good Judgment Project, he uncovered an astonishing truth. Yes, most people have no real notion of how to predict the outcome of future events. However, there are some who can outperform the chimp. They can outperform the famous names. They can outperform think tanks and universities and national intelligence agencies and algorithms. Tetlock calls these people “superforecasters.”These superforecasters were among the tens of thousands that volunteered to be a part of the Good Judgment Project. They were part of a massive, government-funded forecasting tournament. These people – folks from all walks of life, filmmakers and retirees and ballroom dancers and you name it – were asked to predict the outcome of future events. And so they did. These people soon separated themselves from the pack, offering predictions about a vast and varied assemblage of global events with a degree of unmatched accuracy.In “Superforecasters,” we get a chance to look a little closer at some of these remarkably gifted individuals. Tetlock offers analysis of some past predictions that were successful and others that were failures. We also get insight from prominent figures in the intelligence community and from people ensconced in the public and private sectors alike. And as Tetlock and company dig deeper, it becomes clear that the key to forecasting accuracy – to becoming a superforecaster – isn’t about computing power or complex algorithms or secret formulas. Instead, it’s about a mindset, a desire to devote one’s intellectual powers to a flexibility of thought. The accuracy of these superforecasters springs from their ability to understand probability and to work as a team, as well as an acceptance of the possibility of error and a willingness to change one’s mind.There’s something inherently fascinating about predicting the future. One might think that Tetlock’s findings are a bit complex – and they undoubtedly are. However, what he and co-author Gardner have done is condense the myriad complications of his decades of research into something digestible. A wealth of information has been distilled into a compelling and fascinating work. It’s not quite pop science – it’s a bit denser than that – but it’s still perfectly comprehensible to the layman. In essence, this book gives us a clear and demonstrable way to improve the way we predict the future.“Superforecasting” is a fascinating and compelling exploration of something to which many of us may not have given much thought. It’s not all haphazard chance – there are actually ways to improve your ability to predict the future, some of which are laid out right here for you. If nothing else, you’ll never look at a pundit’s prediction the same way again. "
19,0804136696,http://goodreads.com/user/show/1045774-mehrsa,4,"Really interesting. The book shows some common misconceptions about forecasting, well, statistics really. I'm often surprised by how people, including me, misinterpret data. The book also showed what it takes to be an excellent forecaster. Basically, requires the same skills as anything: pay attention, evaluate yourself, know your blindspots, be humble, practice. "
20,0804136696,http://goodreads.com/user/show/16080726-sabin,4,"It sucks when an audiobook is penned by two people but you hear a lot of “I” and “me”. After a little bit of background check, apparently the “I” and “me” guy is Tetlock, the scientist, while Gardner is just here for the ride. And also because he’s a journalist and because he can write. But maybe I’m wrong. Anyway, the end-result is worth it. It’s a very detailed account of two forecasting tournaments, which aim to find out if people are better than chance at predicting the future. Short answer: for some people yes, but on average no. Which means that, while some are better than average, some people are actually worse at predicting the future than the flip of a coin, or Tetlock’s infamous “dart-throwing monkey”.Aside from describing the experiments and drawing conclusions from them, which by themselves would have made this book worth every minute spent reading it, the authors also discuss other experiments and connect their findings to other theories proposed by psychologists Daniel Kahneman and Amos Tversky, essayst of “Black Swan” fame Nassim Nicholas Taleb and a few other theorists, both with civilian and military backgrounds.The authors focus on predictions about international events and especially on their correctness. The tournaments aim to find people who can make very good predictions (a lot better than the monkey) and find out how they go about achieving such good scores. Their conclusions are, as always, common-sense, if you stop to think about it. But their insight also helps avoid the pitfalls we may encounter along the way. While not many of us will be called upon to predict if, upon examination, Yasser Arafat’s body contains traces of Polonium, or if North Korea develops nuclear weapons within a given timeframe, these experiments point out judgement flaws inherent in our human nature and make us aware of our own mistakes.Unlike Kahneman’s Thinking Fast and Slow, this book’s contents are perhaps less directly relevant to everyone. It seems to apply more to people who are in the business of forecasting, like economists, financial analysts and stock brokers. And I say this also because it’s very practical and gives a lot of details on the methods one can use to achieve better forecasting results. What is actually relevant to everyone, is their description of the mindset required for good decision-making and how someone should go about weighing the consequences of important decisions before making them. Of course, going with your gut feelings is one way of making a decision, and, apparently, if you already have experience in that situation, a gut decision is already a lot better than random. But a better way, even if your gut tells you a course of action is good, and you have enough time to analyse the issue, is to check your internal point of view against an external one, which can be quantified, and then adjust your initial estimate accordingly. Actually you get a much better explanation and also a lot of examples in the book, and they do, in fact, make sense.It is one of the better books I read his year, and I found it pleasant to be reminded of some of the concepts discussed previously by Kahneman. Forecasting is an integral part of our humanity, and this book can also help us understand ourselves a bit better. One more thing: listening to it was a delightful experience. The performance, except for a few German names, was admirable, while the pace and the examples kept everything interesting."
21,0804136696,http://goodreads.com/user/show/19505655-ragnar,3,"The point Philip Tetlock and Dan Gardner are trying to make is that in the field of forecasting we seldom measure the accuracy of a prediction retrospectively, it applies especially to talking heads giving vague opinions often with no timeframes in media about the trends in the stock market, crisis in Syria, results of next elections etc. The unfortunate thing is that the same happens or happened previously in the national intelligence services as well. No scores were kept on the accuracy levels of the forecasts, therefor it was impossible to improve as well.To start validating the forecasts Philip Tetlock ran an experiment - the Good Judgement Project – a competition for the ordinary folks to forecast the future, in essence a crowdsourcing project. It was a success, a revelation for the prediction science as it turned out the results were ~40% better than set benchmark. The accuracy of those probabilistic predictions was measured with Brier score, a validation method where the set of possible outcomes can be either binary or categorical and must be mutually exclusive.An example. Suppose that one is forecasting the probability P that it will rain on a given day. Then the Brier score is calculated as follows:If the forecast is 100% (P = 1) and it rains, then the Brier Score is 0, the best score achievable.If the forecast is 100% and it does not rain, then the Brier Score is 1, the worst score achievable.If the forecast is 70% (P = 0.70) and it rains, then the Brier Score is (0.70−1)2 = 0.09.If the forecast is 30% (P = 0.30) and it rains, then the Brier Score is (0.30−1)2 = 0.49.If the forecast is 50% (P = 0.50), then the Brier score is (0.50−1)2 = (0.50−0)2 = 0.25, regardless of whether it rains.By using Brier score, Tetlock managed to find from thousands of forecasters the so called superforecasters who had exceptional results. By interviewing and oberving them he noted that the superforecasters had following characteristics:Cautious: Nothing is certain.Humble: Reality is infinitely complexNondeterministic: What happens is not meant to be and does not have to happenActively open-minded: Beliefs are hypotheses to be tested, not treasures to be protected.Intelligent and knowledgeable, with a need for cognition: Intellectually curious, enjoy puzzles and mental challengesReflective: Introspective and self-criticalNumerate: Comfortable with numbersIn their methods of forecasting they tend to be:Pragmatic: Not wedded to any idea or agendaAnalytical: Capable of stepping back from the tip-of-your-nose perspective and considering other viewsDragonfly-eyed: Value diverse views and synthesize them into their ownProbabilistic: Judge using many grades of maybeThoughtful updaters: When facts change, they change their mindsGood intuitive psychologists: Aware of the value of checking thinking for cognitive and emotional biases.In their work ethic, they tend to have:A growth mindset: Believe it's possible to get betterGrit: Determined to keep at it however long it takesBest quotes:Magnus Carlsen – I often can´t explain move, it just feels right. Usually I know what I will do in 10 seconds; the rest is double-checking.Unknown - Not everything that counts can be counted and not everything that can be counted counts.Galen – All who drink of this remedy recover in a short time except those whom it does not help, who all die."
22,0804136696,http://goodreads.com/user/show/6478339-gabriele,5,"Many of my friends were recommending this book to me and now that I've read it, I regret not doing it sooner. Do yourself a favor and read it asap! "
23,0804136696,http://goodreads.com/user/show/58247692-katrina-stevenson,4,"I truly think I learned a lot about how to forecast more accurately, and what to be aware of when listening to the forecasts of others. It definitely expanded my critical eye when it comes to experts, claims of fact, etc."
24,0804136696,http://goodreads.com/user/show/38623347-simon-eskildsen,4,"The average human forecaster is no better than a monkey throwing darts. Evolutionarily, we've developed a simple three-dial system for making decision: Do I see a huge dangerous predator? Yes, run. Maybe, stay alert / run. No, relax. Whenever we do venture into predictions, it's with a vague vocabulary filled with rubbery words: may, soon, highly likely, unlikely, .. The statement ""Greece may default in the near future"" really doesn't mean anything: may is completely uncertain, and the near future could be a year, decade or tomorrow depending on who you ask. If the medium for predictions is ambiguous English, how are we supposed to evaluate and therethrough make anyone accountable for their predictions?However, aggregate a large enough sample of average dart throwing humans and you'll get a much more useful result. If you have enough people guessing the weight of an ox, the average will run quite close to reality. People all come with different backgrounds, biases and bits of information that they boil down to a single number. Combine enough of those numbers, and a remarkable amount of information is captured in the final average. This is exactly how a stock market works, oodles of traders push new information into the stock.This strategy can be applied just as well to your own predictions. Instead of thinking twice to take another angle, think 12 times, even better 100 times—become your own supplier of diverse views, and aggregate these views. As new information submerges, update your predictions, but only move them little at a time. Super forecasters think in probabilities, not three dial notches, and they're excellent distilling facts into numbers. What sets them apart is their ability to see through confirmation bias, and consider as many angles as they can possibly find. They're experts at bias awareness and balance. They grunt at the smell of false dichotomy. They don't substitute questions for whether they'd do it, but absorb the full context to understand whether the person the prediction question is about will do it. Their growth mindset is what makes this possible, they refuse to believe that everything cannot be learned through hard work—versus a fixed mindset, where you think your only job is to reveal skills you were born with.When an effective forecaster look at a new problem, they start with a baseline. It's easy to get primed by an inside perspective here, but a super forecaster always start from outside. Vietnam and China border dispute? Look at history and see how often it's happened in the past, rather than compiling only from information available right this week which is subject to the availability bias. They don't go incredibly deep into one branch of an issue, but rather develop a nuanced, broad perspective. Ferme predictions are a weapon of choice, breaking a problem into many that can each have a reasonable probability associated with it. They know that the aggregation will result in a reasonable prediction. After developing an outside perspective, they'll dig inside and come back up merging the two into their final prediction.Super forecasters do remarkably in groups, effectively aggregation of aggregations. This is how Nate Silver works too, and 2-level (or even higher) of aggregating can be remarkably effective. They're aware of groupthink: that consensus should not be confused with having found the best possible solution. Friendliness may not spur enough diverse opinions. Chaos is an accepted reality among them, and they understand that the further into the future we venture the more we invite chaos and unpredictability. Taleb, the author and Kahnemann all agree it's unreasonable to predict anything 10 years out. Predictions excel in the 3-18 month range, as longer it becomes subject to the butterfly effect and it becomes more like a seasonal weather prediction than anything. When the book discusses chaos, it takes a detour into Prussian war strategy where localized decision power was always maximized. The higher ups would compose the overall plan, but the field generals would make the final decisions. The vision was shared, but the execution was up to the people with the most information. The famous quote here being ""plans don't survive contact with the enemy"".The author does a good job throughout of applying what he's preaching, questioning what he's saying and arguing against it."
25,0804136696,http://goodreads.com/user/show/3897817-morgan-blackledge,4,"I don’t have a lot to say about this book. Other than it’s good and the author Phil Tetlock is an extremely well regarded social scientist.My uncharacteristic lack of verbiage is not necessarily a slam on this book. It’s more of a reflection of how dang overloaded I am at present with work and school. It all has to get done. But in the meantime, my goodreads output has jumped the shark so to speak. That being said:Superforcasters documents Tetlock’s work studying individuals and teams that have extraordinary (i.e. consistently slightly better than chance over the long term) track records for predicting important events.As it turns out, these individuals are not necessarily experts in the given field they make predictions in. They aren’t necessarily smarter or better at math. They are certainly not psychic or particularly intuitive. But rather, they are meticulous critical thinkers who understand logic and probability, and utilize a bayesian methodology for incrementally adjusting their predictions as the data rolls in. It’s like google maps estimated arrival time (ETA). It’s shockingly accurate because it continuously compares its initial estimate (based on previous data) to real progress, and up and down modulates your ETA based on an algorithm that goes something like:Probability (P) = revised estimate of probability (B) divided by initial estimate of probability (A) multiplied by A, and divided by B.At least I think that’s something like the the way it goes. Please feel free to clarify in the comments. So. It’s at once really neat, and like so many of the findings in social science, kind of a no duh when you consider what is being said i.e. I can tell you what’s about to happen if I am allowed to continuously change my prediction the closer we get to the event. It’s not exactly that simple. Because every datahead can run that same methodology, but not all of them are Superforcasters. So read the book. But that’s it in a nutshell."
26,0804136696,http://goodreads.com/user/show/23857545-aloke,5,"Social scientist Philip Tetlock talks about his multi-year experiment to measure and identify ways to improve our forecasts. A lot of his findings appear to be common sense, i.e. many forecasts are very nebulous and difficult to disprove and so we can't really tell if we are improving, big problems can be forecast by breaking them into more manageable pieces (Fermi-izing in his terminology), our judgement is affected by various biases, etc. The contribution is that Tetlock has brought these ideas together in one place and really proved that applying them can produce forecasts that are significantly better than other approaches. The book is also really well written (it is co-written with the author Dan Gardner) and thought provoking enough to elevate it far above the average pop-sci tome. The anecdotes are well-chosen and persuasive but Tetlock stays humble and avoids sweeping pronouncements: he repeatedly warns against false dichotomies. He isn't interested in absolute certainty. He pokes holes in his own biases (e.g. against forecasters like Thomas Friedman) and he frankly discusses criticisms of his research by Taleb and Kahneman."
27,0804136696,http://goodreads.com/user/show/8676317-a-man-called-ove,4,"I first heard of this book on CNN's GPS podcast, but the name ""Superforecasting"" reminded me of ""Super-freakonomics"", which inturn remined of dubious smartass hindsights and which caused me to ignore the recommendation. Tetlock was cited again by Steven Pinker in his book ""Enlightenment Now"" and that finally got me to pick it up.Can you really forecast geopolitical events ? Surprisingly yes.Do you need a special ability to be a ""super-forecaster"" ? Not really.What then do you need ?The book describes the methods used by super-forecasters and in doing so describes a number of systemic biases in our thinking. Also, there are many relevant examples and except for a couple of complex equations which can be ignored, the author makes his points really well. This was a fun, fast read that was also satisfying.To the author's credit, he has finally made me pick up Thinking, Fast and Slow which I already think will be life-changing as far as books and ideas can be."
28,0804136696,http://goodreads.com/user/show/39544074-nick-pascucci,4,"This is really a book on epistemology: How do you manage uncertainty in a world with tons of it? How can we update our beliefs while taking into account the usefulness of new evidence? In the forecasting arena these questions and their answers are key to producing accurate predictions. I would recommend this book to anyone interested in the art of Bayesian epistemology and probabilistic thinking, and doubly so if one has a predilection for prediction."
29,0804136696,http://goodreads.com/user/show/4828849-michael-burnam-fink,5,"Prediction is hard, especially about the future. And despite the importance people and organizations lay on having a clear view of the future, we're not very good at prediction. The authors, Tetlock and Gardner, argue that the state of prediction is similar to the state of medicine, before randomized clinical trials. Sometime forecasters are right, but mostly they're wrong, and there's no way to separate the potentially useful treatments from quackish nonsense. But there might be a better way. Superforecasters is a write up of the authors' Good Judgement Project, an IARPA (Intelligence Advanced Research Projects Agency) effort to systemically study predictions. The Good Judgement Project smoked the competition, including in-agency experts and prediction markets. Tetlock and Gardner used their background in psychology to find out what made their top 2%, the titular superforecasters, tick better than everyone else.The first finding is that most of us are astonishingly bad at prediction. Generally, people have three settings for probability: impossible, certain, and maybe. Kahneman's System 1, the intuitive rush to judgment, is terrible at complex problems. Perhaps the biggest step is to slow down, and engage System 2, the rational and logical side of the mind. Beyond that, people with ideological blinders are lousy predictors. If everything has to fit into Marxist dialectics, or the immortal science of Friedman-Hayek Thought, you'll overlook evidence that contradicts the theory.The Good Judgement project provide some necessary structure. Instead of weasel words (most of, highly likely) and vague timelines (in the near future), participants are given clear factual statements with a definite endpoint. There are thousands of predictions, and participants are allowed to update their initial assessments as they do more research.Superforecasters are adept at seeing a problem in numerous ways, rather that focusing on grand theories. They tend to be comfortable with statistics. Participants in The Good Judgement Project self-selected as more intelligent and better educated than the population at large, but superforecasters aren't notably smarter or more credentialed than their less accurate peers. Instead, they have a bulldog tendency towards research, an ability to question their own assumptions, update beliefs, and think inside-out and outside-in.Superforecasting is a fascinating and very useful book for anyone who is thinking about the future."
30,0804136696,http://goodreads.com/user/show/12162514-graeme-newell,5,"I really loved this book. The author ran a multi-year study for the government agency DARPA (Defense Advanced Research Projects Agency) attempting to find ways to make predictions of future world events more accurate. The agency recruited thousands and thousands of ordinary people, then asked them to predict events of major importance. Examples included:•Political leaders that might fall from power•The prices of important commodities such as oil, precious metals and food stocks•Major economic occurrences such as recessions and stock market growth.Dozens and dozens of “super forecasters” quickly rose to the top prediction ranks. DARPA then conducted a long-term study of the specific techniques these prediction geniuses used to create their startlingly accurate prognostications. Tetlock’s book skillfully explains the shared best-practices used by these prescient champions. Predicting world events and even important events in personal life is more achievable than you might think - but you must learn to minimize the sabotaging innate biases that taint the average person’s prediction abilities. Over thousand of years of evolution, our human judgments were optimized for evolutionary survival in small hunter-gatherer tribes. Unfortunately, humanity has not had time to evolve a brain that’s great at solving problems in modern society. Our brains are fantastic at making decisions that keep us from starving to death, but pretty terrible at predicting modern problems such as currency fluctuations. Monetary policy just never seemed to be a major discussion point around the homo sapien campfire. Tetlock shows just how remarkably badly our out-of-date brains predict future events. But there is hope! Tetlock dives into the simple yet effective techniques shared by these superstar predictors, then shows how we mere mortals can put these same practices to work on our own personal problems. Tetlock’s writing is delightfully flowing and he weaves a fascinating story about a big government program that brought about a revelation in the intelligence gathering process. It is an inspirational story about how everyday people who practice some very simple mental disciplines were able to bring about some major good in the world."
31,0804136696,http://goodreads.com/user/show/78089723-joe-flynn,5,"5* and must read for people interested in forcasting and geopolitics, super useful for business leaders too. 4* for more casual readers, still interesting though with fewer day to day takeaways. (The best being that commentators on TV/newspapers generally have little actual insight, should be obvious but is not!)This is the ongoing story of a research project that born serious fruit. Showing that it is possible to both make useful, and also systematically improve, forcasts about the future - albeit with some limitations. I knew the jist of the book from articles in the press, this is deeper on both the initial findings, the improvements during the tournament, and the future. Worth the extra effort.Well written and nuanced, takes the lay reader from basic principles to expert insight without overuse of jargon and with good examples. Excellent name/story dropping with Taversky & Khaneman, and Taleb, well done sections. Superforcasting skills are usually held by pragmatic people who work hard on themselves - especially in the acknowledgement and ability to put aside their own biasis, who stay on top of news, and who have strong maths skills (though they don't need to use it much - more about the ability to think probabilistically).It becomes clear that it's more how you think as opposed to what you know that helps. There is a system, to be more of a fox as opposed to a hedgehog, to be able to change your mind with new information, to actively seek out that new and opposing information. Working in the right type of team helps too. The book is s little US based and they are openly working with the US intelligence services, that may not be a good thing depending on your POV - there are deep discussions on the Iraq war intenigence failure. Would have better assessment avoided war? We don't know. I can't see better predictions making it any worse however."
32,0804136696,http://goodreads.com/user/show/6427542-vincent-li,5,"A great read, probably the best light read this summer. A book I would recommend to whole-heartedly to anyone, since we are all forecasters. Usually, I am not a fan of the Predictably Irrational/Freakonomics model of turning an academic paper into a full length book but exceptions must be made. Tetlock was famous for producing the chimp throwing darts study (interestingly, fame was inversely proportional to accuracy, and there's many examples of pundits who project forward a vague forecast and then rationalize their forecast ex post), but the work that produced this book is far more interesting and meaningful. Tetlock starts with a digression into the history of medicine, which consistently spurned randomized trials and empiricism to the harm of patients and the practice of medicine (reminds me particularly of Taleb's work as well as the general narrative of Emperor of All Maladies). Tetlock argues that without randomized testing, our folk wisdom are essentially shots in the dark. As a faint hearted-empiricist, I cannot agree with him more. Tetlock discusses how many forecasts are unfalsifiable because the time frame is too vague, the event is ill defined and language can be ambiguous (maybe, significant, probably). Tetlock argues fairly that sometimes this vagueness comes from a common understanding that is quickly forgotten, a general discomfort with turning probabilities into numbers, or more cynically a method to cover one's own forecasts (this is not totally irrational, as people have trouble understanding probabilistic forecasts, and are quick to pin blame). A point that Tetlock makes that I thought was important is that not all forecasts have the purpose of being accurate. Forecasts have other purposes, such as entertainment, encouraging action, and bringing comfort. The book goes through Tetlock's entry into the IAPRA competition, which tried to measure the accuracy of clearly defined questions with time frames (Tetlock praises IAPRA for daring to run an experiment that could cost them their jobs). The book goes on to quantify the accuracy of forecasts by explaining the concept of a brier score (0-2, the lower the score, the more accurate), calibration (over many forecasts, the match between a predicted probability and the percentage occurrence of an event, i.e. when a forecaster claims that something is likely to occur with 60% confidence, do those events overall actually occur 60% of the time?), and resolution (to adjust for forecasters who just consistently forecast in the midrange). According to Tetlock, his methods for aggregating the wisdom of the forecasters (the wisdom of the crowd idea, that random errors cancel each other out but real information point the same way) with heavier weights for ""superforecasters"" and extremizing the probability was the superior forecasting method to the other entries in the competition. According to one source, the method was 30% more accurate than analysts with access to classified information (Tetlock argues that the superforecasters as teams are superior to even prediction markets, but with intellectual candor admits that his markets might have been too inliquid and had no monetary incentive). The core of the book discusses these superforecasters. I was impressed by the factors that Tetlock tries to control for in order to find the secret to their success. Tetlock systemically eliminates superintelligence (the superforecasters on the whole are a little above average but nowhere near genius level) and luck (Tetlock notes the lack of regression to the mean in time series data) as the driving factors behind the superforecasters' success. Instead, tantalizingly, Tetlock argues that their success is driven by learnable habits that is accessible to nearly everyone. Tetlock's superforecasters tend to be foxes, who are intellectually humble, constantly learn through feedback, use base rates, break down problems into pieces like a consulting interview problem (""Fermize""), update their forecasts in a Bayesian manner (though not strictly mathematically), use fine gradations of probability (an analysis demonstrated that rounding to the nearest 10% of these forecasts actually reduced their accuracy), avoided cognitive errors (like confirmation bias, by taking a different position and arguing it to the earnest, and avoiding hindsight bias), had grit, typically had a growth mindset instead of a fixed mindset, did not believe that things necessarily happened for a reason but instead believed in chance, and practiced a lot(Tetlock does not think there is a magic bullet). Tetlock also studied the effect of putting these superforecasters in teams, and observed the general avoidance of groupthink by these teams who had improved accuracy (more ground covered, diverse opinions, and respectful clarification and pushback). Tetlock also suggests creating psychological safety in order to avoid group think by having dedicated devil advocates, suspending hierarchy, and bringing in fresh perspectives. The team members were also generally givers, which maximized accuracy. Tetlock then responds to a two criticisms from Kahneman and Taleb. Tetlock characterizes Kahneman's objection as an inability for humans to adjust to their system 1 mistakes. Tetlock claims that certain adjustments can be repeated so often that they become reactionary and part of a person's system 1. Tetlock offers potential scope sensitivity as evidence that superforecasters can perhaps overcome their innate biases. Tetlock argues that many of the black swans that people discuss are actually to some degree predictable (though Tetlock admits that extremely rare events would be hard to accurately measure the forecast of), and it is still necessary for forecasts to be used to appropriately allocate resources for different contingencies. Tetlock argues that a leader can be both decisive when decisions are made and still have the good qualities of a forecaster. In particular, Tetlock argues for a decentralized decision making process that only sets out goals, allowing subordinates to achieve this goal through the means they see fit, which he claims was the method used by the German army, US army and several successful businesses. Tetlock spends some time setting out potential implications of his research. He argues that there might be a place for pundits in setting the questions for superforecasters to answer, and the hope that these forecasts will be imported into political discourse in order to bring empirical rigor to economic and political debates. "
33,0804136696,http://goodreads.com/user/show/9609102-zahedul,5,Loved the book!
34,0804136696,http://goodreads.com/user/show/27549015-poorna-kumar,4,"What makes some forecasters systematically more successful than others? In Superforecasting, Philip Tetlock unpacks the strategies behind making a good forecast, based on evidence from the Good Judgement Project, a study of thousands of forecasters who made thousands of forecasts. Central to the thesis of the book is the idea that our forecasts need to be unambiguous, with predictions expressed as numeric probabilities and measurable outcomes that we can track to know how successful the initial forecasts really were. Otherwise, hindsight bias makes it easy to assume we were right all along, and we lose the opportunity to learn from our mistakes. We should also record the reasoning behind our forecasts, so that once the outcome is clear, we are able to distinguish between forecasts that are ""reasonable but wrong"", ""unreasonable and wrong"", ""reasonable and right"", and ""unreasonable but still right"". All of this concrete feedback, averaged over many forecasts, can be used to tease apart luck and skill, and improve skill. While none of the individual strategies to improve skill struck me as revelatory, the body of evidence in their favor made them meaningful, and when presented together, they made me reflect in a deeper way on all that goes into the fuzzy skill we call ""good judgement"". A skillful forecast can be unpacked into a few different pieces, including starting to formulate your answer through an ""outside view"" (reviewing a question in its larger context and thinking about base rates) before moving to the ""inside view"" (being swayed by the specifics of a particular question), utilizing the ""dragonfly eye"" that takes different many perspectives and synthesizes them into a single number, breaking down large questions into subparts, questioning one's assumptions and reframing the question, and working effectively in groups (in particular, ""adversarial collaboration"" between good forecasters which helps to bring many unique and relevant perspectives to the table, rather than groupthink). Some of the research here offers interesting insight into the scope and limits of predictability. For example, I learned that more granular forecasts seem to improve superforecasters' accuracy. Also, no matter the skill of the forecaster, it is difficult/impossible to be especially accurate when making specific forecasts about long-term events. The best we can do is to try to make long-term plans that are resilient to changes. Throughout the book, Tetlock is (perhaps overly) skeptical of media pundits (like Tom Friedman) who make predictions of how the world will turn out, calling their predictions vague, overconfident and riddled with the possibility of hindsight bias. He concedes that pundits' skill may be in identifying good questions to ask, but calls for a move to evidence-based forecasting (and honestly, it seems strange to me that there can be such a thing as non-evidence-based forecasting). Personally, I like to think that I've always aspired to measured and probabilistic thinking, but I realized I have no way of knowing how successful I am. This book has inspired a fun exercise in quantifying my own beliefs in the everyday realm, and maintaining a spreadsheet to check my accuracy!"
35,0804136696,http://goodreads.com/user/show/11302400-marco,3,"TL;DR: a more practical companion to Expert Political Judgment, useful for forecasts in politics, economics, and other areas of human activity, if not quite as compelling as its predecessor. 3 1/2 stars.Superforecasting is a book that presents the results of Philip Tetlock's research on prediction and avoiding the biases that afflict even experts' forecasts within their domain of expertise. The book is something of a sequel to Expert Political Judgment​, which shows that, on average, the political forecasts made by experts don't fare much better than random, when one takes into account both accuracy and calibration (that is, events which were attributed high probabilities occur with higher frequency, and the converse for low-probability events). This dismal scenario is alleviated by the discovery that some experts had a significantly better performance than that, and that those experts shared certain traits.That silver lining led to the creation of the Good Judgment Project led by the author, which studied the behavior and results of those overperforming predictors. The book describes some of the findings of that study: despite the (practical?) impossibility of perfect prediction​, some heuristics and habits can improve the accuracy of forecasts in significant and persistent ways. The author claims, supported by the project's evidence, that testability plays a central role on that improvement: if there are metrics that allow the verification of how right or how wrong a given prediction (or predictor) was, then the higher level of scrutiny will push forecasters towards ""good judgment"" as described by the Good Judgment Project. This programme is open to some critiques that the author acknowledges but -- at least not in my opinion -- does not fully address, especially about the relevance and scope of the kind of predictions which have results that can be easily metrified. Yet, as the author claims, those doubts don't mean that a programme for improving forecast, even if somewhat narrow on its outputs, would not be beneficial for the public debate on the relevant subject matters.While, in general, I think that ​Expert Political Judgment​ is a more interesting book, Superforecasting​ has a much wider and more general scope. As such, it is a good read for those interested in practical forecast at whatever level or topic."
36,0804136696,http://goodreads.com/user/show/37756562-itsadog,4,"I received an ARC in exchange for an honest review through Goodreads First Reads. Thank you, Random House of Canada!!Old joke about American Intelligence and oxymorons aside, Mr Tetlock draws heavy inspiration from American politics in the latest book detailing his life's research. This is the person behind the quip about American forecasters being worse than ""chimps with darts"", a line he's sick of hearing but that's not inaccurate.This book was about half guilty pleasure for nerds *raises hand* and half self-help *puts hand down*. The combination rattled my cage somewhat because they contradict each other (""be a free thinker - like this!""), hence the loss of a star.The book is sprinkled with entertaining trivia and follows Mr Tetlock's campaign to instill a more refined taste in the American news consumer. Why not, the book demands, apply a Moneyball-style rubric to the waffling forecasters who hold so much sway over viewers? And why not, further, apply this rubric to various services the government uses those viewers' taxes to provide?Mr Tetlock does not hold this rubric close to his chest. He intends for every reader to graduate his book-long course with all the tools she needs to out-think the people who run the country.All in all, I'd recommend the book for novelty seekers and bored nerds. I also like the cover. It's neato."
37,0804136696,http://goodreads.com/user/show/1142366-hans,4,"The temptation to accept expert opinion on the future is unquestioningly difficult to resist, until you realize that they rarely perform better than random chance. This book blow start but picks up and definitely worth the read. Understanding the natural limitations and cognitive pitfalls of the brain is important for increasing the accuracy of foresight. The author researched the art of prediction in decision making to see if it is possible to refine one's predictive capabilities thus allowing for better decision. His research suggests that not only is it possible but that anyone is capable of utilizing these techniques irregardless of their education or experience.Forecasting can be comparted to stone sculpting, you are trying to see the end state inside an un-formed block of marble. What you need is tools to remove the pieces that are obscuring the end product. These cognitive tools are:1) Gathering evidence from a variety of sources (Aggregation).2) Thinking probabilistically. The future is still yet undetermined.3) Working in teams, differing perspectives can add clarity to the overall picture.4) Keeping record, in order to measure accuracy.5) Being willing to admit error and change course. New information requires the humility to adjust.The author delves into explaining how each of these tools work giving examples of each. "
38,0804136696,http://goodreads.com/user/show/146952-grumpus,3,"A Christmas gift from my daughters with a note that said, “Now you can predict the lottery numbers.” No pressure.This book is not that type of book, but it is still worth the read to those of us who feel the need to quantify everything. The book documents the inaccuracy of TV pundits and, through a website in which anyone can make quantifiable forecasts, tracks the accuracy of forecasters on key economic and world issues. Through a scoring system, the super forecasters are identified and studied to find out what sets them apart from the rest. This book details those findings.So, what sets them apart? (view spoiler)[The answer is not related to ESP or special intuition, but rather hard work. These super forecasters are relentless in studying the issues and simply making informed decisions. <\spoiler>.I’ve signed up at https://www.gjopen.com/ and am giving it a go as a forecaster. It’s not easy as evidenced by my poor score on one event that has concluded. I had not anticipated the release of the hostages held by Iran after the sanctions were lifted. See, it is not easy. I have great admiration for those that can do it well as every possible angle of an issue must be known and the impact that each may have on the outcome of the topic in question. (hide spoiler)]"
39,0804136696,http://goodreads.com/user/show/34496847-josh-url,5,"Superforecasting: The Art and Science of Prediction is a wonderful read on a fascinating subject which we all do to some degree every single day of our lives - forecasting. Tetlock and Gardner paint the picture of why we struggle at predicting uncertain things and illustrate several ways we can improve our abilities to improve in this area. I work in basketball and one of the core elements is forecasting how players may perform in a different offensive system, defensive system, with different teammates, in different roles, or even against a higher level of competition. To put it lightly, it is challenging to get it right at a high level consistently. This book has given me many ideas on how to improve my ability to reduce uncertainty in scouting/projecting players which will greatly benefit my work. I highly recommend this book to anyone whose work requires answering questions about uncertain events in the not too distant future as well as anyone interested in finding out how to improve her/his ability to predict things that may impact her/his life."
40,0804136696,http://goodreads.com/user/show/4666544-pat,5,"I was one-hundred percent, absolutely blown away by this book. I think it is because it contained so many truths that I, a a project-based learning facilitator in a public high school, preach to my students. Students have to be able to accurately forecast whether their project will be successful and make changes if it won't be. How do they do that? They need to know what is known and unknown. They also need grit, determination, and a growth mindset. They also need to be able to work together without succumbing to group think. What does that have to do with this book?Tetlock held a forecasting contest among 2800 different forecasters over several years in order to determine whether forecasting success is based on pure luck or if their is also a skill to it, and if it is skill-based, what are those skills? He found that you can become a good forecaster through developing a certain set of skills, and some of them are the ones that I mentioned above."
41,0804136696,http://goodreads.com/user/show/9412938-cathy,5,"This book is the ultimate reward for serious readers who are interested in behavioral economics and decision making. It supplements the Charlie Munger-style mental model building for worldly wisdom especially if you've read or are familiar with the following:Thinking Fast and Slow by Daniel KahnemanBlack Swan by Nassim TalebMindset by Carol DweckGive & Take by Adam GrantBlink by Malcolm GladwellWisdom of the Crowds by James SurowieckiThe Success Equation by Michael MauboussinAngela Duckworth's work on gritRichard Thaler and Cass Sunstein's work Professor Tetlock is highly introspective of his work, pointing to competing schools of thought while providing actionable insights from different perspectives. Unlike the rambling writing style of Thinking Fast and Slow, I thought Superforecasting is very well written and constructed, likely through the contributions from co-author Dan Gardner. "
42,0804136696,http://goodreads.com/user/show/15414947-navneet-bhushan,4,"One thing that came out from this book and the study on which it is based is - that so called Experts - Journalists and even key consultants have practically nil ability to forecast about anything meaningful with any reasonable probability. That in essence is the key finding and learning. The Good Judgement Project and the 10 commandments of superforecasting that the book delivers based on relatively unknown forecasters who diligently have been working and giving their inputs is indeed an important contribution and can stand the test of time perhaps, especially in the emerging age of Bayesian Belief Networks, Big Data and Deep Learning. A definite good read for anyone who has any interest in working out the future - at least you will know who and what type of experts to avoid.Giving 4 Stars"
43,0804136696,http://goodreads.com/user/show/76105804-rick-wilson,5,"Great look at the biases we fall prey to when attempting to predict and understand what the hell is going to happen. Most people suck at prediction, but the absence of accountability means there is no incentive to improve. And in many situations, incentives push against accuracy in favor of defensible wrongness. Feels a bit dated already, as the convergence between tech and predictions is absent. There's a great book waiting to be written that takes all the strengths of Superforcasting and combines them with an in depth knowledge of Machine Learning and Predictive Modeling. Overall, this book delivers solid research, connects that to strong conclusions, and presents actionable insights to improve your thinking. All while being delightfully self aware. Highly recommend to anyone who likes to meta-cognate. "
44,0804136696,http://goodreads.com/user/show/2531665-charlene,5,"Of course everyone should read Thinking Fast and Slow, but this is not only the best book I have read on the topics of prediction, logic, and critical thinking since Kahneman's book, it's one of the best books I have read period. Some people are better at logical thinking than others, but even among those who are fairly good, there is room for improvement and this book does a great job of illustrating why. It's so easy to buy into the idea that those who have a good record of prediction have better skills. But do they? Could it be as random as a coin toss? This book made me really question what I think I know. I consider that to be one of the greatest gifts an author can bestow. Reading Superforecasting was extremely rewarding and informative. "
45,0804136696,http://goodreads.com/user/show/27609086-travis-scher,4,"An excellent book on how to think clearly. The short-short version of the forecasting formula set forth by author Philip Tetlock is ""use probabilities and check your ego."" The book elaborates on this, and will teach you how to use probabilities and when and how to check your ego. Perhaps more importantly, it will help you spot others' errors and see through their bs. The book also highlights that much of the ""forecasts"" by professional and amateur pundits are no sincere attempts to accurately predict the future, but are actually aimed at getting attention, signaling something about themselves, or influencing an outcome.The book flows nicely and is punctuated by engaging examples. It's a must-read for anybody interested in how to overcome bias and separate truth from noise."
46,0804136696,http://goodreads.com/user/show/7811034-lindley,3,"I didn't know much about forecasting before reading this book, and found the explanations easy to follow. The ease with which the superforecasters' predictions were explained, however, took some of the magic out of the ability to consistently beat others' forecasts. Yet tips not to over- or under-predict simultaneously make the superforecasters' feats seem even more out of reach. This book will appeal to readers who enjoyed Nate Silver's The Signal and the Noise. "
47,0804136696,http://goodreads.com/user/show/37708090-yifan,2,It was mildly interesting. I wouldn't really recommend if you have taken stats and know basic ideas behind thinking fast and slow.
48,0804136696,http://goodreads.com/user/show/37442561-michael-cestas,4,Super.
49,0804136696,http://goodreads.com/user/show/78628924-joseph-l,4,Watch a detailed review along with my favorite ideas and takeaways at:https://youtu.be/YR2Y3SDSGcU
50,0804136696,http://goodreads.com/user/show/86576157-minervas-owl,5,"Superforecasting introduces Tetlock's research on people's ability to answer hard forecast questions, such as ""will North Korea launch another missile in six months?"" He found that 2% of research subjects outperform the others by enormous margins, and tried to explain how they became such ""superforecasters."" I find the book especially relevant at this very time. As the world seethes with troubles brought by Covid-19, many forecast questions plummet out of the safe haven, where statistical models and common sense generally works. No longer can we comfortably expect an accurate prediction about how many forbearance borrowers will default, or visit a dentist without gauging the probability of being infected. Yet we still need to answer these questions. According to the authors, even when the forecast is hard, we can still make a difference. I find the examples in the book riveting. For example, eight years after Arafat's death, since researchers found high levels of polonium in his belongings, two separate agencies went on to inspect his body. Will the agencies find an elevated level of radioactive in the body? The author introduced how Bill, one of the ""superforecasters,"" approached this question. His first step was to study the science of polonium testing, so he could understand the possibility for the element to stay at a detectable level after eight years in the human body. I missed this step entirely. Like many other people studied by Tetlock, I switched the original forecast question to an emotionally charged one: ""did Israel poison Arafat?"", and I failed to zoom in to the details. My major takeaways from the book are: 1) avoid letting questions push the ideological hot buttons, and 2) refrain from organizing my thinking around some big ideas. I find these errors to be dissidents' Achilles' heel. The questions we try to analyze often swamp us with emotions, and we often attribute everything to the dictators' wrongdoings. It's good that the author suggested a few coping strategies, such as distancing oneself with the ideas by writing them down and reframe the questions. For example, change ""Will South Africa grant a visa to Dalai Lama?"" to ""Will South Africa reject a visa to Dalai Lama""). The book also motivated me to think about how to improve the accuracy of forecasts used in daily life. For example, I need to have a better estimate of the time required to get a specific task done. I find two things mentioned by the author useful: 1) keep score. 2) get clear and prompt feedback.The authors covered enough grounds in the book. Are the superforecasters merely lucky? No. They continued to outperform in 3 years. Is prediction accuracy the only criteria we should use to measure success? No, we can put different weights on false positives/negatives. Are there other dimensions of good judgment in addition to being able to forecast accurately? Yes, by asking good questions. I only wish to see a little more statistics, such as the distribution of the forecasters' brier score. The authors did not adequately explain why extremizing aggregated forecasts make them more accurate, but it's okay since this is a mathematical question outside the book's scope. Side notes:When information is lacking, one could use ""Copernican humility"" to predict how long a war or an epidemic will last. Say it has lasted for a year. If we assume nothing is special about the point of observation, then we can be 95% sure that the ratio of (time elapsed) / (total time) is between 2.5% - 97.5%. Therefore, the war/epidemics will last another 1/39 year or another 39 years. In order to encourage team members to safely express doubts about the leader's plan, run ""premortem,"" in which the team is told to assume the plan has failed and explain why. "
51,0804136696,http://goodreads.com/user/show/22094221-morgan,5,"Superforecasting does a pretty good job of describing what prediction is, how to tell if you're any good at it, and what people who are *very* good at it do. It mixes in a lot of history and anecdotes to make its points, which I found engaging and effective.Tetlock's main historical comparison is medicine. Before the 1800s, medicine was a grab-bag of whatever some rando thought worked. Through the mighty effort of some philosophers and doctors, medicine slowly changed into an evidence driven discipline in which people actually test their treatments. The result has been a major improvement in human health over the past hundred years or so.Forecasting and prediction, according to Tetlock, are in the same place now that medicine was in hundreds of years ago. Nobody knows what works, or even can agree on how to measure that. At least that was the case before Tetlock started doing his research back in the 1980s. Since then, he's figured out some of what works and wants to convince people to follow on with that work and bring prediction into the modern age.His thoughts about how to predict well make intuitive sense. In a nutshell:1. be specific about what, exactly, you are predicting2. break the problem down into sub-problems that are easier to approach3. look at how often those sorts of things happen in general (lots of research happens in this step)4. adjust for the specifics of this case (lots more specific research)5. make your prediction, along with a probability for how much you think it will happen (yes/no thinking is to be avoided)6. revise, revise, reviseThat's great and all, but really unhelpful for people that want to answer more general questions like ""what should I do with my life?"" or ""how will this complicated situation I'm in the middle of play out?"" In it's worst incarnation, this is the beginning of a potentially endless repetition of the question ""predict what would be the best thing to predict?""The first 70% of the book all takes the actual questions that people are trying to predict answers to as given. Luckily, the last few chapters do address the question of what we should be trying to predict.Early in the book, Tetlock introduces Foxes and Hedgehogs. Foxes are people who know many small tricks that they use to predict. Hedgehogs are people who have one large way of looking at the world. Over and over, Tetlock emphasizes that Foxes make the best predictors.When it comes to identifying what should be predicted, Tetlock theorizes that Hedgehogs will do better. Precisely the ideology that gets in the way of predictive accuracy allows them to identify questions that may be important. Tetlock goes so far as to say that the best way of influencing the future (rather than just predicting it) would be for Hedgehogs and Foxes to team up, but with fairly strict roles.I liked this book. I was worried it would be too much a re-hash of Thinking, Fast and Slow or similar books, but it was mostly new material. A lot of it seems obvious, but the history and anecdotes really bring home how much this ""obvious"" method is not being applied in practice. The book was fun, useful, and also addresses the limitations that Tetlock sees with his method."
52,0804136696,http://goodreads.com/user/show/7226831-josh-friedlander,3,"The failure of experts to predict major events (the breakup of the USSR, the terror attacks of 9/11, Saddam Hussein's non-possession of nuclear weapons) is well known. (To this long list I would not append the current global pandemic: as Tetlock himself said on a podcast, the relevant experts have been loudly predicting it for years.) In the Expert Judgement Project he quantified and measured predictions about politics and economics, and proved that people making them are no more accurate than random choice, or dart-throwing bonobos. That brought him to the attention of IARPA, a ""DARPA-for-intelligence"" established by the US government in 2006, where he now works on the Good Judgement Project, an attempt to recruit, track and improve evidence-based forecasters, a motley crew of amateurs (retired farmers, maths professors, etc.) who attempt such predictions without domain knowledge, basing themselves on research and assiduous avoidance of cognitive biases. (One might draw a parallel to the great rationalisation in investing of the past decade or so, where evidence-based assessment has led to a flood of mom-and-pop investors switching from active to passive investing. Both draw inspiration from the work of Kahneman and Tversky discussed in The Undoing Project.)As I read I thought of possible objections: can all problems really be modeled on a three-point outcome scale? Shouldn't they think more about the boundaries of answerable questions? And analyse further the methodology of problem-solving? Isn't this all a bit robotic? But none of them stand up to Tetlock's central conjecture, that we have to measure how wrong we are, and if we are improving. Any gut feeling which can't be measured and improved is equivalent to firing in the dark: it may be wrong or right, but we can never know. And he is humble, avoiding the (great and forgivable) temptation of dunking on Thomas Friedman, and modestly acknowledging that Friedman's domain knowledge and instinct for which questions to ask is important to his superforecasters; but that Friedman, like anyone, needs measurable predictions in order to know if his punditry has value.The book itself is somewhat shallow. It does what I call ""scaring with maths"": quoting something out of context without explanation just to show the reader that here is something Very Scary for Clever People Only (one example: ""his résumé includes papers with occult titles like Scaling Limits for Internal Aggregation Models with Multiple Sources""). It also is a little repetitive. Nonetheless, I feel like reading it made me overall a little wiser. This review of Tetlock's work and one of his other books by the ever-sharp Louis Menand might be a quicker précis - although I think his closing line is exactly what Tetlock is trying to reject! "
53,0804136696,http://goodreads.com/user/show/112553900-raz-pirata,4,"“It turns out forecasting is not a ‘have it or you don’t’ talent. It is a skill that can be cultivated.”Imagine, just for a moment, how different your life would be if you could see into the future. The bets laid and won, the stocks bought, disasters avoided and triumphs celebrated. You could be like Marty McFly on steroids, Delorean and all. But maybe, just maybe, you don’t need a ‘flux capacitor’ to travel through time. Perhaps you don’t need a mad scientist for a mentor either, to catch a glimpse of the future.Superforecasting - The Art and Science of Prediction by Phil Tetlock and Dan Gardner is the pièce de résistance of prediction. Born out of the culmination of the meta forecasting study The Good Judgement Project, Superforcasting reveals what it takes to see the future and how you might do it too.“For superforecasters, beliefs are hypotheses to be tested, not treasures to be guarded.”Rooted in groundbreaking research and expertly crafted, Superforcasting demonstrates how, when it comes to prediction, there are a select set of skills and habits of mind that can allow one to see further into the future with greater accuracy than the so called ‘experts’ barking guarantees on the cable news networks.Written with a style that is both academic and appealing, Superforcasting, and it's fabulous cast of superforecasters, takes us on a journey into what it takes to become the oracle of our own lives. It gives the reader a sneak peek into what these visionaries do, think and see that makes them and their predictions so much more accurate than the rest of us mortals. “If you have to plan for a future beyond the forecasting horizon, plan for surprise.”Chocked full of lessons, analogies, story and science, Superforecasting might be one of the most useful books you could ever read. Practical, scientifically backed, thoughtful and articulate, this is what a self-help book is supposed to be, a book where if you read it and apply its lessons you really can help yourself.Good forecasting is rare, better than good forecasting is priceless. Learn what it takes to see further into the future in Superforecasting and use your new observational super powers for good. Whether you wanna get the Delorean is up to you.Overall Score: 4.6 / 5In a Sentence: If you could learn to see into the future, would you? Because you can."
54,0804136696,http://goodreads.com/user/show/44289250-robert-martin,5,"Superforecasting is the product of Tetlock's research and experiments running a large-scale and long-running forecasting competition, from which he identified a group of individuals who significantly outperformed expert analysts. The book is the story behind this research and a discussion of the characteristics of these ""superforecasters"".A recurring theme is the ability of the superforecasters to minimise the effect of their type 1 system (the ""fast"" brain) which, although well suited to survival on the savannah, is often a major cause of faulty forecasts. At first, I felt that Superforecasting was a rehashed Thinking, Fast and Slow (or a mix of books from that genre), but it's a wonderful book in its own right and a valuable contribution to the genre. It makes interesting philsophical points about the nature of forecasting (to parry those of NNT); it provides rich historical examples of forecasting errors (Bay of Pigs), forecasting non-errors that were treated as errors (controversially, the existence of WMDs in Iraq), and forecasting successes (several examples of superforecaster reasoning); most importantly, the book provides practical prescriptions for improving forecasting. All of this is wrapped in an introspective and rational package – the authors are self-aware enough to point out contentious points in their own methodology and to consider alternate views. As much as I would like to paraphrase the main takeaways, I can't do any better than the author's own summary:""Unpack the question into components. Distinguish as sharply as you can between the known and unknown and leave no assumptions unscrutinized. Adopt the outside view and put the problem into a comparative perspective that downplays its uniqueness and treats it as a special case of a wider class of phenomena. Then adopt the inside view that plays up the uniqueness of the problem. Also explore the similarities and differences between your views and those of others—and pay special attention to prediction markets and other methods of extracting wisdom from crowds. Synthesize all these different views into a single vision as acute as that of a dragonfly. Finally, express your judgment as precisely as you can, using a finely grained scale of probability."""
55,0804136696,http://goodreads.com/user/show/15096157-prasanna,4,"This book had been on my list of to-read for a while and I'm glad I finally picked it up. A lot of the gist of the book is earlier in maybe the 100 pages -- rest are anecdotes and filler material. It's all great read but the core ideas are the following* There are certain number of people who can be described as superforecasters -- people who can make accurate forecasts and change beliefs accurately when the information changes* The author participated in a challenge organized by the IARPA and won with a group of super forecasters with a project called the Good Judgement Project * The groups were self-selected and then based on performance on questions picked by IARPA* The consistently outperformed the intelligence analysts in predicting eventsThe core idea of the book can be even further summarized as ""measure your predictions."" Often the talking heads on TV get away with a vague prediction because they're not being judged on their past predictions on how accurate they are. Tetlock further brings up the Foxes vs Hedgehog mindset where the media rewards hedgehog-like jumping to tip of nose instinct whereas superforecasters tend to be more Fox-like and look at all the different possibilities.Among the stories the author brings up, the one that really sticks with me is the one about the German general Helmuth von Moltke who built a military organization that stood in stark contrast to the existing top down hierarchies. They were still hierarchies but the Prussian military that eventually unified into modern Germany and became basis for Wehrmacht derives on the idea of providing a high level objective and letting the commanders and soldiers decide how they want to execute the goal.> “German junior officers were regularly asked for their opinions and they would criticize the outcome of a large maneuver with several divisions before the attending general had the floor.”This stands in stark contrast to Eisenhower, as a junior officer, being reprimanded for suggesting that they should be looking more into tanks. Eventually lot of militaries adopted the model of command espoused by Moltke. While the link to data gathering and modern algorithmic approach seemed tenuous with this story. It does elaborate the idea that the organization can be robust by allowing diversity of opinions.I think this is a good book and a lot of ideas can be applied in daily predictions and/or business decisions. Giving it less than five stars because of the extra filler material."
56,0804136696,http://goodreads.com/user/show/13206524-john,5,"This was an excellent read, if you have some training in behavioral economics (Kahneman, Thaler, etc.). Without some prerequisites, you may be a bit overwhelmed with the psycho-speak. It makes such logical sense, but the lingo is unconventional. To be fair, Tetlock does a good job bringing the reader along, and a novice would be able to read this effectively with some time for pause and let it all sink in. That being said, there are some awesome kernels of knowledge, like the Fermi-ize system, the process of knowing what you really don't know, and being numerically-centric. As a synopsis see the following:Tetlock has been part of something called the Good Judgment Project (GJP - www.goodjudgement.com) which is a funded group to locate laypeople who make forecasts. The most effective (top 1%-2%) he calls ""Superforecasters"". The book is a breakdown of what makes these people better than average forecasters (and better than the ""pros""). That being said, he also allows for regular people to become better at making forecasts, based upon an analytical system to root out ""guesses"" and move toward ""forecasts"". This is accomplished by working from known informational averages, and breaking down a question into smaller questions, this can lead to a projection. It is very interesting stuff. For some, this may seem boring. However, if you have an interest in how numbers and data can help solve some problems, or at least make life's choices less of a guess, this may be an interesting addition to your reading list."
57,0804136696,http://goodreads.com/user/show/45859662-george-goodall,5,"Yup. This one is good. It turns out that making good forecasts is really hard and we haven't been getting much better at it (see Sherden's 1998 book _The Fortune Sellers_ for an earlier take). Tetlock -- the political scientist who brought the idea of foxes and hedgehogs into the mainstream -- has run a long-term prediction project. With government funding, he and his collaborators asked people to make a series of predictions with binary outcomes. Specifically, he asked for a probability. Participants were scored on their forecasting ability using a Brier Score, typically applied to meteorologists (but not, sadly, appropriate for curling). He then focuses on those people (and teams) that make really good predictions, hence _Superforecasting_. Here's the summary from the appendix: - Break seemingly intractable problems into tractable sub-problems.- Strike the right balance between inside and outside views.- Strike the right balance between under- and overreacting to evidence.- Look for clashing casual forces at work in each problem.- Strike the right balance between under- and overconfidence, between prudence and decisiveness.- Look for the errors behind your mistakes but beware of rearview-mirror hindsight biases.- Bring out the best in other and let others bring out the best in you.- Master the effort-balancing bicycle.- Don't treat commandments as commandments.- Be thou familiar, but by no means vulgar.- Take each man's censure, but reserve thy judgment.- Neither a borrower nor a lender be. Okay, those last three are actually from Polonius, but you get the idea."
58,0804136696,http://goodreads.com/user/show/7716365-kate,4,"This was a fascinating look at ""superforecasters"" - basically people who are the best at predicting the (near) future. They're not always the people you think though - none of the guys who make their living doing forecasts for the news wanted to be tested so who knows how well they're doing! But a lot of the superforecasters described in this book are regular joes who do this for fun on the side. There's a lot of good advice in here on how you too can become a superforecaster - it's a learnable skill, if you're willing to put in the time and effort to practice. Some of the lessons are good to know for everyday rational thinking as well, and reading this book has made me want to try and put some of these suggestions into practice for myself."
59,0804136696,http://goodreads.com/user/show/81213541-cedric-chin,5,"I reread this book in preparation for my series of summaries about the Good Judgment Project, available at https://commoncog.com/blog/forecastin... https://commoncog.com/blog/how-do-you... and https://commoncog.com/blog/how-the-su... respectively. This is truly a tour de force of judgment and decision-making. When I first read it in 2016, I didn't realise that Tetlock had weaved in so many results from across the field of judgment and decision-making. He referenced Gary Klein (whose field of Naturalistic Decision Making is philosophically opposed to the heuristics and biases research program that Tetlock et all belong to), invited Daniel Kahneman for an adversarial collaboration (which is recounted in the book), used Jonathan Baron's Actively Open Minded Thinking test during the GJP (Baron is responsible for a textbook of the field that I regard as life changing; my review here: https://www.goodreads.com/review/show...).This is a definite must-read. "
60,0804136696,http://goodreads.com/user/show/66952286-scott,5,"I enjoyed this book about forecasting future events. It discusses how medicine changed around the time of ww2 when statistics were first used to commonly track likelihood of outcomes - leading to improved standardized treatments. It also discusses how the best self-selected forecasters in a study were consistently able to outperform even dedicated agencies with access to classified info when trained. The author managed this group for years and offers many insights into forecasting. Next, he discusses the use of forecasting (effective and not) in critical decisions like the bay of pigs, Cuban missile crisis, raid on bin laden, and attack on Saddam Hussein. Finally, he discusses his current work with GJOpen.com where anyone can test their skills at forecasting future events like elections, commodity prices, deployment of electric vehicles, etc."
61,0804136696,http://goodreads.com/user/show/64798070-jay-hennessey,4,"I really enjoyed this read, especially in my new role in professional sports. At risk of liking due to confirmation bias, I really enjoyed hearing the application of concepts from other books that I hold in the highest regard, I.e. Thinking Fast and Slow. I also enjoyed learning about Bayes Theory — coincidentally reading, The Theory that would not Die - How Bayes Rule Cracked the Enigma Code....Like most brilliant concepts, the idea of constantly updating one’s forecast, or hypothesis with new information sounded so simple, but the examples showed that this is usually the exception rather than the rule. The book also made me reflect on the concepts from Lean Start Up — Build, Measure, Learn. While the Lean Startup model is more about doing something and making adjustments, broadly, I see the concepts as the same. In Lean, one forms a hypothesis, determines how he will know if he is correct, and then sets out with a minimally viable product to test and measure the result — constantly adapting the product...or in Super-forecasting-Speak, constantly updating the prediction.I recommend this book to growth minded leaders of any organization. The concepts in this book, interleaved with so many other great works, are thought provoking and worthily of consideration in any space."
62,0804136696,http://goodreads.com/user/show/4057945-cristobal,5,"This book reminds of Emerson's phrase “The mind, once stretched by a new idea, never returns to its original dimensions.” which is what experienced after reading it. Mental models are the filters through which we see the world and tint all of our thinking. That is why taking the time and making the effort to screen how we think can make all the difference. ""Superforecasting"" builds a solid case on why making the best forecasting is not something reserved for a few lucky ones but that can be attained through meticulous analysis and keeping an open mind to information and other points of view. Mental biases will always be around, but knowing them and building the tools to try to limit their influence can make anyone a much more accurate forecaster and clear thinker.Definitely one of my favorite books on thinking right alongside ""Thinking Fast and Slow""."
63,0804136696,http://goodreads.com/user/show/7460650-viktor-szathmary,2,"While the book provides a reasonable overview of biases (and methods for correcting) in forecasting games, it is sadly disconnected from real-world decision making. For example, it treats a tennis match the same as international nuclear war. Only in academia can the two be managed the same way (as a binary prediction game, measured by your Brier score). While the author seems to have read Nassim Taleb's works (devoting a chapter to it), unfortunately they have missed the key points."
64,0804136696,http://goodreads.com/user/show/93933488-wen,4,"I liked it but it's not for everybody. The author wants to drive accountability for those who make predictions. I largely resonate with his observations and recommendations for providing better forecasts. However, as stated by the author, his recommendations should not be treated as gospel, but rather as another tool. I do like how the Good Judgement Project predictions and results are publicly available to view. "
65,0804136696,http://goodreads.com/user/show/34741339-robert-meyro,3,"While the title of this book drew me to believe that there could be hope in forecasting, I have definitely become more skeptical after reading this book. It nearly seems that the author felt the same way, the further one progressed in the book. That being said, there were some interesting anecdotes covered."
66,0804136696,http://goodreads.com/user/show/87001455-nathan-runke,5,Phenomenal book. Extremely well written and laid out. For a book on statistics and predicting the future (or as much as you can) it flows very well and is a nice read. Will probably be reading again. Has some valuable information on how you can improve your own personal ability to predict things/understand what is happening around the world.
67,0804136696,http://goodreads.com/user/show/39225478-jiliac,5,"The trilogy of books about prediction is ""Black Swan"" by Taleb, ""Thinking Fast and Slow"" by Kahneman, and this one, ""Superforecasting"" by Tetlock. All authors cite the two others profusely. They all have a different approach. Taleb being on the extremely rare events, Kahneman on the bias our brain creates, and Tetlock on the ""how to predict in practice"".Myself being more interested on the practical aspect of things, I was expecting Tetlock to be my prefered one. But in the end, I feel it's the one that I learned the least from. It comes with a very precise, detailed, and scientifically well studied, set of advices: practice a lot, reconsider yourself, diversify the opinions you base yourself on, and more... However, it doesn't deal with Taleb point, which was what I was expecting. (This expectation was unfair though, since it never advertised as such. That's why I still rate it five stars.)"
68,0804136696,http://goodreads.com/user/show/33662754-escada-emanuel,4,"Interesting book with a pretty smooth reading. Strikes a quite clear balance between what can actually be forecast and what cannot. Gives interesting tips on how to analyse complex realities, for example, through the inside and outside views or the 'fermi-zing' method. Also a highlight for the 'Auftragstaktik' from the Prussian military and now fashionably widespread on the ideal way to deal with uncertainty. "
69,0804136696,http://goodreads.com/user/show/118345071-brock-bank,5,"Late to the party on this one, but definitely deserves the hype. Favorite book I read this year, probably my favorite I’ve ever read around metacognition. Written to be readable, has practicable tips for being a better predictor. Tbf I was very predisposed to enjoying this book; throw in some prescient Michael Flynn criticism and I was sure to love it.Definitely worth the time to read the short appendix which outlines the main takes if you don’t want to give it a full read."
70,0804136696,http://goodreads.com/user/show/45921922-thiago-marzag-o,5,If I had read about Tetlock's work when I first started grad school my interests would have changed completely and I would have produced an entirely different dissertation. Too bad it took me so long to come across this.
71,0804136696,http://goodreads.com/user/show/12310532-taivo,4,"There was a decent concentration of content and good balance between storytelling versus relaying useful models, but I still felt the core ideas of the book could have been fit into a chapter or two, and the rest of the book was written to fill up 300 pages."
72,0804136696,http://goodreads.com/user/show/13493517-cenk-undey,4,An interesting high level summary of approaches to predicting life events. It makes a lot of references to the books I already read such as Kahleman’s Thinking Fast and Slow and Nassim N. Taleb’s The Black Swan. It is a fun book to go through.
73,0804136696,http://goodreads.com/user/show/15701574-allan-aksiim,3,"No point reading the entire thing. Valuable lessons yes, american style unnecessary pop-science fluff also. "
74,0804136696,http://goodreads.com/user/show/52223189-mario-vanhoucke,4,"Great book about the good judgement project and how some people can outperform experts in forecasting events. The book brings together some of my favourite authors/researchers and refers to the “illusion of control” by E. Langer, the work and quotes of Amos Tsversky, good old Bayes and his theorem (we should take an inside and outside view, but start with the outside one) and finally, in chapter 11, the work of Kahneman and Taleb (really great summary and discussion). Evidence based forecasting, it sounds like an easy-to-accept principle, but the authors argue it’s not. But they are hopeful: Where wisdom once was, quantification will now be. Read!"
75,0804136696,http://goodreads.com/user/show/64773125-gabi,5,"A great book, in a few words : ""This book is not about how to be happy.. is about how to be accurate!"" [from-the-end-of-chapter-6]"
76,0804136696,http://goodreads.com/user/show/1300459-sambasivan,4,Novel concept and well articulated. Contains principles that one can use for a lifetime. Be curious and improve yourself regularly to become a super forecaster. 
77,0804136696,http://goodreads.com/user/show/2385396-bradley-eylander,4,"This book is a slight contrast to the book I read before called, ""The Signal and the Noise: Why So Many Predictions Fail - But Some Don't"". Both books acknowledge that there are many things in life we can't predict and the difficulty of predicting them. This book covers people that are statistically good at predicting and why. Turns out its due to collecting data daily (compounding effect) and applying statistics. What I thought was ironic was the people that are good at predicting aren't the ones being paid lots of $$ working for organizations such as the CIA. It was hobbyist who's only incentive was Amazon gift cards."
78,0804136696,http://goodreads.com/user/show/1054712-kent-winward,4,"I would have known how much I'd like the book, if I had read the book first . . ."
79,0804136696,http://goodreads.com/user/show/11228-suman,0,3.5 stars
80,0804136696,http://goodreads.com/user/show/57644603-colin-paulish,5,Easy read with takeaways that you feel you could integrate and apply to your life almost immediately
81,0804136696,http://goodreads.com/user/show/82584091-raph-zindi,5,Wildly disruptive thinking! One to revisit from time to time. 
82,0804136696,http://goodreads.com/user/show/33743305-vinh,4,The book is based on scientific results which are quite interesting and insightful.
83,0804136696,http://goodreads.com/user/show/5822873-kaustubh,3,"A nice introduction (and not a guide) into the world of forecasting: 6.5/10Tetlock and (presumable ghostwriter) Gardner provide a nice introduction into the world of geopolitical forecasting. They detail Tetlock’s long-running (>15 yr) experiments on people’s abilities to forecast future outcomes on a variety of themes. Although the book sets itself up somewhat disingenuously as a guide for improvement in forecasting knowledge and ability, it is more of an introduction to forecasting, histogram statistics, calibration-verification-validation, and several anecdotal examples about people and events that conform to said underlying statistical theory. The examples are illuminating and at times, contradictory - while they are useful, one can’t help but wonder about where exactly they fall on the bell curve and whether they are representative of anything or not. There are many references to Kahneman’s Thinking Fast and Slow littered throughout the book. Sometimes, I got the feeling that I ought to be reading that book instead. Where Tetlock and Gardner excel is in driving down their take-home message: there is no single catch-all formula to “super”forecasting and rather, one should expect a slow-burn process of hard work, attentiveness, and a metered demeanor for marginal successes. However, keep in mind, that margins are everything in this field."
84,0804136696,http://goodreads.com/user/show/13608511-ka,5,What an excellent read. Highly recommend if you’re curious about how to reason more rigorously and want to make better predictions about where the world is headed. Rarely seen such gifted science writing!
85,0804136696,http://goodreads.com/user/show/38189944-mo,5,"Phenomenal! I think this is the one to beat in terms of a popular guide to the Decision and Cognitive Sciences and - more interestingly - how they can be used in the real world to make a real difference. All told in an eminently readable, deceptively simple-seeming fashion to boot. I honestly can not praise or recommend this book enough. "
86,0804136696,http://goodreads.com/user/show/25471547-ahmed,5,"You have to understand that this book is eminently and deeply rooted in the research program Tetlock pioneered and ran, the Good Judgement Project, and about the real people whom he found through the program to be great are forecasting geopolitical events in the three to six month range.My own personal goal before reading the book was to learn rigorous prediction—not necessarily answering geopolitical questions that were handed from somewhere else, but more like how many editors will show up on global Wikipedia today, or possibly more profitably, how much the stock market will lose today. I was prepared for the book to be an overly-academic treatise on the minutiae of the research program, or for a number of ways it could be operationally useless.But at each chapter, Tetlock had something very meaningful and useful to say to me about the business of prediction. At each stage he surprised me with the foxlike connections he drew between what he saw his forecasters doing and with other research programs and discoveries.(I was delighted to see my boy Duncan Watts cited on the emphatically post hoc nature of describing “significant” events. Aaron Brown, my poker-playing risk-managing guru, weighs in on the value of small but consistent wins. The once-cool-but-now-fascist-apologist Nassim Taleb makes appearances as the book struggles with whether its brand of forecasting is useful in the face of an extremistan world (answer: it is, because you can make money in an extremistan world). Of course Danny Kahneman is intertwined with the narrative, as Tetlock’s sounding board and colleague for many decades. Not mentioned in the book but in Tetlock’s five-part master-class on forecasting on Edge.org, my main man Anders Ericsson is cited on the trainability of forecasters.)Chapter one opens with trying to convince us that predictability exists and forecasting can help us (make or save us money, at the simplest). He invites us to contrast the unpredictability of cloud shapes with the predictability of a clock, and exposes this as the first of many false dichotomies throughout the book. “We live in a world of clocks and clouds and a vast jumble of other metaphors. Unpredictability and predictability coexist uneasily in the intricately interlocking systems that make up our bodies, our societies, and the cosmos. How predictable something is depends on what we are trying to predict, how far into the future, and under what circumstances.”He then sets up the core argument of the research program that IARPA paid for: how well can we do? A question like this should leave you speechless and befuddled—the US Government asked Tetlock and other university and industry programs to find out how well we can predict geopolitical outcomes over a 3–6 month time horizon, because nobody knew how well we did. Think of all the pundits, all the terrible books Tom Friedman pooped out, all the bloggers and tooters, and the realization that all that heat gave us no light should leave you breathless.In chapter two, Tetlock dives into the ugly story of medicine.Tetlock revisits this over and over again: medicine only very, very recently became a devotee of the randomized controlled trial. He has a wonderful set of vignettes of the vanguard of physicians who dragged their colleagues kicking and screaming into the evidence-based age after World War II. Its been a few weeks since I read this section but I think I will forever remember this story:‘When hospitals created cardiac care units to treat patients recovering from heart attacks, Cochrane proposed a randomized trial to determine whether the new units delivered better results than the old treatment, which was to send the patient home for monitoring and bed rest. Physicians balked. It was obvious the cardiac care units were superior, they said, and denying patients the best care would be unethical. … [but] Cochrane got his trial: some patients, randomly selected, were sent to the cardiac care units while others were sent home for monitoring and bed rest. Partway through the trial, Cochrane met with a group of the cardiologists who had tried to stop his experiment. He told them that he had preliminary results. The difference in outcomes between the two treatments was not statistically significant, he emphasized, but it appeared that patients might do slightly better in the cardiac care units. “They were vociferous in their abuse: ‘Archie,’ they said, ‘we always thought you were unethical. You must stop the trial at once.’ ” But then Cochrane revealed he had played a little trick. He had reversed the results: home care had done slightly better than the cardiac units. “There was dead silence and I felt rather sick because they were, after all, my medical colleagues.”’Yes, a lot of medicine is terribly “intuition-based” today—“it’s obvious this treatment is better”. But medicine has made great strides, over the last few decades, in acknowledging the risks and flaws of “intuition” and committing itself to the exacting requirements of randomized controlled trials.Programmers are slowly learning this, thankfully with less loss of life. Andrei Alexandrescu, in his “Writing Quick Code in C++, Quickly” talk in 2013, discussed this at length:‘You must measure everything. We all have intuition. And the intuition of programmers is always wrong. Outdated. Intuition ignores a lot of aspects of a complex reality. Today’s machine architectures are so complicated, there’re so many variables in flight at any point in time that it’s essentially impossible to consider them deterministic machines any more. They are not deterministic any more. So we make very often big mistakes when assuming things about what’s going to make fast code. [E.g.,] fewer instructions do not equal faster code. Data [access] is not always faster than computation. The only good intuition is “I should measure this stuff and see what happens.” To quote a classic, who is still alive, Walter Bight: “Measuring gives you a leg up on experts who are so good they don’t need to measure.” Walter and I have been working on optimizing bits and pieces of a project we work on and … whenever we think we know what we’re doing, we measure, and it’s just the other way around.’Here are two of the world’s leading experts in programming language design and implementation, openly saying “whenever we think we know what we’re doing, we measure, and it’s just the other way around.” That is probably worth tattooing on one’s forehead.This is relevant to the Good Judgement Project because it is the first time randomized controlled trials have been applied to geopolitical prediction, but also because dealing with evidence and weighing it is a key component of forecasting. Putting a collar on intuition helps to prevent it from ruining your predictions.The next chapter (chapter three) discusses the intricacies of keeping score. A project like this, and the task of improving one’s own forecasting, lives and dies by the pesky pernicious questions of exactly how you measure performance, and other experimental details. Tetlock shows how everything we might consider as “forecasts” (intelligence agencies, the revolting Thomas Friedman) is trash, is intellectual weasel-worded garbage. He details the kinds of questions amenable to his experiment, how to elicit probability estimates, how to enforce time horizons, how to factor in update frequencies, how to fuse predictions from groups, various research tools, etc.This chapter details how the Brier score works: forecasters answer a yes/no question with a probability. The score rewards emphatically correct answer, and punishes incorrect confidence.Chapter four gives a detailed overview of the project’s findings: how well Tetlock’s superforecasters did, and analyses of how and why they did so well. Tetlock really surprised me here by offering a very humble and honestly rigorous analysis of regression to the mean. He explains how in games of chance, regression to the mean crushes the winners after repeated rounds, whereas exercises of skill sees the winners only improve after succeeding rounds.“Each year, roughly 30% of the individual superforecasters fall from the ranks of the top 2% next year. But that also implies a good deal of consistency over time: 70% of superforecasters remain superforecasters.”Tetlock could have gone all business-book “Good to Great” (trash) on me. No. This is a well-reasoned and thoughtful argument that honestly explored the role of luck in forecasting. 30% annual replacement suggests some luck, but a lot of skill. That is a very powerful finding.Chapter five breaks down the intelligence of superforecasters, and six their math savvy. Findings: superforecasters are intelligent and also generically math-savvy, but intelligence and math skills are neither necessary nor sufficient for superforecasting performance.Chapter five has a really interesting discussion of Fermi analyses—you know, “how many piano tuners are in Chicago”. I did this piano tuner exercise for the first time while reading this book (despite having read about it here and there in the past), and that was a very insightful experience. Fermi analysis shows that, rather than making a big prediction that might have a lot of error, you can break the problem up into smaller problems whose errors are smaller, and whose errors stay small after combining them. Fermi analysis is cool, and I finally appreciate them.Chapter six also deals, almost spiritually, with the misguided “quest for meaning” and the herculean discipline needed to maintain a probabilistic outlook on life:‘Even in the face of tragedy, the probabilistic thinker will say, “Yes, there was an almost infinite number of paths that events could have taken, and it was incredibly unlikely that events would take the path that ended in my child’s death. But they had to take a path and that’s the one they took. That’s all there is to it.” In Kahneman’s terms, probabilistic thinkers take the outside view toward even profoundly identity-defining events, seeing them as quasi-random draws from distributions of once-possible worlds.’Forget living Biblically for a year. Try living like this for a day.Chapter seven examines whether superforecasters are plugged into the global news streams. Answer: yes to some degree, but it doesn’t really explain their performance versus regular non-super forecasters.Chapter eight examines the twin vexing problems of updating beliefs in light of new evidence, and getting better at making predictions in light of your past predictions. Because Black Lives Matter, consider police officers:“police officers spend a lot of time figuring out who is telling the truth and who is lying, but research has found they aren’t nearly as good at it as they think they are and they tend not to get better with experience. That’s because experience isn’t enough. It must be accompanied by clear feedback. … Psychologists who test police officers’ ability to spot lies in a controlled setting find a big gap between their confidence and their skill. And that gap grows as officers become more experienced and they assume, not unreasonably, that their experience has made them better lie detectors. As a result, officers grow confident faster than they grow accurate, meaning they grow increasingly overconfident.”This chapter also talks about the discipline to keep hindsight bias in the kennel, and the difficulty in acknowledging the role of luck:“People often assume that when a decision is followed by a good outcome, the decision was good, which isn’t always true, and can be dangerous if it blinds us to the flaws in our thinking.”In a book full of actionably valuable insights (to the aspiring forecaster), this discussion might be the most helpful. Chapter nine deals with teams, team dynamics, fusing algorithms for merging individuals’ predictions, and lots of interesting related things. Chapter ten discusses how leaders might respond to a team of forecasters, and the changes leaders have to make to best utilize them.Chapter eleven talks about the problems with Tetlock’s research platform. (As he himself states earlier in the book, a scientist will always specify the conditions under which they would change their minds.)“I see Kahneman’s and Taleb’s critiques as the strongest challenges to the notion of superforecasting.”Kahneman’s critique is, can forecasters permanently tame cognitive biases, and keep churning out winning forecasts year after year (or at least long enough to be useful)? Taleb’s critique is, can forecasters say anything about black swan events that dominate history?Both of these critiques, in my personal opinion, are surmountable, making Tetlock’s research agenda and this book well worth reading.Chapter twelve closes with Tetlock’s hopes for a future world where we keep score about forecast. It could be awesome. But we’d get used to it fast and start worrying about the next problem."
87,0804136696,http://goodreads.com/user/show/347507-pietro,5,"This book is ostensibly about predicting (some aspects of) the future *reliably*. But that framing sells it far short. Anyone interested in holding true beliefs about the world could benefit immensely from taking in this short book. As far as I'm concerned, predictions are just a really good way of keeping you honest, as they have been in science for centuries: if your beliefs lead to wrong predictions often, you need new beliefs.Lots of people --- newscasters, politicians, pundits --- make claims about what the future holds. The book opens by noting that, although we listen to them raptly, nobody keeps track of how reliable they are as forecasters. With a couple compelling bits of history, it draws an analogy with medicine before the 20th century, which was largely evidence-free and doctors were judged by credentials instead of track records.One big reason nobody keeps track of forecasters' abilities is that most forecasts are vague: ""there is a risk inflation will go up"". How much of a risk? Up by how much? Over what time frame? Unless these are specified, it is impossible to tell whether a forecast was right or wrong, and thus impossible to keep track of how good a forecaster is. The importance of precision is a MAJOR theme in the book; it's the only way to know how well we're doing as forecasters, and knowing that is the only way we can hope to improve.Obviously, high-status pundits and analysts who write op-eds and appear in the news have no incentive to have their performance tracked. They're already high-status; why would they risk being debunked? So Tetlock ran a massive experiment over several years where thousands of volunteers made predictions about hundreds of precise questions such as:""In the next year, will any country withdraw from the eurozone?"" ""How many additional countries will report cases of the Ebola virus before the end of this year?""""If a non-Chinese telecom firm wins a contract to provide internet in the Shanghai Free Trade Zone in the next two years, will Chinese citizens have access to Facebook or Twitter?""Volunteers, working in isolation, made probabilistic forecasts: ""there is a 65% chance Russia will annex a part of Ukraine in the next 6 months"". In the end, each was scored based on their hundreds of forecasts (using a ""Brier score""). As it turns out, about 2% of the volunteers did SUBSTANTIALLY better not only than chance, but better than famous analysts, pundits, and even the intelligence community, which has access to privileged information. And this wasn't luck either; these ""superforecasters"" continued to do incredibly well year in and year out.Let me say that again: there are normal, unknown people out there who can predict important geopolitical events SUBSTANTIALLY better than chance; better than all the pundits who get paid shitloads of money and invited to speak at Davos and meet with presidents; better even than intelligence analysts with access to classified information.How do they do it??That's what the rest of the book is about. In short, these are pretty smart, numerate, and informed people; but they're not OFF-THE-CHARTS smart, math-savvy, hyper news junkies. They're slightly above-average people who simply cultivate healthy mental habits.Habits like ""seek out many different opinions/analyses"", ""challenge your own beliefs"", ""examine past mistakes honestly""; ""put a question in context by relating it to similar questions""; habits we can all cultivate.This truly is one of those books that make your mind more powerful, that give you a new set of eyes. Highly recommended."
88,0804136696,http://goodreads.com/user/show/51856061-bilal-hafeez,4,"The talking heads on TV give such a convincing story about what the future holds that its hard not to believe them. But pinning them down to a time-frame and discrete future event is often next to impossible, so you can never determine whether they were right or wrong. When such talking heads are pinned down, their track-record turns out to be very poor, according to Philip Tetlock in his latest book ""Superforecasting: The Art and Science of Prediction"". This makes sense as they are picked more for their entertainment value, than their track-records.The real super-forecasters are people like Doug Lorch (ex-IBM, retired, lives in Santa Barbara), Mary Simpson (independent financial consultant, formerly regulatory affairs at utility Southern California Edison) and Devyn Duffy (welfare case worker, Pittsburgh). What they lack in TV appeal they make up for in their approach to forecasting.Tetlock and team found them as part of an IARPA, which is part of US Intelligence, project to improve forecasting of political and other events. He found that the way you approach forecasting rather than your credentials or political leanings matter most. If you're a one-big idea person (i.e. a hedgehog) you will struggle, while if you're a pragmatic tinkerer (i.e. a fox) you will do well. He listed the eleven commandments for aspiring super forecasters:1.Focus on questions where your handwork will pay off. So don't bother with a question like ""who will win the US election in 2028?"". It's too far out to bother predicting.2.Break big problems into smaller ones. 3.Strike the right balance between inside and outside views. There is nothing new under the sun. So first calculate the odds of an event happening based on similar ones in the past (i.e. top-down or outside view), then work-out the bottom-up/inside view of the specific event.4.Strike the right balance between under- and overreacting to evidence. Careful incorporate new evidence and update beliefs accordingly. The best forecasters are incremental belief updaters.5.Acknowledge the counter-arguments. If you believe military action never works, be open to the possibility that it might. Take both views and synthesise.6.Distinguish as many degrees of doubt as the problem permits. Your uncertainty dials needs more than 3 settings (certain, maybe, impossible). Translate vague hunches into numeric probabilities.7.Strike the right balance between under- and over-confidence. Understand the risks of rushing to judgment and of dawdling too long near ""maybe"".8.Don't justify or excuse your failures. Conduct unflinching postmortem.9.Bring out the best in others and let others bring out the best in you. Master team management: perspective taking (being able to reproduce the others argument to their satisfaction), precision questioning (clarify arguments so there is no misunderstanding), and constructive confrontation (learning to disagree without being disagreeable).10.Master the error-balancing bicycle. Learning requires doing, with good feedback that leaves no ambiguity about whether you are succeeding.11.Don't treat commandments as commandments!You can practise your prediction skills by joining Tetlock's Good Judgment Projectbilalhafeez.com"
89,0804136696,http://goodreads.com/user/show/7876548-chirayu-batra,4,"Seems like a true companion of Dan Kahneman's Thinking:fast and slow and surely influenced by his judgement psychology. Book is a reminder that world is complex equation and forecasting anything in it is not a simple phenomenon. The book also seems a good promotional material for the good judgement project, but that makes sense. I hope to join the project and make some of my own predictions, smart predictions. A nice read for psychology lovers and people interested in reading about the art of forecasting. "
90,0804136696,http://goodreads.com/user/show/66952286-scott,5,"I enjoyed this book about forecasting future events. It discusses how medicine changed around the time of ww2 when statistics were first used to commonly track likelihood of outcomes - leading to improved standardized treatments. It also discusses how the best self-selected forecasters in a study were consistently able to outperform even dedicated agencies with access to classified info when trained. The author managed this group for years and offers many insights into forecasting. Next, he discusses the use of forecasting (effective and not) in critical decisions like the bay of pigs, Cuban missile crisis, raid on bin laden, and attack on Saddam Hussein. Finally, he discusses his current work with GJOpen.com where anyone can test their skills at forecasting future events like elections, commodity prices, deployment of electric vehicles, etc."
91,0804136696,http://goodreads.com/user/show/64798070-jay-hennessey,4,"I really enjoyed this read, especially in my new role in professional sports. At risk of liking due to confirmation bias, I really enjoyed hearing the application of concepts from other books that I hold in the highest regard, I.e. Thinking Fast and Slow. I also enjoyed learning about Bayes Theory — coincidentally reading, The Theory that would not Die - How Bayes Rule Cracked the Enigma Code....Like most brilliant concepts, the idea of constantly updating one’s forecast, or hypothesis with new information sounded so simple, but the examples showed that this is usually the exception rather than the rule. The book also made me reflect on the concepts from Lean Start Up — Build, Measure, Learn. While the Lean Startup model is more about doing something and making adjustments, broadly, I see the concepts as the same. In Lean, one forms a hypothesis, determines how he will know if he is correct, and then sets out with a minimally viable product to test and measure the result — constantly adapting the product...or in Super-forecasting-Speak, constantly updating the prediction.I recommend this book to growth minded leaders of any organization. The concepts in this book, interleaved with so many other great works, are thought provoking and worthily of consideration in any space."
92,0804136696,http://goodreads.com/user/show/4057945-cristobal,5,"This book reminds of Emerson's phrase “The mind, once stretched by a new idea, never returns to its original dimensions.” which is what experienced after reading it. Mental models are the filters through which we see the world and tint all of our thinking. That is why taking the time and making the effort to screen how we think can make all the difference. ""Superforecasting"" builds a solid case on why making the best forecasting is not something reserved for a few lucky ones but that can be attained through meticulous analysis and keeping an open mind to information and other points of view. Mental biases will always be around, but knowing them and building the tools to try to limit their influence can make anyone a much more accurate forecaster and clear thinker.Definitely one of my favorite books on thinking right alongside ""Thinking Fast and Slow""."
93,0804136696,http://goodreads.com/user/show/7460650-viktor-szathmary,2,"While the book provides a reasonable overview of biases (and methods for correcting) in forecasting games, it is sadly disconnected from real-world decision making. For example, it treats a tennis match the same as international nuclear war. Only in academia can the two be managed the same way (as a binary prediction game, measured by your Brier score). While the author seems to have read Nassim Taleb's works (devoting a chapter to it), unfortunately they have missed the key points."
94,0804136696,http://goodreads.com/user/show/93933488-wen,4,"I liked it but it's not for everybody. The author wants to drive accountability for those who make predictions. I largely resonate with his observations and recommendations for providing better forecasts. However, as stated by the author, his recommendations should not be treated as gospel, but rather as another tool. I do like how the Good Judgement Project predictions and results are publicly available to view. "
95,0804136696,http://goodreads.com/user/show/34741339-robert-meyro,3,"While the title of this book drew me to believe that there could be hope in forecasting, I have definitely become more skeptical after reading this book. It nearly seems that the author felt the same way, the further one progressed in the book. That being said, there were some interesting anecdotes covered."
96,0804136696,http://goodreads.com/user/show/87001455-nathan-runke,5,Phenomenal book. Extremely well written and laid out. For a book on statistics and predicting the future (or as much as you can) it flows very well and is a nice read. Will probably be reading again. Has some valuable information on how you can improve your own personal ability to predict things/understand what is happening around the world.
97,0804136696,http://goodreads.com/user/show/39225478-jiliac,5,"The trilogy of books about prediction is ""Black Swan"" by Taleb, ""Thinking Fast and Slow"" by Kahneman, and this one, ""Superforecasting"" by Tetlock. All authors cite the two others profusely. They all have a different approach. Taleb being on the extremely rare events, Kahneman on the bias our brain creates, and Tetlock on the ""how to predict in practice"".Myself being more interested on the practical aspect of things, I was expecting Tetlock to be my prefered one. But in the end, I feel it's the one that I learned the least from. It comes with a very precise, detailed, and scientifically well studied, set of advices: practice a lot, reconsider yourself, diversify the opinions you base yourself on, and more... However, it doesn't deal with Taleb point, which was what I was expecting. (This expectation was unfair though, since it never advertised as such. That's why I still rate it five stars.)"
98,0804136696,http://goodreads.com/user/show/33662754-escada-emanuel,4,"Interesting book with a pretty smooth reading. Strikes a quite clear balance between what can actually be forecast and what cannot. Gives interesting tips on how to analyse complex realities, for example, through the inside and outside views or the 'fermi-zing' method. Also a highlight for the 'Auftragstaktik' from the Prussian military and now fashionably widespread on the ideal way to deal with uncertainty. "
99,0804136696,http://goodreads.com/user/show/118345071-brock-bank,5,"Late to the party on this one, but definitely deserves the hype. Favorite book I read this year, probably my favorite I’ve ever read around metacognition. Written to be readable, has practicable tips for being a better predictor. Tbf I was very predisposed to enjoying this book; throw in some prescient Michael Flynn criticism and I was sure to love it.Definitely worth the time to read the short appendix which outlines the main takes if you don’t want to give it a full read."
100,0804136696,http://goodreads.com/user/show/45921922-thiago-marzag-o,5,If I had read about Tetlock's work when I first started grad school my interests would have changed completely and I would have produced an entirely different dissertation. Too bad it took me so long to come across this.
101,0804136696,http://goodreads.com/user/show/12310532-taivo,4,"There was a decent concentration of content and good balance between storytelling versus relaying useful models, but I still felt the core ideas of the book could have been fit into a chapter or two, and the rest of the book was written to fill up 300 pages."
102,0804136696,http://goodreads.com/user/show/13493517-cenk-undey,4,An interesting high level summary of approaches to predicting life events. It makes a lot of references to the books I already read such as Kahleman’s Thinking Fast and Slow and Nassim N. Taleb’s The Black Swan. It is a fun book to go through.
103,0804136696,http://goodreads.com/user/show/15701574-allan-aksiim,3,"No point reading the entire thing. Valuable lessons yes, american style unnecessary pop-science fluff also. "
104,0804136696,http://goodreads.com/user/show/52223189-mario-vanhoucke,4,"Great book about the good judgement project and how some people can outperform experts in forecasting events. The book brings together some of my favourite authors/researchers and refers to the “illusion of control” by E. Langer, the work and quotes of Amos Tsversky, good old Bayes and his theorem (we should take an inside and outside view, but start with the outside one) and finally, in chapter 11, the work of Kahneman and Taleb (really great summary and discussion). Evidence based forecasting, it sounds like an easy-to-accept principle, but the authors argue it’s not. But they are hopeful: Where wisdom once was, quantification will now be. Read!"
105,0804136696,http://goodreads.com/user/show/64773125-gabi,5,"A great book, in a few words : ""This book is not about how to be happy.. is about how to be accurate!"" [from-the-end-of-chapter-6]"
106,0804136696,http://goodreads.com/user/show/1300459-sambasivan,4,Novel concept and well articulated. Contains principles that one can use for a lifetime. Be curious and improve yourself regularly to become a super forecaster. 
107,0804136696,http://goodreads.com/user/show/2385396-bradley-eylander,4,"This book is a slight contrast to the book I read before called, ""The Signal and the Noise: Why So Many Predictions Fail - But Some Don't"". Both books acknowledge that there are many things in life we can't predict and the difficulty of predicting them. This book covers people that are statistically good at predicting and why. Turns out its due to collecting data daily (compounding effect) and applying statistics. What I thought was ironic was the people that are good at predicting aren't the ones being paid lots of $$ working for organizations such as the CIA. It was hobbyist who's only incentive was Amazon gift cards."
108,0804136696,http://goodreads.com/user/show/1054712-kent-winward,4,"I would have known how much I'd like the book, if I had read the book first . . ."
109,0804136696,http://goodreads.com/user/show/11228-suman,0,3.5 stars
110,0804136696,http://goodreads.com/user/show/57644603-colin-paulish,5,Easy read with takeaways that you feel you could integrate and apply to your life almost immediately
111,0804136696,http://goodreads.com/user/show/82584091-raph-zindi,5,Wildly disruptive thinking! One to revisit from time to time. 
112,0804136696,http://goodreads.com/user/show/33743305-vinh,4,The book is based on scientific results which are quite interesting and insightful.
113,0804136696,http://goodreads.com/user/show/5822873-kaustubh,3,"A nice introduction (and not a guide) into the world of forecasting: 6.5/10Tetlock and (presumable ghostwriter) Gardner provide a nice introduction into the world of geopolitical forecasting. They detail Tetlock’s long-running (>15 yr) experiments on people’s abilities to forecast future outcomes on a variety of themes. Although the book sets itself up somewhat disingenuously as a guide for improvement in forecasting knowledge and ability, it is more of an introduction to forecasting, histogram statistics, calibration-verification-validation, and several anecdotal examples about people and events that conform to said underlying statistical theory. The examples are illuminating and at times, contradictory - while they are useful, one can’t help but wonder about where exactly they fall on the bell curve and whether they are representative of anything or not. There are many references to Kahneman’s Thinking Fast and Slow littered throughout the book. Sometimes, I got the feeling that I ought to be reading that book instead. Where Tetlock and Gardner excel is in driving down their take-home message: there is no single catch-all formula to “super”forecasting and rather, one should expect a slow-burn process of hard work, attentiveness, and a metered demeanor for marginal successes. However, keep in mind, that margins are everything in this field."
114,0804136696,http://goodreads.com/user/show/13608511-ka,5,What an excellent read. Highly recommend if you’re curious about how to reason more rigorously and want to make better predictions about where the world is headed. Rarely seen such gifted science writing!
115,0804136696,http://goodreads.com/user/show/38189944-mo,5,"Phenomenal! I think this is the one to beat in terms of a popular guide to the Decision and Cognitive Sciences and - more interestingly - how they can be used in the real world to make a real difference. All told in an eminently readable, deceptively simple-seeming fashion to boot. I honestly can not praise or recommend this book enough. "
116,0804136696,http://goodreads.com/user/show/25471547-ahmed,5,"You have to understand that this book is eminently and deeply rooted in the research program Tetlock pioneered and ran, the Good Judgement Project, and about the real people whom he found through the program to be great are forecasting geopolitical events in the three to six month range.My own personal goal before reading the book was to learn rigorous prediction—not necessarily answering geopolitical questions that were handed from somewhere else, but more like how many editors will show up on global Wikipedia today, or possibly more profitably, how much the stock market will lose today. I was prepared for the book to be an overly-academic treatise on the minutiae of the research program, or for a number of ways it could be operationally useless.But at each chapter, Tetlock had something very meaningful and useful to say to me about the business of prediction. At each stage he surprised me with the foxlike connections he drew between what he saw his forecasters doing and with other research programs and discoveries.(I was delighted to see my boy Duncan Watts cited on the emphatically post hoc nature of describing “significant” events. Aaron Brown, my poker-playing risk-managing guru, weighs in on the value of small but consistent wins. The once-cool-but-now-fascist-apologist Nassim Taleb makes appearances as the book struggles with whether its brand of forecasting is useful in the face of an extremistan world (answer: it is, because you can make money in an extremistan world). Of course Danny Kahneman is intertwined with the narrative, as Tetlock’s sounding board and colleague for many decades. Not mentioned in the book but in Tetlock’s five-part master-class on forecasting on Edge.org, my main man Anders Ericsson is cited on the trainability of forecasters.)Chapter one opens with trying to convince us that predictability exists and forecasting can help us (make or save us money, at the simplest). He invites us to contrast the unpredictability of cloud shapes with the predictability of a clock, and exposes this as the first of many false dichotomies throughout the book. “We live in a world of clocks and clouds and a vast jumble of other metaphors. Unpredictability and predictability coexist uneasily in the intricately interlocking systems that make up our bodies, our societies, and the cosmos. How predictable something is depends on what we are trying to predict, how far into the future, and under what circumstances.”He then sets up the core argument of the research program that IARPA paid for: how well can we do? A question like this should leave you speechless and befuddled—the US Government asked Tetlock and other university and industry programs to find out how well we can predict geopolitical outcomes over a 3–6 month time horizon, because nobody knew how well we did. Think of all the pundits, all the terrible books Tom Friedman pooped out, all the bloggers and tooters, and the realization that all that heat gave us no light should leave you breathless.In chapter two, Tetlock dives into the ugly story of medicine.Tetlock revisits this over and over again: medicine only very, very recently became a devotee of the randomized controlled trial. He has a wonderful set of vignettes of the vanguard of physicians who dragged their colleagues kicking and screaming into the evidence-based age after World War II. Its been a few weeks since I read this section but I think I will forever remember this story:‘When hospitals created cardiac care units to treat patients recovering from heart attacks, Cochrane proposed a randomized trial to determine whether the new units delivered better results than the old treatment, which was to send the patient home for monitoring and bed rest. Physicians balked. It was obvious the cardiac care units were superior, they said, and denying patients the best care would be unethical. … [but] Cochrane got his trial: some patients, randomly selected, were sent to the cardiac care units while others were sent home for monitoring and bed rest. Partway through the trial, Cochrane met with a group of the cardiologists who had tried to stop his experiment. He told them that he had preliminary results. The difference in outcomes between the two treatments was not statistically significant, he emphasized, but it appeared that patients might do slightly better in the cardiac care units. “They were vociferous in their abuse: ‘Archie,’ they said, ‘we always thought you were unethical. You must stop the trial at once.’ ” But then Cochrane revealed he had played a little trick. He had reversed the results: home care had done slightly better than the cardiac units. “There was dead silence and I felt rather sick because they were, after all, my medical colleagues.”’Yes, a lot of medicine is terribly “intuition-based” today—“it’s obvious this treatment is better”. But medicine has made great strides, over the last few decades, in acknowledging the risks and flaws of “intuition” and committing itself to the exacting requirements of randomized controlled trials.Programmers are slowly learning this, thankfully with less loss of life. Andrei Alexandrescu, in his “Writing Quick Code in C++, Quickly” talk in 2013, discussed this at length:‘You must measure everything. We all have intuition. And the intuition of programmers is always wrong. Outdated. Intuition ignores a lot of aspects of a complex reality. Today’s machine architectures are so complicated, there’re so many variables in flight at any point in time that it’s essentially impossible to consider them deterministic machines any more. They are not deterministic any more. So we make very often big mistakes when assuming things about what’s going to make fast code. [E.g.,] fewer instructions do not equal faster code. Data [access] is not always faster than computation. The only good intuition is “I should measure this stuff and see what happens.” To quote a classic, who is still alive, Walter Bight: “Measuring gives you a leg up on experts who are so good they don’t need to measure.” Walter and I have been working on optimizing bits and pieces of a project we work on and … whenever we think we know what we’re doing, we measure, and it’s just the other way around.’Here are two of the world’s leading experts in programming language design and implementation, openly saying “whenever we think we know what we’re doing, we measure, and it’s just the other way around.” That is probably worth tattooing on one’s forehead.This is relevant to the Good Judgement Project because it is the first time randomized controlled trials have been applied to geopolitical prediction, but also because dealing with evidence and weighing it is a key component of forecasting. Putting a collar on intuition helps to prevent it from ruining your predictions.The next chapter (chapter three) discusses the intricacies of keeping score. A project like this, and the task of improving one’s own forecasting, lives and dies by the pesky pernicious questions of exactly how you measure performance, and other experimental details. Tetlock shows how everything we might consider as “forecasts” (intelligence agencies, the revolting Thomas Friedman) is trash, is intellectual weasel-worded garbage. He details the kinds of questions amenable to his experiment, how to elicit probability estimates, how to enforce time horizons, how to factor in update frequencies, how to fuse predictions from groups, various research tools, etc.This chapter details how the Brier score works: forecasters answer a yes/no question with a probability. The score rewards emphatically correct answer, and punishes incorrect confidence.Chapter four gives a detailed overview of the project’s findings: how well Tetlock’s superforecasters did, and analyses of how and why they did so well. Tetlock really surprised me here by offering a very humble and honestly rigorous analysis of regression to the mean. He explains how in games of chance, regression to the mean crushes the winners after repeated rounds, whereas exercises of skill sees the winners only improve after succeeding rounds.“Each year, roughly 30% of the individual superforecasters fall from the ranks of the top 2% next year. But that also implies a good deal of consistency over time: 70% of superforecasters remain superforecasters.”Tetlock could have gone all business-book “Good to Great” (trash) on me. No. This is a well-reasoned and thoughtful argument that honestly explored the role of luck in forecasting. 30% annual replacement suggests some luck, but a lot of skill. That is a very powerful finding.Chapter five breaks down the intelligence of superforecasters, and six their math savvy. Findings: superforecasters are intelligent and also generically math-savvy, but intelligence and math skills are neither necessary nor sufficient for superforecasting performance.Chapter five has a really interesting discussion of Fermi analyses—you know, “how many piano tuners are in Chicago”. I did this piano tuner exercise for the first time while reading this book (despite having read about it here and there in the past), and that was a very insightful experience. Fermi analysis shows that, rather than making a big prediction that might have a lot of error, you can break the problem up into smaller problems whose errors are smaller, and whose errors stay small after combining them. Fermi analysis is cool, and I finally appreciate them.Chapter six also deals, almost spiritually, with the misguided “quest for meaning” and the herculean discipline needed to maintain a probabilistic outlook on life:‘Even in the face of tragedy, the probabilistic thinker will say, “Yes, there was an almost infinite number of paths that events could have taken, and it was incredibly unlikely that events would take the path that ended in my child’s death. But they had to take a path and that’s the one they took. That’s all there is to it.” In Kahneman’s terms, probabilistic thinkers take the outside view toward even profoundly identity-defining events, seeing them as quasi-random draws from distributions of once-possible worlds.’Forget living Biblically for a year. Try living like this for a day.Chapter seven examines whether superforecasters are plugged into the global news streams. Answer: yes to some degree, but it doesn’t really explain their performance versus regular non-super forecasters.Chapter eight examines the twin vexing problems of updating beliefs in light of new evidence, and getting better at making predictions in light of your past predictions. Because Black Lives Matter, consider police officers:“police officers spend a lot of time figuring out who is telling the truth and who is lying, but research has found they aren’t nearly as good at it as they think they are and they tend not to get better with experience. That’s because experience isn’t enough. It must be accompanied by clear feedback. … Psychologists who test police officers’ ability to spot lies in a controlled setting find a big gap between their confidence and their skill. And that gap grows as officers become more experienced and they assume, not unreasonably, that their experience has made them better lie detectors. As a result, officers grow confident faster than they grow accurate, meaning they grow increasingly overconfident.”This chapter also talks about the discipline to keep hindsight bias in the kennel, and the difficulty in acknowledging the role of luck:“People often assume that when a decision is followed by a good outcome, the decision was good, which isn’t always true, and can be dangerous if it blinds us to the flaws in our thinking.”In a book full of actionably valuable insights (to the aspiring forecaster), this discussion might be the most helpful. Chapter nine deals with teams, team dynamics, fusing algorithms for merging individuals’ predictions, and lots of interesting related things. Chapter ten discusses how leaders might respond to a team of forecasters, and the changes leaders have to make to best utilize them.Chapter eleven talks about the problems with Tetlock’s research platform. (As he himself states earlier in the book, a scientist will always specify the conditions under which they would change their minds.)“I see Kahneman’s and Taleb’s critiques as the strongest challenges to the notion of superforecasting.”Kahneman’s critique is, can forecasters permanently tame cognitive biases, and keep churning out winning forecasts year after year (or at least long enough to be useful)? Taleb’s critique is, can forecasters say anything about black swan events that dominate history?Both of these critiques, in my personal opinion, are surmountable, making Tetlock’s research agenda and this book well worth reading.Chapter twelve closes with Tetlock’s hopes for a future world where we keep score about forecast. It could be awesome. But we’d get used to it fast and start worrying about the next problem."
117,0804136696,http://goodreads.com/user/show/347507-pietro,5,"This book is ostensibly about predicting (some aspects of) the future *reliably*. But that framing sells it far short. Anyone interested in holding true beliefs about the world could benefit immensely from taking in this short book. As far as I'm concerned, predictions are just a really good way of keeping you honest, as they have been in science for centuries: if your beliefs lead to wrong predictions often, you need new beliefs.Lots of people --- newscasters, politicians, pundits --- make claims about what the future holds. The book opens by noting that, although we listen to them raptly, nobody keeps track of how reliable they are as forecasters. With a couple compelling bits of history, it draws an analogy with medicine before the 20th century, which was largely evidence-free and doctors were judged by credentials instead of track records.One big reason nobody keeps track of forecasters' abilities is that most forecasts are vague: ""there is a risk inflation will go up"". How much of a risk? Up by how much? Over what time frame? Unless these are specified, it is impossible to tell whether a forecast was right or wrong, and thus impossible to keep track of how good a forecaster is. The importance of precision is a MAJOR theme in the book; it's the only way to know how well we're doing as forecasters, and knowing that is the only way we can hope to improve.Obviously, high-status pundits and analysts who write op-eds and appear in the news have no incentive to have their performance tracked. They're already high-status; why would they risk being debunked? So Tetlock ran a massive experiment over several years where thousands of volunteers made predictions about hundreds of precise questions such as:""In the next year, will any country withdraw from the eurozone?"" ""How many additional countries will report cases of the Ebola virus before the end of this year?""""If a non-Chinese telecom firm wins a contract to provide internet in the Shanghai Free Trade Zone in the next two years, will Chinese citizens have access to Facebook or Twitter?""Volunteers, working in isolation, made probabilistic forecasts: ""there is a 65% chance Russia will annex a part of Ukraine in the next 6 months"". In the end, each was scored based on their hundreds of forecasts (using a ""Brier score""). As it turns out, about 2% of the volunteers did SUBSTANTIALLY better not only than chance, but better than famous analysts, pundits, and even the intelligence community, which has access to privileged information. And this wasn't luck either; these ""superforecasters"" continued to do incredibly well year in and year out.Let me say that again: there are normal, unknown people out there who can predict important geopolitical events SUBSTANTIALLY better than chance; better than all the pundits who get paid shitloads of money and invited to speak at Davos and meet with presidents; better even than intelligence analysts with access to classified information.How do they do it??That's what the rest of the book is about. In short, these are pretty smart, numerate, and informed people; but they're not OFF-THE-CHARTS smart, math-savvy, hyper news junkies. They're slightly above-average people who simply cultivate healthy mental habits.Habits like ""seek out many different opinions/analyses"", ""challenge your own beliefs"", ""examine past mistakes honestly""; ""put a question in context by relating it to similar questions""; habits we can all cultivate.This truly is one of those books that make your mind more powerful, that give you a new set of eyes. Highly recommended."
118,0804136696,http://goodreads.com/user/show/51856061-bilal-hafeez,4,"The talking heads on TV give such a convincing story about what the future holds that its hard not to believe them. But pinning them down to a time-frame and discrete future event is often next to impossible, so you can never determine whether they were right or wrong. When such talking heads are pinned down, their track-record turns out to be very poor, according to Philip Tetlock in his latest book ""Superforecasting: The Art and Science of Prediction"". This makes sense as they are picked more for their entertainment value, than their track-records.The real super-forecasters are people like Doug Lorch (ex-IBM, retired, lives in Santa Barbara), Mary Simpson (independent financial consultant, formerly regulatory affairs at utility Southern California Edison) and Devyn Duffy (welfare case worker, Pittsburgh). What they lack in TV appeal they make up for in their approach to forecasting.Tetlock and team found them as part of an IARPA, which is part of US Intelligence, project to improve forecasting of political and other events. He found that the way you approach forecasting rather than your credentials or political leanings matter most. If you're a one-big idea person (i.e. a hedgehog) you will struggle, while if you're a pragmatic tinkerer (i.e. a fox) you will do well. He listed the eleven commandments for aspiring super forecasters:1.Focus on questions where your handwork will pay off. So don't bother with a question like ""who will win the US election in 2028?"". It's too far out to bother predicting.2.Break big problems into smaller ones. 3.Strike the right balance between inside and outside views. There is nothing new under the sun. So first calculate the odds of an event happening based on similar ones in the past (i.e. top-down or outside view), then work-out the bottom-up/inside view of the specific event.4.Strike the right balance between under- and overreacting to evidence. Careful incorporate new evidence and update beliefs accordingly. The best forecasters are incremental belief updaters.5.Acknowledge the counter-arguments. If you believe military action never works, be open to the possibility that it might. Take both views and synthesise.6.Distinguish as many degrees of doubt as the problem permits. Your uncertainty dials needs more than 3 settings (certain, maybe, impossible). Translate vague hunches into numeric probabilities.7.Strike the right balance between under- and over-confidence. Understand the risks of rushing to judgment and of dawdling too long near ""maybe"".8.Don't justify or excuse your failures. Conduct unflinching postmortem.9.Bring out the best in others and let others bring out the best in you. Master team management: perspective taking (being able to reproduce the others argument to their satisfaction), precision questioning (clarify arguments so there is no misunderstanding), and constructive confrontation (learning to disagree without being disagreeable).10.Master the error-balancing bicycle. Learning requires doing, with good feedback that leaves no ambiguity about whether you are succeeding.11.Don't treat commandments as commandments!You can practise your prediction skills by joining Tetlock's Good Judgment Projectbilalhafeez.com"
119,0804136696,http://goodreads.com/user/show/7876548-chirayu-batra,4,"Seems like a true companion of Dan Kahneman's Thinking:fast and slow and surely influenced by his judgement psychology. Book is a reminder that world is complex equation and forecasting anything in it is not a simple phenomenon. The book also seems a good promotional material for the good judgement project, but that makes sense. I hope to join the project and make some of my own predictions, smart predictions. A nice read for psychology lovers and people interested in reading about the art of forecasting. "
120,0804136696,http://goodreads.com/user/show/66952286-scott,5,"I enjoyed this book about forecasting future events. It discusses how medicine changed around the time of ww2 when statistics were first used to commonly track likelihood of outcomes - leading to improved standardized treatments. It also discusses how the best self-selected forecasters in a study were consistently able to outperform even dedicated agencies with access to classified info when trained. The author managed this group for years and offers many insights into forecasting. Next, he discusses the use of forecasting (effective and not) in critical decisions like the bay of pigs, Cuban missile crisis, raid on bin laden, and attack on Saddam Hussein. Finally, he discusses his current work with GJOpen.com where anyone can test their skills at forecasting future events like elections, commodity prices, deployment of electric vehicles, etc."
121,0804136696,http://goodreads.com/user/show/64798070-jay-hennessey,4,"I really enjoyed this read, especially in my new role in professional sports. At risk of liking due to confirmation bias, I really enjoyed hearing the application of concepts from other books that I hold in the highest regard, I.e. Thinking Fast and Slow. I also enjoyed learning about Bayes Theory — coincidentally reading, The Theory that would not Die - How Bayes Rule Cracked the Enigma Code....Like most brilliant concepts, the idea of constantly updating one’s forecast, or hypothesis with new information sounded so simple, but the examples showed that this is usually the exception rather than the rule. The book also made me reflect on the concepts from Lean Start Up — Build, Measure, Learn. While the Lean Startup model is more about doing something and making adjustments, broadly, I see the concepts as the same. In Lean, one forms a hypothesis, determines how he will know if he is correct, and then sets out with a minimally viable product to test and measure the result — constantly adapting the product...or in Super-forecasting-Speak, constantly updating the prediction.I recommend this book to growth minded leaders of any organization. The concepts in this book, interleaved with so many other great works, are thought provoking and worthily of consideration in any space."
122,0804136696,http://goodreads.com/user/show/4057945-cristobal,5,"This book reminds of Emerson's phrase “The mind, once stretched by a new idea, never returns to its original dimensions.” which is what experienced after reading it. Mental models are the filters through which we see the world and tint all of our thinking. That is why taking the time and making the effort to screen how we think can make all the difference. ""Superforecasting"" builds a solid case on why making the best forecasting is not something reserved for a few lucky ones but that can be attained through meticulous analysis and keeping an open mind to information and other points of view. Mental biases will always be around, but knowing them and building the tools to try to limit their influence can make anyone a much more accurate forecaster and clear thinker.Definitely one of my favorite books on thinking right alongside ""Thinking Fast and Slow""."
123,0804136696,http://goodreads.com/user/show/7460650-viktor-szathmary,2,"While the book provides a reasonable overview of biases (and methods for correcting) in forecasting games, it is sadly disconnected from real-world decision making. For example, it treats a tennis match the same as international nuclear war. Only in academia can the two be managed the same way (as a binary prediction game, measured by your Brier score). While the author seems to have read Nassim Taleb's works (devoting a chapter to it), unfortunately they have missed the key points."
124,0804136696,http://goodreads.com/user/show/93933488-wen,4,"I liked it but it's not for everybody. The author wants to drive accountability for those who make predictions. I largely resonate with his observations and recommendations for providing better forecasts. However, as stated by the author, his recommendations should not be treated as gospel, but rather as another tool. I do like how the Good Judgement Project predictions and results are publicly available to view. "
125,0804136696,http://goodreads.com/user/show/34741339-robert-meyro,3,"While the title of this book drew me to believe that there could be hope in forecasting, I have definitely become more skeptical after reading this book. It nearly seems that the author felt the same way, the further one progressed in the book. That being said, there were some interesting anecdotes covered."
126,0804136696,http://goodreads.com/user/show/87001455-nathan-runke,5,Phenomenal book. Extremely well written and laid out. For a book on statistics and predicting the future (or as much as you can) it flows very well and is a nice read. Will probably be reading again. Has some valuable information on how you can improve your own personal ability to predict things/understand what is happening around the world.
127,0804136696,http://goodreads.com/user/show/39225478-jiliac,5,"The trilogy of books about prediction is ""Black Swan"" by Taleb, ""Thinking Fast and Slow"" by Kahneman, and this one, ""Superforecasting"" by Tetlock. All authors cite the two others profusely. They all have a different approach. Taleb being on the extremely rare events, Kahneman on the bias our brain creates, and Tetlock on the ""how to predict in practice"".Myself being more interested on the practical aspect of things, I was expecting Tetlock to be my prefered one. But in the end, I feel it's the one that I learned the least from. It comes with a very precise, detailed, and scientifically well studied, set of advices: practice a lot, reconsider yourself, diversify the opinions you base yourself on, and more... However, it doesn't deal with Taleb point, which was what I was expecting. (This expectation was unfair though, since it never advertised as such. That's why I still rate it five stars.)"
128,0804136696,http://goodreads.com/user/show/33662754-escada-emanuel,4,"Interesting book with a pretty smooth reading. Strikes a quite clear balance between what can actually be forecast and what cannot. Gives interesting tips on how to analyse complex realities, for example, through the inside and outside views or the 'fermi-zing' method. Also a highlight for the 'Auftragstaktik' from the Prussian military and now fashionably widespread on the ideal way to deal with uncertainty. "
129,0804136696,http://goodreads.com/user/show/118345071-brock-bank,5,"Late to the party on this one, but definitely deserves the hype. Favorite book I read this year, probably my favorite I’ve ever read around metacognition. Written to be readable, has practicable tips for being a better predictor. Tbf I was very predisposed to enjoying this book; throw in some prescient Michael Flynn criticism and I was sure to love it.Definitely worth the time to read the short appendix which outlines the main takes if you don’t want to give it a full read."
130,0804136696,http://goodreads.com/user/show/45921922-thiago-marzag-o,5,If I had read about Tetlock's work when I first started grad school my interests would have changed completely and I would have produced an entirely different dissertation. Too bad it took me so long to come across this.
131,0804136696,http://goodreads.com/user/show/12310532-taivo,4,"There was a decent concentration of content and good balance between storytelling versus relaying useful models, but I still felt the core ideas of the book could have been fit into a chapter or two, and the rest of the book was written to fill up 300 pages."
132,0804136696,http://goodreads.com/user/show/13493517-cenk-undey,4,An interesting high level summary of approaches to predicting life events. It makes a lot of references to the books I already read such as Kahleman’s Thinking Fast and Slow and Nassim N. Taleb’s The Black Swan. It is a fun book to go through.
133,0804136696,http://goodreads.com/user/show/15701574-allan-aksiim,3,"No point reading the entire thing. Valuable lessons yes, american style unnecessary pop-science fluff also. "
134,0804136696,http://goodreads.com/user/show/52223189-mario-vanhoucke,4,"Great book about the good judgement project and how some people can outperform experts in forecasting events. The book brings together some of my favourite authors/researchers and refers to the “illusion of control” by E. Langer, the work and quotes of Amos Tsversky, good old Bayes and his theorem (we should take an inside and outside view, but start with the outside one) and finally, in chapter 11, the work of Kahneman and Taleb (really great summary and discussion). Evidence based forecasting, it sounds like an easy-to-accept principle, but the authors argue it’s not. But they are hopeful: Where wisdom once was, quantification will now be. Read!"
135,0804136696,http://goodreads.com/user/show/64773125-gabi,5,"A great book, in a few words : ""This book is not about how to be happy.. is about how to be accurate!"" [from-the-end-of-chapter-6]"
136,0804136696,http://goodreads.com/user/show/1300459-sambasivan,4,Novel concept and well articulated. Contains principles that one can use for a lifetime. Be curious and improve yourself regularly to become a super forecaster. 
137,0804136696,http://goodreads.com/user/show/2385396-bradley-eylander,4,"This book is a slight contrast to the book I read before called, ""The Signal and the Noise: Why So Many Predictions Fail - But Some Don't"". Both books acknowledge that there are many things in life we can't predict and the difficulty of predicting them. This book covers people that are statistically good at predicting and why. Turns out its due to collecting data daily (compounding effect) and applying statistics. What I thought was ironic was the people that are good at predicting aren't the ones being paid lots of $$ working for organizations such as the CIA. It was hobbyist who's only incentive was Amazon gift cards."
138,0804136696,http://goodreads.com/user/show/1054712-kent-winward,4,"I would have known how much I'd like the book, if I had read the book first . . ."
139,0804136696,http://goodreads.com/user/show/11228-suman,0,3.5 stars
140,0804136696,http://goodreads.com/user/show/57644603-colin-paulish,5,Easy read with takeaways that you feel you could integrate and apply to your life almost immediately
141,0804136696,http://goodreads.com/user/show/82584091-raph-zindi,5,Wildly disruptive thinking! One to revisit from time to time. 
142,0804136696,http://goodreads.com/user/show/33743305-vinh,4,The book is based on scientific results which are quite interesting and insightful.
143,0804136696,http://goodreads.com/user/show/5822873-kaustubh,3,"A nice introduction (and not a guide) into the world of forecasting: 6.5/10Tetlock and (presumable ghostwriter) Gardner provide a nice introduction into the world of geopolitical forecasting. They detail Tetlock’s long-running (>15 yr) experiments on people’s abilities to forecast future outcomes on a variety of themes. Although the book sets itself up somewhat disingenuously as a guide for improvement in forecasting knowledge and ability, it is more of an introduction to forecasting, histogram statistics, calibration-verification-validation, and several anecdotal examples about people and events that conform to said underlying statistical theory. The examples are illuminating and at times, contradictory - while they are useful, one can’t help but wonder about where exactly they fall on the bell curve and whether they are representative of anything or not. There are many references to Kahneman’s Thinking Fast and Slow littered throughout the book. Sometimes, I got the feeling that I ought to be reading that book instead. Where Tetlock and Gardner excel is in driving down their take-home message: there is no single catch-all formula to “super”forecasting and rather, one should expect a slow-burn process of hard work, attentiveness, and a metered demeanor for marginal successes. However, keep in mind, that margins are everything in this field."
144,0804136696,http://goodreads.com/user/show/13608511-ka,5,What an excellent read. Highly recommend if you’re curious about how to reason more rigorously and want to make better predictions about where the world is headed. Rarely seen such gifted science writing!
145,0804136696,http://goodreads.com/user/show/38189944-mo,5,"Phenomenal! I think this is the one to beat in terms of a popular guide to the Decision and Cognitive Sciences and - more interestingly - how they can be used in the real world to make a real difference. All told in an eminently readable, deceptively simple-seeming fashion to boot. I honestly can not praise or recommend this book enough. "
146,0804136696,http://goodreads.com/user/show/25471547-ahmed,5,"You have to understand that this book is eminently and deeply rooted in the research program Tetlock pioneered and ran, the Good Judgement Project, and about the real people whom he found through the program to be great are forecasting geopolitical events in the three to six month range.My own personal goal before reading the book was to learn rigorous prediction—not necessarily answering geopolitical questions that were handed from somewhere else, but more like how many editors will show up on global Wikipedia today, or possibly more profitably, how much the stock market will lose today. I was prepared for the book to be an overly-academic treatise on the minutiae of the research program, or for a number of ways it could be operationally useless.But at each chapter, Tetlock had something very meaningful and useful to say to me about the business of prediction. At each stage he surprised me with the foxlike connections he drew between what he saw his forecasters doing and with other research programs and discoveries.(I was delighted to see my boy Duncan Watts cited on the emphatically post hoc nature of describing “significant” events. Aaron Brown, my poker-playing risk-managing guru, weighs in on the value of small but consistent wins. The once-cool-but-now-fascist-apologist Nassim Taleb makes appearances as the book struggles with whether its brand of forecasting is useful in the face of an extremistan world (answer: it is, because you can make money in an extremistan world). Of course Danny Kahneman is intertwined with the narrative, as Tetlock’s sounding board and colleague for many decades. Not mentioned in the book but in Tetlock’s five-part master-class on forecasting on Edge.org, my main man Anders Ericsson is cited on the trainability of forecasters.)Chapter one opens with trying to convince us that predictability exists and forecasting can help us (make or save us money, at the simplest). He invites us to contrast the unpredictability of cloud shapes with the predictability of a clock, and exposes this as the first of many false dichotomies throughout the book. “We live in a world of clocks and clouds and a vast jumble of other metaphors. Unpredictability and predictability coexist uneasily in the intricately interlocking systems that make up our bodies, our societies, and the cosmos. How predictable something is depends on what we are trying to predict, how far into the future, and under what circumstances.”He then sets up the core argument of the research program that IARPA paid for: how well can we do? A question like this should leave you speechless and befuddled—the US Government asked Tetlock and other university and industry programs to find out how well we can predict geopolitical outcomes over a 3–6 month time horizon, because nobody knew how well we did. Think of all the pundits, all the terrible books Tom Friedman pooped out, all the bloggers and tooters, and the realization that all that heat gave us no light should leave you breathless.In chapter two, Tetlock dives into the ugly story of medicine.Tetlock revisits this over and over again: medicine only very, very recently became a devotee of the randomized controlled trial. He has a wonderful set of vignettes of the vanguard of physicians who dragged their colleagues kicking and screaming into the evidence-based age after World War II. Its been a few weeks since I read this section but I think I will forever remember this story:‘When hospitals created cardiac care units to treat patients recovering from heart attacks, Cochrane proposed a randomized trial to determine whether the new units delivered better results than the old treatment, which was to send the patient home for monitoring and bed rest. Physicians balked. It was obvious the cardiac care units were superior, they said, and denying patients the best care would be unethical. … [but] Cochrane got his trial: some patients, randomly selected, were sent to the cardiac care units while others were sent home for monitoring and bed rest. Partway through the trial, Cochrane met with a group of the cardiologists who had tried to stop his experiment. He told them that he had preliminary results. The difference in outcomes between the two treatments was not statistically significant, he emphasized, but it appeared that patients might do slightly better in the cardiac care units. “They were vociferous in their abuse: ‘Archie,’ they said, ‘we always thought you were unethical. You must stop the trial at once.’ ” But then Cochrane revealed he had played a little trick. He had reversed the results: home care had done slightly better than the cardiac units. “There was dead silence and I felt rather sick because they were, after all, my medical colleagues.”’Yes, a lot of medicine is terribly “intuition-based” today—“it’s obvious this treatment is better”. But medicine has made great strides, over the last few decades, in acknowledging the risks and flaws of “intuition” and committing itself to the exacting requirements of randomized controlled trials.Programmers are slowly learning this, thankfully with less loss of life. Andrei Alexandrescu, in his “Writing Quick Code in C++, Quickly” talk in 2013, discussed this at length:‘You must measure everything. We all have intuition. And the intuition of programmers is always wrong. Outdated. Intuition ignores a lot of aspects of a complex reality. Today’s machine architectures are so complicated, there’re so many variables in flight at any point in time that it’s essentially impossible to consider them deterministic machines any more. They are not deterministic any more. So we make very often big mistakes when assuming things about what’s going to make fast code. [E.g.,] fewer instructions do not equal faster code. Data [access] is not always faster than computation. The only good intuition is “I should measure this stuff and see what happens.” To quote a classic, who is still alive, Walter Bight: “Measuring gives you a leg up on experts who are so good they don’t need to measure.” Walter and I have been working on optimizing bits and pieces of a project we work on and … whenever we think we know what we’re doing, we measure, and it’s just the other way around.’Here are two of the world’s leading experts in programming language design and implementation, openly saying “whenever we think we know what we’re doing, we measure, and it’s just the other way around.” That is probably worth tattooing on one’s forehead.This is relevant to the Good Judgement Project because it is the first time randomized controlled trials have been applied to geopolitical prediction, but also because dealing with evidence and weighing it is a key component of forecasting. Putting a collar on intuition helps to prevent it from ruining your predictions.The next chapter (chapter three) discusses the intricacies of keeping score. A project like this, and the task of improving one’s own forecasting, lives and dies by the pesky pernicious questions of exactly how you measure performance, and other experimental details. Tetlock shows how everything we might consider as “forecasts” (intelligence agencies, the revolting Thomas Friedman) is trash, is intellectual weasel-worded garbage. He details the kinds of questions amenable to his experiment, how to elicit probability estimates, how to enforce time horizons, how to factor in update frequencies, how to fuse predictions from groups, various research tools, etc.This chapter details how the Brier score works: forecasters answer a yes/no question with a probability. The score rewards emphatically correct answer, and punishes incorrect confidence.Chapter four gives a detailed overview of the project’s findings: how well Tetlock’s superforecasters did, and analyses of how and why they did so well. Tetlock really surprised me here by offering a very humble and honestly rigorous analysis of regression to the mean. He explains how in games of chance, regression to the mean crushes the winners after repeated rounds, whereas exercises of skill sees the winners only improve after succeeding rounds.“Each year, roughly 30% of the individual superforecasters fall from the ranks of the top 2% next year. But that also implies a good deal of consistency over time: 70% of superforecasters remain superforecasters.”Tetlock could have gone all business-book “Good to Great” (trash) on me. No. This is a well-reasoned and thoughtful argument that honestly explored the role of luck in forecasting. 30% annual replacement suggests some luck, but a lot of skill. That is a very powerful finding.Chapter five breaks down the intelligence of superforecasters, and six their math savvy. Findings: superforecasters are intelligent and also generically math-savvy, but intelligence and math skills are neither necessary nor sufficient for superforecasting performance.Chapter five has a really interesting discussion of Fermi analyses—you know, “how many piano tuners are in Chicago”. I did this piano tuner exercise for the first time while reading this book (despite having read about it here and there in the past), and that was a very insightful experience. Fermi analysis shows that, rather than making a big prediction that might have a lot of error, you can break the problem up into smaller problems whose errors are smaller, and whose errors stay small after combining them. Fermi analysis is cool, and I finally appreciate them.Chapter six also deals, almost spiritually, with the misguided “quest for meaning” and the herculean discipline needed to maintain a probabilistic outlook on life:‘Even in the face of tragedy, the probabilistic thinker will say, “Yes, there was an almost infinite number of paths that events could have taken, and it was incredibly unlikely that events would take the path that ended in my child’s death. But they had to take a path and that’s the one they took. That’s all there is to it.” In Kahneman’s terms, probabilistic thinkers take the outside view toward even profoundly identity-defining events, seeing them as quasi-random draws from distributions of once-possible worlds.’Forget living Biblically for a year. Try living like this for a day.Chapter seven examines whether superforecasters are plugged into the global news streams. Answer: yes to some degree, but it doesn’t really explain their performance versus regular non-super forecasters.Chapter eight examines the twin vexing problems of updating beliefs in light of new evidence, and getting better at making predictions in light of your past predictions. Because Black Lives Matter, consider police officers:“police officers spend a lot of time figuring out who is telling the truth and who is lying, but research has found they aren’t nearly as good at it as they think they are and they tend not to get better with experience. That’s because experience isn’t enough. It must be accompanied by clear feedback. … Psychologists who test police officers’ ability to spot lies in a controlled setting find a big gap between their confidence and their skill. And that gap grows as officers become more experienced and they assume, not unreasonably, that their experience has made them better lie detectors. As a result, officers grow confident faster than they grow accurate, meaning they grow increasingly overconfident.”This chapter also talks about the discipline to keep hindsight bias in the kennel, and the difficulty in acknowledging the role of luck:“People often assume that when a decision is followed by a good outcome, the decision was good, which isn’t always true, and can be dangerous if it blinds us to the flaws in our thinking.”In a book full of actionably valuable insights (to the aspiring forecaster), this discussion might be the most helpful. Chapter nine deals with teams, team dynamics, fusing algorithms for merging individuals’ predictions, and lots of interesting related things. Chapter ten discusses how leaders might respond to a team of forecasters, and the changes leaders have to make to best utilize them.Chapter eleven talks about the problems with Tetlock’s research platform. (As he himself states earlier in the book, a scientist will always specify the conditions under which they would change their minds.)“I see Kahneman’s and Taleb’s critiques as the strongest challenges to the notion of superforecasting.”Kahneman’s critique is, can forecasters permanently tame cognitive biases, and keep churning out winning forecasts year after year (or at least long enough to be useful)? Taleb’s critique is, can forecasters say anything about black swan events that dominate history?Both of these critiques, in my personal opinion, are surmountable, making Tetlock’s research agenda and this book well worth reading.Chapter twelve closes with Tetlock’s hopes for a future world where we keep score about forecast. It could be awesome. But we’d get used to it fast and start worrying about the next problem."
147,0804136696,http://goodreads.com/user/show/347507-pietro,5,"This book is ostensibly about predicting (some aspects of) the future *reliably*. But that framing sells it far short. Anyone interested in holding true beliefs about the world could benefit immensely from taking in this short book. As far as I'm concerned, predictions are just a really good way of keeping you honest, as they have been in science for centuries: if your beliefs lead to wrong predictions often, you need new beliefs.Lots of people --- newscasters, politicians, pundits --- make claims about what the future holds. The book opens by noting that, although we listen to them raptly, nobody keeps track of how reliable they are as forecasters. With a couple compelling bits of history, it draws an analogy with medicine before the 20th century, which was largely evidence-free and doctors were judged by credentials instead of track records.One big reason nobody keeps track of forecasters' abilities is that most forecasts are vague: ""there is a risk inflation will go up"". How much of a risk? Up by how much? Over what time frame? Unless these are specified, it is impossible to tell whether a forecast was right or wrong, and thus impossible to keep track of how good a forecaster is. The importance of precision is a MAJOR theme in the book; it's the only way to know how well we're doing as forecasters, and knowing that is the only way we can hope to improve.Obviously, high-status pundits and analysts who write op-eds and appear in the news have no incentive to have their performance tracked. They're already high-status; why would they risk being debunked? So Tetlock ran a massive experiment over several years where thousands of volunteers made predictions about hundreds of precise questions such as:""In the next year, will any country withdraw from the eurozone?"" ""How many additional countries will report cases of the Ebola virus before the end of this year?""""If a non-Chinese telecom firm wins a contract to provide internet in the Shanghai Free Trade Zone in the next two years, will Chinese citizens have access to Facebook or Twitter?""Volunteers, working in isolation, made probabilistic forecasts: ""there is a 65% chance Russia will annex a part of Ukraine in the next 6 months"". In the end, each was scored based on their hundreds of forecasts (using a ""Brier score""). As it turns out, about 2% of the volunteers did SUBSTANTIALLY better not only than chance, but better than famous analysts, pundits, and even the intelligence community, which has access to privileged information. And this wasn't luck either; these ""superforecasters"" continued to do incredibly well year in and year out.Let me say that again: there are normal, unknown people out there who can predict important geopolitical events SUBSTANTIALLY better than chance; better than all the pundits who get paid shitloads of money and invited to speak at Davos and meet with presidents; better even than intelligence analysts with access to classified information.How do they do it??That's what the rest of the book is about. In short, these are pretty smart, numerate, and informed people; but they're not OFF-THE-CHARTS smart, math-savvy, hyper news junkies. They're slightly above-average people who simply cultivate healthy mental habits.Habits like ""seek out many different opinions/analyses"", ""challenge your own beliefs"", ""examine past mistakes honestly""; ""put a question in context by relating it to similar questions""; habits we can all cultivate.This truly is one of those books that make your mind more powerful, that give you a new set of eyes. Highly recommended."
148,0804136696,http://goodreads.com/user/show/51856061-bilal-hafeez,4,"The talking heads on TV give such a convincing story about what the future holds that its hard not to believe them. But pinning them down to a time-frame and discrete future event is often next to impossible, so you can never determine whether they were right or wrong. When such talking heads are pinned down, their track-record turns out to be very poor, according to Philip Tetlock in his latest book ""Superforecasting: The Art and Science of Prediction"". This makes sense as they are picked more for their entertainment value, than their track-records.The real super-forecasters are people like Doug Lorch (ex-IBM, retired, lives in Santa Barbara), Mary Simpson (independent financial consultant, formerly regulatory affairs at utility Southern California Edison) and Devyn Duffy (welfare case worker, Pittsburgh). What they lack in TV appeal they make up for in their approach to forecasting.Tetlock and team found them as part of an IARPA, which is part of US Intelligence, project to improve forecasting of political and other events. He found that the way you approach forecasting rather than your credentials or political leanings matter most. If you're a one-big idea person (i.e. a hedgehog) you will struggle, while if you're a pragmatic tinkerer (i.e. a fox) you will do well. He listed the eleven commandments for aspiring super forecasters:1.Focus on questions where your handwork will pay off. So don't bother with a question like ""who will win the US election in 2028?"". It's too far out to bother predicting.2.Break big problems into smaller ones. 3.Strike the right balance between inside and outside views. There is nothing new under the sun. So first calculate the odds of an event happening based on similar ones in the past (i.e. top-down or outside view), then work-out the bottom-up/inside view of the specific event.4.Strike the right balance between under- and overreacting to evidence. Careful incorporate new evidence and update beliefs accordingly. The best forecasters are incremental belief updaters.5.Acknowledge the counter-arguments. If you believe military action never works, be open to the possibility that it might. Take both views and synthesise.6.Distinguish as many degrees of doubt as the problem permits. Your uncertainty dials needs more than 3 settings (certain, maybe, impossible). Translate vague hunches into numeric probabilities.7.Strike the right balance between under- and over-confidence. Understand the risks of rushing to judgment and of dawdling too long near ""maybe"".8.Don't justify or excuse your failures. Conduct unflinching postmortem.9.Bring out the best in others and let others bring out the best in you. Master team management: perspective taking (being able to reproduce the others argument to their satisfaction), precision questioning (clarify arguments so there is no misunderstanding), and constructive confrontation (learning to disagree without being disagreeable).10.Master the error-balancing bicycle. Learning requires doing, with good feedback that leaves no ambiguity about whether you are succeeding.11.Don't treat commandments as commandments!You can practise your prediction skills by joining Tetlock's Good Judgment Projectbilalhafeez.com"
149,0804136696,http://goodreads.com/user/show/7876548-chirayu-batra,4,"Seems like a true companion of Dan Kahneman's Thinking:fast and slow and surely influenced by his judgement psychology. Book is a reminder that world is complex equation and forecasting anything in it is not a simple phenomenon. The book also seems a good promotional material for the good judgement project, but that makes sense. I hope to join the project and make some of my own predictions, smart predictions. A nice read for psychology lovers and people interested in reading about the art of forecasting. "
150,0804136696,http://goodreads.com/user/show/66952286-scott,5,"I enjoyed this book about forecasting future events. It discusses how medicine changed around the time of ww2 when statistics were first used to commonly track likelihood of outcomes - leading to improved standardized treatments. It also discusses how the best self-selected forecasters in a study were consistently able to outperform even dedicated agencies with access to classified info when trained. The author managed this group for years and offers many insights into forecasting. Next, he discusses the use of forecasting (effective and not) in critical decisions like the bay of pigs, Cuban missile crisis, raid on bin laden, and attack on Saddam Hussein. Finally, he discusses his current work with GJOpen.com where anyone can test their skills at forecasting future events like elections, commodity prices, deployment of electric vehicles, etc."
151,0804136696,http://goodreads.com/user/show/64798070-jay-hennessey,4,"I really enjoyed this read, especially in my new role in professional sports. At risk of liking due to confirmation bias, I really enjoyed hearing the application of concepts from other books that I hold in the highest regard, I.e. Thinking Fast and Slow. I also enjoyed learning about Bayes Theory — coincidentally reading, The Theory that would not Die - How Bayes Rule Cracked the Enigma Code....Like most brilliant concepts, the idea of constantly updating one’s forecast, or hypothesis with new information sounded so simple, but the examples showed that this is usually the exception rather than the rule. The book also made me reflect on the concepts from Lean Start Up — Build, Measure, Learn. While the Lean Startup model is more about doing something and making adjustments, broadly, I see the concepts as the same. In Lean, one forms a hypothesis, determines how he will know if he is correct, and then sets out with a minimally viable product to test and measure the result — constantly adapting the product...or in Super-forecasting-Speak, constantly updating the prediction.I recommend this book to growth minded leaders of any organization. The concepts in this book, interleaved with so many other great works, are thought provoking and worthily of consideration in any space."
152,0804136696,http://goodreads.com/user/show/4057945-cristobal,5,"This book reminds of Emerson's phrase “The mind, once stretched by a new idea, never returns to its original dimensions.” which is what experienced after reading it. Mental models are the filters through which we see the world and tint all of our thinking. That is why taking the time and making the effort to screen how we think can make all the difference. ""Superforecasting"" builds a solid case on why making the best forecasting is not something reserved for a few lucky ones but that can be attained through meticulous analysis and keeping an open mind to information and other points of view. Mental biases will always be around, but knowing them and building the tools to try to limit their influence can make anyone a much more accurate forecaster and clear thinker.Definitely one of my favorite books on thinking right alongside ""Thinking Fast and Slow""."
153,0804136696,http://goodreads.com/user/show/7460650-viktor-szathmary,2,"While the book provides a reasonable overview of biases (and methods for correcting) in forecasting games, it is sadly disconnected from real-world decision making. For example, it treats a tennis match the same as international nuclear war. Only in academia can the two be managed the same way (as a binary prediction game, measured by your Brier score). While the author seems to have read Nassim Taleb's works (devoting a chapter to it), unfortunately they have missed the key points."
154,0804136696,http://goodreads.com/user/show/93933488-wen,4,"I liked it but it's not for everybody. The author wants to drive accountability for those who make predictions. I largely resonate with his observations and recommendations for providing better forecasts. However, as stated by the author, his recommendations should not be treated as gospel, but rather as another tool. I do like how the Good Judgement Project predictions and results are publicly available to view. "
155,0804136696,http://goodreads.com/user/show/34741339-robert-meyro,3,"While the title of this book drew me to believe that there could be hope in forecasting, I have definitely become more skeptical after reading this book. It nearly seems that the author felt the same way, the further one progressed in the book. That being said, there were some interesting anecdotes covered."
156,0804136696,http://goodreads.com/user/show/87001455-nathan-runke,5,Phenomenal book. Extremely well written and laid out. For a book on statistics and predicting the future (or as much as you can) it flows very well and is a nice read. Will probably be reading again. Has some valuable information on how you can improve your own personal ability to predict things/understand what is happening around the world.
157,0804136696,http://goodreads.com/user/show/39225478-jiliac,5,"The trilogy of books about prediction is ""Black Swan"" by Taleb, ""Thinking Fast and Slow"" by Kahneman, and this one, ""Superforecasting"" by Tetlock. All authors cite the two others profusely. They all have a different approach. Taleb being on the extremely rare events, Kahneman on the bias our brain creates, and Tetlock on the ""how to predict in practice"".Myself being more interested on the practical aspect of things, I was expecting Tetlock to be my prefered one. But in the end, I feel it's the one that I learned the least from. It comes with a very precise, detailed, and scientifically well studied, set of advices: practice a lot, reconsider yourself, diversify the opinions you base yourself on, and more... However, it doesn't deal with Taleb point, which was what I was expecting. (This expectation was unfair though, since it never advertised as such. That's why I still rate it five stars.)"
158,0804136696,http://goodreads.com/user/show/33662754-escada-emanuel,4,"Interesting book with a pretty smooth reading. Strikes a quite clear balance between what can actually be forecast and what cannot. Gives interesting tips on how to analyse complex realities, for example, through the inside and outside views or the 'fermi-zing' method. Also a highlight for the 'Auftragstaktik' from the Prussian military and now fashionably widespread on the ideal way to deal with uncertainty. "
159,0804136696,http://goodreads.com/user/show/118345071-brock-bank,5,"Late to the party on this one, but definitely deserves the hype. Favorite book I read this year, probably my favorite I’ve ever read around metacognition. Written to be readable, has practicable tips for being a better predictor. Tbf I was very predisposed to enjoying this book; throw in some prescient Michael Flynn criticism and I was sure to love it.Definitely worth the time to read the short appendix which outlines the main takes if you don’t want to give it a full read."
160,0804136696,http://goodreads.com/user/show/45921922-thiago-marzag-o,5,If I had read about Tetlock's work when I first started grad school my interests would have changed completely and I would have produced an entirely different dissertation. Too bad it took me so long to come across this.
161,0804136696,http://goodreads.com/user/show/12310532-taivo,4,"There was a decent concentration of content and good balance between storytelling versus relaying useful models, but I still felt the core ideas of the book could have been fit into a chapter or two, and the rest of the book was written to fill up 300 pages."
162,0804136696,http://goodreads.com/user/show/13493517-cenk-undey,4,An interesting high level summary of approaches to predicting life events. It makes a lot of references to the books I already read such as Kahleman’s Thinking Fast and Slow and Nassim N. Taleb’s The Black Swan. It is a fun book to go through.
163,0804136696,http://goodreads.com/user/show/15701574-allan-aksiim,3,"No point reading the entire thing. Valuable lessons yes, american style unnecessary pop-science fluff also. "
164,0804136696,http://goodreads.com/user/show/52223189-mario-vanhoucke,4,"Great book about the good judgement project and how some people can outperform experts in forecasting events. The book brings together some of my favourite authors/researchers and refers to the “illusion of control” by E. Langer, the work and quotes of Amos Tsversky, good old Bayes and his theorem (we should take an inside and outside view, but start with the outside one) and finally, in chapter 11, the work of Kahneman and Taleb (really great summary and discussion). Evidence based forecasting, it sounds like an easy-to-accept principle, but the authors argue it’s not. But they are hopeful: Where wisdom once was, quantification will now be. Read!"
165,0804136696,http://goodreads.com/user/show/64773125-gabi,5,"A great book, in a few words : ""This book is not about how to be happy.. is about how to be accurate!"" [from-the-end-of-chapter-6]"
166,0804136696,http://goodreads.com/user/show/1300459-sambasivan,4,Novel concept and well articulated. Contains principles that one can use for a lifetime. Be curious and improve yourself regularly to become a super forecaster. 
167,0804136696,http://goodreads.com/user/show/2385396-bradley-eylander,4,"This book is a slight contrast to the book I read before called, ""The Signal and the Noise: Why So Many Predictions Fail - But Some Don't"". Both books acknowledge that there are many things in life we can't predict and the difficulty of predicting them. This book covers people that are statistically good at predicting and why. Turns out its due to collecting data daily (compounding effect) and applying statistics. What I thought was ironic was the people that are good at predicting aren't the ones being paid lots of $$ working for organizations such as the CIA. It was hobbyist who's only incentive was Amazon gift cards."
168,0804136696,http://goodreads.com/user/show/1054712-kent-winward,4,"I would have known how much I'd like the book, if I had read the book first . . ."
169,0804136696,http://goodreads.com/user/show/11228-suman,0,3.5 stars
170,0804136696,http://goodreads.com/user/show/57644603-colin-paulish,5,Easy read with takeaways that you feel you could integrate and apply to your life almost immediately
171,0804136696,http://goodreads.com/user/show/82584091-raph-zindi,5,Wildly disruptive thinking! One to revisit from time to time. 
172,0804136696,http://goodreads.com/user/show/33743305-vinh,4,The book is based on scientific results which are quite interesting and insightful.
173,0804136696,http://goodreads.com/user/show/5822873-kaustubh,3,"A nice introduction (and not a guide) into the world of forecasting: 6.5/10Tetlock and (presumable ghostwriter) Gardner provide a nice introduction into the world of geopolitical forecasting. They detail Tetlock’s long-running (>15 yr) experiments on people’s abilities to forecast future outcomes on a variety of themes. Although the book sets itself up somewhat disingenuously as a guide for improvement in forecasting knowledge and ability, it is more of an introduction to forecasting, histogram statistics, calibration-verification-validation, and several anecdotal examples about people and events that conform to said underlying statistical theory. The examples are illuminating and at times, contradictory - while they are useful, one can’t help but wonder about where exactly they fall on the bell curve and whether they are representative of anything or not. There are many references to Kahneman’s Thinking Fast and Slow littered throughout the book. Sometimes, I got the feeling that I ought to be reading that book instead. Where Tetlock and Gardner excel is in driving down their take-home message: there is no single catch-all formula to “super”forecasting and rather, one should expect a slow-burn process of hard work, attentiveness, and a metered demeanor for marginal successes. However, keep in mind, that margins are everything in this field."
174,0804136696,http://goodreads.com/user/show/13608511-ka,5,What an excellent read. Highly recommend if you’re curious about how to reason more rigorously and want to make better predictions about where the world is headed. Rarely seen such gifted science writing!
175,0804136696,http://goodreads.com/user/show/38189944-mo,5,"Phenomenal! I think this is the one to beat in terms of a popular guide to the Decision and Cognitive Sciences and - more interestingly - how they can be used in the real world to make a real difference. All told in an eminently readable, deceptively simple-seeming fashion to boot. I honestly can not praise or recommend this book enough. "
176,0804136696,http://goodreads.com/user/show/25471547-ahmed,5,"You have to understand that this book is eminently and deeply rooted in the research program Tetlock pioneered and ran, the Good Judgement Project, and about the real people whom he found through the program to be great are forecasting geopolitical events in the three to six month range.My own personal goal before reading the book was to learn rigorous prediction—not necessarily answering geopolitical questions that were handed from somewhere else, but more like how many editors will show up on global Wikipedia today, or possibly more profitably, how much the stock market will lose today. I was prepared for the book to be an overly-academic treatise on the minutiae of the research program, or for a number of ways it could be operationally useless.But at each chapter, Tetlock had something very meaningful and useful to say to me about the business of prediction. At each stage he surprised me with the foxlike connections he drew between what he saw his forecasters doing and with other research programs and discoveries.(I was delighted to see my boy Duncan Watts cited on the emphatically post hoc nature of describing “significant” events. Aaron Brown, my poker-playing risk-managing guru, weighs in on the value of small but consistent wins. The once-cool-but-now-fascist-apologist Nassim Taleb makes appearances as the book struggles with whether its brand of forecasting is useful in the face of an extremistan world (answer: it is, because you can make money in an extremistan world). Of course Danny Kahneman is intertwined with the narrative, as Tetlock’s sounding board and colleague for many decades. Not mentioned in the book but in Tetlock’s five-part master-class on forecasting on Edge.org, my main man Anders Ericsson is cited on the trainability of forecasters.)Chapter one opens with trying to convince us that predictability exists and forecasting can help us (make or save us money, at the simplest). He invites us to contrast the unpredictability of cloud shapes with the predictability of a clock, and exposes this as the first of many false dichotomies throughout the book. “We live in a world of clocks and clouds and a vast jumble of other metaphors. Unpredictability and predictability coexist uneasily in the intricately interlocking systems that make up our bodies, our societies, and the cosmos. How predictable something is depends on what we are trying to predict, how far into the future, and under what circumstances.”He then sets up the core argument of the research program that IARPA paid for: how well can we do? A question like this should leave you speechless and befuddled—the US Government asked Tetlock and other university and industry programs to find out how well we can predict geopolitical outcomes over a 3–6 month time horizon, because nobody knew how well we did. Think of all the pundits, all the terrible books Tom Friedman pooped out, all the bloggers and tooters, and the realization that all that heat gave us no light should leave you breathless.In chapter two, Tetlock dives into the ugly story of medicine.Tetlock revisits this over and over again: medicine only very, very recently became a devotee of the randomized controlled trial. He has a wonderful set of vignettes of the vanguard of physicians who dragged their colleagues kicking and screaming into the evidence-based age after World War II. Its been a few weeks since I read this section but I think I will forever remember this story:‘When hospitals created cardiac care units to treat patients recovering from heart attacks, Cochrane proposed a randomized trial to determine whether the new units delivered better results than the old treatment, which was to send the patient home for monitoring and bed rest. Physicians balked. It was obvious the cardiac care units were superior, they said, and denying patients the best care would be unethical. … [but] Cochrane got his trial: some patients, randomly selected, were sent to the cardiac care units while others were sent home for monitoring and bed rest. Partway through the trial, Cochrane met with a group of the cardiologists who had tried to stop his experiment. He told them that he had preliminary results. The difference in outcomes between the two treatments was not statistically significant, he emphasized, but it appeared that patients might do slightly better in the cardiac care units. “They were vociferous in their abuse: ‘Archie,’ they said, ‘we always thought you were unethical. You must stop the trial at once.’ ” But then Cochrane revealed he had played a little trick. He had reversed the results: home care had done slightly better than the cardiac units. “There was dead silence and I felt rather sick because they were, after all, my medical colleagues.”’Yes, a lot of medicine is terribly “intuition-based” today—“it’s obvious this treatment is better”. But medicine has made great strides, over the last few decades, in acknowledging the risks and flaws of “intuition” and committing itself to the exacting requirements of randomized controlled trials.Programmers are slowly learning this, thankfully with less loss of life. Andrei Alexandrescu, in his “Writing Quick Code in C++, Quickly” talk in 2013, discussed this at length:‘You must measure everything. We all have intuition. And the intuition of programmers is always wrong. Outdated. Intuition ignores a lot of aspects of a complex reality. Today’s machine architectures are so complicated, there’re so many variables in flight at any point in time that it’s essentially impossible to consider them deterministic machines any more. They are not deterministic any more. So we make very often big mistakes when assuming things about what’s going to make fast code. [E.g.,] fewer instructions do not equal faster code. Data [access] is not always faster than computation. The only good intuition is “I should measure this stuff and see what happens.” To quote a classic, who is still alive, Walter Bight: “Measuring gives you a leg up on experts who are so good they don’t need to measure.” Walter and I have been working on optimizing bits and pieces of a project we work on and … whenever we think we know what we’re doing, we measure, and it’s just the other way around.’Here are two of the world’s leading experts in programming language design and implementation, openly saying “whenever we think we know what we’re doing, we measure, and it’s just the other way around.” That is probably worth tattooing on one’s forehead.This is relevant to the Good Judgement Project because it is the first time randomized controlled trials have been applied to geopolitical prediction, but also because dealing with evidence and weighing it is a key component of forecasting. Putting a collar on intuition helps to prevent it from ruining your predictions.The next chapter (chapter three) discusses the intricacies of keeping score. A project like this, and the task of improving one’s own forecasting, lives and dies by the pesky pernicious questions of exactly how you measure performance, and other experimental details. Tetlock shows how everything we might consider as “forecasts” (intelligence agencies, the revolting Thomas Friedman) is trash, is intellectual weasel-worded garbage. He details the kinds of questions amenable to his experiment, how to elicit probability estimates, how to enforce time horizons, how to factor in update frequencies, how to fuse predictions from groups, various research tools, etc.This chapter details how the Brier score works: forecasters answer a yes/no question with a probability. The score rewards emphatically correct answer, and punishes incorrect confidence.Chapter four gives a detailed overview of the project’s findings: how well Tetlock’s superforecasters did, and analyses of how and why they did so well. Tetlock really surprised me here by offering a very humble and honestly rigorous analysis of regression to the mean. He explains how in games of chance, regression to the mean crushes the winners after repeated rounds, whereas exercises of skill sees the winners only improve after succeeding rounds.“Each year, roughly 30% of the individual superforecasters fall from the ranks of the top 2% next year. But that also implies a good deal of consistency over time: 70% of superforecasters remain superforecasters.”Tetlock could have gone all business-book “Good to Great” (trash) on me. No. This is a well-reasoned and thoughtful argument that honestly explored the role of luck in forecasting. 30% annual replacement suggests some luck, but a lot of skill. That is a very powerful finding.Chapter five breaks down the intelligence of superforecasters, and six their math savvy. Findings: superforecasters are intelligent and also generically math-savvy, but intelligence and math skills are neither necessary nor sufficient for superforecasting performance.Chapter five has a really interesting discussion of Fermi analyses—you know, “how many piano tuners are in Chicago”. I did this piano tuner exercise for the first time while reading this book (despite having read about it here and there in the past), and that was a very insightful experience. Fermi analysis shows that, rather than making a big prediction that might have a lot of error, you can break the problem up into smaller problems whose errors are smaller, and whose errors stay small after combining them. Fermi analysis is cool, and I finally appreciate them.Chapter six also deals, almost spiritually, with the misguided “quest for meaning” and the herculean discipline needed to maintain a probabilistic outlook on life:‘Even in the face of tragedy, the probabilistic thinker will say, “Yes, there was an almost infinite number of paths that events could have taken, and it was incredibly unlikely that events would take the path that ended in my child’s death. But they had to take a path and that’s the one they took. That’s all there is to it.” In Kahneman’s terms, probabilistic thinkers take the outside view toward even profoundly identity-defining events, seeing them as quasi-random draws from distributions of once-possible worlds.’Forget living Biblically for a year. Try living like this for a day.Chapter seven examines whether superforecasters are plugged into the global news streams. Answer: yes to some degree, but it doesn’t really explain their performance versus regular non-super forecasters.Chapter eight examines the twin vexing problems of updating beliefs in light of new evidence, and getting better at making predictions in light of your past predictions. Because Black Lives Matter, consider police officers:“police officers spend a lot of time figuring out who is telling the truth and who is lying, but research has found they aren’t nearly as good at it as they think they are and they tend not to get better with experience. That’s because experience isn’t enough. It must be accompanied by clear feedback. … Psychologists who test police officers’ ability to spot lies in a controlled setting find a big gap between their confidence and their skill. And that gap grows as officers become more experienced and they assume, not unreasonably, that their experience has made them better lie detectors. As a result, officers grow confident faster than they grow accurate, meaning they grow increasingly overconfident.”This chapter also talks about the discipline to keep hindsight bias in the kennel, and the difficulty in acknowledging the role of luck:“People often assume that when a decision is followed by a good outcome, the decision was good, which isn’t always true, and can be dangerous if it blinds us to the flaws in our thinking.”In a book full of actionably valuable insights (to the aspiring forecaster), this discussion might be the most helpful. Chapter nine deals with teams, team dynamics, fusing algorithms for merging individuals’ predictions, and lots of interesting related things. Chapter ten discusses how leaders might respond to a team of forecasters, and the changes leaders have to make to best utilize them.Chapter eleven talks about the problems with Tetlock’s research platform. (As he himself states earlier in the book, a scientist will always specify the conditions under which they would change their minds.)“I see Kahneman’s and Taleb’s critiques as the strongest challenges to the notion of superforecasting.”Kahneman’s critique is, can forecasters permanently tame cognitive biases, and keep churning out winning forecasts year after year (or at least long enough to be useful)? Taleb’s critique is, can forecasters say anything about black swan events that dominate history?Both of these critiques, in my personal opinion, are surmountable, making Tetlock’s research agenda and this book well worth reading.Chapter twelve closes with Tetlock’s hopes for a future world where we keep score about forecast. It could be awesome. But we’d get used to it fast and start worrying about the next problem."
177,0804136696,http://goodreads.com/user/show/347507-pietro,5,"This book is ostensibly about predicting (some aspects of) the future *reliably*. But that framing sells it far short. Anyone interested in holding true beliefs about the world could benefit immensely from taking in this short book. As far as I'm concerned, predictions are just a really good way of keeping you honest, as they have been in science for centuries: if your beliefs lead to wrong predictions often, you need new beliefs.Lots of people --- newscasters, politicians, pundits --- make claims about what the future holds. The book opens by noting that, although we listen to them raptly, nobody keeps track of how reliable they are as forecasters. With a couple compelling bits of history, it draws an analogy with medicine before the 20th century, which was largely evidence-free and doctors were judged by credentials instead of track records.One big reason nobody keeps track of forecasters' abilities is that most forecasts are vague: ""there is a risk inflation will go up"". How much of a risk? Up by how much? Over what time frame? Unless these are specified, it is impossible to tell whether a forecast was right or wrong, and thus impossible to keep track of how good a forecaster is. The importance of precision is a MAJOR theme in the book; it's the only way to know how well we're doing as forecasters, and knowing that is the only way we can hope to improve.Obviously, high-status pundits and analysts who write op-eds and appear in the news have no incentive to have their performance tracked. They're already high-status; why would they risk being debunked? So Tetlock ran a massive experiment over several years where thousands of volunteers made predictions about hundreds of precise questions such as:""In the next year, will any country withdraw from the eurozone?"" ""How many additional countries will report cases of the Ebola virus before the end of this year?""""If a non-Chinese telecom firm wins a contract to provide internet in the Shanghai Free Trade Zone in the next two years, will Chinese citizens have access to Facebook or Twitter?""Volunteers, working in isolation, made probabilistic forecasts: ""there is a 65% chance Russia will annex a part of Ukraine in the next 6 months"". In the end, each was scored based on their hundreds of forecasts (using a ""Brier score""). As it turns out, about 2% of the volunteers did SUBSTANTIALLY better not only than chance, but better than famous analysts, pundits, and even the intelligence community, which has access to privileged information. And this wasn't luck either; these ""superforecasters"" continued to do incredibly well year in and year out.Let me say that again: there are normal, unknown people out there who can predict important geopolitical events SUBSTANTIALLY better than chance; better than all the pundits who get paid shitloads of money and invited to speak at Davos and meet with presidents; better even than intelligence analysts with access to classified information.How do they do it??That's what the rest of the book is about. In short, these are pretty smart, numerate, and informed people; but they're not OFF-THE-CHARTS smart, math-savvy, hyper news junkies. They're slightly above-average people who simply cultivate healthy mental habits.Habits like ""seek out many different opinions/analyses"", ""challenge your own beliefs"", ""examine past mistakes honestly""; ""put a question in context by relating it to similar questions""; habits we can all cultivate.This truly is one of those books that make your mind more powerful, that give you a new set of eyes. Highly recommended."
178,0804136696,http://goodreads.com/user/show/51856061-bilal-hafeez,4,"The talking heads on TV give such a convincing story about what the future holds that its hard not to believe them. But pinning them down to a time-frame and discrete future event is often next to impossible, so you can never determine whether they were right or wrong. When such talking heads are pinned down, their track-record turns out to be very poor, according to Philip Tetlock in his latest book ""Superforecasting: The Art and Science of Prediction"". This makes sense as they are picked more for their entertainment value, than their track-records.The real super-forecasters are people like Doug Lorch (ex-IBM, retired, lives in Santa Barbara), Mary Simpson (independent financial consultant, formerly regulatory affairs at utility Southern California Edison) and Devyn Duffy (welfare case worker, Pittsburgh). What they lack in TV appeal they make up for in their approach to forecasting.Tetlock and team found them as part of an IARPA, which is part of US Intelligence, project to improve forecasting of political and other events. He found that the way you approach forecasting rather than your credentials or political leanings matter most. If you're a one-big idea person (i.e. a hedgehog) you will struggle, while if you're a pragmatic tinkerer (i.e. a fox) you will do well. He listed the eleven commandments for aspiring super forecasters:1.Focus on questions where your handwork will pay off. So don't bother with a question like ""who will win the US election in 2028?"". It's too far out to bother predicting.2.Break big problems into smaller ones. 3.Strike the right balance between inside and outside views. There is nothing new under the sun. So first calculate the odds of an event happening based on similar ones in the past (i.e. top-down or outside view), then work-out the bottom-up/inside view of the specific event.4.Strike the right balance between under- and overreacting to evidence. Careful incorporate new evidence and update beliefs accordingly. The best forecasters are incremental belief updaters.5.Acknowledge the counter-arguments. If you believe military action never works, be open to the possibility that it might. Take both views and synthesise.6.Distinguish as many degrees of doubt as the problem permits. Your uncertainty dials needs more than 3 settings (certain, maybe, impossible). Translate vague hunches into numeric probabilities.7.Strike the right balance between under- and over-confidence. Understand the risks of rushing to judgment and of dawdling too long near ""maybe"".8.Don't justify or excuse your failures. Conduct unflinching postmortem.9.Bring out the best in others and let others bring out the best in you. Master team management: perspective taking (being able to reproduce the others argument to their satisfaction), precision questioning (clarify arguments so there is no misunderstanding), and constructive confrontation (learning to disagree without being disagreeable).10.Master the error-balancing bicycle. Learning requires doing, with good feedback that leaves no ambiguity about whether you are succeeding.11.Don't treat commandments as commandments!You can practise your prediction skills by joining Tetlock's Good Judgment Projectbilalhafeez.com"
179,0804136696,http://goodreads.com/user/show/7876548-chirayu-batra,4,"Seems like a true companion of Dan Kahneman's Thinking:fast and slow and surely influenced by his judgement psychology. Book is a reminder that world is complex equation and forecasting anything in it is not a simple phenomenon. The book also seems a good promotional material for the good judgement project, but that makes sense. I hope to join the project and make some of my own predictions, smart predictions. A nice read for psychology lovers and people interested in reading about the art of forecasting. "
180,0804136696,http://goodreads.com/user/show/66952286-scott,5,"I enjoyed this book about forecasting future events. It discusses how medicine changed around the time of ww2 when statistics were first used to commonly track likelihood of outcomes - leading to improved standardized treatments. It also discusses how the best self-selected forecasters in a study were consistently able to outperform even dedicated agencies with access to classified info when trained. The author managed this group for years and offers many insights into forecasting. Next, he discusses the use of forecasting (effective and not) in critical decisions like the bay of pigs, Cuban missile crisis, raid on bin laden, and attack on Saddam Hussein. Finally, he discusses his current work with GJOpen.com where anyone can test their skills at forecasting future events like elections, commodity prices, deployment of electric vehicles, etc."
181,0804136696,http://goodreads.com/user/show/64798070-jay-hennessey,4,"I really enjoyed this read, especially in my new role in professional sports. At risk of liking due to confirmation bias, I really enjoyed hearing the application of concepts from other books that I hold in the highest regard, I.e. Thinking Fast and Slow. I also enjoyed learning about Bayes Theory — coincidentally reading, The Theory that would not Die - How Bayes Rule Cracked the Enigma Code....Like most brilliant concepts, the idea of constantly updating one’s forecast, or hypothesis with new information sounded so simple, but the examples showed that this is usually the exception rather than the rule. The book also made me reflect on the concepts from Lean Start Up — Build, Measure, Learn. While the Lean Startup model is more about doing something and making adjustments, broadly, I see the concepts as the same. In Lean, one forms a hypothesis, determines how he will know if he is correct, and then sets out with a minimally viable product to test and measure the result — constantly adapting the product...or in Super-forecasting-Speak, constantly updating the prediction.I recommend this book to growth minded leaders of any organization. The concepts in this book, interleaved with so many other great works, are thought provoking and worthily of consideration in any space."
182,0804136696,http://goodreads.com/user/show/4057945-cristobal,5,"This book reminds of Emerson's phrase “The mind, once stretched by a new idea, never returns to its original dimensions.” which is what experienced after reading it. Mental models are the filters through which we see the world and tint all of our thinking. That is why taking the time and making the effort to screen how we think can make all the difference. ""Superforecasting"" builds a solid case on why making the best forecasting is not something reserved for a few lucky ones but that can be attained through meticulous analysis and keeping an open mind to information and other points of view. Mental biases will always be around, but knowing them and building the tools to try to limit their influence can make anyone a much more accurate forecaster and clear thinker.Definitely one of my favorite books on thinking right alongside ""Thinking Fast and Slow""."
183,0804136696,http://goodreads.com/user/show/7460650-viktor-szathmary,2,"While the book provides a reasonable overview of biases (and methods for correcting) in forecasting games, it is sadly disconnected from real-world decision making. For example, it treats a tennis match the same as international nuclear war. Only in academia can the two be managed the same way (as a binary prediction game, measured by your Brier score). While the author seems to have read Nassim Taleb's works (devoting a chapter to it), unfortunately they have missed the key points."
184,0804136696,http://goodreads.com/user/show/93933488-wen,4,"I liked it but it's not for everybody. The author wants to drive accountability for those who make predictions. I largely resonate with his observations and recommendations for providing better forecasts. However, as stated by the author, his recommendations should not be treated as gospel, but rather as another tool. I do like how the Good Judgement Project predictions and results are publicly available to view. "
185,0804136696,http://goodreads.com/user/show/34741339-robert-meyro,3,"While the title of this book drew me to believe that there could be hope in forecasting, I have definitely become more skeptical after reading this book. It nearly seems that the author felt the same way, the further one progressed in the book. That being said, there were some interesting anecdotes covered."
186,0804136696,http://goodreads.com/user/show/87001455-nathan-runke,5,Phenomenal book. Extremely well written and laid out. For a book on statistics and predicting the future (or as much as you can) it flows very well and is a nice read. Will probably be reading again. Has some valuable information on how you can improve your own personal ability to predict things/understand what is happening around the world.
187,0804136696,http://goodreads.com/user/show/39225478-jiliac,5,"The trilogy of books about prediction is ""Black Swan"" by Taleb, ""Thinking Fast and Slow"" by Kahneman, and this one, ""Superforecasting"" by Tetlock. All authors cite the two others profusely. They all have a different approach. Taleb being on the extremely rare events, Kahneman on the bias our brain creates, and Tetlock on the ""how to predict in practice"".Myself being more interested on the practical aspect of things, I was expecting Tetlock to be my prefered one. But in the end, I feel it's the one that I learned the least from. It comes with a very precise, detailed, and scientifically well studied, set of advices: practice a lot, reconsider yourself, diversify the opinions you base yourself on, and more... However, it doesn't deal with Taleb point, which was what I was expecting. (This expectation was unfair though, since it never advertised as such. That's why I still rate it five stars.)"
188,0804136696,http://goodreads.com/user/show/33662754-escada-emanuel,4,"Interesting book with a pretty smooth reading. Strikes a quite clear balance between what can actually be forecast and what cannot. Gives interesting tips on how to analyse complex realities, for example, through the inside and outside views or the 'fermi-zing' method. Also a highlight for the 'Auftragstaktik' from the Prussian military and now fashionably widespread on the ideal way to deal with uncertainty. "
189,0804136696,http://goodreads.com/user/show/118345071-brock-bank,5,"Late to the party on this one, but definitely deserves the hype. Favorite book I read this year, probably my favorite I’ve ever read around metacognition. Written to be readable, has practicable tips for being a better predictor. Tbf I was very predisposed to enjoying this book; throw in some prescient Michael Flynn criticism and I was sure to love it.Definitely worth the time to read the short appendix which outlines the main takes if you don’t want to give it a full read."
190,0804136696,http://goodreads.com/user/show/45921922-thiago-marzag-o,5,If I had read about Tetlock's work when I first started grad school my interests would have changed completely and I would have produced an entirely different dissertation. Too bad it took me so long to come across this.
191,0804136696,http://goodreads.com/user/show/12310532-taivo,4,"There was a decent concentration of content and good balance between storytelling versus relaying useful models, but I still felt the core ideas of the book could have been fit into a chapter or two, and the rest of the book was written to fill up 300 pages."
192,0804136696,http://goodreads.com/user/show/13493517-cenk-undey,4,An interesting high level summary of approaches to predicting life events. It makes a lot of references to the books I already read such as Kahleman’s Thinking Fast and Slow and Nassim N. Taleb’s The Black Swan. It is a fun book to go through.
193,0804136696,http://goodreads.com/user/show/15701574-allan-aksiim,3,"No point reading the entire thing. Valuable lessons yes, american style unnecessary pop-science fluff also. "
194,0804136696,http://goodreads.com/user/show/52223189-mario-vanhoucke,4,"Great book about the good judgement project and how some people can outperform experts in forecasting events. The book brings together some of my favourite authors/researchers and refers to the “illusion of control” by E. Langer, the work and quotes of Amos Tsversky, good old Bayes and his theorem (we should take an inside and outside view, but start with the outside one) and finally, in chapter 11, the work of Kahneman and Taleb (really great summary and discussion). Evidence based forecasting, it sounds like an easy-to-accept principle, but the authors argue it’s not. But they are hopeful: Where wisdom once was, quantification will now be. Read!"
195,0804136696,http://goodreads.com/user/show/64773125-gabi,5,"A great book, in a few words : ""This book is not about how to be happy.. is about how to be accurate!"" [from-the-end-of-chapter-6]"
196,0804136696,http://goodreads.com/user/show/1300459-sambasivan,4,Novel concept and well articulated. Contains principles that one can use for a lifetime. Be curious and improve yourself regularly to become a super forecaster. 
197,0804136696,http://goodreads.com/user/show/2385396-bradley-eylander,4,"This book is a slight contrast to the book I read before called, ""The Signal and the Noise: Why So Many Predictions Fail - But Some Don't"". Both books acknowledge that there are many things in life we can't predict and the difficulty of predicting them. This book covers people that are statistically good at predicting and why. Turns out its due to collecting data daily (compounding effect) and applying statistics. What I thought was ironic was the people that are good at predicting aren't the ones being paid lots of $$ working for organizations such as the CIA. It was hobbyist who's only incentive was Amazon gift cards."
198,0804136696,http://goodreads.com/user/show/1054712-kent-winward,4,"I would have known how much I'd like the book, if I had read the book first . . ."
199,0804136696,http://goodreads.com/user/show/11228-suman,0,3.5 stars
200,0804136696,http://goodreads.com/user/show/57644603-colin-paulish,5,Easy read with takeaways that you feel you could integrate and apply to your life almost immediately
201,0804136696,http://goodreads.com/user/show/82584091-raph-zindi,5,Wildly disruptive thinking! One to revisit from time to time. 
202,0804136696,http://goodreads.com/user/show/33743305-vinh,4,The book is based on scientific results which are quite interesting and insightful.
203,0804136696,http://goodreads.com/user/show/5822873-kaustubh,3,"A nice introduction (and not a guide) into the world of forecasting: 6.5/10Tetlock and (presumable ghostwriter) Gardner provide a nice introduction into the world of geopolitical forecasting. They detail Tetlock’s long-running (>15 yr) experiments on people’s abilities to forecast future outcomes on a variety of themes. Although the book sets itself up somewhat disingenuously as a guide for improvement in forecasting knowledge and ability, it is more of an introduction to forecasting, histogram statistics, calibration-verification-validation, and several anecdotal examples about people and events that conform to said underlying statistical theory. The examples are illuminating and at times, contradictory - while they are useful, one can’t help but wonder about where exactly they fall on the bell curve and whether they are representative of anything or not. There are many references to Kahneman’s Thinking Fast and Slow littered throughout the book. Sometimes, I got the feeling that I ought to be reading that book instead. Where Tetlock and Gardner excel is in driving down their take-home message: there is no single catch-all formula to “super”forecasting and rather, one should expect a slow-burn process of hard work, attentiveness, and a metered demeanor for marginal successes. However, keep in mind, that margins are everything in this field."
204,0804136696,http://goodreads.com/user/show/13608511-ka,5,What an excellent read. Highly recommend if you’re curious about how to reason more rigorously and want to make better predictions about where the world is headed. Rarely seen such gifted science writing!
205,0804136696,http://goodreads.com/user/show/38189944-mo,5,"Phenomenal! I think this is the one to beat in terms of a popular guide to the Decision and Cognitive Sciences and - more interestingly - how they can be used in the real world to make a real difference. All told in an eminently readable, deceptively simple-seeming fashion to boot. I honestly can not praise or recommend this book enough. "
206,0804136696,http://goodreads.com/user/show/25471547-ahmed,5,"You have to understand that this book is eminently and deeply rooted in the research program Tetlock pioneered and ran, the Good Judgement Project, and about the real people whom he found through the program to be great are forecasting geopolitical events in the three to six month range.My own personal goal before reading the book was to learn rigorous prediction—not necessarily answering geopolitical questions that were handed from somewhere else, but more like how many editors will show up on global Wikipedia today, or possibly more profitably, how much the stock market will lose today. I was prepared for the book to be an overly-academic treatise on the minutiae of the research program, or for a number of ways it could be operationally useless.But at each chapter, Tetlock had something very meaningful and useful to say to me about the business of prediction. At each stage he surprised me with the foxlike connections he drew between what he saw his forecasters doing and with other research programs and discoveries.(I was delighted to see my boy Duncan Watts cited on the emphatically post hoc nature of describing “significant” events. Aaron Brown, my poker-playing risk-managing guru, weighs in on the value of small but consistent wins. The once-cool-but-now-fascist-apologist Nassim Taleb makes appearances as the book struggles with whether its brand of forecasting is useful in the face of an extremistan world (answer: it is, because you can make money in an extremistan world). Of course Danny Kahneman is intertwined with the narrative, as Tetlock’s sounding board and colleague for many decades. Not mentioned in the book but in Tetlock’s five-part master-class on forecasting on Edge.org, my main man Anders Ericsson is cited on the trainability of forecasters.)Chapter one opens with trying to convince us that predictability exists and forecasting can help us (make or save us money, at the simplest). He invites us to contrast the unpredictability of cloud shapes with the predictability of a clock, and exposes this as the first of many false dichotomies throughout the book. “We live in a world of clocks and clouds and a vast jumble of other metaphors. Unpredictability and predictability coexist uneasily in the intricately interlocking systems that make up our bodies, our societies, and the cosmos. How predictable something is depends on what we are trying to predict, how far into the future, and under what circumstances.”He then sets up the core argument of the research program that IARPA paid for: how well can we do? A question like this should leave you speechless and befuddled—the US Government asked Tetlock and other university and industry programs to find out how well we can predict geopolitical outcomes over a 3–6 month time horizon, because nobody knew how well we did. Think of all the pundits, all the terrible books Tom Friedman pooped out, all the bloggers and tooters, and the realization that all that heat gave us no light should leave you breathless.In chapter two, Tetlock dives into the ugly story of medicine.Tetlock revisits this over and over again: medicine only very, very recently became a devotee of the randomized controlled trial. He has a wonderful set of vignettes of the vanguard of physicians who dragged their colleagues kicking and screaming into the evidence-based age after World War II. Its been a few weeks since I read this section but I think I will forever remember this story:‘When hospitals created cardiac care units to treat patients recovering from heart attacks, Cochrane proposed a randomized trial to determine whether the new units delivered better results than the old treatment, which was to send the patient home for monitoring and bed rest. Physicians balked. It was obvious the cardiac care units were superior, they said, and denying patients the best care would be unethical. … [but] Cochrane got his trial: some patients, randomly selected, were sent to the cardiac care units while others were sent home for monitoring and bed rest. Partway through the trial, Cochrane met with a group of the cardiologists who had tried to stop his experiment. He told them that he had preliminary results. The difference in outcomes between the two treatments was not statistically significant, he emphasized, but it appeared that patients might do slightly better in the cardiac care units. “They were vociferous in their abuse: ‘Archie,’ they said, ‘we always thought you were unethical. You must stop the trial at once.’ ” But then Cochrane revealed he had played a little trick. He had reversed the results: home care had done slightly better than the cardiac units. “There was dead silence and I felt rather sick because they were, after all, my medical colleagues.”’Yes, a lot of medicine is terribly “intuition-based” today—“it’s obvious this treatment is better”. But medicine has made great strides, over the last few decades, in acknowledging the risks and flaws of “intuition” and committing itself to the exacting requirements of randomized controlled trials.Programmers are slowly learning this, thankfully with less loss of life. Andrei Alexandrescu, in his “Writing Quick Code in C++, Quickly” talk in 2013, discussed this at length:‘You must measure everything. We all have intuition. And the intuition of programmers is always wrong. Outdated. Intuition ignores a lot of aspects of a complex reality. Today’s machine architectures are so complicated, there’re so many variables in flight at any point in time that it’s essentially impossible to consider them deterministic machines any more. They are not deterministic any more. So we make very often big mistakes when assuming things about what’s going to make fast code. [E.g.,] fewer instructions do not equal faster code. Data [access] is not always faster than computation. The only good intuition is “I should measure this stuff and see what happens.” To quote a classic, who is still alive, Walter Bight: “Measuring gives you a leg up on experts who are so good they don’t need to measure.” Walter and I have been working on optimizing bits and pieces of a project we work on and … whenever we think we know what we’re doing, we measure, and it’s just the other way around.’Here are two of the world’s leading experts in programming language design and implementation, openly saying “whenever we think we know what we’re doing, we measure, and it’s just the other way around.” That is probably worth tattooing on one’s forehead.This is relevant to the Good Judgement Project because it is the first time randomized controlled trials have been applied to geopolitical prediction, but also because dealing with evidence and weighing it is a key component of forecasting. Putting a collar on intuition helps to prevent it from ruining your predictions.The next chapter (chapter three) discusses the intricacies of keeping score. A project like this, and the task of improving one’s own forecasting, lives and dies by the pesky pernicious questions of exactly how you measure performance, and other experimental details. Tetlock shows how everything we might consider as “forecasts” (intelligence agencies, the revolting Thomas Friedman) is trash, is intellectual weasel-worded garbage. He details the kinds of questions amenable to his experiment, how to elicit probability estimates, how to enforce time horizons, how to factor in update frequencies, how to fuse predictions from groups, various research tools, etc.This chapter details how the Brier score works: forecasters answer a yes/no question with a probability. The score rewards emphatically correct answer, and punishes incorrect confidence.Chapter four gives a detailed overview of the project’s findings: how well Tetlock’s superforecasters did, and analyses of how and why they did so well. Tetlock really surprised me here by offering a very humble and honestly rigorous analysis of regression to the mean. He explains how in games of chance, regression to the mean crushes the winners after repeated rounds, whereas exercises of skill sees the winners only improve after succeeding rounds.“Each year, roughly 30% of the individual superforecasters fall from the ranks of the top 2% next year. But that also implies a good deal of consistency over time: 70% of superforecasters remain superforecasters.”Tetlock could have gone all business-book “Good to Great” (trash) on me. No. This is a well-reasoned and thoughtful argument that honestly explored the role of luck in forecasting. 30% annual replacement suggests some luck, but a lot of skill. That is a very powerful finding.Chapter five breaks down the intelligence of superforecasters, and six their math savvy. Findings: superforecasters are intelligent and also generically math-savvy, but intelligence and math skills are neither necessary nor sufficient for superforecasting performance.Chapter five has a really interesting discussion of Fermi analyses—you know, “how many piano tuners are in Chicago”. I did this piano tuner exercise for the first time while reading this book (despite having read about it here and there in the past), and that was a very insightful experience. Fermi analysis shows that, rather than making a big prediction that might have a lot of error, you can break the problem up into smaller problems whose errors are smaller, and whose errors stay small after combining them. Fermi analysis is cool, and I finally appreciate them.Chapter six also deals, almost spiritually, with the misguided “quest for meaning” and the herculean discipline needed to maintain a probabilistic outlook on life:‘Even in the face of tragedy, the probabilistic thinker will say, “Yes, there was an almost infinite number of paths that events could have taken, and it was incredibly unlikely that events would take the path that ended in my child’s death. But they had to take a path and that’s the one they took. That’s all there is to it.” In Kahneman’s terms, probabilistic thinkers take the outside view toward even profoundly identity-defining events, seeing them as quasi-random draws from distributions of once-possible worlds.’Forget living Biblically for a year. Try living like this for a day.Chapter seven examines whether superforecasters are plugged into the global news streams. Answer: yes to some degree, but it doesn’t really explain their performance versus regular non-super forecasters.Chapter eight examines the twin vexing problems of updating beliefs in light of new evidence, and getting better at making predictions in light of your past predictions. Because Black Lives Matter, consider police officers:“police officers spend a lot of time figuring out who is telling the truth and who is lying, but research has found they aren’t nearly as good at it as they think they are and they tend not to get better with experience. That’s because experience isn’t enough. It must be accompanied by clear feedback. … Psychologists who test police officers’ ability to spot lies in a controlled setting find a big gap between their confidence and their skill. And that gap grows as officers become more experienced and they assume, not unreasonably, that their experience has made them better lie detectors. As a result, officers grow confident faster than they grow accurate, meaning they grow increasingly overconfident.”This chapter also talks about the discipline to keep hindsight bias in the kennel, and the difficulty in acknowledging the role of luck:“People often assume that when a decision is followed by a good outcome, the decision was good, which isn’t always true, and can be dangerous if it blinds us to the flaws in our thinking.”In a book full of actionably valuable insights (to the aspiring forecaster), this discussion might be the most helpful. Chapter nine deals with teams, team dynamics, fusing algorithms for merging individuals’ predictions, and lots of interesting related things. Chapter ten discusses how leaders might respond to a team of forecasters, and the changes leaders have to make to best utilize them.Chapter eleven talks about the problems with Tetlock’s research platform. (As he himself states earlier in the book, a scientist will always specify the conditions under which they would change their minds.)“I see Kahneman’s and Taleb’s critiques as the strongest challenges to the notion of superforecasting.”Kahneman’s critique is, can forecasters permanently tame cognitive biases, and keep churning out winning forecasts year after year (or at least long enough to be useful)? Taleb’s critique is, can forecasters say anything about black swan events that dominate history?Both of these critiques, in my personal opinion, are surmountable, making Tetlock’s research agenda and this book well worth reading.Chapter twelve closes with Tetlock’s hopes for a future world where we keep score about forecast. It could be awesome. But we’d get used to it fast and start worrying about the next problem."
207,0804136696,http://goodreads.com/user/show/347507-pietro,5,"This book is ostensibly about predicting (some aspects of) the future *reliably*. But that framing sells it far short. Anyone interested in holding true beliefs about the world could benefit immensely from taking in this short book. As far as I'm concerned, predictions are just a really good way of keeping you honest, as they have been in science for centuries: if your beliefs lead to wrong predictions often, you need new beliefs.Lots of people --- newscasters, politicians, pundits --- make claims about what the future holds. The book opens by noting that, although we listen to them raptly, nobody keeps track of how reliable they are as forecasters. With a couple compelling bits of history, it draws an analogy with medicine before the 20th century, which was largely evidence-free and doctors were judged by credentials instead of track records.One big reason nobody keeps track of forecasters' abilities is that most forecasts are vague: ""there is a risk inflation will go up"". How much of a risk? Up by how much? Over what time frame? Unless these are specified, it is impossible to tell whether a forecast was right or wrong, and thus impossible to keep track of how good a forecaster is. The importance of precision is a MAJOR theme in the book; it's the only way to know how well we're doing as forecasters, and knowing that is the only way we can hope to improve.Obviously, high-status pundits and analysts who write op-eds and appear in the news have no incentive to have their performance tracked. They're already high-status; why would they risk being debunked? So Tetlock ran a massive experiment over several years where thousands of volunteers made predictions about hundreds of precise questions such as:""In the next year, will any country withdraw from the eurozone?"" ""How many additional countries will report cases of the Ebola virus before the end of this year?""""If a non-Chinese telecom firm wins a contract to provide internet in the Shanghai Free Trade Zone in the next two years, will Chinese citizens have access to Facebook or Twitter?""Volunteers, working in isolation, made probabilistic forecasts: ""there is a 65% chance Russia will annex a part of Ukraine in the next 6 months"". In the end, each was scored based on their hundreds of forecasts (using a ""Brier score""). As it turns out, about 2% of the volunteers did SUBSTANTIALLY better not only than chance, but better than famous analysts, pundits, and even the intelligence community, which has access to privileged information. And this wasn't luck either; these ""superforecasters"" continued to do incredibly well year in and year out.Let me say that again: there are normal, unknown people out there who can predict important geopolitical events SUBSTANTIALLY better than chance; better than all the pundits who get paid shitloads of money and invited to speak at Davos and meet with presidents; better even than intelligence analysts with access to classified information.How do they do it??That's what the rest of the book is about. In short, these are pretty smart, numerate, and informed people; but they're not OFF-THE-CHARTS smart, math-savvy, hyper news junkies. They're slightly above-average people who simply cultivate healthy mental habits.Habits like ""seek out many different opinions/analyses"", ""challenge your own beliefs"", ""examine past mistakes honestly""; ""put a question in context by relating it to similar questions""; habits we can all cultivate.This truly is one of those books that make your mind more powerful, that give you a new set of eyes. Highly recommended."
208,0804136696,http://goodreads.com/user/show/51856061-bilal-hafeez,4,"The talking heads on TV give such a convincing story about what the future holds that its hard not to believe them. But pinning them down to a time-frame and discrete future event is often next to impossible, so you can never determine whether they were right or wrong. When such talking heads are pinned down, their track-record turns out to be very poor, according to Philip Tetlock in his latest book ""Superforecasting: The Art and Science of Prediction"". This makes sense as they are picked more for their entertainment value, than their track-records.The real super-forecasters are people like Doug Lorch (ex-IBM, retired, lives in Santa Barbara), Mary Simpson (independent financial consultant, formerly regulatory affairs at utility Southern California Edison) and Devyn Duffy (welfare case worker, Pittsburgh). What they lack in TV appeal they make up for in their approach to forecasting.Tetlock and team found them as part of an IARPA, which is part of US Intelligence, project to improve forecasting of political and other events. He found that the way you approach forecasting rather than your credentials or political leanings matter most. If you're a one-big idea person (i.e. a hedgehog) you will struggle, while if you're a pragmatic tinkerer (i.e. a fox) you will do well. He listed the eleven commandments for aspiring super forecasters:1.Focus on questions where your handwork will pay off. So don't bother with a question like ""who will win the US election in 2028?"". It's too far out to bother predicting.2.Break big problems into smaller ones. 3.Strike the right balance between inside and outside views. There is nothing new under the sun. So first calculate the odds of an event happening based on similar ones in the past (i.e. top-down or outside view), then work-out the bottom-up/inside view of the specific event.4.Strike the right balance between under- and overreacting to evidence. Careful incorporate new evidence and update beliefs accordingly. The best forecasters are incremental belief updaters.5.Acknowledge the counter-arguments. If you believe military action never works, be open to the possibility that it might. Take both views and synthesise.6.Distinguish as many degrees of doubt as the problem permits. Your uncertainty dials needs more than 3 settings (certain, maybe, impossible). Translate vague hunches into numeric probabilities.7.Strike the right balance between under- and over-confidence. Understand the risks of rushing to judgment and of dawdling too long near ""maybe"".8.Don't justify or excuse your failures. Conduct unflinching postmortem.9.Bring out the best in others and let others bring out the best in you. Master team management: perspective taking (being able to reproduce the others argument to their satisfaction), precision questioning (clarify arguments so there is no misunderstanding), and constructive confrontation (learning to disagree without being disagreeable).10.Master the error-balancing bicycle. Learning requires doing, with good feedback that leaves no ambiguity about whether you are succeeding.11.Don't treat commandments as commandments!You can practise your prediction skills by joining Tetlock's Good Judgment Projectbilalhafeez.com"
209,0804136696,http://goodreads.com/user/show/7876548-chirayu-batra,4,"Seems like a true companion of Dan Kahneman's Thinking:fast and slow and surely influenced by his judgement psychology. Book is a reminder that world is complex equation and forecasting anything in it is not a simple phenomenon. The book also seems a good promotional material for the good judgement project, but that makes sense. I hope to join the project and make some of my own predictions, smart predictions. A nice read for psychology lovers and people interested in reading about the art of forecasting. "
210,0804136696,http://goodreads.com/user/show/66952286-scott,5,"I enjoyed this book about forecasting future events. It discusses how medicine changed around the time of ww2 when statistics were first used to commonly track likelihood of outcomes - leading to improved standardized treatments. It also discusses how the best self-selected forecasters in a study were consistently able to outperform even dedicated agencies with access to classified info when trained. The author managed this group for years and offers many insights into forecasting. Next, he discusses the use of forecasting (effective and not) in critical decisions like the bay of pigs, Cuban missile crisis, raid on bin laden, and attack on Saddam Hussein. Finally, he discusses his current work with GJOpen.com where anyone can test their skills at forecasting future events like elections, commodity prices, deployment of electric vehicles, etc."
211,0804136696,http://goodreads.com/user/show/64798070-jay-hennessey,4,"I really enjoyed this read, especially in my new role in professional sports. At risk of liking due to confirmation bias, I really enjoyed hearing the application of concepts from other books that I hold in the highest regard, I.e. Thinking Fast and Slow. I also enjoyed learning about Bayes Theory — coincidentally reading, The Theory that would not Die - How Bayes Rule Cracked the Enigma Code....Like most brilliant concepts, the idea of constantly updating one’s forecast, or hypothesis with new information sounded so simple, but the examples showed that this is usually the exception rather than the rule. The book also made me reflect on the concepts from Lean Start Up — Build, Measure, Learn. While the Lean Startup model is more about doing something and making adjustments, broadly, I see the concepts as the same. In Lean, one forms a hypothesis, determines how he will know if he is correct, and then sets out with a minimally viable product to test and measure the result — constantly adapting the product...or in Super-forecasting-Speak, constantly updating the prediction.I recommend this book to growth minded leaders of any organization. The concepts in this book, interleaved with so many other great works, are thought provoking and worthily of consideration in any space."
212,0804136696,http://goodreads.com/user/show/4057945-cristobal,5,"This book reminds of Emerson's phrase “The mind, once stretched by a new idea, never returns to its original dimensions.” which is what experienced after reading it. Mental models are the filters through which we see the world and tint all of our thinking. That is why taking the time and making the effort to screen how we think can make all the difference. ""Superforecasting"" builds a solid case on why making the best forecasting is not something reserved for a few lucky ones but that can be attained through meticulous analysis and keeping an open mind to information and other points of view. Mental biases will always be around, but knowing them and building the tools to try to limit their influence can make anyone a much more accurate forecaster and clear thinker.Definitely one of my favorite books on thinking right alongside ""Thinking Fast and Slow""."
213,0804136696,http://goodreads.com/user/show/7460650-viktor-szathmary,2,"While the book provides a reasonable overview of biases (and methods for correcting) in forecasting games, it is sadly disconnected from real-world decision making. For example, it treats a tennis match the same as international nuclear war. Only in academia can the two be managed the same way (as a binary prediction game, measured by your Brier score). While the author seems to have read Nassim Taleb's works (devoting a chapter to it), unfortunately they have missed the key points."
214,0804136696,http://goodreads.com/user/show/93933488-wen,4,"I liked it but it's not for everybody. The author wants to drive accountability for those who make predictions. I largely resonate with his observations and recommendations for providing better forecasts. However, as stated by the author, his recommendations should not be treated as gospel, but rather as another tool. I do like how the Good Judgement Project predictions and results are publicly available to view. "
215,0804136696,http://goodreads.com/user/show/34741339-robert-meyro,3,"While the title of this book drew me to believe that there could be hope in forecasting, I have definitely become more skeptical after reading this book. It nearly seems that the author felt the same way, the further one progressed in the book. That being said, there were some interesting anecdotes covered."
216,0804136696,http://goodreads.com/user/show/87001455-nathan-runke,5,Phenomenal book. Extremely well written and laid out. For a book on statistics and predicting the future (or as much as you can) it flows very well and is a nice read. Will probably be reading again. Has some valuable information on how you can improve your own personal ability to predict things/understand what is happening around the world.
217,0804136696,http://goodreads.com/user/show/39225478-jiliac,5,"The trilogy of books about prediction is ""Black Swan"" by Taleb, ""Thinking Fast and Slow"" by Kahneman, and this one, ""Superforecasting"" by Tetlock. All authors cite the two others profusely. They all have a different approach. Taleb being on the extremely rare events, Kahneman on the bias our brain creates, and Tetlock on the ""how to predict in practice"".Myself being more interested on the practical aspect of things, I was expecting Tetlock to be my prefered one. But in the end, I feel it's the one that I learned the least from. It comes with a very precise, detailed, and scientifically well studied, set of advices: practice a lot, reconsider yourself, diversify the opinions you base yourself on, and more... However, it doesn't deal with Taleb point, which was what I was expecting. (This expectation was unfair though, since it never advertised as such. That's why I still rate it five stars.)"
218,0804136696,http://goodreads.com/user/show/33662754-escada-emanuel,4,"Interesting book with a pretty smooth reading. Strikes a quite clear balance between what can actually be forecast and what cannot. Gives interesting tips on how to analyse complex realities, for example, through the inside and outside views or the 'fermi-zing' method. Also a highlight for the 'Auftragstaktik' from the Prussian military and now fashionably widespread on the ideal way to deal with uncertainty. "
219,0804136696,http://goodreads.com/user/show/118345071-brock-bank,5,"Late to the party on this one, but definitely deserves the hype. Favorite book I read this year, probably my favorite I’ve ever read around metacognition. Written to be readable, has practicable tips for being a better predictor. Tbf I was very predisposed to enjoying this book; throw in some prescient Michael Flynn criticism and I was sure to love it.Definitely worth the time to read the short appendix which outlines the main takes if you don’t want to give it a full read."
220,0804136696,http://goodreads.com/user/show/45921922-thiago-marzag-o,5,If I had read about Tetlock's work when I first started grad school my interests would have changed completely and I would have produced an entirely different dissertation. Too bad it took me so long to come across this.
221,0804136696,http://goodreads.com/user/show/12310532-taivo,4,"There was a decent concentration of content and good balance between storytelling versus relaying useful models, but I still felt the core ideas of the book could have been fit into a chapter or two, and the rest of the book was written to fill up 300 pages."
222,0804136696,http://goodreads.com/user/show/13493517-cenk-undey,4,An interesting high level summary of approaches to predicting life events. It makes a lot of references to the books I already read such as Kahleman’s Thinking Fast and Slow and Nassim N. Taleb’s The Black Swan. It is a fun book to go through.
223,0804136696,http://goodreads.com/user/show/15701574-allan-aksiim,3,"No point reading the entire thing. Valuable lessons yes, american style unnecessary pop-science fluff also. "
224,0804136696,http://goodreads.com/user/show/52223189-mario-vanhoucke,4,"Great book about the good judgement project and how some people can outperform experts in forecasting events. The book brings together some of my favourite authors/researchers and refers to the “illusion of control” by E. Langer, the work and quotes of Amos Tsversky, good old Bayes and his theorem (we should take an inside and outside view, but start with the outside one) and finally, in chapter 11, the work of Kahneman and Taleb (really great summary and discussion). Evidence based forecasting, it sounds like an easy-to-accept principle, but the authors argue it’s not. But they are hopeful: Where wisdom once was, quantification will now be. Read!"
225,0804136696,http://goodreads.com/user/show/64773125-gabi,5,"A great book, in a few words : ""This book is not about how to be happy.. is about how to be accurate!"" [from-the-end-of-chapter-6]"
226,0804136696,http://goodreads.com/user/show/1300459-sambasivan,4,Novel concept and well articulated. Contains principles that one can use for a lifetime. Be curious and improve yourself regularly to become a super forecaster. 
227,0804136696,http://goodreads.com/user/show/2385396-bradley-eylander,4,"This book is a slight contrast to the book I read before called, ""The Signal and the Noise: Why So Many Predictions Fail - But Some Don't"". Both books acknowledge that there are many things in life we can't predict and the difficulty of predicting them. This book covers people that are statistically good at predicting and why. Turns out its due to collecting data daily (compounding effect) and applying statistics. What I thought was ironic was the people that are good at predicting aren't the ones being paid lots of $$ working for organizations such as the CIA. It was hobbyist who's only incentive was Amazon gift cards."
228,0804136696,http://goodreads.com/user/show/1054712-kent-winward,4,"I would have known how much I'd like the book, if I had read the book first . . ."
229,0804136696,http://goodreads.com/user/show/11228-suman,0,3.5 stars
230,0804136696,http://goodreads.com/user/show/57644603-colin-paulish,5,Easy read with takeaways that you feel you could integrate and apply to your life almost immediately
231,0804136696,http://goodreads.com/user/show/82584091-raph-zindi,5,Wildly disruptive thinking! One to revisit from time to time. 
232,0804136696,http://goodreads.com/user/show/33743305-vinh,4,The book is based on scientific results which are quite interesting and insightful.
233,0804136696,http://goodreads.com/user/show/5822873-kaustubh,3,"A nice introduction (and not a guide) into the world of forecasting: 6.5/10Tetlock and (presumable ghostwriter) Gardner provide a nice introduction into the world of geopolitical forecasting. They detail Tetlock’s long-running (>15 yr) experiments on people’s abilities to forecast future outcomes on a variety of themes. Although the book sets itself up somewhat disingenuously as a guide for improvement in forecasting knowledge and ability, it is more of an introduction to forecasting, histogram statistics, calibration-verification-validation, and several anecdotal examples about people and events that conform to said underlying statistical theory. The examples are illuminating and at times, contradictory - while they are useful, one can’t help but wonder about where exactly they fall on the bell curve and whether they are representative of anything or not. There are many references to Kahneman’s Thinking Fast and Slow littered throughout the book. Sometimes, I got the feeling that I ought to be reading that book instead. Where Tetlock and Gardner excel is in driving down their take-home message: there is no single catch-all formula to “super”forecasting and rather, one should expect a slow-burn process of hard work, attentiveness, and a metered demeanor for marginal successes. However, keep in mind, that margins are everything in this field."
234,0804136696,http://goodreads.com/user/show/13608511-ka,5,What an excellent read. Highly recommend if you’re curious about how to reason more rigorously and want to make better predictions about where the world is headed. Rarely seen such gifted science writing!
235,0804136696,http://goodreads.com/user/show/38189944-mo,5,"Phenomenal! I think this is the one to beat in terms of a popular guide to the Decision and Cognitive Sciences and - more interestingly - how they can be used in the real world to make a real difference. All told in an eminently readable, deceptively simple-seeming fashion to boot. I honestly can not praise or recommend this book enough. "
236,0804136696,http://goodreads.com/user/show/25471547-ahmed,5,"You have to understand that this book is eminently and deeply rooted in the research program Tetlock pioneered and ran, the Good Judgement Project, and about the real people whom he found through the program to be great are forecasting geopolitical events in the three to six month range.My own personal goal before reading the book was to learn rigorous prediction—not necessarily answering geopolitical questions that were handed from somewhere else, but more like how many editors will show up on global Wikipedia today, or possibly more profitably, how much the stock market will lose today. I was prepared for the book to be an overly-academic treatise on the minutiae of the research program, or for a number of ways it could be operationally useless.But at each chapter, Tetlock had something very meaningful and useful to say to me about the business of prediction. At each stage he surprised me with the foxlike connections he drew between what he saw his forecasters doing and with other research programs and discoveries.(I was delighted to see my boy Duncan Watts cited on the emphatically post hoc nature of describing “significant” events. Aaron Brown, my poker-playing risk-managing guru, weighs in on the value of small but consistent wins. The once-cool-but-now-fascist-apologist Nassim Taleb makes appearances as the book struggles with whether its brand of forecasting is useful in the face of an extremistan world (answer: it is, because you can make money in an extremistan world). Of course Danny Kahneman is intertwined with the narrative, as Tetlock’s sounding board and colleague for many decades. Not mentioned in the book but in Tetlock’s five-part master-class on forecasting on Edge.org, my main man Anders Ericsson is cited on the trainability of forecasters.)Chapter one opens with trying to convince us that predictability exists and forecasting can help us (make or save us money, at the simplest). He invites us to contrast the unpredictability of cloud shapes with the predictability of a clock, and exposes this as the first of many false dichotomies throughout the book. “We live in a world of clocks and clouds and a vast jumble of other metaphors. Unpredictability and predictability coexist uneasily in the intricately interlocking systems that make up our bodies, our societies, and the cosmos. How predictable something is depends on what we are trying to predict, how far into the future, and under what circumstances.”He then sets up the core argument of the research program that IARPA paid for: how well can we do? A question like this should leave you speechless and befuddled—the US Government asked Tetlock and other university and industry programs to find out how well we can predict geopolitical outcomes over a 3–6 month time horizon, because nobody knew how well we did. Think of all the pundits, all the terrible books Tom Friedman pooped out, all the bloggers and tooters, and the realization that all that heat gave us no light should leave you breathless.In chapter two, Tetlock dives into the ugly story of medicine.Tetlock revisits this over and over again: medicine only very, very recently became a devotee of the randomized controlled trial. He has a wonderful set of vignettes of the vanguard of physicians who dragged their colleagues kicking and screaming into the evidence-based age after World War II. Its been a few weeks since I read this section but I think I will forever remember this story:‘When hospitals created cardiac care units to treat patients recovering from heart attacks, Cochrane proposed a randomized trial to determine whether the new units delivered better results than the old treatment, which was to send the patient home for monitoring and bed rest. Physicians balked. It was obvious the cardiac care units were superior, they said, and denying patients the best care would be unethical. … [but] Cochrane got his trial: some patients, randomly selected, were sent to the cardiac care units while others were sent home for monitoring and bed rest. Partway through the trial, Cochrane met with a group of the cardiologists who had tried to stop his experiment. He told them that he had preliminary results. The difference in outcomes between the two treatments was not statistically significant, he emphasized, but it appeared that patients might do slightly better in the cardiac care units. “They were vociferous in their abuse: ‘Archie,’ they said, ‘we always thought you were unethical. You must stop the trial at once.’ ” But then Cochrane revealed he had played a little trick. He had reversed the results: home care had done slightly better than the cardiac units. “There was dead silence and I felt rather sick because they were, after all, my medical colleagues.”’Yes, a lot of medicine is terribly “intuition-based” today—“it’s obvious this treatment is better”. But medicine has made great strides, over the last few decades, in acknowledging the risks and flaws of “intuition” and committing itself to the exacting requirements of randomized controlled trials.Programmers are slowly learning this, thankfully with less loss of life. Andrei Alexandrescu, in his “Writing Quick Code in C++, Quickly” talk in 2013, discussed this at length:‘You must measure everything. We all have intuition. And the intuition of programmers is always wrong. Outdated. Intuition ignores a lot of aspects of a complex reality. Today’s machine architectures are so complicated, there’re so many variables in flight at any point in time that it’s essentially impossible to consider them deterministic machines any more. They are not deterministic any more. So we make very often big mistakes when assuming things about what’s going to make fast code. [E.g.,] fewer instructions do not equal faster code. Data [access] is not always faster than computation. The only good intuition is “I should measure this stuff and see what happens.” To quote a classic, who is still alive, Walter Bight: “Measuring gives you a leg up on experts who are so good they don’t need to measure.” Walter and I have been working on optimizing bits and pieces of a project we work on and … whenever we think we know what we’re doing, we measure, and it’s just the other way around.’Here are two of the world’s leading experts in programming language design and implementation, openly saying “whenever we think we know what we’re doing, we measure, and it’s just the other way around.” That is probably worth tattooing on one’s forehead.This is relevant to the Good Judgement Project because it is the first time randomized controlled trials have been applied to geopolitical prediction, but also because dealing with evidence and weighing it is a key component of forecasting. Putting a collar on intuition helps to prevent it from ruining your predictions.The next chapter (chapter three) discusses the intricacies of keeping score. A project like this, and the task of improving one’s own forecasting, lives and dies by the pesky pernicious questions of exactly how you measure performance, and other experimental details. Tetlock shows how everything we might consider as “forecasts” (intelligence agencies, the revolting Thomas Friedman) is trash, is intellectual weasel-worded garbage. He details the kinds of questions amenable to his experiment, how to elicit probability estimates, how to enforce time horizons, how to factor in update frequencies, how to fuse predictions from groups, various research tools, etc.This chapter details how the Brier score works: forecasters answer a yes/no question with a probability. The score rewards emphatically correct answer, and punishes incorrect confidence.Chapter four gives a detailed overview of the project’s findings: how well Tetlock’s superforecasters did, and analyses of how and why they did so well. Tetlock really surprised me here by offering a very humble and honestly rigorous analysis of regression to the mean. He explains how in games of chance, regression to the mean crushes the winners after repeated rounds, whereas exercises of skill sees the winners only improve after succeeding rounds.“Each year, roughly 30% of the individual superforecasters fall from the ranks of the top 2% next year. But that also implies a good deal of consistency over time: 70% of superforecasters remain superforecasters.”Tetlock could have gone all business-book “Good to Great” (trash) on me. No. This is a well-reasoned and thoughtful argument that honestly explored the role of luck in forecasting. 30% annual replacement suggests some luck, but a lot of skill. That is a very powerful finding.Chapter five breaks down the intelligence of superforecasters, and six their math savvy. Findings: superforecasters are intelligent and also generically math-savvy, but intelligence and math skills are neither necessary nor sufficient for superforecasting performance.Chapter five has a really interesting discussion of Fermi analyses—you know, “how many piano tuners are in Chicago”. I did this piano tuner exercise for the first time while reading this book (despite having read about it here and there in the past), and that was a very insightful experience. Fermi analysis shows that, rather than making a big prediction that might have a lot of error, you can break the problem up into smaller problems whose errors are smaller, and whose errors stay small after combining them. Fermi analysis is cool, and I finally appreciate them.Chapter six also deals, almost spiritually, with the misguided “quest for meaning” and the herculean discipline needed to maintain a probabilistic outlook on life:‘Even in the face of tragedy, the probabilistic thinker will say, “Yes, there was an almost infinite number of paths that events could have taken, and it was incredibly unlikely that events would take the path that ended in my child’s death. But they had to take a path and that’s the one they took. That’s all there is to it.” In Kahneman’s terms, probabilistic thinkers take the outside view toward even profoundly identity-defining events, seeing them as quasi-random draws from distributions of once-possible worlds.’Forget living Biblically for a year. Try living like this for a day.Chapter seven examines whether superforecasters are plugged into the global news streams. Answer: yes to some degree, but it doesn’t really explain their performance versus regular non-super forecasters.Chapter eight examines the twin vexing problems of updating beliefs in light of new evidence, and getting better at making predictions in light of your past predictions. Because Black Lives Matter, consider police officers:“police officers spend a lot of time figuring out who is telling the truth and who is lying, but research has found they aren’t nearly as good at it as they think they are and they tend not to get better with experience. That’s because experience isn’t enough. It must be accompanied by clear feedback. … Psychologists who test police officers’ ability to spot lies in a controlled setting find a big gap between their confidence and their skill. And that gap grows as officers become more experienced and they assume, not unreasonably, that their experience has made them better lie detectors. As a result, officers grow confident faster than they grow accurate, meaning they grow increasingly overconfident.”This chapter also talks about the discipline to keep hindsight bias in the kennel, and the difficulty in acknowledging the role of luck:“People often assume that when a decision is followed by a good outcome, the decision was good, which isn’t always true, and can be dangerous if it blinds us to the flaws in our thinking.”In a book full of actionably valuable insights (to the aspiring forecaster), this discussion might be the most helpful. Chapter nine deals with teams, team dynamics, fusing algorithms for merging individuals’ predictions, and lots of interesting related things. Chapter ten discusses how leaders might respond to a team of forecasters, and the changes leaders have to make to best utilize them.Chapter eleven talks about the problems with Tetlock’s research platform. (As he himself states earlier in the book, a scientist will always specify the conditions under which they would change their minds.)“I see Kahneman’s and Taleb’s critiques as the strongest challenges to the notion of superforecasting.”Kahneman’s critique is, can forecasters permanently tame cognitive biases, and keep churning out winning forecasts year after year (or at least long enough to be useful)? Taleb’s critique is, can forecasters say anything about black swan events that dominate history?Both of these critiques, in my personal opinion, are surmountable, making Tetlock’s research agenda and this book well worth reading.Chapter twelve closes with Tetlock’s hopes for a future world where we keep score about forecast. It could be awesome. But we’d get used to it fast and start worrying about the next problem."
237,0804136696,http://goodreads.com/user/show/347507-pietro,5,"This book is ostensibly about predicting (some aspects of) the future *reliably*. But that framing sells it far short. Anyone interested in holding true beliefs about the world could benefit immensely from taking in this short book. As far as I'm concerned, predictions are just a really good way of keeping you honest, as they have been in science for centuries: if your beliefs lead to wrong predictions often, you need new beliefs.Lots of people --- newscasters, politicians, pundits --- make claims about what the future holds. The book opens by noting that, although we listen to them raptly, nobody keeps track of how reliable they are as forecasters. With a couple compelling bits of history, it draws an analogy with medicine before the 20th century, which was largely evidence-free and doctors were judged by credentials instead of track records.One big reason nobody keeps track of forecasters' abilities is that most forecasts are vague: ""there is a risk inflation will go up"". How much of a risk? Up by how much? Over what time frame? Unless these are specified, it is impossible to tell whether a forecast was right or wrong, and thus impossible to keep track of how good a forecaster is. The importance of precision is a MAJOR theme in the book; it's the only way to know how well we're doing as forecasters, and knowing that is the only way we can hope to improve.Obviously, high-status pundits and analysts who write op-eds and appear in the news have no incentive to have their performance tracked. They're already high-status; why would they risk being debunked? So Tetlock ran a massive experiment over several years where thousands of volunteers made predictions about hundreds of precise questions such as:""In the next year, will any country withdraw from the eurozone?"" ""How many additional countries will report cases of the Ebola virus before the end of this year?""""If a non-Chinese telecom firm wins a contract to provide internet in the Shanghai Free Trade Zone in the next two years, will Chinese citizens have access to Facebook or Twitter?""Volunteers, working in isolation, made probabilistic forecasts: ""there is a 65% chance Russia will annex a part of Ukraine in the next 6 months"". In the end, each was scored based on their hundreds of forecasts (using a ""Brier score""). As it turns out, about 2% of the volunteers did SUBSTANTIALLY better not only than chance, but better than famous analysts, pundits, and even the intelligence community, which has access to privileged information. And this wasn't luck either; these ""superforecasters"" continued to do incredibly well year in and year out.Let me say that again: there are normal, unknown people out there who can predict important geopolitical events SUBSTANTIALLY better than chance; better than all the pundits who get paid shitloads of money and invited to speak at Davos and meet with presidents; better even than intelligence analysts with access to classified information.How do they do it??That's what the rest of the book is about. In short, these are pretty smart, numerate, and informed people; but they're not OFF-THE-CHARTS smart, math-savvy, hyper news junkies. They're slightly above-average people who simply cultivate healthy mental habits.Habits like ""seek out many different opinions/analyses"", ""challenge your own beliefs"", ""examine past mistakes honestly""; ""put a question in context by relating it to similar questions""; habits we can all cultivate.This truly is one of those books that make your mind more powerful, that give you a new set of eyes. Highly recommended."
238,0804136696,http://goodreads.com/user/show/51856061-bilal-hafeez,4,"The talking heads on TV give such a convincing story about what the future holds that its hard not to believe them. But pinning them down to a time-frame and discrete future event is often next to impossible, so you can never determine whether they were right or wrong. When such talking heads are pinned down, their track-record turns out to be very poor, according to Philip Tetlock in his latest book ""Superforecasting: The Art and Science of Prediction"". This makes sense as they are picked more for their entertainment value, than their track-records.The real super-forecasters are people like Doug Lorch (ex-IBM, retired, lives in Santa Barbara), Mary Simpson (independent financial consultant, formerly regulatory affairs at utility Southern California Edison) and Devyn Duffy (welfare case worker, Pittsburgh). What they lack in TV appeal they make up for in their approach to forecasting.Tetlock and team found them as part of an IARPA, which is part of US Intelligence, project to improve forecasting of political and other events. He found that the way you approach forecasting rather than your credentials or political leanings matter most. If you're a one-big idea person (i.e. a hedgehog) you will struggle, while if you're a pragmatic tinkerer (i.e. a fox) you will do well. He listed the eleven commandments for aspiring super forecasters:1.Focus on questions where your handwork will pay off. So don't bother with a question like ""who will win the US election in 2028?"". It's too far out to bother predicting.2.Break big problems into smaller ones. 3.Strike the right balance between inside and outside views. There is nothing new under the sun. So first calculate the odds of an event happening based on similar ones in the past (i.e. top-down or outside view), then work-out the bottom-up/inside view of the specific event.4.Strike the right balance between under- and overreacting to evidence. Careful incorporate new evidence and update beliefs accordingly. The best forecasters are incremental belief updaters.5.Acknowledge the counter-arguments. If you believe military action never works, be open to the possibility that it might. Take both views and synthesise.6.Distinguish as many degrees of doubt as the problem permits. Your uncertainty dials needs more than 3 settings (certain, maybe, impossible). Translate vague hunches into numeric probabilities.7.Strike the right balance between under- and over-confidence. Understand the risks of rushing to judgment and of dawdling too long near ""maybe"".8.Don't justify or excuse your failures. Conduct unflinching postmortem.9.Bring out the best in others and let others bring out the best in you. Master team management: perspective taking (being able to reproduce the others argument to their satisfaction), precision questioning (clarify arguments so there is no misunderstanding), and constructive confrontation (learning to disagree without being disagreeable).10.Master the error-balancing bicycle. Learning requires doing, with good feedback that leaves no ambiguity about whether you are succeeding.11.Don't treat commandments as commandments!You can practise your prediction skills by joining Tetlock's Good Judgment Projectbilalhafeez.com"
239,0804136696,http://goodreads.com/user/show/7876548-chirayu-batra,4,"Seems like a true companion of Dan Kahneman's Thinking:fast and slow and surely influenced by his judgement psychology. Book is a reminder that world is complex equation and forecasting anything in it is not a simple phenomenon. The book also seems a good promotional material for the good judgement project, but that makes sense. I hope to join the project and make some of my own predictions, smart predictions. A nice read for psychology lovers and people interested in reading about the art of forecasting. "
240,0804136696,http://goodreads.com/user/show/66952286-scott,5,"I enjoyed this book about forecasting future events. It discusses how medicine changed around the time of ww2 when statistics were first used to commonly track likelihood of outcomes - leading to improved standardized treatments. It also discusses how the best self-selected forecasters in a study were consistently able to outperform even dedicated agencies with access to classified info when trained. The author managed this group for years and offers many insights into forecasting. Next, he discusses the use of forecasting (effective and not) in critical decisions like the bay of pigs, Cuban missile crisis, raid on bin laden, and attack on Saddam Hussein. Finally, he discusses his current work with GJOpen.com where anyone can test their skills at forecasting future events like elections, commodity prices, deployment of electric vehicles, etc."
241,0804136696,http://goodreads.com/user/show/64798070-jay-hennessey,4,"I really enjoyed this read, especially in my new role in professional sports. At risk of liking due to confirmation bias, I really enjoyed hearing the application of concepts from other books that I hold in the highest regard, I.e. Thinking Fast and Slow. I also enjoyed learning about Bayes Theory — coincidentally reading, The Theory that would not Die - How Bayes Rule Cracked the Enigma Code....Like most brilliant concepts, the idea of constantly updating one’s forecast, or hypothesis with new information sounded so simple, but the examples showed that this is usually the exception rather than the rule. The book also made me reflect on the concepts from Lean Start Up — Build, Measure, Learn. While the Lean Startup model is more about doing something and making adjustments, broadly, I see the concepts as the same. In Lean, one forms a hypothesis, determines how he will know if he is correct, and then sets out with a minimally viable product to test and measure the result — constantly adapting the product...or in Super-forecasting-Speak, constantly updating the prediction.I recommend this book to growth minded leaders of any organization. The concepts in this book, interleaved with so many other great works, are thought provoking and worthily of consideration in any space."
242,0804136696,http://goodreads.com/user/show/4057945-cristobal,5,"This book reminds of Emerson's phrase “The mind, once stretched by a new idea, never returns to its original dimensions.” which is what experienced after reading it. Mental models are the filters through which we see the world and tint all of our thinking. That is why taking the time and making the effort to screen how we think can make all the difference. ""Superforecasting"" builds a solid case on why making the best forecasting is not something reserved for a few lucky ones but that can be attained through meticulous analysis and keeping an open mind to information and other points of view. Mental biases will always be around, but knowing them and building the tools to try to limit their influence can make anyone a much more accurate forecaster and clear thinker.Definitely one of my favorite books on thinking right alongside ""Thinking Fast and Slow""."
243,0804136696,http://goodreads.com/user/show/7460650-viktor-szathmary,2,"While the book provides a reasonable overview of biases (and methods for correcting) in forecasting games, it is sadly disconnected from real-world decision making. For example, it treats a tennis match the same as international nuclear war. Only in academia can the two be managed the same way (as a binary prediction game, measured by your Brier score). While the author seems to have read Nassim Taleb's works (devoting a chapter to it), unfortunately they have missed the key points."
244,0804136696,http://goodreads.com/user/show/93933488-wen,4,"I liked it but it's not for everybody. The author wants to drive accountability for those who make predictions. I largely resonate with his observations and recommendations for providing better forecasts. However, as stated by the author, his recommendations should not be treated as gospel, but rather as another tool. I do like how the Good Judgement Project predictions and results are publicly available to view. "
245,0804136696,http://goodreads.com/user/show/34741339-robert-meyro,3,"While the title of this book drew me to believe that there could be hope in forecasting, I have definitely become more skeptical after reading this book. It nearly seems that the author felt the same way, the further one progressed in the book. That being said, there were some interesting anecdotes covered."
246,0804136696,http://goodreads.com/user/show/87001455-nathan-runke,5,Phenomenal book. Extremely well written and laid out. For a book on statistics and predicting the future (or as much as you can) it flows very well and is a nice read. Will probably be reading again. Has some valuable information on how you can improve your own personal ability to predict things/understand what is happening around the world.
247,0804136696,http://goodreads.com/user/show/39225478-jiliac,5,"The trilogy of books about prediction is ""Black Swan"" by Taleb, ""Thinking Fast and Slow"" by Kahneman, and this one, ""Superforecasting"" by Tetlock. All authors cite the two others profusely. They all have a different approach. Taleb being on the extremely rare events, Kahneman on the bias our brain creates, and Tetlock on the ""how to predict in practice"".Myself being more interested on the practical aspect of things, I was expecting Tetlock to be my prefered one. But in the end, I feel it's the one that I learned the least from. It comes with a very precise, detailed, and scientifically well studied, set of advices: practice a lot, reconsider yourself, diversify the opinions you base yourself on, and more... However, it doesn't deal with Taleb point, which was what I was expecting. (This expectation was unfair though, since it never advertised as such. That's why I still rate it five stars.)"
248,0804136696,http://goodreads.com/user/show/33662754-escada-emanuel,4,"Interesting book with a pretty smooth reading. Strikes a quite clear balance between what can actually be forecast and what cannot. Gives interesting tips on how to analyse complex realities, for example, through the inside and outside views or the 'fermi-zing' method. Also a highlight for the 'Auftragstaktik' from the Prussian military and now fashionably widespread on the ideal way to deal with uncertainty. "
249,0804136696,http://goodreads.com/user/show/118345071-brock-bank,5,"Late to the party on this one, but definitely deserves the hype. Favorite book I read this year, probably my favorite I’ve ever read around metacognition. Written to be readable, has practicable tips for being a better predictor. Tbf I was very predisposed to enjoying this book; throw in some prescient Michael Flynn criticism and I was sure to love it.Definitely worth the time to read the short appendix which outlines the main takes if you don’t want to give it a full read."
250,0804136696,http://goodreads.com/user/show/45921922-thiago-marzag-o,5,If I had read about Tetlock's work when I first started grad school my interests would have changed completely and I would have produced an entirely different dissertation. Too bad it took me so long to come across this.
251,0804136696,http://goodreads.com/user/show/12310532-taivo,4,"There was a decent concentration of content and good balance between storytelling versus relaying useful models, but I still felt the core ideas of the book could have been fit into a chapter or two, and the rest of the book was written to fill up 300 pages."
252,0804136696,http://goodreads.com/user/show/13493517-cenk-undey,4,An interesting high level summary of approaches to predicting life events. It makes a lot of references to the books I already read such as Kahleman’s Thinking Fast and Slow and Nassim N. Taleb’s The Black Swan. It is a fun book to go through.
253,0804136696,http://goodreads.com/user/show/15701574-allan-aksiim,3,"No point reading the entire thing. Valuable lessons yes, american style unnecessary pop-science fluff also. "
254,0804136696,http://goodreads.com/user/show/52223189-mario-vanhoucke,4,"Great book about the good judgement project and how some people can outperform experts in forecasting events. The book brings together some of my favourite authors/researchers and refers to the “illusion of control” by E. Langer, the work and quotes of Amos Tsversky, good old Bayes and his theorem (we should take an inside and outside view, but start with the outside one) and finally, in chapter 11, the work of Kahneman and Taleb (really great summary and discussion). Evidence based forecasting, it sounds like an easy-to-accept principle, but the authors argue it’s not. But they are hopeful: Where wisdom once was, quantification will now be. Read!"
255,0804136696,http://goodreads.com/user/show/64773125-gabi,5,"A great book, in a few words : ""This book is not about how to be happy.. is about how to be accurate!"" [from-the-end-of-chapter-6]"
256,0804136696,http://goodreads.com/user/show/1300459-sambasivan,4,Novel concept and well articulated. Contains principles that one can use for a lifetime. Be curious and improve yourself regularly to become a super forecaster. 
257,0804136696,http://goodreads.com/user/show/2385396-bradley-eylander,4,"This book is a slight contrast to the book I read before called, ""The Signal and the Noise: Why So Many Predictions Fail - But Some Don't"". Both books acknowledge that there are many things in life we can't predict and the difficulty of predicting them. This book covers people that are statistically good at predicting and why. Turns out its due to collecting data daily (compounding effect) and applying statistics. What I thought was ironic was the people that are good at predicting aren't the ones being paid lots of $$ working for organizations such as the CIA. It was hobbyist who's only incentive was Amazon gift cards."
258,0804136696,http://goodreads.com/user/show/1054712-kent-winward,4,"I would have known how much I'd like the book, if I had read the book first . . ."
259,0804136696,http://goodreads.com/user/show/11228-suman,0,3.5 stars
260,0804136696,http://goodreads.com/user/show/57644603-colin-paulish,5,Easy read with takeaways that you feel you could integrate and apply to your life almost immediately
261,0804136696,http://goodreads.com/user/show/82584091-raph-zindi,5,Wildly disruptive thinking! One to revisit from time to time. 
262,0804136696,http://goodreads.com/user/show/33743305-vinh,4,The book is based on scientific results which are quite interesting and insightful.
263,0804136696,http://goodreads.com/user/show/5822873-kaustubh,3,"A nice introduction (and not a guide) into the world of forecasting: 6.5/10Tetlock and (presumable ghostwriter) Gardner provide a nice introduction into the world of geopolitical forecasting. They detail Tetlock’s long-running (>15 yr) experiments on people’s abilities to forecast future outcomes on a variety of themes. Although the book sets itself up somewhat disingenuously as a guide for improvement in forecasting knowledge and ability, it is more of an introduction to forecasting, histogram statistics, calibration-verification-validation, and several anecdotal examples about people and events that conform to said underlying statistical theory. The examples are illuminating and at times, contradictory - while they are useful, one can’t help but wonder about where exactly they fall on the bell curve and whether they are representative of anything or not. There are many references to Kahneman’s Thinking Fast and Slow littered throughout the book. Sometimes, I got the feeling that I ought to be reading that book instead. Where Tetlock and Gardner excel is in driving down their take-home message: there is no single catch-all formula to “super”forecasting and rather, one should expect a slow-burn process of hard work, attentiveness, and a metered demeanor for marginal successes. However, keep in mind, that margins are everything in this field."
264,0804136696,http://goodreads.com/user/show/13608511-ka,5,What an excellent read. Highly recommend if you’re curious about how to reason more rigorously and want to make better predictions about where the world is headed. Rarely seen such gifted science writing!
265,0804136696,http://goodreads.com/user/show/38189944-mo,5,"Phenomenal! I think this is the one to beat in terms of a popular guide to the Decision and Cognitive Sciences and - more interestingly - how they can be used in the real world to make a real difference. All told in an eminently readable, deceptively simple-seeming fashion to boot. I honestly can not praise or recommend this book enough. "
266,0804136696,http://goodreads.com/user/show/25471547-ahmed,5,"You have to understand that this book is eminently and deeply rooted in the research program Tetlock pioneered and ran, the Good Judgement Project, and about the real people whom he found through the program to be great are forecasting geopolitical events in the three to six month range.My own personal goal before reading the book was to learn rigorous prediction—not necessarily answering geopolitical questions that were handed from somewhere else, but more like how many editors will show up on global Wikipedia today, or possibly more profitably, how much the stock market will lose today. I was prepared for the book to be an overly-academic treatise on the minutiae of the research program, or for a number of ways it could be operationally useless.But at each chapter, Tetlock had something very meaningful and useful to say to me about the business of prediction. At each stage he surprised me with the foxlike connections he drew between what he saw his forecasters doing and with other research programs and discoveries.(I was delighted to see my boy Duncan Watts cited on the emphatically post hoc nature of describing “significant” events. Aaron Brown, my poker-playing risk-managing guru, weighs in on the value of small but consistent wins. The once-cool-but-now-fascist-apologist Nassim Taleb makes appearances as the book struggles with whether its brand of forecasting is useful in the face of an extremistan world (answer: it is, because you can make money in an extremistan world). Of course Danny Kahneman is intertwined with the narrative, as Tetlock’s sounding board and colleague for many decades. Not mentioned in the book but in Tetlock’s five-part master-class on forecasting on Edge.org, my main man Anders Ericsson is cited on the trainability of forecasters.)Chapter one opens with trying to convince us that predictability exists and forecasting can help us (make or save us money, at the simplest). He invites us to contrast the unpredictability of cloud shapes with the predictability of a clock, and exposes this as the first of many false dichotomies throughout the book. “We live in a world of clocks and clouds and a vast jumble of other metaphors. Unpredictability and predictability coexist uneasily in the intricately interlocking systems that make up our bodies, our societies, and the cosmos. How predictable something is depends on what we are trying to predict, how far into the future, and under what circumstances.”He then sets up the core argument of the research program that IARPA paid for: how well can we do? A question like this should leave you speechless and befuddled—the US Government asked Tetlock and other university and industry programs to find out how well we can predict geopolitical outcomes over a 3–6 month time horizon, because nobody knew how well we did. Think of all the pundits, all the terrible books Tom Friedman pooped out, all the bloggers and tooters, and the realization that all that heat gave us no light should leave you breathless.In chapter two, Tetlock dives into the ugly story of medicine.Tetlock revisits this over and over again: medicine only very, very recently became a devotee of the randomized controlled trial. He has a wonderful set of vignettes of the vanguard of physicians who dragged their colleagues kicking and screaming into the evidence-based age after World War II. Its been a few weeks since I read this section but I think I will forever remember this story:‘When hospitals created cardiac care units to treat patients recovering from heart attacks, Cochrane proposed a randomized trial to determine whether the new units delivered better results than the old treatment, which was to send the patient home for monitoring and bed rest. Physicians balked. It was obvious the cardiac care units were superior, they said, and denying patients the best care would be unethical. … [but] Cochrane got his trial: some patients, randomly selected, were sent to the cardiac care units while others were sent home for monitoring and bed rest. Partway through the trial, Cochrane met with a group of the cardiologists who had tried to stop his experiment. He told them that he had preliminary results. The difference in outcomes between the two treatments was not statistically significant, he emphasized, but it appeared that patients might do slightly better in the cardiac care units. “They were vociferous in their abuse: ‘Archie,’ they said, ‘we always thought you were unethical. You must stop the trial at once.’ ” But then Cochrane revealed he had played a little trick. He had reversed the results: home care had done slightly better than the cardiac units. “There was dead silence and I felt rather sick because they were, after all, my medical colleagues.”’Yes, a lot of medicine is terribly “intuition-based” today—“it’s obvious this treatment is better”. But medicine has made great strides, over the last few decades, in acknowledging the risks and flaws of “intuition” and committing itself to the exacting requirements of randomized controlled trials.Programmers are slowly learning this, thankfully with less loss of life. Andrei Alexandrescu, in his “Writing Quick Code in C++, Quickly” talk in 2013, discussed this at length:‘You must measure everything. We all have intuition. And the intuition of programmers is always wrong. Outdated. Intuition ignores a lot of aspects of a complex reality. Today’s machine architectures are so complicated, there’re so many variables in flight at any point in time that it’s essentially impossible to consider them deterministic machines any more. They are not deterministic any more. So we make very often big mistakes when assuming things about what’s going to make fast code. [E.g.,] fewer instructions do not equal faster code. Data [access] is not always faster than computation. The only good intuition is “I should measure this stuff and see what happens.” To quote a classic, who is still alive, Walter Bight: “Measuring gives you a leg up on experts who are so good they don’t need to measure.” Walter and I have been working on optimizing bits and pieces of a project we work on and … whenever we think we know what we’re doing, we measure, and it’s just the other way around.’Here are two of the world’s leading experts in programming language design and implementation, openly saying “whenever we think we know what we’re doing, we measure, and it’s just the other way around.” That is probably worth tattooing on one’s forehead.This is relevant to the Good Judgement Project because it is the first time randomized controlled trials have been applied to geopolitical prediction, but also because dealing with evidence and weighing it is a key component of forecasting. Putting a collar on intuition helps to prevent it from ruining your predictions.The next chapter (chapter three) discusses the intricacies of keeping score. A project like this, and the task of improving one’s own forecasting, lives and dies by the pesky pernicious questions of exactly how you measure performance, and other experimental details. Tetlock shows how everything we might consider as “forecasts” (intelligence agencies, the revolting Thomas Friedman) is trash, is intellectual weasel-worded garbage. He details the kinds of questions amenable to his experiment, how to elicit probability estimates, how to enforce time horizons, how to factor in update frequencies, how to fuse predictions from groups, various research tools, etc.This chapter details how the Brier score works: forecasters answer a yes/no question with a probability. The score rewards emphatically correct answer, and punishes incorrect confidence.Chapter four gives a detailed overview of the project’s findings: how well Tetlock’s superforecasters did, and analyses of how and why they did so well. Tetlock really surprised me here by offering a very humble and honestly rigorous analysis of regression to the mean. He explains how in games of chance, regression to the mean crushes the winners after repeated rounds, whereas exercises of skill sees the winners only improve after succeeding rounds.“Each year, roughly 30% of the individual superforecasters fall from the ranks of the top 2% next year. But that also implies a good deal of consistency over time: 70% of superforecasters remain superforecasters.”Tetlock could have gone all business-book “Good to Great” (trash) on me. No. This is a well-reasoned and thoughtful argument that honestly explored the role of luck in forecasting. 30% annual replacement suggests some luck, but a lot of skill. That is a very powerful finding.Chapter five breaks down the intelligence of superforecasters, and six their math savvy. Findings: superforecasters are intelligent and also generically math-savvy, but intelligence and math skills are neither necessary nor sufficient for superforecasting performance.Chapter five has a really interesting discussion of Fermi analyses—you know, “how many piano tuners are in Chicago”. I did this piano tuner exercise for the first time while reading this book (despite having read about it here and there in the past), and that was a very insightful experience. Fermi analysis shows that, rather than making a big prediction that might have a lot of error, you can break the problem up into smaller problems whose errors are smaller, and whose errors stay small after combining them. Fermi analysis is cool, and I finally appreciate them.Chapter six also deals, almost spiritually, with the misguided “quest for meaning” and the herculean discipline needed to maintain a probabilistic outlook on life:‘Even in the face of tragedy, the probabilistic thinker will say, “Yes, there was an almost infinite number of paths that events could have taken, and it was incredibly unlikely that events would take the path that ended in my child’s death. But they had to take a path and that’s the one they took. That’s all there is to it.” In Kahneman’s terms, probabilistic thinkers take the outside view toward even profoundly identity-defining events, seeing them as quasi-random draws from distributions of once-possible worlds.’Forget living Biblically for a year. Try living like this for a day.Chapter seven examines whether superforecasters are plugged into the global news streams. Answer: yes to some degree, but it doesn’t really explain their performance versus regular non-super forecasters.Chapter eight examines the twin vexing problems of updating beliefs in light of new evidence, and getting better at making predictions in light of your past predictions. Because Black Lives Matter, consider police officers:“police officers spend a lot of time figuring out who is telling the truth and who is lying, but research has found they aren’t nearly as good at it as they think they are and they tend not to get better with experience. That’s because experience isn’t enough. It must be accompanied by clear feedback. … Psychologists who test police officers’ ability to spot lies in a controlled setting find a big gap between their confidence and their skill. And that gap grows as officers become more experienced and they assume, not unreasonably, that their experience has made them better lie detectors. As a result, officers grow confident faster than they grow accurate, meaning they grow increasingly overconfident.”This chapter also talks about the discipline to keep hindsight bias in the kennel, and the difficulty in acknowledging the role of luck:“People often assume that when a decision is followed by a good outcome, the decision was good, which isn’t always true, and can be dangerous if it blinds us to the flaws in our thinking.”In a book full of actionably valuable insights (to the aspiring forecaster), this discussion might be the most helpful. Chapter nine deals with teams, team dynamics, fusing algorithms for merging individuals’ predictions, and lots of interesting related things. Chapter ten discusses how leaders might respond to a team of forecasters, and the changes leaders have to make to best utilize them.Chapter eleven talks about the problems with Tetlock’s research platform. (As he himself states earlier in the book, a scientist will always specify the conditions under which they would change their minds.)“I see Kahneman’s and Taleb’s critiques as the strongest challenges to the notion of superforecasting.”Kahneman’s critique is, can forecasters permanently tame cognitive biases, and keep churning out winning forecasts year after year (or at least long enough to be useful)? Taleb’s critique is, can forecasters say anything about black swan events that dominate history?Both of these critiques, in my personal opinion, are surmountable, making Tetlock’s research agenda and this book well worth reading.Chapter twelve closes with Tetlock’s hopes for a future world where we keep score about forecast. It could be awesome. But we’d get used to it fast and start worrying about the next problem."
267,0804136696,http://goodreads.com/user/show/347507-pietro,5,"This book is ostensibly about predicting (some aspects of) the future *reliably*. But that framing sells it far short. Anyone interested in holding true beliefs about the world could benefit immensely from taking in this short book. As far as I'm concerned, predictions are just a really good way of keeping you honest, as they have been in science for centuries: if your beliefs lead to wrong predictions often, you need new beliefs.Lots of people --- newscasters, politicians, pundits --- make claims about what the future holds. The book opens by noting that, although we listen to them raptly, nobody keeps track of how reliable they are as forecasters. With a couple compelling bits of history, it draws an analogy with medicine before the 20th century, which was largely evidence-free and doctors were judged by credentials instead of track records.One big reason nobody keeps track of forecasters' abilities is that most forecasts are vague: ""there is a risk inflation will go up"". How much of a risk? Up by how much? Over what time frame? Unless these are specified, it is impossible to tell whether a forecast was right or wrong, and thus impossible to keep track of how good a forecaster is. The importance of precision is a MAJOR theme in the book; it's the only way to know how well we're doing as forecasters, and knowing that is the only way we can hope to improve.Obviously, high-status pundits and analysts who write op-eds and appear in the news have no incentive to have their performance tracked. They're already high-status; why would they risk being debunked? So Tetlock ran a massive experiment over several years where thousands of volunteers made predictions about hundreds of precise questions such as:""In the next year, will any country withdraw from the eurozone?"" ""How many additional countries will report cases of the Ebola virus before the end of this year?""""If a non-Chinese telecom firm wins a contract to provide internet in the Shanghai Free Trade Zone in the next two years, will Chinese citizens have access to Facebook or Twitter?""Volunteers, working in isolation, made probabilistic forecasts: ""there is a 65% chance Russia will annex a part of Ukraine in the next 6 months"". In the end, each was scored based on their hundreds of forecasts (using a ""Brier score""). As it turns out, about 2% of the volunteers did SUBSTANTIALLY better not only than chance, but better than famous analysts, pundits, and even the intelligence community, which has access to privileged information. And this wasn't luck either; these ""superforecasters"" continued to do incredibly well year in and year out.Let me say that again: there are normal, unknown people out there who can predict important geopolitical events SUBSTANTIALLY better than chance; better than all the pundits who get paid shitloads of money and invited to speak at Davos and meet with presidents; better even than intelligence analysts with access to classified information.How do they do it??That's what the rest of the book is about. In short, these are pretty smart, numerate, and informed people; but they're not OFF-THE-CHARTS smart, math-savvy, hyper news junkies. They're slightly above-average people who simply cultivate healthy mental habits.Habits like ""seek out many different opinions/analyses"", ""challenge your own beliefs"", ""examine past mistakes honestly""; ""put a question in context by relating it to similar questions""; habits we can all cultivate.This truly is one of those books that make your mind more powerful, that give you a new set of eyes. Highly recommended."
268,0804136696,http://goodreads.com/user/show/51856061-bilal-hafeez,4,"The talking heads on TV give such a convincing story about what the future holds that its hard not to believe them. But pinning them down to a time-frame and discrete future event is often next to impossible, so you can never determine whether they were right or wrong. When such talking heads are pinned down, their track-record turns out to be very poor, according to Philip Tetlock in his latest book ""Superforecasting: The Art and Science of Prediction"". This makes sense as they are picked more for their entertainment value, than their track-records.The real super-forecasters are people like Doug Lorch (ex-IBM, retired, lives in Santa Barbara), Mary Simpson (independent financial consultant, formerly regulatory affairs at utility Southern California Edison) and Devyn Duffy (welfare case worker, Pittsburgh). What they lack in TV appeal they make up for in their approach to forecasting.Tetlock and team found them as part of an IARPA, which is part of US Intelligence, project to improve forecasting of political and other events. He found that the way you approach forecasting rather than your credentials or political leanings matter most. If you're a one-big idea person (i.e. a hedgehog) you will struggle, while if you're a pragmatic tinkerer (i.e. a fox) you will do well. He listed the eleven commandments for aspiring super forecasters:1.Focus on questions where your handwork will pay off. So don't bother with a question like ""who will win the US election in 2028?"". It's too far out to bother predicting.2.Break big problems into smaller ones. 3.Strike the right balance between inside and outside views. There is nothing new under the sun. So first calculate the odds of an event happening based on similar ones in the past (i.e. top-down or outside view), then work-out the bottom-up/inside view of the specific event.4.Strike the right balance between under- and overreacting to evidence. Careful incorporate new evidence and update beliefs accordingly. The best forecasters are incremental belief updaters.5.Acknowledge the counter-arguments. If you believe military action never works, be open to the possibility that it might. Take both views and synthesise.6.Distinguish as many degrees of doubt as the problem permits. Your uncertainty dials needs more than 3 settings (certain, maybe, impossible). Translate vague hunches into numeric probabilities.7.Strike the right balance between under- and over-confidence. Understand the risks of rushing to judgment and of dawdling too long near ""maybe"".8.Don't justify or excuse your failures. Conduct unflinching postmortem.9.Bring out the best in others and let others bring out the best in you. Master team management: perspective taking (being able to reproduce the others argument to their satisfaction), precision questioning (clarify arguments so there is no misunderstanding), and constructive confrontation (learning to disagree without being disagreeable).10.Master the error-balancing bicycle. Learning requires doing, with good feedback that leaves no ambiguity about whether you are succeeding.11.Don't treat commandments as commandments!You can practise your prediction skills by joining Tetlock's Good Judgment Projectbilalhafeez.com"
269,0804136696,http://goodreads.com/user/show/7876548-chirayu-batra,4,"Seems like a true companion of Dan Kahneman's Thinking:fast and slow and surely influenced by his judgement psychology. Book is a reminder that world is complex equation and forecasting anything in it is not a simple phenomenon. The book also seems a good promotional material for the good judgement project, but that makes sense. I hope to join the project and make some of my own predictions, smart predictions. A nice read for psychology lovers and people interested in reading about the art of forecasting. "
270,0804136696,http://goodreads.com/user/show/66952286-scott,5,"I enjoyed this book about forecasting future events. It discusses how medicine changed around the time of ww2 when statistics were first used to commonly track likelihood of outcomes - leading to improved standardized treatments. It also discusses how the best self-selected forecasters in a study were consistently able to outperform even dedicated agencies with access to classified info when trained. The author managed this group for years and offers many insights into forecasting. Next, he discusses the use of forecasting (effective and not) in critical decisions like the bay of pigs, Cuban missile crisis, raid on bin laden, and attack on Saddam Hussein. Finally, he discusses his current work with GJOpen.com where anyone can test their skills at forecasting future events like elections, commodity prices, deployment of electric vehicles, etc."
271,0804136696,http://goodreads.com/user/show/64798070-jay-hennessey,4,"I really enjoyed this read, especially in my new role in professional sports. At risk of liking due to confirmation bias, I really enjoyed hearing the application of concepts from other books that I hold in the highest regard, I.e. Thinking Fast and Slow. I also enjoyed learning about Bayes Theory — coincidentally reading, The Theory that would not Die - How Bayes Rule Cracked the Enigma Code....Like most brilliant concepts, the idea of constantly updating one’s forecast, or hypothesis with new information sounded so simple, but the examples showed that this is usually the exception rather than the rule. The book also made me reflect on the concepts from Lean Start Up — Build, Measure, Learn. While the Lean Startup model is more about doing something and making adjustments, broadly, I see the concepts as the same. In Lean, one forms a hypothesis, determines how he will know if he is correct, and then sets out with a minimally viable product to test and measure the result — constantly adapting the product...or in Super-forecasting-Speak, constantly updating the prediction.I recommend this book to growth minded leaders of any organization. The concepts in this book, interleaved with so many other great works, are thought provoking and worthily of consideration in any space."
272,0804136696,http://goodreads.com/user/show/4057945-cristobal,5,"This book reminds of Emerson's phrase “The mind, once stretched by a new idea, never returns to its original dimensions.” which is what experienced after reading it. Mental models are the filters through which we see the world and tint all of our thinking. That is why taking the time and making the effort to screen how we think can make all the difference. ""Superforecasting"" builds a solid case on why making the best forecasting is not something reserved for a few lucky ones but that can be attained through meticulous analysis and keeping an open mind to information and other points of view. Mental biases will always be around, but knowing them and building the tools to try to limit their influence can make anyone a much more accurate forecaster and clear thinker.Definitely one of my favorite books on thinking right alongside ""Thinking Fast and Slow""."
273,0804136696,http://goodreads.com/user/show/7460650-viktor-szathmary,2,"While the book provides a reasonable overview of biases (and methods for correcting) in forecasting games, it is sadly disconnected from real-world decision making. For example, it treats a tennis match the same as international nuclear war. Only in academia can the two be managed the same way (as a binary prediction game, measured by your Brier score). While the author seems to have read Nassim Taleb's works (devoting a chapter to it), unfortunately they have missed the key points."
274,0804136696,http://goodreads.com/user/show/93933488-wen,4,"I liked it but it's not for everybody. The author wants to drive accountability for those who make predictions. I largely resonate with his observations and recommendations for providing better forecasts. However, as stated by the author, his recommendations should not be treated as gospel, but rather as another tool. I do like how the Good Judgement Project predictions and results are publicly available to view. "
275,0804136696,http://goodreads.com/user/show/34741339-robert-meyro,3,"While the title of this book drew me to believe that there could be hope in forecasting, I have definitely become more skeptical after reading this book. It nearly seems that the author felt the same way, the further one progressed in the book. That being said, there were some interesting anecdotes covered."
276,0804136696,http://goodreads.com/user/show/87001455-nathan-runke,5,Phenomenal book. Extremely well written and laid out. For a book on statistics and predicting the future (or as much as you can) it flows very well and is a nice read. Will probably be reading again. Has some valuable information on how you can improve your own personal ability to predict things/understand what is happening around the world.
277,0804136696,http://goodreads.com/user/show/39225478-jiliac,5,"The trilogy of books about prediction is ""Black Swan"" by Taleb, ""Thinking Fast and Slow"" by Kahneman, and this one, ""Superforecasting"" by Tetlock. All authors cite the two others profusely. They all have a different approach. Taleb being on the extremely rare events, Kahneman on the bias our brain creates, and Tetlock on the ""how to predict in practice"".Myself being more interested on the practical aspect of things, I was expecting Tetlock to be my prefered one. But in the end, I feel it's the one that I learned the least from. It comes with a very precise, detailed, and scientifically well studied, set of advices: practice a lot, reconsider yourself, diversify the opinions you base yourself on, and more... However, it doesn't deal with Taleb point, which was what I was expecting. (This expectation was unfair though, since it never advertised as such. That's why I still rate it five stars.)"
278,0804136696,http://goodreads.com/user/show/33662754-escada-emanuel,4,"Interesting book with a pretty smooth reading. Strikes a quite clear balance between what can actually be forecast and what cannot. Gives interesting tips on how to analyse complex realities, for example, through the inside and outside views or the 'fermi-zing' method. Also a highlight for the 'Auftragstaktik' from the Prussian military and now fashionably widespread on the ideal way to deal with uncertainty. "
279,0804136696,http://goodreads.com/user/show/118345071-brock-bank,5,"Late to the party on this one, but definitely deserves the hype. Favorite book I read this year, probably my favorite I’ve ever read around metacognition. Written to be readable, has practicable tips for being a better predictor. Tbf I was very predisposed to enjoying this book; throw in some prescient Michael Flynn criticism and I was sure to love it.Definitely worth the time to read the short appendix which outlines the main takes if you don’t want to give it a full read."
280,0804136696,http://goodreads.com/user/show/45921922-thiago-marzag-o,5,If I had read about Tetlock's work when I first started grad school my interests would have changed completely and I would have produced an entirely different dissertation. Too bad it took me so long to come across this.
281,0804136696,http://goodreads.com/user/show/12310532-taivo,4,"There was a decent concentration of content and good balance between storytelling versus relaying useful models, but I still felt the core ideas of the book could have been fit into a chapter or two, and the rest of the book was written to fill up 300 pages."
282,0804136696,http://goodreads.com/user/show/13493517-cenk-undey,4,An interesting high level summary of approaches to predicting life events. It makes a lot of references to the books I already read such as Kahleman’s Thinking Fast and Slow and Nassim N. Taleb’s The Black Swan. It is a fun book to go through.
283,0804136696,http://goodreads.com/user/show/15701574-allan-aksiim,3,"No point reading the entire thing. Valuable lessons yes, american style unnecessary pop-science fluff also. "
284,0804136696,http://goodreads.com/user/show/52223189-mario-vanhoucke,4,"Great book about the good judgement project and how some people can outperform experts in forecasting events. The book brings together some of my favourite authors/researchers and refers to the “illusion of control” by E. Langer, the work and quotes of Amos Tsversky, good old Bayes and his theorem (we should take an inside and outside view, but start with the outside one) and finally, in chapter 11, the work of Kahneman and Taleb (really great summary and discussion). Evidence based forecasting, it sounds like an easy-to-accept principle, but the authors argue it’s not. But they are hopeful: Where wisdom once was, quantification will now be. Read!"
285,0804136696,http://goodreads.com/user/show/64773125-gabi,5,"A great book, in a few words : ""This book is not about how to be happy.. is about how to be accurate!"" [from-the-end-of-chapter-6]"
286,0804136696,http://goodreads.com/user/show/1300459-sambasivan,4,Novel concept and well articulated. Contains principles that one can use for a lifetime. Be curious and improve yourself regularly to become a super forecaster. 
287,0804136696,http://goodreads.com/user/show/2385396-bradley-eylander,4,"This book is a slight contrast to the book I read before called, ""The Signal and the Noise: Why So Many Predictions Fail - But Some Don't"". Both books acknowledge that there are many things in life we can't predict and the difficulty of predicting them. This book covers people that are statistically good at predicting and why. Turns out its due to collecting data daily (compounding effect) and applying statistics. What I thought was ironic was the people that are good at predicting aren't the ones being paid lots of $$ working for organizations such as the CIA. It was hobbyist who's only incentive was Amazon gift cards."
288,0804136696,http://goodreads.com/user/show/1054712-kent-winward,4,"I would have known how much I'd like the book, if I had read the book first . . ."
289,0804136696,http://goodreads.com/user/show/11228-suman,0,3.5 stars
290,0804136696,http://goodreads.com/user/show/57644603-colin-paulish,5,Easy read with takeaways that you feel you could integrate and apply to your life almost immediately
291,0804136696,http://goodreads.com/user/show/82584091-raph-zindi,5,Wildly disruptive thinking! One to revisit from time to time. 
292,0804136696,http://goodreads.com/user/show/33743305-vinh,4,The book is based on scientific results which are quite interesting and insightful.
293,0804136696,http://goodreads.com/user/show/5822873-kaustubh,3,"A nice introduction (and not a guide) into the world of forecasting: 6.5/10Tetlock and (presumable ghostwriter) Gardner provide a nice introduction into the world of geopolitical forecasting. They detail Tetlock’s long-running (>15 yr) experiments on people’s abilities to forecast future outcomes on a variety of themes. Although the book sets itself up somewhat disingenuously as a guide for improvement in forecasting knowledge and ability, it is more of an introduction to forecasting, histogram statistics, calibration-verification-validation, and several anecdotal examples about people and events that conform to said underlying statistical theory. The examples are illuminating and at times, contradictory - while they are useful, one can’t help but wonder about where exactly they fall on the bell curve and whether they are representative of anything or not. There are many references to Kahneman’s Thinking Fast and Slow littered throughout the book. Sometimes, I got the feeling that I ought to be reading that book instead. Where Tetlock and Gardner excel is in driving down their take-home message: there is no single catch-all formula to “super”forecasting and rather, one should expect a slow-burn process of hard work, attentiveness, and a metered demeanor for marginal successes. However, keep in mind, that margins are everything in this field."
294,0804136696,http://goodreads.com/user/show/13608511-ka,5,What an excellent read. Highly recommend if you’re curious about how to reason more rigorously and want to make better predictions about where the world is headed. Rarely seen such gifted science writing!
295,0804136696,http://goodreads.com/user/show/38189944-mo,5,"Phenomenal! I think this is the one to beat in terms of a popular guide to the Decision and Cognitive Sciences and - more interestingly - how they can be used in the real world to make a real difference. All told in an eminently readable, deceptively simple-seeming fashion to boot. I honestly can not praise or recommend this book enough. "
296,0804136696,http://goodreads.com/user/show/25471547-ahmed,5,"You have to understand that this book is eminently and deeply rooted in the research program Tetlock pioneered and ran, the Good Judgement Project, and about the real people whom he found through the program to be great are forecasting geopolitical events in the three to six month range.My own personal goal before reading the book was to learn rigorous prediction—not necessarily answering geopolitical questions that were handed from somewhere else, but more like how many editors will show up on global Wikipedia today, or possibly more profitably, how much the stock market will lose today. I was prepared for the book to be an overly-academic treatise on the minutiae of the research program, or for a number of ways it could be operationally useless.But at each chapter, Tetlock had something very meaningful and useful to say to me about the business of prediction. At each stage he surprised me with the foxlike connections he drew between what he saw his forecasters doing and with other research programs and discoveries.(I was delighted to see my boy Duncan Watts cited on the emphatically post hoc nature of describing “significant” events. Aaron Brown, my poker-playing risk-managing guru, weighs in on the value of small but consistent wins. The once-cool-but-now-fascist-apologist Nassim Taleb makes appearances as the book struggles with whether its brand of forecasting is useful in the face of an extremistan world (answer: it is, because you can make money in an extremistan world). Of course Danny Kahneman is intertwined with the narrative, as Tetlock’s sounding board and colleague for many decades. Not mentioned in the book but in Tetlock’s five-part master-class on forecasting on Edge.org, my main man Anders Ericsson is cited on the trainability of forecasters.)Chapter one opens with trying to convince us that predictability exists and forecasting can help us (make or save us money, at the simplest). He invites us to contrast the unpredictability of cloud shapes with the predictability of a clock, and exposes this as the first of many false dichotomies throughout the book. “We live in a world of clocks and clouds and a vast jumble of other metaphors. Unpredictability and predictability coexist uneasily in the intricately interlocking systems that make up our bodies, our societies, and the cosmos. How predictable something is depends on what we are trying to predict, how far into the future, and under what circumstances.”He then sets up the core argument of the research program that IARPA paid for: how well can we do? A question like this should leave you speechless and befuddled—the US Government asked Tetlock and other university and industry programs to find out how well we can predict geopolitical outcomes over a 3–6 month time horizon, because nobody knew how well we did. Think of all the pundits, all the terrible books Tom Friedman pooped out, all the bloggers and tooters, and the realization that all that heat gave us no light should leave you breathless.In chapter two, Tetlock dives into the ugly story of medicine.Tetlock revisits this over and over again: medicine only very, very recently became a devotee of the randomized controlled trial. He has a wonderful set of vignettes of the vanguard of physicians who dragged their colleagues kicking and screaming into the evidence-based age after World War II. Its been a few weeks since I read this section but I think I will forever remember this story:‘When hospitals created cardiac care units to treat patients recovering from heart attacks, Cochrane proposed a randomized trial to determine whether the new units delivered better results than the old treatment, which was to send the patient home for monitoring and bed rest. Physicians balked. It was obvious the cardiac care units were superior, they said, and denying patients the best care would be unethical. … [but] Cochrane got his trial: some patients, randomly selected, were sent to the cardiac care units while others were sent home for monitoring and bed rest. Partway through the trial, Cochrane met with a group of the cardiologists who had tried to stop his experiment. He told them that he had preliminary results. The difference in outcomes between the two treatments was not statistically significant, he emphasized, but it appeared that patients might do slightly better in the cardiac care units. “They were vociferous in their abuse: ‘Archie,’ they said, ‘we always thought you were unethical. You must stop the trial at once.’ ” But then Cochrane revealed he had played a little trick. He had reversed the results: home care had done slightly better than the cardiac units. “There was dead silence and I felt rather sick because they were, after all, my medical colleagues.”’Yes, a lot of medicine is terribly “intuition-based” today—“it’s obvious this treatment is better”. But medicine has made great strides, over the last few decades, in acknowledging the risks and flaws of “intuition” and committing itself to the exacting requirements of randomized controlled trials.Programmers are slowly learning this, thankfully with less loss of life. Andrei Alexandrescu, in his “Writing Quick Code in C++, Quickly” talk in 2013, discussed this at length:‘You must measure everything. We all have intuition. And the intuition of programmers is always wrong. Outdated. Intuition ignores a lot of aspects of a complex reality. Today’s machine architectures are so complicated, there’re so many variables in flight at any point in time that it’s essentially impossible to consider them deterministic machines any more. They are not deterministic any more. So we make very often big mistakes when assuming things about what’s going to make fast code. [E.g.,] fewer instructions do not equal faster code. Data [access] is not always faster than computation. The only good intuition is “I should measure this stuff and see what happens.” To quote a classic, who is still alive, Walter Bight: “Measuring gives you a leg up on experts who are so good they don’t need to measure.” Walter and I have been working on optimizing bits and pieces of a project we work on and … whenever we think we know what we’re doing, we measure, and it’s just the other way around.’Here are two of the world’s leading experts in programming language design and implementation, openly saying “whenever we think we know what we’re doing, we measure, and it’s just the other way around.” That is probably worth tattooing on one’s forehead.This is relevant to the Good Judgement Project because it is the first time randomized controlled trials have been applied to geopolitical prediction, but also because dealing with evidence and weighing it is a key component of forecasting. Putting a collar on intuition helps to prevent it from ruining your predictions.The next chapter (chapter three) discusses the intricacies of keeping score. A project like this, and the task of improving one’s own forecasting, lives and dies by the pesky pernicious questions of exactly how you measure performance, and other experimental details. Tetlock shows how everything we might consider as “forecasts” (intelligence agencies, the revolting Thomas Friedman) is trash, is intellectual weasel-worded garbage. He details the kinds of questions amenable to his experiment, how to elicit probability estimates, how to enforce time horizons, how to factor in update frequencies, how to fuse predictions from groups, various research tools, etc.This chapter details how the Brier score works: forecasters answer a yes/no question with a probability. The score rewards emphatically correct answer, and punishes incorrect confidence.Chapter four gives a detailed overview of the project’s findings: how well Tetlock’s superforecasters did, and analyses of how and why they did so well. Tetlock really surprised me here by offering a very humble and honestly rigorous analysis of regression to the mean. He explains how in games of chance, regression to the mean crushes the winners after repeated rounds, whereas exercises of skill sees the winners only improve after succeeding rounds.“Each year, roughly 30% of the individual superforecasters fall from the ranks of the top 2% next year. But that also implies a good deal of consistency over time: 70% of superforecasters remain superforecasters.”Tetlock could have gone all business-book “Good to Great” (trash) on me. No. This is a well-reasoned and thoughtful argument that honestly explored the role of luck in forecasting. 30% annual replacement suggests some luck, but a lot of skill. That is a very powerful finding.Chapter five breaks down the intelligence of superforecasters, and six their math savvy. Findings: superforecasters are intelligent and also generically math-savvy, but intelligence and math skills are neither necessary nor sufficient for superforecasting performance.Chapter five has a really interesting discussion of Fermi analyses—you know, “how many piano tuners are in Chicago”. I did this piano tuner exercise for the first time while reading this book (despite having read about it here and there in the past), and that was a very insightful experience. Fermi analysis shows that, rather than making a big prediction that might have a lot of error, you can break the problem up into smaller problems whose errors are smaller, and whose errors stay small after combining them. Fermi analysis is cool, and I finally appreciate them.Chapter six also deals, almost spiritually, with the misguided “quest for meaning” and the herculean discipline needed to maintain a probabilistic outlook on life:‘Even in the face of tragedy, the probabilistic thinker will say, “Yes, there was an almost infinite number of paths that events could have taken, and it was incredibly unlikely that events would take the path that ended in my child’s death. But they had to take a path and that’s the one they took. That’s all there is to it.” In Kahneman’s terms, probabilistic thinkers take the outside view toward even profoundly identity-defining events, seeing them as quasi-random draws from distributions of once-possible worlds.’Forget living Biblically for a year. Try living like this for a day.Chapter seven examines whether superforecasters are plugged into the global news streams. Answer: yes to some degree, but it doesn’t really explain their performance versus regular non-super forecasters.Chapter eight examines the twin vexing problems of updating beliefs in light of new evidence, and getting better at making predictions in light of your past predictions. Because Black Lives Matter, consider police officers:“police officers spend a lot of time figuring out who is telling the truth and who is lying, but research has found they aren’t nearly as good at it as they think they are and they tend not to get better with experience. That’s because experience isn’t enough. It must be accompanied by clear feedback. … Psychologists who test police officers’ ability to spot lies in a controlled setting find a big gap between their confidence and their skill. And that gap grows as officers become more experienced and they assume, not unreasonably, that their experience has made them better lie detectors. As a result, officers grow confident faster than they grow accurate, meaning they grow increasingly overconfident.”This chapter also talks about the discipline to keep hindsight bias in the kennel, and the difficulty in acknowledging the role of luck:“People often assume that when a decision is followed by a good outcome, the decision was good, which isn’t always true, and can be dangerous if it blinds us to the flaws in our thinking.”In a book full of actionably valuable insights (to the aspiring forecaster), this discussion might be the most helpful. Chapter nine deals with teams, team dynamics, fusing algorithms for merging individuals’ predictions, and lots of interesting related things. Chapter ten discusses how leaders might respond to a team of forecasters, and the changes leaders have to make to best utilize them.Chapter eleven talks about the problems with Tetlock’s research platform. (As he himself states earlier in the book, a scientist will always specify the conditions under which they would change their minds.)“I see Kahneman’s and Taleb’s critiques as the strongest challenges to the notion of superforecasting.”Kahneman’s critique is, can forecasters permanently tame cognitive biases, and keep churning out winning forecasts year after year (or at least long enough to be useful)? Taleb’s critique is, can forecasters say anything about black swan events that dominate history?Both of these critiques, in my personal opinion, are surmountable, making Tetlock’s research agenda and this book well worth reading.Chapter twelve closes with Tetlock’s hopes for a future world where we keep score about forecast. It could be awesome. But we’d get used to it fast and start worrying about the next problem."
297,0804136696,http://goodreads.com/user/show/347507-pietro,5,"This book is ostensibly about predicting (some aspects of) the future *reliably*. But that framing sells it far short. Anyone interested in holding true beliefs about the world could benefit immensely from taking in this short book. As far as I'm concerned, predictions are just a really good way of keeping you honest, as they have been in science for centuries: if your beliefs lead to wrong predictions often, you need new beliefs.Lots of people --- newscasters, politicians, pundits --- make claims about what the future holds. The book opens by noting that, although we listen to them raptly, nobody keeps track of how reliable they are as forecasters. With a couple compelling bits of history, it draws an analogy with medicine before the 20th century, which was largely evidence-free and doctors were judged by credentials instead of track records.One big reason nobody keeps track of forecasters' abilities is that most forecasts are vague: ""there is a risk inflation will go up"". How much of a risk? Up by how much? Over what time frame? Unless these are specified, it is impossible to tell whether a forecast was right or wrong, and thus impossible to keep track of how good a forecaster is. The importance of precision is a MAJOR theme in the book; it's the only way to know how well we're doing as forecasters, and knowing that is the only way we can hope to improve.Obviously, high-status pundits and analysts who write op-eds and appear in the news have no incentive to have their performance tracked. They're already high-status; why would they risk being debunked? So Tetlock ran a massive experiment over several years where thousands of volunteers made predictions about hundreds of precise questions such as:""In the next year, will any country withdraw from the eurozone?"" ""How many additional countries will report cases of the Ebola virus before the end of this year?""""If a non-Chinese telecom firm wins a contract to provide internet in the Shanghai Free Trade Zone in the next two years, will Chinese citizens have access to Facebook or Twitter?""Volunteers, working in isolation, made probabilistic forecasts: ""there is a 65% chance Russia will annex a part of Ukraine in the next 6 months"". In the end, each was scored based on their hundreds of forecasts (using a ""Brier score""). As it turns out, about 2% of the volunteers did SUBSTANTIALLY better not only than chance, but better than famous analysts, pundits, and even the intelligence community, which has access to privileged information. And this wasn't luck either; these ""superforecasters"" continued to do incredibly well year in and year out.Let me say that again: there are normal, unknown people out there who can predict important geopolitical events SUBSTANTIALLY better than chance; better than all the pundits who get paid shitloads of money and invited to speak at Davos and meet with presidents; better even than intelligence analysts with access to classified information.How do they do it??That's what the rest of the book is about. In short, these are pretty smart, numerate, and informed people; but they're not OFF-THE-CHARTS smart, math-savvy, hyper news junkies. They're slightly above-average people who simply cultivate healthy mental habits.Habits like ""seek out many different opinions/analyses"", ""challenge your own beliefs"", ""examine past mistakes honestly""; ""put a question in context by relating it to similar questions""; habits we can all cultivate.This truly is one of those books that make your mind more powerful, that give you a new set of eyes. Highly recommended."
298,0804136696,http://goodreads.com/user/show/51856061-bilal-hafeez,4,"The talking heads on TV give such a convincing story about what the future holds that its hard not to believe them. But pinning them down to a time-frame and discrete future event is often next to impossible, so you can never determine whether they were right or wrong. When such talking heads are pinned down, their track-record turns out to be very poor, according to Philip Tetlock in his latest book ""Superforecasting: The Art and Science of Prediction"". This makes sense as they are picked more for their entertainment value, than their track-records.The real super-forecasters are people like Doug Lorch (ex-IBM, retired, lives in Santa Barbara), Mary Simpson (independent financial consultant, formerly regulatory affairs at utility Southern California Edison) and Devyn Duffy (welfare case worker, Pittsburgh). What they lack in TV appeal they make up for in their approach to forecasting.Tetlock and team found them as part of an IARPA, which is part of US Intelligence, project to improve forecasting of political and other events. He found that the way you approach forecasting rather than your credentials or political leanings matter most. If you're a one-big idea person (i.e. a hedgehog) you will struggle, while if you're a pragmatic tinkerer (i.e. a fox) you will do well. He listed the eleven commandments for aspiring super forecasters:1.Focus on questions where your handwork will pay off. So don't bother with a question like ""who will win the US election in 2028?"". It's too far out to bother predicting.2.Break big problems into smaller ones. 3.Strike the right balance between inside and outside views. There is nothing new under the sun. So first calculate the odds of an event happening based on similar ones in the past (i.e. top-down or outside view), then work-out the bottom-up/inside view of the specific event.4.Strike the right balance between under- and overreacting to evidence. Careful incorporate new evidence and update beliefs accordingly. The best forecasters are incremental belief updaters.5.Acknowledge the counter-arguments. If you believe military action never works, be open to the possibility that it might. Take both views and synthesise.6.Distinguish as many degrees of doubt as the problem permits. Your uncertainty dials needs more than 3 settings (certain, maybe, impossible). Translate vague hunches into numeric probabilities.7.Strike the right balance between under- and over-confidence. Understand the risks of rushing to judgment and of dawdling too long near ""maybe"".8.Don't justify or excuse your failures. Conduct unflinching postmortem.9.Bring out the best in others and let others bring out the best in you. Master team management: perspective taking (being able to reproduce the others argument to their satisfaction), precision questioning (clarify arguments so there is no misunderstanding), and constructive confrontation (learning to disagree without being disagreeable).10.Master the error-balancing bicycle. Learning requires doing, with good feedback that leaves no ambiguity about whether you are succeeding.11.Don't treat commandments as commandments!You can practise your prediction skills by joining Tetlock's Good Judgment Projectbilalhafeez.com"
299,0804136696,http://goodreads.com/user/show/7876548-chirayu-batra,4,"Seems like a true companion of Dan Kahneman's Thinking:fast and slow and surely influenced by his judgement psychology. Book is a reminder that world is complex equation and forecasting anything in it is not a simple phenomenon. The book also seems a good promotional material for the good judgement project, but that makes sense. I hope to join the project and make some of my own predictions, smart predictions. A nice read for psychology lovers and people interested in reading about the art of forecasting. "
