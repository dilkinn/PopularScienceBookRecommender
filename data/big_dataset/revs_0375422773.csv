,isbn,user_link,ranking,review
0,0,http://goodreads.com/user/show/68050592-lori,3,"I loved the history. Dyson’s enthusiasm and love for the subject and scientists comes through loud and clear. It’s rich in detail on researchers with emphasis on John von Neumann.As for speculation on the future of a living machine, I think the then ninety-one-year-old Edward Teller gave sound advice.
”That seems reasonable,” I agreed. “My own personal theory is that extraterrestrial life could be here already … and how would we necessarily know? If there is life in the universe, the form of life that will prove to be most successful at propagating itself will be digital life; it will adopt a form that is independent of the local chemistry, and migrate from one place to another as an electromagnetic signal, as long as there’s a digital world—a civilization that has discovered the Universal Turing Machine—for it to colonize when it gets there. And that’s why von Neumann and you other Martians (the five Hungarian “Martians”: John von Neumann, Theodore von Kármán, Leo Szilard, and Eugene Wigner) got us to build all these computers, to create a home for this kind of life.”There was a long, drawn-out pause. “Look,” Teller finally said, lowering his voice to a raspy whisper, “may I suggest that instead of explaining this which would be hard … you write a science-fiction book about it.”“Probably someone has,” I said.“Probably,” answered Teller, “someone has not.” 
***************************************https://www.nytimes.com/2012/05/06/bo..."
1,0,http://goodreads.com/user/show/4215702-jenny-brown,2,"This book is fatally marred by Dyson's failure to understand computer architecture. I note many reviewers assuming that they are confused because they are math phobic. But I was a programmer in the late 1970s and 1980s. I wrote in Assembly language and have read machine language (in hex) when debugging, so when I read Dyson's long passages of gibberish purporting to describe what is going on in a computer I knew they were just plain gibberish. The stories about the people involved in the project were very interesting, as was the description of the environment--the IAS--and its politics, but Dyson's failure to explain the most important technical concepts to the reader (or to understand them himself) limit the books usefulness. These concepts are not all that complex. They were explained to a class of recent high school graduates in Tennessee back when I took my first computer class in 1979 and we all wrote a simplified machine language program for our final exam. So there isn't any reason that the intelligent people this book was designed for couldn't have had the architecture of a Van Neumann machine explained in more depth--in a way that would have made sense to them. I found this book saddening because it made me really want to understand the technology whose description it butchers. I'd really love to know more about how the engineers built these early computers and how they differ from the ones in use today. I gleaned many facts from the mishmash presented here, for example that CRTs were used in place of what was later called ""core"" memory in slightly later computers. But Dyson clearly doesn't understand what a register is, nor does he understand why shifting bits would do arithmetic and reading the same bits in a CPU provides the machine with its program. For that matter, he doesn't seem to understand what a program is. A simple explanation of what a computer algorithm is and why 17 instructions can lead to thousands of operations being performed would make long tracts of this book make sense, which they currently don't. But statements like one claiming that the CRT screen on a PC represented a CPU buffer were sheer imbecility. (The CPU on that computer functions as an output device, similar to the tape or punched paper on the early computers.) And there were dozens more of these uninformed statements.Bottom line: If you read this book and are confused you are getting the authors message, since he, too, was terminally confused by the technology he was attempting to describe. "
2,0,http://goodreads.com/user/show/17744555-blackoxford,4,"Knowledge To Kill ForThis is not your average paean to the pioneers of the high-tech industry. Who knew, for example, that Turing’s insight had to overcome two centuries of mathematical obsession with Newton’s (but not Leibniz's) infinitesimal calculus? And who knew that the development of the first digital computers was triggered by the military drive to create the hydrogen bomb? And who knew that the victory of binary arithmetic would be ensured by molecular biology? Certainly not me, and I suspect a number of other ignorant sods who presumed that this industry ‘just happened’, like milk suddenly appearing on the supermarket shelves with no clue about its origins in muck and mud.Dyson, a son of the manse so to speak (son of Freeman Dyson, brother of Esther Dyson, and the grandson of Sir George Dyson), can be as concise as he is illuminating: “Three technological revolutions dawned in 1953: thermonuclear weapons, stored-program computers, and the elucidation of how life stores its own instructions as strings of DNA.” When these events are considered together rather than as independent strands of modern science, it becomes clear that nothing in our lives almost 70 years later is unconnected to war and the organisation for war provoked directly by the Second World War (and indirectly by the First). The American President Eisenhower’s concerns about the ‘military-industrial complex’ were proven justified not just about the defence industry but also about a new global society built upon inherently lethal knowledge.The sources of this lethal knowledge were places like the Aberdeen Proving Grounds in Maryland, the Los Alamos compound in New Mexico, and the Institute for Advanced Research In Princeton, New Jersey. These were modern monastic establishments whose existence was justified not by prayer but by thought, largely mathematical, and not by the construction of physical edifices but the creation of weapons of destruction. These were the forerunners of what would later be known as ‘think tanks’ and ‘skunk works,’ organisational entities devoted exclusively neither to economic success nor industrial productivity but technological innovation that would facilitate mass killing.These new centres of thought were not isolated academic enclaves. They did assemble and concentrate the best intellects and coordinated their collective efforts in highly abstruse areas. But they also set agendas for university (and even high school) scientific education, successfully lobbied government about the priorities for military research spending, and shaped the interests of the most important private foundations that funded research from medicine to astrophysics. Because they had no factories, no significant labour force, and no immediately commercial products, these establishments engaged in a sort of parallel politics. Although they were the driving force of the new military industrial complex, they were functionally invisible, in part because their work was confidential, but mostly because no one outside them could really understand what they were up to. They effectively constituted an independent empire of the mind, a Platonic haven of pure rationality, or at least what military requirements implied as rationality.Most of the men (and they were almost all men) recruited into these establishments as thinkers or administrators were undoubtedly exceptionally clever in their respective fields. However, it is clear from the personal and institutional biographical detail which Dyson provides that very few of them would have achieved their ‘potential’ without this new form of scientific organisation. It is likely that they would have spent their lives in interesting but inconclusive research in dispersed academic institutions, or teaching Latin to high school seniors. The legendary names - Shannon, von Neumann, Ashby, Wiener, Mandelbrot, etc - would probably have been known but not with anything like the cultural force that they now have. These new organisations were intellectual king-makers.So these military/intellectual enterprises, dedicated to refining the efficiency of human conflict, have transformed scientific culture. The concentration of intellectual talent, money and professional dominance means that there is only one path to scientific innovation - national defence, however widely that might be defined. Subsequent commercialisation, organised on similar lines in the Silicon Valleys and University Science Parks of the world, are functional subsidiaries of an invisible network, which few of us know anything about except when some ‘breakthrough’ (or breakdown) is announced in Wired or featured in Fast Company. My lifetime is almost exactly contemporaneous with the digital epoch (Von Neumann died on my 10th birthday; Steve Jobs had just turned 2; Gates had just begun to walk). The presumptions, intentions, and fallacies of this epoch are things I share intellectually and emotionally with my generational cohort. This is Turing’s Cathedral, a cultural state of mind rather than a physical edifice. It took substantially less time to build than its medieval version. But its cultural influence is at least as great. Whether it will maintain itself as durably or with continued centrality is an open question, the answer to which seems to depend upon our fundamental but repressed attitude toward the god of war."
3,0,http://goodreads.com/user/show/4213258-bradley,4,"I had an issue with this non-fiction, but also a whole lot of love. So this is about the mathematicians who heralded the whole computer movement. You know, the OTHER, more disreputable and crazy smart people like Von Neumann, Gödel, and all the other nutters like Turing who ushered in the computer age from just a thought experiment into a hand-made lab and later into the co-authors of the nuclear age.Yeah. THOSE crazy nutters. The ones that ran enough physics programs on their automatic machines to model nuclear explosions and bring about the bomb. Computers, and not the poor women (and a few men) who got paid to crunch math by hand for years, are the real reason we have the nuclear age. And also why we have genetic sciences.Pretty obvious, I know, but still, these guys are some unsung heroes. Just programmers. Sheesh. Whatever.The book is full of love. I love the people. And then there was a wholly appropriate section expounding on science fiction and the future of AIs and I LOVED that, too, especially the form a realistic alien might take.So what issues did I have?WAY too much time was spent on the schools. Early schools, history, blah blah blah. Sure. Colleges are important and such, but I lost my caring factor until a while after we were introduced to Von Neumann. And what an interesting guy he was! :)A side issue I should have more problem with is the role of women in this non-fiction, but like real history, too much idiocy prevents half our population from having more active roles. I'm not too fond of how the women here were relegated to being facilitators, suicidal wives, or footnotes to Crick and Watson. But let's be real here. We have a horrible track record at pushing these people aside in reality, not just in history. I can appreciate the minds SHOWN HERE while still wishing the other minds had a chance. It didn't diminish my fascination. I can have MORE fascination to spare elsewhere. :)So. Maybe not the best non-fiction I've ever read, but I did learn a hell of a lot about the people who ushered in the computer age and it's quite a story. And honestly, it makes for a more realistic story than the others I've read that focused more on WWII encryption engines as the real focus and impetus for computers. Making nukes is pretty damn huge. And obvious. :)"
4,0,http://goodreads.com/user/show/913238-justin,2,"	I might have easily given this book four stars if Dyson could have stuck to history instead of indulging himself in inane speculations, and commentaries that are sadly meant to sound profound. The connections he draws between completely unrelated aspects of technology and biology are so strained that whenever I read a particularly grievous one, I'm forced to put the book down and walk around the room until the waves of stupidity subside a bit. For example, at one point Dyson asks us to consider whether digital computers might be ""optimizing our genetic code ... so we can better assist them."" At another he explains the reason we can't predict the evolution of the digital universe is because algorithms that predicted airplane movements in WW2 had to be normalized to the reference frame of the target... or something? Throughout the entire book there's a complete disconnect between the technical nature of the things he describes and the vague abstractions that he twists into obscenely trite metaphors.	Dyson seems to live in some sort of science-fiction wonderland where every computer program is a kind of non-organic organism. He calls code ""symbiotic associations of self-reproducing numbers"" that ""evolved into collector societies, bringing memory allocations and other resources back to the collective nest."" They are active, autonomous entities which ""learned how to divide into packets, traverse the network, correct any errors suffered along the way, and reassemble themselves at the other end."" By the end of the book I'm not even sure if Dyson means this as a metaphor - he appears to genuinely believe that it's merely a matter of perspective.		The truth is, if every human died tomorrow and the internet was left to run from now to infinity, not a single advance would be made in the state of computing. The viruses would quickly burn themselves away, the servers would grind monotonously at their maintenance routines, and the Google webcrawlers would stoically trudge through every porn site on Earth, an infinite number of times.	Dyson might respond that programs integrate humans as a symbiotic part of their evolution, but in that case you could say the same thing about clothing, music, or furniture. In this light the IKEA franchise must be viewed as a great self-replicating organism, conscripting humans in the propagation of its global hegemony of coffee tables."
5,0,http://goodreads.com/user/show/53165-warwick,3,"A fascinating and illuminating book, but also a frustrating one because it should have been a lot better than it is.The heart of the story is more or less on target – a collection of very interesting anecdotes and narratives about the personalities involved in building America's first computer, at Princeton's Institute for Advanced Study after the Second World War. Leading the team was the quite extraordinary figure of John von Neumann, about whom I knew rather little before reading this. He comes across as by far the most brilliant mind in these pages (not excluding the presence of one A. Einstein), with a near-eidetic memory, an ability to understand new concepts instantly and make staggering leaps of reasoning to advance them further. Not a very endearing character, though – a refugee from 1930s Europe, he pushed the nuclear programme hard and argued to the end of his life that the best way to create world peace was to launch a full ‘preventive’ hydrogen bomb strike against the USSR, mass civilian deaths and all.The nuclear project was central to the invention of the computer. The first incarnation of the machine (the ‘ENIAC’, later nicknamed ‘MANIAC’) was developed specifically to model fission reactions, which involve some rather tricky maths. But von Neumann and other thinkers realised early on that a machine capable of doing that would also be able to fulfil Alan Turing's description of a ‘universal computer’: if it could do the arithmetic, it turned out, it could do practically anything else too, provided there was a way of feeding it instructions. ‘It is an irony of fate,’ observes Françoise Ulam, ‘that much of the hi-tech world we live in today, the conquest of space, the extraordinary advances in biology and medicine, were spurred on by one man's monomania and the need to develop electronic computers to calculate whether an H-bomb could be built or not.’What is particularly fascinating is how these studies gradually let to a blurring of the distinction between life and technology. The development of computing coincided with the discovery of DNA, which showed that life is essentially built from a digital code (Nils Barricelli described strings of DNA as ‘molecule-shaped numbers’), and early programmers soon discovered that lines of code in replicating systems would display survival-of-the-fittest type phenomena. This is entering a new era with the advent of cloud-sourcing and other systems by which computing is, in effect, becoming analog and statistics-based again – search engines are a fair example.How can this be intelligence, since we are just throwing statistical, probabilistic horsepower at the problem, and seeing what sticks, without any underlying understanding? There's no model. And how does a brain do it? With a model? These are not models of intelligent processes. They are intelligent processes.All of this is very bracing and very interesting. Unfortunately the book is let down by a couple of major problems. The first is a compulsion to give what the author presumably thinks is ‘historical colour’ every two minutes – so the IAS is introduced by way of an entire chapter tracing the history of local Algonquin tribes, Dutch traders, William Penn and the Quakers, the War of Independence – all of which is at best totally irrelevant and at worst a fatal distraction.The second, even more serious failing is that the technology involved remains extremely opaque. One of the most crucial parts of this story should be following the developments of cathode-ray storage, early transistors, the first moves from machine language to codes and programs. But the explanations in here are poor or non-existant. Terms like ‘shift register’, ‘stored-program computer’, ‘pulse-frequency-coded’, are thrown around as though we should all be familiar with them.My favourite story to do with the invention of the digital world involves Claude Shannon and his remarkable realisation – one of the most civilisation-altering ideas of our species – that electronic transistors could work as logic gates. It's not even mentioned in this book. And so the crucial early building blocks of what a computer actually is – how, on a fundamental level, it really does what it does – that is all missing. And it's a pretty serious omission for someone that finds it necessary to go back to the Civil War every couple of chapters.A lot of reviews here, especially from more technical experts, really hate this book, but on balance, I'd still recommend it. It's a very important story about a very important period, and the later chapters especially have a lot of great material. But reading around the text will probably be necessary, and this book should have offered a complete package."
6,0,http://goodreads.com/user/show/155663-david,5,"Despite the title, this book is not primarily about Alan Turing. It is really about the group of people at the Institute of Advanced Studies at Princeton. Much of the book focuses on John von Neumann, who spearheaded the effort to build some of the earliest electronic computers. These first computers were very unreliable--incorrect results were as likely due to faulty vacuum tubes as coding errors. In fact, circuits had to be designed to be robust to vacuum tubes that did not follow specs.Quite a large chunk of the book--and the most fascinating--dealt with the types of mathematical and physical problems that the earliest computers could solve. In fact, that was the principal interest of von Neumann--learning what types of problems could be solved using computers. Here, Alan Turing and Kurt Godel played a large role in defining what sorts of problems might be solvable.Among the problems that the earliest computers attacked was weather forecasting. In the late 1940's, there was much controversy about the feasibility of numerically computing forecasts in principle. Of course, weathermen wanted to continue to use their gut feelings to forecast the weather, while some scientists thought that, given sufficient spatial resolution, the weather could be forecast far in advance. It was not until later that people like Lorenz discovered that there are fundamental limits to how far in advance weather can be forecast. The earliest computers were also used for developing the atomic bomb. Many aspects of the physics were not solvable using direct means. Simulations using a brand new numerical method called ""Monte Carlo"" were extremely significant for solving them. For this method to work, random numbers are required for initiating independent simulated trajectories. But random numbers were not easy to come by, so a special program helped to develop algorithms for computing them.This book goes into considerable depth, in describing the people who developed and used the first computers at the institute. There are fascinating descriptions of the mathematical, physics, and biological puzzles that were attempted. I recommend the book highly, for those interested in the history of numerical computation."
7,0,http://goodreads.com/user/show/1711431-eric-w,5,"If you are looking for information about Alan Turing, look elsewhere. The title is a metaphor.The Nazis did the U.S. a huge favor with their boorish and stupid racial policies. Many prominent Jews were brilliant mathematicians and physicists, and when the “cleansing” of universities began by the Nazis, people like Van Neumann, Einstein, and many others fled to the United States where they were of immense assistance in the development of the atomic bomb.This book is about the origins and development of the digital age and Dyson spends considerable space on the people and institutions key to that development. The Princeton Institute for Advanced Research, for example, under Abraham Flexner and Oswald Veblen, recruited many of these refugees who helped build the Institute into one of the premier research institutions. I suppose it all has special interest for me as my life span parallels the development of the computer. I was born in 1947. In the 7th grade I became fascinated by ham radio and electrons and studied the intricate workings of the vacuum tube, a device for which I still have some reverence. I’m still dismantling and messing with the insides of computers.Ironically, given the book’s title, John Van Neumann takes center stage with Turing playing only a peripheral role. Van Neumann’s interest in digital computation was apparently sparked by reading Turing’s seminal article. “On Computational Numbers” that led him to the realization of the importance of stored program processing.What Turing did that was so crucial was to take Gödel’s proof of the incompleteness theorem that permitted numbers to carry two meanings. Turing took that and thought up the paper tape computer that produced both data and code simultaneously. That realization alone was fundamental in providing the basic building block for the computer.The builders had conflicting views of the incredible computational power they had unleashed that was to be used for both ill and good. Van Neumann recognized this: “ A tidal wave of computational power was about to break and inundate everything in science and much elsewhere, and things would never be the same.”It would have been impossible to develop the atomic bomb without the computational abilities of the new “computers.” So naturally, the Manhattan Project is covered along with the influence of the evil Dr. Teller (I must remember to get his biography,) who was the character (Dr. Strangelove) brilliantly played by Peter Sellers. After the war, Teller pushed very hard for the development of the “super-bomb” even though he knew, or must have known, that his initial calculations were flawed because he didn’t have the computational power to do them completely. One number that I questioned was the Dyson’s reporting that when the Russians exploded a three-stage hydrogen bomb in 1961, the force released was equivalent to 1% of the sun’s power. That sounds wildly improbable. Anyone able to contradict number?Some interesting little tidbits. One computational scientist refused to use the new VDTs, preferring to stick with punched cards (he obviously never dropped a box of them) which seemed far more tangible to him than dots on a screen. I guess fear of new technology is not reserved for non-scientists. One of the major and very interesting questions addressed by Turing and reported on in the book is what we now call artificial intelligence. When we use a search engine are we learning from the search engine? or is the search engine learning from us? It would appear currently the latter may be true. Clearly, the search engines have been designed to store information and use that information to learn things about us both as a group and individually. I suspect that programs now make decisions based on that accumulation of knowledge. Is that not one definition of intelligence? (I will again highly recommend a book written and read quite a while ago that foresaw many of these issues: The Adolescence of P-1 by Thomas Ryan (1977)** . Note that Turing talked about the adolescence of computers and likening them to children.)Some reviewers have taken Dyson to task for emphasizing abstract reasoning that went into the development of the computer while downplaying the role of electrical engineers (Eckert and Mauchly) in actually building the things. I’ll leave that argument to others, not caring a whit for who should get the credit and being in awe of both parties. On the other hand, the book does dwell more on the personalities than the intricacies of computing. There are some fascinating digressions, however, such as the examination of digital vs analog and how the future of computing might have been altered had Vann Neumann not tragically died so young as he had a great interest in biological computing and the relationship of the brain to the computer.**For a plot summary of The Adolescence of P-1 see https://en.wikipedia.org/wiki/The_Ado..."
8,0,http://goodreads.com/user/show/4696922-ushan,2,"This book covers essentially the same material as William Aspray's 1990 John von Neumann and the Origin of Modern Computing, the life and times of John von Neumann and the IAS computer. Aspray's book is much more to the point, though, while Dyson's takes large detours into the history of the atomic and hydrogen bomb, World War II cryptography and the like - all these topics have better books dedicated to them. George Dyson has a personal connection to the Institute of Advanced Study because his father, famous physicist and mathematician Freeman Dyson, worked there in the 1950s, when the IAS computer was being constructed and operated, and George was a child. Dyson Jr. had the run of the Institute's archives but he did not fish any radically new discoveries out of them.As in Dyson's books about Project Orion and the evolution of artificial intelligence, I wish that someone better educated had written on this topic. For example, Dyson mentions the von Neumann architecture without clearly defining, what it is. The ENIAC and the Colossus were early non-von Neumann-architecture computers whose programs were input using a plugboard and switches; a modern equivalent is the FPGA. A better book would make this distinction more explicit. Dyson also talks about digital versus analog computation. A digital computation represents different digits of a mathematical variable with separate physical variables, and an analog one represents them with different digits of the same physical variable; an abacus is digital and a slide rule is analog. It is not true that search engines and social networks are enormous analog computers! They are not fault tolerant: a single bug in the implementation of the core algorithms can wreck them both!"
9,0,http://goodreads.com/user/show/53179837-george,3,"This book is almost a biography of John von Neumann, almost a history of the MANIAC computer, almost the story of the beginning of the Computer Science field. All of these topics are connected but it's edited in such a way that it seems like 3 different books collided and were glued together with a lot of unnecessary detours. And that is a real shame, there is a lot of good info in here.I can still recommend this book to other people, but I will have to warn them of these frustrations. "
10,0,http://goodreads.com/user/show/8101737-peter-tillman,3,"An interesting and discursive early history of the electronic digital computer, around and after WW2, with a history of the Princeton NJ area back to colonial days, and the history of the founding of the Institute for Advanced Study there. The local history stuff you can safely skim or skip, the IAS stuff is also peripheral (but interesting). But when Dyson gets to John von Neumann's biography (chapter 4), the pace picks up, and picks up even more when he gets to the computer history. Von Neumann was a genius, and things move quickly in wartime. I love reading about bright engineers at work. Many of the parts for the first IAS computer were war-surplus and/or picked for easy troubleshooting. The academic mathematicians hated the engineers, and succeeded in getting the IAS program shut down in the 1950s. Too bad.There are more more detours to come, but Dyson is a fine historian and did his homework. At his best, this is 5-star writing -- from hydrogen bombs to stellar evolution! -- but the odd structure and detours were distracting. 3.5 stars, rounded down for that. For real reviews, I recommend Alan Scott's https://www.goodreads.com/review/show... and John Gribbin's https://www.goodreads.com/review/show... I expected to like this one more than I did, for the reasons Alan (and others) spell out."
11,0,http://goodreads.com/user/show/11951948-paul,2,"Three quarters of a century ago a small number of men and women gathered in Princeton, New Jersey. Under the direction of John von Neumann they were to begin building one of the world’s first computers driven by the vison that Alan Turing had of a Universal machine. Using cutting edge technology, valves and vacuum tubes to store the data, the first computer was born. This unit took 19.5kW to work and had a memory size of five, yes five kilobytes. It caused a number of revolutions, it was this machine that laid the foundations for every single computing device that exists on the planet today, it changed the way that we think about numbers and what they could do for us and the calculations that it ran gave us the hydrogen bomb…I had picked this up mostly because of the title, Turing's Cathedral, thinking that it would be about that great man, the way that he thought and the legacy that he left us with regards to computing and cryptography. There was some of the on Turing and his collaboration with the American computer scientists and engineers through the war, but the main focus was on the development of the computer in America and the characters that were involved in the foundation of today’s technological society. Some parts were fascinating, but it could be quite tedious at times. There were lots and lots of detail in the book, the characters and political games that they were playing and subject to, not completely sure why we needed to go so far back in time on the origins of Princeton. Definitely one for the computer geek, not for the general reader. "
12,0,http://goodreads.com/user/show/7247043-jodip,1,"This book started off rather confusingly--without a clear description of what it was to be about. It did not improve. For some reaosn,the author thought it very important to tell how Princeton was founded and had a lengthy chapter on William Penn from the 1600s. I thought it might be the format--I was reading on Kindle, and entertained the idea of getting the hard copy to flip through irrelevant sections. I then checked reviews and decided to give it a pass all together. Other folks have found it rambling as well. I didn't finish it."
13,0,http://goodreads.com/user/show/5671213-thore-husfeldt,3,"The history of the universal electronic computer at the Institute of Advanced Studies in Princeton, pioneered by the leading genius of his time, John von Neumann, and driven largely by the computational requirements of building a nuclear bomb, makes for a good book. George Dyson’s Turing’s Cathedral is not that book.At his best, Dyson writes compelling, erudite, witty, and idiosyncratic prose with a gift for poetic analogies and elegant turns of phrase. The opening of chapter XVII, on the vast computational power we have today and in the future, is a good example:Von Neumann made a deal with “the other party” in 1946. The scientists would get the computers, and the military would get their bombs. This seems to have turned out well enough so far, because, contrary to von Neumann’s expectations, it was the computers that exploded, not the bombs.Alas, these morsels are thinly spread. Worse, many of them are abject nonsense. Dyson seems to have no internal error correction mechanisms that shield him from stretching his analogies far beyond what their flimsy fabric can endure. He combines this infatuation with his own literacy with digital, mathematical, and technological illiteracy: a cavalier attitude towards the details of the technology that he aims to describe and reason about. Search engines and social networks are analog computes of unprecedented scale. Information is being encoded (and operated upon) as continuous (and noise-tolerant) variables such as frequencies (of connection or occurrence) and the topology of what connects where, with location being increasingly defined by a fault-tolerant template rather than by an unforgiving numerical address. Pulse-frequency coding for the Internet is one way to describe the working architecture of a search engine, and PageRank for neurons is one way to describe the working architecture of the brain. These computational structures use digital components, but the analog computing being performed by the system as a whole exceeds the complexity of the digital code on which it runs. [p. 280](The chapter ends with a bold one-liner, “Analog is back, and here to stay.”) It’s unfair to cherry-pick particularly egregious passages from a book about a complicated subject, but the book is full of stuff like that. With the first few instances I tried to adopt a charitable stance and wanted to understand Dyson is trying to tell me, behind the noise of half-understood technical inaccuracies. But after a few chapters I just became annoyed. Generously put, the technical passages of this book are inspiring, in the sense that I was inspired to actually find out how the ENIAC and other machines worked. Using other sources, such as Wikipedia, because Dyson’s book does very little to tell me. Dyson is clearly out of his depth here, and his confused and confusing descriptions read like the account of a blind man explaining the concept of colour. The result is dense, conceited, and just plain annoying. As Goodreads reviewer Jenny Brown puts it, “this book is fatally marred by Dyson’s failure to understand computer architecture.” Other reviewers of this book, both professional and amateur, seem to be appropriately humbled and impressed by the opaque technology, and write off their confusion to their own cognitive inadequacy. Here’s an example from judy’s review: “I stand in awe of the geniuses who envisioned and constructed the digital universe—largely because I haven't a prayer of understanding what they did. Although written in plain English, somehow my brain will simply not grasp the concepts.” Well, neither does Dyson’s.Our message should be that computers are simple. Instead, we get yet another book that makes technology into magic, and its inventors into Byronic heroes.Which leads us to the biographical sketches. The book gives us a rich anecdotes about many of the leading figures associated with Princeton’s computer group in the 40s and 50s. Bigelow, Ulam, Gödel, — all secondary to the book’s main character, the titanic John von Neumann. Many of these descriptions are entertaining and insightful, it’s clearly the best part of the book. Dyson tells much of the story in the voice of others, by quoting at length from interviews and biographies. This works well. However, even these sketches remain disjointed, erratic, meandering, and quirky. Dyson clearly has had access to unique sources at Princeton’s Institute of Advanced Studies, which make for the most interesting parts of the book. Examples include endlessly entertaining log messages from ENIAC programmers. On the other hand, I really don’t need to know that Baricelli had his $1,800 stipend was renewed in 1954. These random facts are many, obviously motivated by the author’s access to their sources, and never play a role in the greater narrative.Even Alan Turing, after whom Dyson’s book is named, makes an appearance in chapter XIII. Otherwise the book has nothing to do with Turing, and little to do with universal computing, so the book’s title remains a mystery. Von Neumann’s Cathedral would have been a fine title for this book, or better Von Neumann’s Bomb Bay. This is a book about von Neumann, not Turing, and his monomaniacal dream of building a computer. Von Neumann’s motivation was mainly militaristic: to leverage computational power for simulation of complex phenomena, such as the physics of nuclear bombs. As such, the early history of computing co-evolves at the ENIAC project in Princeton and the Manhattan project in Los Alamos. This is a story worth telling, and from that perspective, Dyson’s book is a book worth reading. Just remember to read the technological passages like the techno-babble in a Star Trek episode: It’s window-dressing, not there in order to be understood. The final prophetic chapters about the future of computation and mankind are worthless and incompetent.In summary, a misleadingly-titled, meandering, technologically illiterate, annoying, beautifully written, confused, and sometimes entertaining book about an important topic."
14,0,http://goodreads.com/user/show/679134-bryan-alexander,4,"I'm fascinating by the history of computing. There are so many delights there, both geeky and otherwise: glimpses of our present, odd characters, brilliant technical solutions, politics. Turing's Cathedral is a delightful and useful contribution to this field.George Dyson's book takes place during the 1940s and 1950s, focusing on the extraordinary collection of geniuses in Princeton's Institute for Advanced Study, and who created a huge amount of modern computing. A large part of Turing's Cathedral consists of biographical sketches of key players in early computing, including Stan Ulam, Nils Barracelli, Klari Dán Von Neumann, Thorstein Veblen's nephew, Alan Turing, of course, and especially Johnny von Neumann. This is really von Neumann's book. He gets the lion's share of the text (and photos), becoming the Institute's prime driver and most productive thinker. Turing's Cathedral essentially ends with von Neumann's death.Other chapters explore the hardware and theory of early computing, from cathode ray tubes and punch cards to stored program architecture and the Monte Carlo method. Dyson also races off in related directions with something like glee, showing, for example, how the IIS was right on top of major Revolutionary War sites, and how one scientist was related to Olivia Newtown John. Arching across all of these themes is the intertwined history of atomic weapons and computing. This is a controversial theme in cyber-history, and Dyson assembles a great deal of evidence to demonstrate their deep connections.Turing's Cathedral is filled with energy and some fun writing. A few quotes demonstrate this:What if the price of machines that think is people who don't? (314)""Information was never 'volatile' in transit; it was as secure as an acrophobic inchworm on the crest of a sequoia."" -Julian Bigelow (137)Our ever-expanding digital universe is directly descended from the image tube that imploded in the back seat of [Vladimir] Zworykin's car. (81)We owe the existence of high-speed digital computers to pilots who preferred to be shot down intentionally by their enemies rather than accidentally by their friends. (116)Von Neumann made a deal with ""the other party"" in 1946. The scientists would get the computers, and the military would get the bombs. This seems to have turned out well enough so far, because, contrary to von Neumann's expectations, it was the computers that exploded, not the bombs. (303)""It is an irony of fate that much of the high-tech world we live in today, the conquest of space, the extraordinary advances in biology and medicine, were spurred on by one man's monomania and the need to develop electronic computers to calculate whether an H-bomb could be built or not."" -Francoise Ulam (216)""A clock keeps track of time. A modern general purpose computer keeps track of events."" This distinction separates the digital universe from our universe, and is one of the few distinctions left. (300)""We are Martians who have come to Earth to change everything - and we are afraid we will not be so well received. So we try to keep it a secret, try to appear as Americans... but that we could not do, because of our accent. So we settled in a country nobody ever has heard about and now we are claiming to be Hungarians."" -Edward Teller (40)The book has plenty of gems, like the Swedish scientist Hannes Alfvén and his utopian novel about an AI-ruled future, von Neumann's possibly accidental vision of what post-robotic economics might look like (289), and Nils Barracelli's early visions of artificial life. The sheer ambition, nigh unto mad science, of these early explorers, seeking even to control the weather, is infectious.There are a few weaknesses, starting with Dyson's aphoristic tendency occasionally backfiring (I don't buy the return of analog idea). Some of the explanations aren't as lucid as they need to be - I'm still not sure what template-based addressing means. And the focus on Princeton and some historical figures doesn't recognize their extraordinary privilege. I was also surprised that Tyson didn't reference Howard Rheingold's groundbreaking and accessible Tools for Thought. But that's nitpicking. Turing's Cathedral is a treat for geeks, history buffs, and anyone interested in how our digital era came to be."
15,0,http://goodreads.com/user/show/11004626-gwern,4,"Mixed feelings. On the one hand, Dyson digs up all sorts of quotable lines and anecdotes and biographical details, many genuinely new to me. I enjoyed those greatly. For these I give it 4 stars. On the other hand...He is obsessed with Von Neumann's IAS/MANIAC, to the detriment of the rest of the book. The pre-WWII history is OK but signally fails to explain things like the Hilbert program, Goedel or Turing's actual halting theorem. Someone who read this expecting to understand 'Turing's cathedral' would be vastly better served reading a book like Hofstadter's _Goedel, Escher, Bach_ (as old as it is). Instead, countless pages are taken up with detailed technical information that is simultaneously in depth and also poorly explained. I repeatedly got the feeling that Dyson is indulging in that common temptation, allocating material based on how much effort it took to find, not what would inform the reader - he went through a lot of work documenting MANIAC and the rest of us must enjoy (suffer) the fruits of it. I felt that if I didn't already know a great deal of this material, I would be completely lost inside the book; I wonder how much other people could get out of it.The repeated analogies to search engines and modern computing come off very poorly (search engines are analogue? Oookkaayyy....); much could have been said about how modern chip architectures and cloud computing designs are not very Von Neumannian now, so here again I wonder if it's a forced attempt to show contemporary relevance or perhaps just influence from his Google visit.Other parts make one question how much Dyson understands: he links Goedelian/Turing incompleteness to computer viruses and concludes with grand '90s-esque visions (pace Kevin Kelly's old _Out of Control_ book) of viruses spreading out through the Internet and beating on the walls of clean computers - but viruses aren't really a problem these days, nothing like they used to be, and the situation seems apt to only improve! Like spam, the solutions are not perfect and require a great deal of manpower & cleverness, but they are working and currently seem likely to steadily improve; this wouldn't be a surprise to him if he had really appreciated that Goedelian/Turing-incompleteness implies that there *are* large decidable subsets of programs and we can build our systems out of those. (Every programmer who uses a language with a decent type system is doing something a naive understanding of incompleteness says is impossible: he's executing nontrivial predicates over his program.)For those reasons and others, this will never get 5 stars from me, and if there were a 3.5 stars, I'd go with that."
16,0,http://goodreads.com/user/show/17435739-greg,3,"The title is a little misleading. This book is mostly a biography of John von Neumann and concurrently, a story of the early decades of the Institute for Advanced Study in Princeton. The stories are well researched and rich in detail, but at times hard to follow. I think this comes from abrupt changes in the timeline within related chapters. What comes across clearly is the value of interdisciplinary collaboration among genius level scientists and engineers in the presence of new electronic tools. von Neumann is a perfectly chosen example in that regard. I'm also grateful to the author for including and highlighting valued wives, technologists, secretaries, etc. who were important either to the groundbreaking work itself or to the stability of the community, without which progress would undoubtedly have suffered. This is actually more than a three star effort, but not quite four stars. Thank you Mr. Dyson for this enlightening work. And thank you to Powell's (Portland, OR) for carrying it on your shelves. "
17,0,http://goodreads.com/user/show/1153204-alan,3,"Turing's Cathedral is a long, enthusiastic and articulate ramble throughout the early history of computing, a solid work constructed over a great deal of time by a keen observer who has an insider's perspective on many of that history's most pivotal moments. George Dyson is the son of the famous physicist Freeman Dyson, and as a child he must have met many of the principals of this story while they were working at the Institute for Advanced Study (IAS) in Princeton, New Jersey (although at the time his historical interests were, I'm sure, not yet fully developed). Dyson's chapter of Acknowledgements tells the tale of the many surviving participants he interviewed and the many records the archivists of the IAS unearthed for him during the creation of this book; he notes that ""Julian Bigelow and his colleagues designed and built the new computer in less time than it took me to write this book."" (p.xvi)But... Turing's Cathedral isn't going to be for everyone. It is quite a ramble, for one thing—there's definitely a forward, linear flow to Dyson's overall narrative, but it's full of eddies and backwaters, digressions and diversions. It's also a very thorough story about math and machines, and the military imperatives—like ballistics computations, code-breaking, and the simulation of blast effects for new kinds of explosives—that drove the transition of ""computers"" from being a job description for women who operate adding machines in rooms to describing self-contained electronic devices that filled rooms. You should already have at least some familiarity with computer architecture and design, in order to appreciate the raw roots of the technology that are exposed here—vacuum tubes and wires, to be sure, but more importantly their underlying logic: deep concepts, like memory addresses, Boolean arithmetic and self-modifying code, the very notion of ""software"" itself, that still make up the mostly-unseen bedrock of today's graphically-oriented operating systems.And... I'll admit to feeling a bit misled, at least to start with—despite its name, Turing's Cathedral's early chapters seem not so much about Alan Turing himself as they are about John von Neumann and the IAS... in fact, George Dyson sends his gaze back all the way to William Penn's acquisition of the land that later became the Institute, before slowly moving forward into the 20th Century.It is not until Chapter 13 that Dyson digs into what Alan Turing really meant to the history (and future) of computing. In this chapter, Dyson takes us from the initial, purely mathematical insights of Turing's Universal Machine, through Turing's more concrete notion that an artificial intelligence must be grown, not made (a sentiment with which I find myself wholly in agreement)... to Dyson's own final observation that our search engines are now searching us. This chapter is the core of the book, the part that most fully justifies its name.After Dyson is done, for the most part, with history, he goes on to engage in some intriguing speculations, such as his musing (on pp.278-281) about the resurgence of analog, continuous modes of computing, and what that might mean for the segmented digital universe he spent so much time explaining in the earlier parts of the book. Along the way he also has a couple of interesting book recommendations; I'm going to have to look up Aldous Huxley's lesser-known post-apocalyptic work Ape and Essence sometime, for example. And... I'll admit I'd never seen this statistic put this way before:Global production of optical fiber reached Mach 20 (15,000 miles per hour) in 2011, barely keeping up with the demand.—p.300Computers are ubiquitous now, and intimately interconnected... but there was a time not so long ago when they weren't. George Dyson's Turing's Cathedral is an instructive and often entertaining examination of the moment just before that amazing phase transition took place."
18,0,http://goodreads.com/user/show/4763049-holly,4,"Though I would never dare to participate in anything but a cursory, general conversation about Turing machines or Gödel's theorem or the Monte Carlo method, I just love histories of science and this book made me happy. I listened to the audio version, and often arcane concepts requiring visualization or anything involving equations would blow past me, but all the wonderful details and biographies and momentum more than made up for my muddled moments. I love that it started with a disorienting-orienting historical story about the ground on which the Princeton Institute for Advanced Study was built (with details about the Lenni Lenape Indians and then William Penn!?), then the Flexner family, and Thorstein Veblen and his brother .... who'd have thought? and weather predicting, and Los Alamos, and Einstein, and the roles of women - such as Clari von Neumann - whom I'd never read about, etc. It's all here. I also liked the book's spiral structure that gradually moved linearly forward but always, by necessity, returned to von Neumann, the true center of the book. (Like every other reader I feel obligated to mention that the eponymous Turing had very little screen-time.) "
19,0,http://goodreads.com/user/show/16404987-john-behle,4,"In several reviews, this book has been called a nerd's labor of love. Okay, but it is also exceptionally well written. The sentences are crafted to keep pulling one in to the action. This is not a direct timeline book, though. Dyson introduces the players as they enter the drama of advancing computing. It is not bog down with old techno speak and specifications. Dyson sprinkles in the interesting facts just as needed. The massive 30 ton computers of the late 1940s did have over 17,000 vacuum tubes. Yes, there were programs with one million punch cards, with run times measured in weeks.But the very smart people who wrote the programs and built those machines had fun, foibles and flaws. Dyson is quite the raconteur and weaves these personalities in to tell his tale. He wraps with how so many of the visions of '40s and '50s have indeed become the way we share information. The ease of access and the freedom to contribute to a worldwide forum like goodreads is a stunning example of how far we have come. Arthur Morey is a velvet hammer of a narrator. His distinct style and precise pronunciation added to the enjoyment of this book for me."
20,0,http://goodreads.com/user/show/4989865-tom-lee,4,"I keep this photo over my desk at work. I think it looks a bit like a microscopic close-up of a drop of milk, or maybe a bacterial colony. In fact it's a shot of the Trinity Test, the planet's first atomic detonation. To me, this event and the context surrounding it are the most fascinating and amazing chapter in all of human engineering: in a panicked fight against evil, a collection of human intelligence was assembled that, through sheer intellectual might, wielded abstract mathematics and applied engineering to bend reality in an astounding new way. It's hard to imagine an engineering problem with such dizzying historical, moral and political consequences.George Dyson has delivered a fascinating and flawed book that connects this project to my own profession: digital computing. He tells the story of Princeton's Institute for Advanced Study and its quest, led by the brilliant John Von Neumann, to build one of the world's first electronic computers, a project that was birthed by -- but, Dyson argues, destined to be even more transformative than -- the US military's atomic weapons program.Dyson is the son of Freeman Dyson, and this is his greatest asset: he actually grew up among these brilliant minds. This puts him in an incredible position to describe what these men and women were like, and he does a very fine job. Von Neumann's own reticent, complicated brilliance leaves him something of an enigma, but Dyson conveys this well. And, although generally shorter, the portraits Dyson draws of Julian Bigelow, Alan Turing, Kurt Godel, Stan Ulam and Klari Von Neumann -- all of whom (astoundingly) were personally involved in this story in one way or another -- are fascinating, inspiring and heartbreaking.But this book has problems. I'll start with a quibble. The beginning is pretentiously overloaded: it's hard to imagine why a reader would want or need an explanation of William Penn's immigration and the events that led to George Washington once marching through what would become the IAS's backyard. But the real sins occur later in the book.The book itself diagnoses Dyson's failings via a quote from Turing on page 263: ""I agree with you about 'thinking in analogies,' but I do not think of the brain as 'searching for analogies' so much as having analogies forced upon it by its own limitations."" Dyson doesn't take this limitation seriously. Having ably and charmingly described the creation of general-purpose digital computing, Dyson is incapable of critically evaluating the musings of its creators. Having built the a-bomb, these inventors -- understandably -- could be similarly oblivious as they applied computational metaphors to problems of biology and the mind. These can be helpful conceptual frames, but Dyson is not equipped to see their limitations, or to acknowledge the modest returns they have yielded over a subsequent half-century of investigation. The stories he tells are instead about avenues of investigation cut short by untimely deaths, the military industrial complex or short-sighted academic administrators. He doesn't acknowledge that Barricelli's ideas about evolutionary algorithms, for instance, have continued to be studied, and have become a useful but far-from-universal (or life-creating!) technique.By the end, Dyson has descended into mysticism. He misreads Turing's discussion of o-machines as a tragically unrealized proposal rather than a not-implementable thought experiment deployed for theoretical ends. He thinks search engines and ""Web 2.0"" are evidence of the kind of emergent properties associated with guesses about the spontaneous origins of consciousness. He points to the complexity of online social networks as exemplars of new forms of computation, subtly implying that this may have philosophical significance (without bothering to ask himself what, then, a market economy, postal system or beehive might be computing). He genuinely seems about half-convinced that extraterrestrials have transmitted themselves digitally into our computer networks, where they reside, hidden. On page 293 he visits with an elderly Edward Teller, to whom he explains this last theory. Teller, gently and sensibly, suggests that Dyson write a science fiction novel instead of his current project. It's good advice.Certainly, others have made these mistakes before. One can hardly blame the creators of this incredibly powerful technology for optimistically imagining applications beyond its eventual reach. A great example comes in Chapter 9, which tells the story of the birth of computational meteorology. The meteorological status quo felt that their discipline was destined to remain an art; some upstarts felt that computational approaches were the path forward. The latter camp, with Von Neumann as their midwife, were thoroughly vindicated both in their own time and the decades since. Yet one ought to note Von Neumann's triumphalist predictions that, once weather systems could be predicted, manipulating them would be trivial. He predicted weather control and meteorological warfare! This has proven to have been a wild overestimation of that problem's tractability. Yet as the book progresses, Dyson takes the IAS staff's increasingly implausible speculations about consciousness and man's eventual subservience to machine and spins them out through his own, much-less-grounded imagination (Cory Doctorow thinks this is all great stuff, by the way).The basic problem is that Dyson doesn't truly understand much of the technology he's writing about. His grasp of technical detail is often very good for a non-engineer. But he lacks the virtuosic comprehension that made the individuals at the heart of this story so remarkable, and which is a prerequisite for the kind of speculation he wishes to indulge in. Let me be quick to add that I don't have that kind of virtuosity -- aside from a general lack of brilliance, the demotion of mathematics from computer science's essential foundation to its mere supplement (perhaps inevitable as CS's complexity has increased) arguably makes it much harder to achieve that kind of understanding these days -- but the big picture should be evident to anyone who's read even a little Hofstadter. The conceptual story here is about abstraction, the Unreasonable Effectiveness of Mathematics, and the doors opened by driving computation to a rate that can achieve results that are unattainable through more elegant and precise theoretical methods*. Instead Dyson often gets bogged down in meaningless errata about dimensionality, floating point arithmetic and whether data is represented spatially or temporally. The point of the story is that none of this matters! But Dyson doesn't grasp this. On pages 255 and 256, in particular, it becomes clear that the universality of the Turing Machine -- the whole point of the damn idea -- is lost on him.This is a shame, and it makes much of the book's end a waste. Things really start go off the rails in the chapter on Turing (the book is largely organized into chapters focusing on individuals, which proves to be a wise choice), though Engineer's Dreams, the chapter that follows it, is well worth reading for its portrait of Von Neumann's inability to confront death -- it's a highlight of the book -- even if it then descends into some of the book's most risible speculation. Klari Von Neumann's fate, explained at the book's very end, also packs emotional weight, though you'll have had to wade through a lot of nonsense to get there.Still, this book was a pleasure, and I'm grateful to Dyson for his portrait of a remarkable time filled with remarkable people. Highly recommended as a history, but whatever you do don't take its analogizing and speculation seriously.* to be fair, Dyson is actually very good on this last point"
21,0,http://goodreads.com/user/show/2996674-jeff,3,"I enjoyed reading this, and learned several new things while doing so. The book is not at all about Alan Turing. If it is a biography of anybody, it is John von Neuman; but really it is about many people, centered around the IAS in Princeton, who played a role in early computer development. There is also a lot of discussion about the development of nuclear and thermo-nuclear weapons, as one of the first applications of electronic computing. Two big downsides prevent me from rating this book higher: 1) Too much historical minutia. This is my number one complaint about this genre, so maybe I'm more impatient than most. However, I was several times frustrated to find myself reading far too many pages about things like the pre-european history of the land on which the IAS would eventually be built, or the when, who, what and how the IAS dealt with housing shortages by relocating housing from elsewhere. 2) The author's apparent lack of understanding of the computer architectures he was describing. As a computer engineer, many of the problems, ideas, or devices described intrigued me. Unfortunately, the ideas which were new to me I had to look elsewhere to learn anything meaningful about, and the descriptions of concepts I was familiar with I found to be somewhere between confusing and outright misleading. In light of this, I am highly suspect of many of the authors claims in the latter sections of the book regarding, e.g., search engines as a return to analog computing, or cellphone ""apps"" as equivalent to Baricelli's ""symbiogenesis"" simulations. It is my impression that he has completely misunderstood and misrepresented these concepts. Basically, don't read this for its technical content, and please don't take his interpretations too seriously. However, it is a thorough, well researched, and well written historical account of an exciting time of technology development. "
22,0,http://goodreads.com/user/show/6316773-brendan-dolan-gavitt,1,"The IAS MANIAC project was indeed a truly revolutionary computing endeavor, and it deserves a well-written history. Unfortunately, you will not find it here. Dyson doesn't seem to understand most of the technical issues he tries to describe, and he often resorts to vague attempts at seemingly profound statements (see the end of almost every chapter for examples). Dyson is at his best when he describes the personalities of those who contributed to the project, but this doesn't really save the work as a whole.The book is also suffused with Dyson's theory that the Internet is a living (and possibly sentient) creature. This kind of idiocy is annoying enough by itself, but it really has no place in a history of a computing project. The most concentrated dose of this is found in the latter half of the chapter on Nils Barricelli, so feel free to skip it -- Barricelli had some interesting ideas, but in the end I think he falls closer to the side of ""crank"" than ""visionary"".Finally, the writing is often a bit tortured and hard to follow, and his biographies jump confusingly between various parts of the individual's life, seemingly whenever the author thought of something new to add. These sapped what was left of my will to finish the book, and had I not been on transatlantic plane flight with no other source of entertainment I doubt I would have made it.I hope someone comes along and does a better job of this, because the early history of computing is a story that deserves the same kind of treatment that the Manhattan Project got in Richard Rhodes's ""The Making of the Atomic Bomb""."
23,0,http://goodreads.com/user/show/5686119-book,3,"Turing's Cathedral: The Origins of the Digital Universe by George Dyson""Turing's Cathedral"" is the uninspiring and rather dry book about the origins of the digital universe. With a title like, ""Turing's Cathedral"" I was expecting a riveting account about the heroic acts of Alan Turing the father of modern computer science and whose work was instrumental in breaking the wartime Enigma codes. Instead, I get a solid albeit ""research-feeling"" book about John von Neumann's project to construct Turing's vision of a Universal Machine. The book covers the ""explosion"" of the digital universe and those applications that propelled them in the aftermath of World War II. Historian of technology, George Dyson does a commendable job of research and provide some interesting stories involving the birth and development of the digital age and the great minds behind it. This 432-page book is composed of the following eighteen chapters: 1.1953, 2. Olden Farm, 3. Veblen's Circle, 4. Neumann Janos, 5. MANIAC, 6. Fuld 219, 7. 6J6, 8. V-40, 9. Cyclogenesis, 10. Monte Carlo, 11. Ulam's Demons, 12. Barricelli's Universe, 13. Turing's Cathedral, 14. Engineer's Dreams, 15. Theory of Self-Reproducing Automota, 16. Mach 9, 17. The Tale of the Big Computer, and 18. The Thirty-ninth Step.Positives: 1.	A well researched book. The author faces a daunting task of research but pulls it together. 2.	The fascinating topic of the birth of the digital universe.3.	A who's who of science and engineering icons of what will eventually become computer science. A list of principal characters was very welcomed. 4.	For those computer lovers who want to learn the history behind the pioneers behind digital computing this book is for you.5.	Some facts will ""blow"" you away, ""In March 1953 there were 53 kilobytes of high-speed random-access memory on planet Earth"".6.	Some goals are counterintuitive. ""The new computer was assigned two problems: how to destroy life as we know it, and how to create life of unknown forms"".7.	There are some interesting philosophical considerations.8.	As an engineer, I enjoy the engineering challenges involved with some of their projects.9.	Amazing how the Nazi threat gave America access to some of the greatest minds. The author does a good job of describing these stories.10.	The fascinating life of the main character of this book, John von Neumann.11.	So much history interspersed throughout this book.12.	The ENIAC..."" a very personal computer"". A large portion of this book is dedicated to the original computer concepts, challenges, parts, testing, etc...13.	The fundamental importance of Turing's paper of 1936. It's the inspiration behind the history of the digital universe.14.	Some amusing tidbits here and there, including Einstein's diet.15.	The influence of Godel. How he set the stage for the digital revolution.16.	Blown away with Leibniz. In 1679, yes that is correct 1679 he already imagined a digital computer with binary numbers...17.	So many great stories of how these great minds attacked engineering challenges. Computer scientists will get plenty of chuckles with some of these stories involving the types of parts used in the genesis of computing. Vacuum tubes as an example.18.	There are many engineering principles devised early on that remain intact today. Many examples, Bigelow provides plenty of axioms.19.	I enjoyed the stories involving how computers improved the art of forecasting the weather.20.	""Filter out the noise"". A recurring theme and engineering practice that makes its presence felt in this book.21.	Computers and nuclear weapons.22.	The Monte Carlo method a new, key domain in mathematical physics and its invaluable contribution to the digital age.23.	The fascinating story of the summer of 1943 at Los Alamos. 24.	The Teller-Ulam invention.25.	How the digital universe and the hydrogen bomb were brought into existence simultaneously.26.	Barricelli and an interesting perspective on biological evolution.27.	The amazing life of Alan Mathison Turing and his heroic contributions.28.	A fascinating look at the philosophy of artificial intelligence and its future.29.	The collision between digital universe and two existing stores of information: genetic codes and information stored in brains.30.	The basis for the power of computers.31.	The five distinct sets of problems running on the MANIAC by mid-1953. All in JUST 5 kilobytes.32.	A look at global digital expansion and where we are today.33.	The unique perspective of Hannes Alfven. Cosmology.34.	The future of computer science.35.	Great quotes, ""What if the price of machines that think is people who don't?""36.	The author does a great job of providing a ""where are they now"" narration of all the main characters of the book.37.	Links worked great.38.	Some great illustrations in the appendix of the book. It's always great to put a face on people involved in this story.Negatives: 1.	It wasn't an enjoyable read. Plain and simple this book was tedious to read. The author lacked panache. 2.	The title is misleading. This title is a metaphor regarding Google's headquarters in California. The author who was given a glimpse inside the aforementioned organization sensed Turing's vision of a gathering of all available answers and possible equations mapped out in this awe-inspiring facility. My disappointment is that this book despite being inspired by Alan Turing's vision, in fact, has only one chapter dedicated to him. The main driver behind this book was really, John von Neumann.3.	A timeline chart would have added value. With so many stories going back and forth it would help the reader ground their focus within the context of the time that it occurred.4.	Some of the stories really took the scenic route to get to the point. 5.	The photos should have been included within the context of the book instead of a separate section of its own.6.	The book was probably a hundred pages too long.In summary, I didn't enjoy reading this book. The topic was of interest to me but between the misleading title and the very dry prose, the book became tedious and not intellectually satisfying. The book felt more like a research paper than a book intended for the general audience. For the record, I am engineer and a lot of the topics covered in this book are near and dear my heart but the author was never able to connect with me. This book is well researched and includes some fascinating stories about some of the icons of science and the engineering involved with the digital origins but I felt like I was reading code instead of a story. This book will have a limited audience; if you are an engineer, scientist or in the computer field this book may be of interest but be forewarned it is a monotonous and an uninspiring read. Recommendations: ""Steve Jobs"" by Walter Isaacson, """"The Quantum Universe"" by Brian Cox, ""The Physics of the Future"" Michio Kaku, ""Warnings: The True Story of how Science Tamed the Weather"" by Mike Smith, ""Spycraft"" by Robert Wallace and H. Keith Melton."
24,0,http://goodreads.com/user/show/22224713-john-gribbin,4,"The title of George Dyson’s latest book about the scientists who worked at the Institute for Advanced Study in Princeton during its glory days is a little misleading; the story is not so much about Alan Turing, the man who came up with the idea of the modern computer, but John von Neumann, who did more than anyone else to make it a practical reality. No matter; like Dyson’s previous books, thus is a glorious insight into how science -- in this case, computer science -- was done at Princeton in the middle decades of the twentieth century. It is as much a story of people and personalities as a story of the science which they were involved in, and you do not need any knowledge of computers or mathematics to enjoy the ride.This time, Dyson sets the historical context of the origin of the Institute itself, before the story proper takes off with the dramatic early life of Hungarian-born von Neumann, leading up to his arrival in Princeton in 1931, one of the first major scientists to see the way the political wind was blowing in Europe and get as far away from the Nazi threat as possible. In the process, Hungarian Janos became “Johnny” to his friends and colleagues; he became a US citizen in 1937.By then, Turing had published his paradigm-shifting paper “On Computable Numbers”, which spelled out the basis of the modern computer, in terms of what we now call hardware and software. He even established the possibility of a “universal computer”, now called a Turing Machine, which could simulate the activity of any specialist computer, using different sets of software. This is exactly what my iPhone does. It can be a phone, a TV, play chess, solve certain kinds of mathematical problems, and do many other things. It can even do things its designers never thought of, as when an outside programmer devises a new app. Most people in the developed world now own a Turing Machine, less than eighty years after the publication of “On Computable Numbers”.Turing visited Princeton in the 1930s, and even completed a PhD there, although by then he hardly needed one. His own future in computing famously involved working at Bletchley Park in World War Two as a key (perhaps the key) member of the team that cracked the German Enigma code. This led on to post-war work on computers, crippled by a lack of funding, and his untimely death.In the US, the impetus for developing the electronic computer, using Turing’s ideas which von Neumann picked up and ran off with, came from the effort to develop the hydrogen bomb. As a bonus to the main thread of his history of computing, Dyson provides one of the clearest succinct accounts of the machinations involved in that project that I have ever seen. The outcome was that for that project, in that country, funding was not a problem, and Britain was soon left far behind in the art of manufacturing fast, number-crunching machines. There is an element here of other familiar tales. Turing’s approach was more elegant, and would probably have led to more versatile machines more quickly, given proper support; the Princeton (especially, the von Neumann) approach was brute force, succeeding by building bigger and faster without necessarily being better.The cathedral of Dyson’s title comes in to the book late on, and in the context of a statement made by Turing in 1950, concerning the idea of machines that think:	In attempting to construct such machines we should not be 	usurping His power of creating souls, any more than we are in the 	procreation of children: rather we are, in either case, instruments of 	His will providing mansions for the souls that He creates . . .Slightly curious terminology, coming from an atheist, but highlighting Turing’s belief that computers would one day become self-aware and able to think in the same way that we do -- or better. As Turing’s wartime assistant Jack Good once put it, “the ultraintelligent machine . . . is a machine that believes people cannot think.”One of the standard tests which may one day convince people that a machine can think was devised by Turing and is called the Turing Test. In this, a human investigator communicates with a machine and another person by some impersonal means (perhaps, these days, by email) and has to decide which is which by posing a series of questions. No computer has yet passed the test, but the heirs of Turing and von Neumann believe it is only a matter of time.How much time? Dyson points out that the first transistor radio, sold in 1954, contained just four transistors. Today, after allowing for inflation, for the same cost as that radio you can buy a computer with the equivalent of a billion transistors (which, among other things, will simulate a radio). And now, people are developing computers based on quantum principles, as far in advance of “classical” computers as the classical computer is in advance of the abacus. As the man said, you ain’t seen nothin’ yet. If you want to be mentally prepared for the next revolution in computing, Dyson’s book is a must read. But it is also a must read if you just want a ripping yarn about the way real scientists (at least, some real scientists) work and think.This first appeared in the Literary Review "
25,0,http://goodreads.com/user/show/5072847-will-ansbacher,2,"This is such a maddening book! Is it history? Is it biography? Is it science? Is it speculation? Well, that would be yes, yes, no and yes.It’s not quite what I was expecting as it has little to say about Turing and his theories or the Colossus machine he is known for (although that’s my fault for not reading the blurbs). Rather it’s about the subsequent computer revolution that developed from it after WW2, and the ENIAC computer in particular.But this book is not only about the mathematicians and computer engineers who built ENIAC; it’s also about the environment in which it “grew up”. Here’s where it all gets maddeningly muddled. Turing’s Cathedral mixes the historical and biographical rather haphazardly. Dyson brings in all the players (and I do mean ALL - like a Dostoyevsky novel, the book even begins with a 5-page list of “principal characters”) – John von Neumann, his wife, Teller, Feynmann and the rest, but it’s not linear in time: John keeps popping up in earlier periods in later chapters; the real purpose of ENIAC – to perform calculations for the H-bomb project (which was of course secret at the time) is mentioned but isn’t discussed until nearly midway through the book. It’s preceded by a number of cover projects such as weather calculations and stellar astrophysics that really didn’t need their own separate chapters, and especially not potted biographies of all the researchers involved, and their ancestors! Dyson even veers into speculation with a chapter on biology calculations and self-reproducing automata. That’s quite apart from the chapters on the actual construction of ENIAC.There are even longish chapters on the philanthropists who founded Princeton’s Institute of Advanced Studies where ENIAC was built, and the claustrophobic and snobbish society that apparently enveloped Princeton. All moderately interesting but really quite irrelevant. Topics are introduced out of sequence and abandoned abruptly, and the focus is often obscure – e.g., the chapter about the H-bomb is buried in one called Ulam’s Demons as though the physicist Ulam was the principal player in the whole thing, and there is nothing in the chapter about the aftermath or morality of the bomb, the destruction of Eniwetok atoll and so on, it simply ends with an unrelated problem that Ulam posed.The other maddening thing is that his explanations are such crap. Dyson quotes extensively from the people he interviewed but without interpreting any of it; in a way it’s much more like journalism than science writing. I don’t know if Dyson simply doesn’t understand much of what he was told or read about (the interviews and notes, at least, are extensive), or whether he thought his readers would not understand or find it too boring. But in this book about digital computing there isn’t even one numerical example. Instead there is endless, pointless speculation about whether computers really “think” or could reproduce.  There are only two places where Dyson seems to be in his element – one, in the extensive interviews he conducted over some ten years (he appears to be a very enthusiastic interviewer); and two, in a curious chapter where he waxes poetic about the Scandinavian physicist Alfven who wrote a forgotten speculative novel about computers taking over the world. The rest seems like Dyson bluffing his way through. To me, this little excerpt says it all: in an interview with Teller, they inevitably drift from computers and the H-bomb to extraterrestrial life. The aging physicist asks Dyson what he thinks about it; afterwards, there is a long, critical pause while Teller contemplates Dyson’s answer, then he pronounces, “Look ... instead of explaining this ... you should write a science fiction book about it”."
26,0,http://goodreads.com/user/show/17534024-raghu-chilukuri,5,"I have no idea why people claim this book is so bad. I agree the narration is non-linear, and possibly confusing, but it doesn't deserve all this flak.I'm not sure if these ranting people understand the concept of non-linear story-telling. There are people who said ""I'm not so technical, but..."" and some are ready to burn the book for not explaining von Neumann architecture in detail. I remember the book mentioning the ability to store code and data in the same place (address space) -- I'm not sure how deep one should go to explain this to a spectrum of readers.Some people (including me) have reservations about claims of cellular automata, the web being analogous to a cell and in this respect, it is ""analog"" engine made of digital computer, just like DNA in a cell. However, if you're intelligent enough to put down the book as rubbish, I wonder if they can't even distinguish facts and speculation (speculation of possible applications of some idea - which is plentiful in pop-sci books).People should remember that computers were mostly 'invented' to solve scientific computation problems like hydrogen bomb, weather modelling, anti-aircraft radar, cipher breaking etc. described in the book, preceding the discovery of transistors or such other knowledge which, for us, now is 'obvious' or 'logical'. For that matter, the 'von Neumann architecture' which we now call easy or logical or perfect choice wasn't completely the obvious choice. In fact, several computer scientists were (are?) proposing alternate ideas. For example, see this paper by Turing award winner John Backus, who worked in compiler theory: http://www.thocp.net/biographies/pape...I found it good to study the lives of scientists and engineers who were building the first generation computers; their challenges, their internal rivalries and often conflicting aims. It is good to remember that these days, most innovations are the result of group work and gone are the days when one person locked himself in a lab and emerged days/years later with a theory/device. And I think this book made quite a good effort at presenting such details, although not always page-turning.I liked this book pretty much overall, and would definitely recommend it to friends."
27,0,http://goodreads.com/user/show/667292-espen,5,"This is a tour de force history of the birth of the modern computer - and, specifically, the role of Princeton's Institute of Advanced Study in it - their ""IAS machine"" was a widely copied design, forming the basis for many research computers and IBM's early 701 model.We hear of John von Neumann (who tragically died of cancer at 53), Alan Turing (stripped of his security clearing and probably driven to suicide at 41), Stan Ulam, and many others, some famous, some (quite undeservedly) less so. I continue to be amazed at how far ahead some of the thinkers were - Alan Turing discussed multiprocessor and evolutionary approaches to artificial intelligence in 1946, for example.On a side note, I was pleased to see that a number of Norwegian academics, mostly within meteorology, played an important part in the development and use of the IAS computer. Nils Aall Baricelli, an Italian-Norwegian, was someone I previously had not heard of, one of those thinkers who is way ahead of his time and (perhaps because he was independently wealthy and led a somewhat nomadic academic existence, hence may have been considered something of a dilettante, though Dyson certainly don't see him as such and credits him with the ability to see a possible way from programmed computer to independently learning mechanism (and, perhaps at some point, organism).The book is a bit uneven - partly standard history, partly relatively deep computer science discussions (some of them certainly over my head), and partly - with no warning - brilliant leaps of extrapolating visioneering into both what computers have meant for us as a species and what they might mean in the future. It also shows some of the power struggles that take place in academics, and the important role IAS played in the development of the hydrogen bomb.All in all, an excellent history of the early days of computing - a more recent history than many are aware of. As George Dyson says in his Ted lecture in 2003: ""If these people hadn't done it, someone else would have. It was an idea whose time had come."" That may be true, but it takes nothing away from the tremendous achievements of the early pioneers."
28,0,http://goodreads.com/user/show/799620-christopher,4,"Ultimately, this is a very good book. The only thing keeping it from being a great book is the author's almost messianic fascination with the role cellular automata and its ilk played in the digital computing revolution, and the role the results of that revolution is playing in society.I realize this might seem counterintuitive, but the religiosity that comes through in Dyson's meandering ruminations on the ramifications of the history he is recounting do not, in my opinion anyway, actually lend itself to a proper telling of this story.That said, there is an awful lot to recommend this book to anyone who has an interest in computing, past, present, or future, or even for anyone who can appreciate the monumental engineering achievements Von Neumann's team enacted.I am still awe struck by some of the hardships they endured and overcame - like the fact that solid state memory didn't exist at the time, so they had to use CRT tubes as stored memory - the instant between when electrons painted an image onto the surface of a CRT and the next when it faded before the next refresh were JUST enough to impart a zero or a one. That concept alone is mind boggling to me.One area in which this book does an excellent job is exploring the incredible connections between the development of the stored matrix digital computer and that of the hydrogen bomb, charting a path for the reader through the tempestuous times, personalities and organizations that culminated in the successful detonation of one of the most fearsome weapons ever created by human kind.On point I am very glad to have read this book. There were definite moments of frustration where I found myself wishing that Mr. Dyson would spend less time prostrate at the altar of digital life and more detailing the technical challenges involved, but the net result was still an incredibly rewarding experience.I learned an enormous amount from this book, and that alone makes it priceless and really worth reading, even if it at times frustrated me :) "
29,0,http://goodreads.com/user/show/625444-diane,5,"I listened to this on audiodisc and it was well done. However, the book has wonderful pictures and if you do listen, I recommend getting a copy of the book for the pictures.This is the story of the making of the first computers. It is the story of the ideas, the machine, the math, the physics, the engineering, the people, the politics, and the physical and social environment. There were sections of the book that were over my head, but that was okay. In a page or two there would be a beautiful discussion of one person’s view of the universe or a description of the land being acquired for the Institute for Advanced Study (IAS). I am making it sound random, but the book is beautifully crafted.I was most impressed with the people (of course). Have so many brilliant people ever assembled before? So many geniuses from Europe, especially from Hungary, came to the United States during the 1930’s and 1940’s to escape the war and ended up working on or supporting work on the atomic bomb to save the world. Dyson tells us the history of many of these amazing people. And he then puts the people, the world events, the work on the computer and on the bomb in context. The story continues well into the 1950’s and 1960’s.My favorite people were Johnny von Neumann, a true polymath, and Julian Bigelow. Von Neumann was a mathematician and physicist, but he could see the bigger picture of any project. He was also empathetic and inclusive. Under his leadership, the IAS included many different people working together. After his death (from cancer at age 54) the IAS lost its big vision. Bigelow was an engineer and I think I liked him because he reminded me of Roy. He was raised in an eccentric family that lived without electricity or running water (etc.), and he loved tinkering with things and making do – liked working on the machinery as much or more than having the machinery work. He co-authored the seminal paper on cybernetics and teleology.This is a classic book and I expect it to be read for many decades. "
30,0,http://goodreads.com/user/show/3617921-david-schwan,5,"A history of the first Von Neumann computer at Princeton's Institute for Advanced Studies (almost all computers today copy this architecture). This book is way more technical than expected, not so much technical about the computer being discussed but much more what was run on the computer. The author grew up near the computer as his father is Freeman Dyson the astrophysicist. A good book but the author is way too hung up on the impact of Monte Carlo analysis (some I have used in the past for circuit design and something I'm suggesting to my company to provide an enhancement to allow certain parts of a transistor model (which currently don't take advantage of Monte Carlo) to allow them to. The cast of characters is wide and deep. Turing, Godel, Stan Ulam, Teller, Oppenheimer, Einstein and others make there appearance. Godel was well involved in the design. Stan Ulam was an influential user and is the father of of Monte Carlo analysis, the Hydrogen bomb (based on computer simulations he developed an design that works--a non obvious one at that). That the US government felt that nuclear bomb technology should be classified and computer technology should not has had wide implications on history (the UK choose to classify both)."
31,0,http://goodreads.com/user/show/4530266-vuk-trifkovic,4,"Fascinating book. It is much needed social history / genealogy of computing based on IAS in Princeton. Yet, it is the social history bit that really attracted me to the book and that absolutely shines through. In some ways it's almost a real-life sequel to something like von Rezzori novel. The cast of all these odd and stray Mitteleuropa scientists is just fascinating.It's just as fascinating to read about the very early computers and discover all these small design decisions that end up having huge repercussions. Or to realize the very physicality of the act of creating the first computers.The book wavers a little bit in the latter part as Dyson starts to extrapolate a bit, and when he lets his ideas of the future of machines and humans run a bit. This is not a problem per se, but his ideas never quite spark fully. Eventually, they end up as a bit of a burden on an otherwise excellent book. I reckon he should have either edited these bits our or been a bit braver and maybe blown them up to another 100 pages. The book is detailed, but not a mammoth, and the first part certainly garnered enough good will on my part to keep me reading."
32,0,http://goodreads.com/user/show/13237624-justin-heyes-jones,4,"I wasn't quite sure what this was about when I picked it up in the library, except I will read anything about Alan Turing. Well it turns out the book is largely about the Princeton Institute of Advanced Study, home to geniuses like John Von Neumann and Kurt Godel. It talks of the birth of the first computers and also of the hydrogen bomb and how the two inventions were intertwined. An interesting read but somewhat jumbled in its presentation. One minute you're reading about the finances and building supplies of the math department, the next it's the technical details of the first random access memory using cathode ray tubes.Despite that, it's worth reading if you are interested in the birth of the digital age, or enjoy reading about the intellectual heavy weights such as Von Neumann that show up so many times in the popular science books of that era."
33,0,http://goodreads.com/user/show/1397766-paul,2,"A disappointment, mainly due to a lack of coherent organization. Dyson assembles a great deal of information, anecdote, and explanation -- some of it fascinating and engaging, but not all of it lucid -- without providing the necessary connecting tissue. The reader is left to do the author's work. This baggy enterprise made me go back to a book by Steve Heims called JOHN VON NEUMANN & NORBERT WIENER: From Mathematics to the Technologies of Life and Death, published some thirty years ago, now sadly out of print. It was a lot clearer. "
34,0,http://goodreads.com/user/show/3897817-morgan-blackledge,5,"This book is a gorgeous, heart felt, nerdy labor of love. George B. Dyson writes like a dream and researches like a hard boiled heart detective. I can't recommend this book to everyone. But if you love science, tech, information theory, 20th century history and biography (you know who you are), than you just might have found your new page turner. "
35,0,http://goodreads.com/user/show/71560858-clemens-wisniewski,3,I believe the author of this book is unaware of the existence of women. Very interesting though. 
36,0,http://goodreads.com/user/show/6326292-boris-limpopo,5,"Dyson, George (2012). Turing’s Cathedral: The Origins of the Digital Universe. New York: Pantheon. 2012. ISBN 9780307907066. Pagine 338. 10,99 €Chi mi conosce o mi segue sa ormai che non mi entusiasmo troppo facilmente per un libro. Eppure questo lungo saggio di George Dyson merita un’acclamazione. È stato tradotto da poco da Codice edizioni (La cattedrale di Turing. Le origini dell’universo digitale) e quindi, se non volete fare la fatica di leggerlo in inglese, potete correre a leggerlo in italiano: ma ve lo raccomando, in qualunque lingua.Ho impiegato molto tempo a leggere il libro (prevalentemente nei viaggi in metropolitana) e ho avuto perciò il tempo di anticipare alcune impressioni e riflessioni, qui (a proposito di previsioni) e qui (su Ape and Essence di Aldous Huxley).Scopro proprio adesso, tra l’altro, che Dyson ha presentato questo suo libro al Festival della scienza di Genova pochi giorni fa, il 28 ottobre 2012. Io peraltro lo avevo incontrato qualche anno fa a Roma, al Festival delle scienze all’Auditorium di Roma dove aveva parlato dello stesso libro (all’epoca in gestazione) con lo stesso interlocutore italiano, Vittorio Bo (Roma, Auditorium Parco della musica, “Tra Possibile e Immaginario” Festival delle Scienze 2010, giovedì 14 gennaio 2010 Sala Petrassi ore 21: La Cattedrale di Turing e l’universo digitale. Conferenza con George Dyson, John Brockman, Vittorio Bo).Nel post che gli avevo dedicato all’epoca raccontavo della sua bella metafora su kaiak e canoe, in risposta alla domanda annuale di John Brockman, che nel 2009 era stata «In che modo Internet ha cambiato la tua vita?». Se non l’avevate letta allora, correte a rileggerla ora, perché è bellissima, vera e molto profonda. Anzi, la rimetto, così non avrete scuse: KAYAKS vs CANOES In the North Pacific ocean, there were two approaches to boatbuilding. The Aleuts (and their kayak-building relatives) lived on barren, treeless islands and built their vessels by piecing together skeletal frameworks from fragments of beach-combed wood. The Tlingit (and their dugout canoe-building relatives) built their vessels by selecting entire trees out of the rainforest and removing wood until there was nothing left but a canoe. The Aleut and the Tlingit achieved similar results — maximum boat / minimum material — by opposite means. The flood of information unleashed by the Internet has produced a similar cultural split. We used to be kayak builders, collecting all available fragments of information to assemble the framework that kept us afloat. Now, we have to learn to become dugout-canoe builders, discarding unneccessary information to reveal the shape of knowledge hidden within. I was a hardened kayak builder, trained to collect every available stick. I resent having to learn the new skills. But those who don’t will be left paddling logs, not canoes.Quello che all’epoca non sapevo è che George Dyson, nipote figlio e fratello d’arte (di questo parleremo dopo), a 16 si era trasferito in British Columbia, nel Burrard Inlet a nord di Vancouver, abitando a lungo in una casa costruita con le sue mani con materiali di risulta a 30 metri d’altezza su un albero e fondando un laboratorio per kaiak di tipo baidarka.Baidarkaguillemot-kayaks.comIn Turing’s Cathedral Dyson ricostruisce meticolosamente la nascita del primo computer digitale a Princeton nell’immediato dopoguerra. Il libro è ricchissimo di testimonianze e informazioni di prima mano. In questo, George Dyson è aiutato dalla circostanza di essere figlio del fisico Freeman Dyson e della sua prima moglie, la matematica Verena Huber-Dyson (oltre che fratello minore di Esther Dyson, e nipote del compositore inglese Sir George Dyson) e di aver passato l’infanzia e l’adolescenza all’Institute for Advenced Studies di Princeton. [Se posso raccontare un piccolissimo aneddoto personale: ho avuto l'avventura di passare un giorno allo IAS in visita a una persona che vi stava trascorrendo un periodo di ricerca, di pranzare alla sua leggendaria mensa e di incontrarvi il fragilissimo Freeman Dyson all'epoca 87enne.]Ma il libro non è soltanto un’accurata ricostruzione storica. I singoli capitoli sono costruiti intorno alle tantissime persone interessanti, più o meno note, che hanno collaborato al progetto: su tutti giganteggia John von Neumann, ma Alan Turing e Kurt Gödel sono comprimari di lusso, per non parlare di comparse della stazza di Thorstein Veblen o di Stanislaw Ulam. Inoltre, ognuno dei 18 capitoli è incentrato – oltre che su uno o più degli scienziati che hanno contribuito al progetto – ai contributi che essi hanno dato a uno o più degli avanzamenti scientifici. Infine, emerge con nettezza, soprattutto nelle ultime pagine del saggio, l’idea cara a George Dyson che l’universo digitale percorra una sua propria traiettoria evoluzionistica. In questo senso è vero quanto ha scritto Janet Maslin nella recensione pubblicata sul San Jose Mercury News del 10 giugno 2012 (non vi metto il link perché è dietro un odioso paywall) che ha definito Turing’s Cathedral «a creation myth of the digital universe.»Non vi dico altro: dovete leggerlo, se siete interessati agli argomenti che tratta, e anche se siete interessati a uno stile storiografico piuttosto originale.Oltre alle solite citazione, che sono comprensibilmente moltissime e che riporterò alla fine, vorrei mettere il filmato di un intervento di George Dyson al TED e i link ad alcune recensioni comparse sulla stampa internazionale.Ecco il link alla recensione di William Poundstone, pubblicata sul New York Times del 4 maggio 2012 (Unleashing the Power. ‘Turing’s Cathedral,’ by George Dyson), e quello alla recensione di Evgeny Morozov, pubblicata su The Observer del 25 marzo 2012 (Turing’s Cathedral by George Dyson – review).* * *Ecco le mie numerose annotazioni, con riferimento alle posizioni sul Kindle (sono annotazioni personali, che siete invitati ma non obbligati a leggere, naturalmente). The term bit (the contraction, by 40 bits, of “binary digit”) was coined by statistician John W. Tukey shortly after he joined von Neumann’s project in November of 1945. [315] The new machine was christened MANIAC (Mathematical and Numerical Integrator and Computer) and put to its first test, during the summer of 1951, with a thermonuclear calculation that ran for sixty days nonstop. [405] What could be wiser than to give people who can think the leisure in which to do it? — Walter W. Stewart to Abraham Flexner, 1939 [597] Equations for gravitation, relativity, quantum theory, five perfect solids, and three conic sections were set into leaded glass windows, and the central mantelpiece featured a carving of a fly traversing the one-sided surface of a Möbius strip. [790] Benoît Mandelbrot, who arrived at von Neumann’s invitation in the fall of 1953 to begin a study of word frequency distributions (sampling the occurrence of probably, sex, and Africa) that would lead to the field known as fractals […] [1039] We are Martians who have come to Earth to change everything—and we are afraid we will not be so well received. So we try to keep it a secret, try to appear as Americans … but that we could not do, because of our accent. So we settled in a country nobody ever has heard about and now we are claiming to be Hungarians. — Edward Teller, 1999 [1061] The good news is that, as Leibniz suggested, we appear to live in the best of all possible worlds, where the computable functions make life predictable enough to be survivable, while the noncomputable functions make life (and mathematical truth) unpredictable enough to remain interesting, no matter how far computers continue to advance. [1291] “It was his genius at synthesizing and analyzing things. He could take large units, rings of operators, measures, continuous geometry, direct integrals, and express the unit in terms of infinitesimal little bits. And he could take infinitesimal little bits and put together large units with arbitrarily prescribed properties. That’s what Johnny could do, and what no one else could do as well.” [1300] Vladimir Kosma Zworykin was a pioneer of television (and the last entry in many encyclopedias) […] [1602] Mathematicians produce their best work at about the same time that they produce their children, and the nursery school helped keep the two apart. [2209] “I can see no essential difference between the materialism which includes soul as a complicated type of material particle and a spiritualism which includes material particles as a primitive type of soul,” Wiener added in 1934. [2475] Leibniz saw binary coding as the key to a universal language and credited its invention to the Chinese, seeing in the hexagrams of the I Ching the remnants of “a Binary Arithmetic … which I have rediscovered some thousands of years later.” [2497] “There it might be said that the complete description of its behavior is infinite because, in view of the non existence of a decision procedure predicting its behavior, the complete description could be given only by an enumeration of all instances. The universal Turing machine, where the ratio of the two complexities is infinity, might then be considered to be a limiting case.” [2545] Brownian motion — the random trajectory followed by a microscopic particle in response to background thermodynamic noise. [2628] Maxim 7 advised “Never estimate what may be accurately computed”; Maxim 8 advised “Never guess what may be estimated”; and, if a guess was absolutely necessary, “Never guess blindly” was Maxim 9. [2667] “We should clear any fog surrounding the notion of ‘prediction,’ ” Bigelow confessed. “Strictly and absolutely, no network operator—or human operator—can predict the future of a function of time.… So-called ‘leads’ evaluated by networks or any other means are actually ‘lags’ (functions of the known past) artificially reversed and added to the present value of the function.” [2675] “A binary counter is simply a pair of bistable cells communicating by gates having the connectivity of a Möbius strip.” [2951] […] 1951 “Reliable Organizations of Unreliable Elements” and 1952 “Probabilistic Logics and the Synthesis of Reliable Organisms from Unreliable Components,” […] [2978] Some failures stem from lack of vision, and some failures from too much. [3450] “Consideration was given to the theory that the carbon dioxide content of the atmosphere has been increasing since the beginning of the industrial revolution, and that this increase has resulted in a warming of the atmosphere since that time,” the proceedings report. “Von Neumann questioned the validity of this theory, stating that there is reason to believe that most of the industrial carbon dioxide introduced into the atmosphere must already have been absorbed by the ocean.” The debate was on. [3962] Imagine a future, combining the visions of Lewis Fry Richardson with those of von Neumann, where the Earth (including much of its oceans) is covered by wind turbines immersed in the momentum flux of the atmosphere, and photovoltaics immersed in the radiation flux from the sun. Eventually enough of these energy-absorbing and energy-dissipating surfaces will be connected to the integrated global computing and power grid, to form, in effect, the great Laplacian lattice of which Charney and Richardson dreamed. Every cell in this system would account for its relations with its neighbors, keeping track of whether it was dark, or sunny, or windy, or calm, and how those conditions may be expected to change. Coupled directly to the real, physical energy flux would be a computational network that was no longer a model—or rather, was a model, in Charney and Richardson’s sense of the atmosphere constituting a model of itself. [4008] Monte Carlo opened a new domain in mathematical physics: distinct from classical physics, which considers the precise behavior of a small number of idealized objects, or statistical mechanics, which considers the collective behavior, on average, of a very large number of objects, Monte Carlo considers the individual, probabilistic behavior of an arbitrarily large number of individual objects, and is thus closer than either of the other two methods to the way the physical universe actually works. [4400] Biological evolution is, in essence, a Monte Carlo search of the fitness landscape, and whatever the next stage in the evolution of evolution turns out to be, computer-assisted Monte Carlo will get there first. Monte Carlo is able to discover practical solutions to otherwise intractable problems because the most efficient search of an unmapped territory takes the form of a random walk. […] The genius of Monte Carlo—and its search-engine descendants—lies in the ability to extract meaningful solutions, in the face of overwhelming information, by recognizing that meaning resides less in the data at the end points and more in the intervening paths. [4563-4568] “He was simultaneously one of the smartest people that I’ve ever met and one of the laziest—an interesting combination.” [4594] Ulam’s self-reproducing cellular automata—patterns of information persisting across time—evolve by letting order in but not letting order out. [5013] “GOD DOES NOT play dice with the Universe,” Albert Einstein advised physicist Max Born (Olivia Newton-John’s grandfather) in 1936. [5137] “Make life difficult but not impossible,” Barricelli recommended. “Let the difficulties be various and serious but not too serious; let the conditions be changing frequently but not too radically and not in the whole universe at the same time.” [5275] No matter how long you wait, numbers will never become organisms, just as nucleotides will never become proteins. But they may learn to code for them. [5335] “I would suspect, that a truly efficient and economical organism is a combination of the ‘digital’ and ‘analogy’ principle,” he wrote in his preliminary notes on “Reliable Organizations of Unreliable Elements” (1951). “The ‘analogy’ procedure loses precision, and thereby endangers significance, rather fast … hence the ‘analogy’ method can probably not be used by itself—‘digital’ restandardizations will from time to time have to be interposed.” [5343] How did complex polynucleotides originate, and how did these molecules learn to coordinate the gathering of amino acids and the construction of proteins as a result? He saw the genetic code “as a language used by primordial ‘collector societies’ of t[ransfer]RNA molecules … specialized in the collection of amino acids and possibly other molecular objects, as a means to organize the delivery of collected material.” He drew analogies between this language and the languages used by other collector societies, such as social insects, but warned against “trying to use the ant and bee languages as an explanation of the origin of the genetic code.” [5425] Aggregations of order codes evolved into collector societies, bringing memory allocations and other resources back to the collective nest. Numerical organisms were replicated, nourished, and rewarded according to their ability to go out and do things: they performed arithmetic, processed words, designed nuclear weapons, and accounted for money in all its forms. They made their creators fabulously wealthy, securing contracts for the national laboratories and fortunes for Remington Rand and IBM. [5448] Twenty-five years later, much of the communication between computers is not passive data, but active instructions to construct specific machines, as needed, on the remote host. [5462] Barricelli believed in intelligent design, but the intelligence was bottom-up. [5470] The origin of species was not the origin of evolution, and the end of species will not be its end. And the evening and the morning were the fifth day. [5529] “One of the facets of extreme originality is not to regard as obvious the things that lesser minds call obvious,” […] [5611] Complicated behavior does not require complicated states of mind. [5648] The title “On Computable Numbers” (rather than “On Computable Functions”) signaled a fundamental shift. Before Turing, things were done to numbers. After Turing, numbers began doing things. By showing that a machine could be encoded as a number, and a number decoded as a machine, “On Computable Numbers” led to numbers (now called “software”) that were “computable” in a way that was entirely new. [5702] The relations between patience, ingenuity, and intuition led Turing to begin thinking about cryptography, where a little ingenuity in encoding a message can resist a large amount of ingenuity if the message is intercepted along the way. […] A Turing machine can also be instructed to search for meaningful statements, but since there will always be uncountably more meaningless statements than meaningful ones, concealment would appear to win. [5752-5755] “When the war started probably only two people thought that the Naval Enigma could be broken,” explained Hugh Alexander, in an internal history written at the end of the war. “Birch [Alexander’s boss] thought it could be broken because it had to be broken and Turing thought it could be broken because it would be so interesting to break it.” [5793] Jack Good would later explain that “the ultraintelligent machine … is a machine that believes people cannot think.” Digital computers are able to answer most—but not all—questions stated in finite, unambiguous terms. They may, however, take a very long time to produce an answer (in which case you build faster computers) or it may take a very long time to ask the question (in which case you hire more programmers). Computers have been getting better and better at providing answers—but only to questions that programmers are able to ask. What about questions that computers can give useful answers to but that are difficult to define? [5969-5971] The paradox of artificial intelligence is that any system simple enough to be understandable is not complicated enough to behave intelligently, and any system complicated enough to behave intelligently is not simple enough to understand. The path to artificial intelligence, suggested Turing, is to construct a machine with the curiosity of a child, and let intelligence evolve. [5987] Search engines are copy engines: replicating everything they find. When a search result is retrieved, the data are locally replicated: on the host computer and at various servers and caches along the way. Data that are widely replicated, or associated frequently by search requests, establish physical proximity that is manifested as proximity in time. More meaningful results appear higher on the list not only because of some mysterious, top-down, weighting algorithm, but because when microseconds count, they are closer, from the bottom up, in time. Meaning just seems to “come to mind” first. [6009] Structure can always be replaced by code. [6223] Biology has been doing this all along. Life relies on digitally coded instructions, translating between sequence and structure (from nucleotides to proteins), with ribosomes reading, duplicating, and interpreting the sequences on the tape. [6262] In biology, the instructions say, “DO THIS with the next copy of THAT which comes along.” THAT is identified not by a numerical address defining a physical location, but by a molecular template that identifies a larger, complex molecule by some smaller, identifiable part. This is the reason that organisms are composed of microscopic (or near-microscopic) cells, since only by keeping all the components in close physical proximity will a stochastic, template-based addressing scheme work fast enough. There is no central address authority and no c"
37,0,http://goodreads.com/user/show/5954293-david-dinaburg,4,"I was surprised to see a dramatis personae in the opening of Turing’s Cathedral, but it is both apt and necessary. This is a dense character study of the dozens of mathematicians, engineers, and scientists that built the first electronic computing machine; the circumstances of their lives are detailed in lurid and occasionally ponderous detail. It is not a brisk read and it certainly takes some work to access. “Budapest, the city of bridges, produced a string of geniuses who bridged artistic and scientific gaps.” I find this line so pedestrian it borders on insulting, blog fodder that an editor should have culled in the first round. I do not fault the writer, who was likely quite pleased; it falls into the seductive “too cute” territory—a witty rejoinder during a casual conversation, maybe, but a strained metaphor for a written page. I picture a publisher wearily sighing as he or she opts not to edit a math-centric author on this sophomoric literary faux pas. But something happened about a hundred pages in; as I became used to the style, meeting each member of the cast, complete with detailing on how—and why—they came to Princeton radiated a sense of purpose. The pace is dictated by the nature of the material. These were some of the best and brightest of a generation; modernization as well as two world wars sent people from all over the globe to New Jersey. It give a sense that the subject requires a deep read, and if that sets a meandering pace, then so be it; the background of each player is required for comprehension of the how and why of our digital age. Sections and paragraphs and pages are devoted to minutia of travel or familial background: The problem was how to squeeze displaced scholars into a shrinking job market without provoking the very anti-Semitism those scholars were trying to escape. The United States offered non-quota visas to teachers and professors, but with insufficient openings for American candidates, finding positions for the refugees, especially in Princeton, was a difficult sell. An invitations to the Institute for Advanced Study allowed Princeton University, historically resistant to Jewish students and faculty, to reap the benefits of the refugee scientists without incurring any of the associated costs. The arrival of Einstein helped open the door. Princeton had become one of the more conservative enclaves in the United States, “a quaint and ceremonious little village of puny demigods on stilts,” as Einstein described it to the Queen of Belgium in 1933.What you get, then, is not a book about the creation of digital computers, but a book about the people who happened to create computers.Gödel proved that within any formal system sufficiently powerful to include ordinary arithmetic, there will always be undecidable statements that cannot be proved true, yet cannot be proved false. Turing proved that within any formal (or mechanical) system, not only are there functions that can be given a finite description yet cannot be computed by any finite machine in a finite amount of time, but there is no definite method to distinguish computable from noncomputable functions in advance. That’s the bad news. The good news is that, as Leibniz suggested, we appear to live in the best of all possible worlds, where the computable functions make life predictable enough to be survivable, while the noncomputable functions make life (and mathematical truth) unpredictable enough to remain interesting, no matter how far computers continue to advance. The mathematical theories are still prevalent, and you will pick up bits of atomic age etymology, like how a “flip-flop” became a “toggle.” “'Flip-flop' is not the right word for a bi-stable circuit which stays in whatever state you put it in. 'Toggle' constituted a far more secure representation of binary data than an element whose state is represented by simply being on or off—where failure is indistinguishable from one of the operational states.” Turing's Cathedral is the genesis chapter of the electronic computer bible, written now before apocrypha finds a way to alter the message. A message that computers, however they may be used now, were born of warfare:The behavior of both high-explosive detonations and supersonic projectiles depended on the effects of shock waves whose behavior was nonlinear and poorly understood. What happens when a discontinuity is propagated faster than the local speed of information ahead of the disturbance (for pressure waves, this being the speed of sound)? What happens when two (or more) shockwaves collide?Von Neumann’s theory of reflected shock waves could then be used to maximize a bomb’s effects. “If you had an explosion a little above the ground and you wanted to know how the original wave would hit the ground, form a reflected wave, then combine near the ground with the original wave, and have an extra strong blast wave go out near the ground, that was a problem involving highly non-linear hydrodynamics,” recalls Martin Schwarzschild. Dozens of human computers—people with paper, pencil, and patience—would have to work for hundreds of hours to complete the math required for shock wave analysis and bomb anticipation. Anti-aircraft guns, during World War Two, required a gunner to estimate how far to lead and when to light the shell before firing. They were not very accurate. Mathematical tables were brought in; creating the tables took time. Non-human computers made this work practical. It also opened the door, down the road, to the creation of weapons so powerful they defied then-current imagining: After the Soviet explosion of a nuclear weapon on August 29, 1949, the General Advisory Committee of the Atomic Energy Commission was asked for their opinion on whether the United States should undertake the development of the hydrogen bomb. The answer was no. “It is not a weapon which can be used exclusively for the destruction of material installations of military or semi-military purposes,” Oppenheimer explained in the introduction to the committee’s report. “Its use therefore carries much further than the atomic bomb itself the policy of exterminating civilian populations. We all hope that by one means or another, the development of these weapons can be avoided.” “Its use would involve a decision to slaughter a vast number of civilians,” the majority, including James Conant as well as Oppenheimer, concurred. “We believe that the psychological effect of the weapon in our hands would be adverse to our interest…. In determining not to proceed to develop the Super bomb, we see a unique opportunity of providing by example some limitations on the totality of war and thus of limiting the fear and arousing the hopes of mankind.” Before the terrifying spectacles at Bikini Atoll were made possible, electronic computers had to be up to the task. If we devote in this manner several years to experimentation with such a machine, without a need for immediate applications, we shall be much better off at the end of that period in every respect, including the applications. The importance of accelerating, approximating, and computing mathematics by factors like 10,000 or more, lies not only in that one might thereby do in 10,000 times less time problems which one is now doing, or say 100 times more of them in 100 times less time—but rather that one will be able to handle problems which are considered completely unassailable at present. That is a quote from John von Neumann, who “...left Europe with an unforgiving hatred for the Nazis, a growing distrust of the Russians, and a determination never again to let the free world fall into a position of military weakness that would force the compromises that had been made with Hitler while the German war machine was gaining strength."" He was the driving force behind the digital computing age and a staunch advocate for using the power of computers for weapons testing. Without his ability to secure funding and allow mathematicians and engineers space for creative and non-practical experimentation—all while still providing classified government sponsors usable and politically leveragable wartime and ""preventative"" weapon capabilities—electronic computers and our current information age would likely have been stymied for decades or denied outright.Yet even during the darkness of World War Two, the newfound precision that computing power allowed was not used solely for destruction: While von Neumann was looking for targets that should be bombed, the Institute’s humanists were enlisted (by the American Commission for the Protection and Salvage of Artistic and Historic Monuments in War Areas) to help identify targets that should not be bombed. Erwin Panofsky, the art historian, was responsible for identifying culturally important resources in Germany, while the Institute’s classicists and archaeologists helped supply similar intelligence for the Mediterranean and Middle East. Even Einstein was debriefed. It is in these small details rather than the breathtaking scope of world-altering events that elucidate just how much research and effort went into Turing’s Cathedral. Bits and quotes from the major players are dropped liberally throughout the text so the reader can create their own unadulterated view and not be forced into the image of events as seen through the lens of the narrator. ""The ENIAC was limited by storage, not by speed. 'Imagine that you take 20 people, lock them up in a room for 3 years, provide them with 20 desk multipliers, and institute this rule: during the entire proceedings all of them together may never have more than one page written full,' von Neumann observed. 'They can erase any amount, put it back again, but they are entitled at any given time only to one page. It’s clear where the bottleneck…lies.'"" It is beneficial that the author has familial ties to the time and places being discussed, and a reader gets the feeling he is transcribing events as near to accurate as we who were not there are likely to get.As time marches forward and electronic computers become ubiquitous, it borders on the imperative to understand how and who created the expanding digital universe:In the real world, most of the time, finding an answer is easier than defining the question. It is easier to draw something that looks like a cat than to define what, exactly, makes something look like a cat. A child scribbles indiscriminately, and eventually something appears that resembles a cat. An answer finds a question, not the other way around. The world starts to make sense, and the meaningless scribbles (the unused neural connections) are left behind. “I agree with you about ‘thinking in analogies,’ but I do not think of the brain as ‘searching for analogies’ so much as having analogies forced upon it by its own limitations,” Turing wrote in 1948.Turing’s Cathedral imparts a near-biblical understanding of the dawn of the Information Age. ""The information in our genes turned out to be more digital, more sequential, and more logical than expected, and the information in our brains turned out to be less digital, less sequential, and less logical than expected."" Computers and humanity have become symbiotically intertwined and both sides grow more similar as they take cues from each other:Search engines and social networks are analog computers of unprecedented scale. Information is being encoded (and operated upon) as continuous (and noise-tolerant) variables such as frequencies (of connection or occurrence) and the topology of what connects where, with location being increasingly defined by a fault-tolerant template rather than by an unforgiving numerical address. Pulse-frequency coding for the Internet is one way to describe the working architecture of a search engine, and PageRank for neurons is one way to describe the working architecture of the brain. These computational structures use digital components, but the analog computing being performed by the systems as a whole exceeds the complexity if the digital code on which it runs. It is difficult to keep the obvious truth—that computers were recently created by people—in plain view. George Dyson is Hesiod for the Prometheus tale of the men and women of Princeton’s Institute for Advanced Study. Access to accurate documentation of a creation myth—the origin of electronic computers, technology as simultaneously useful and dangerous as fire—makes Turing’s Cathedral a strong recommendation for anyone."
38,0,http://goodreads.com/user/show/5109654-marks54,3,"I liked this book but have some issues with it.1) The link between the book and its title is odd. I suspect it refers to a reference late in the book when the author visits Google's headquarters, but why is Turing central to what is really a book about John von Neumann and the team he led in building the famous computer at the Institute for Advanced Studies. This computer was to fit with the idea for a universal computer first proposed by Turing. OK, that is even a better link, but Turing - fascinating though he is - is not central to the book. The book is really a collective biography of the IAS computer and its team.2) . . . but it is not just about the computer, it concerns the different studies that were enabled by the computer, from the nature of fission and fusion, to blast patterns from nuclear weapons, to detailed weather predictions, to evolutionary modelling, to modelling the growth of the universe - not bad for a single machine! This is an attractive feature of the book. In fact, one of the more interesting parts of the book enumerated the links between the studies of population migrations carried out by Von Neumann's wife Klara and how nuclear fission and fusion were to be modelled.3) A negative is that in fashioning a balance between the nature of the science and technology involved in a given project, the author comes down on the side of detail and complexity. That is not necessarily bad, but here it makes the book overly long and tedious without sufficient clarification to compensate. The trouble with this approach is that if one writes to please the specialists, then they don't really need the material, since they already likely know about it. It is the non-specialists that need the education. That is the author's choice, however. It did not work for me with this book.4) The entire storyline of the collective biography is a bit much and too many lines are opened but not closed sufficiently. Be prepared to go on the web for supplementary materials on some of the more obscure parts of the story (Barricelli; Alfven).Overall, the book was fairly rewarding for the effort and there was new material to be gained."
39,0,http://goodreads.com/user/show/2707186-greg-meyer,5,"I had a few problems with this book. It was a little meandering at times. It had a bit of an odd structure, being organized by topic, rather than chronology -- which was especially odd because the first few chapters were in a chronological order. But, I think that the writing is excellent enough that it overcame these problems and ended up an excellent book.His ability to describe very technical details is very good, though I occasionally had to go over passages a few times in order to understand it. (It would have, actually, been a lot easier if I had realized that all of the pictures in the Kindle version are just at the back of the book. I would have looked at those a lot more.) And, his ability to convey the mystery, wonder and exuberance that everyone felt during those early days is amazing. At times, this lead to a bit of a weakness, as the end of the book focused a little bit more on stranger, almost more mystical powers of computers. But, I think that these are very accurate visions of the future. Our world is largely shaped by computers, and our future will be influenced more and more by the intelligence and predictions of computers. I am unable to comment on the morality or utility of these influences, nor in how they will play out in our future. In Frank Herbert's Dune, we saw a universe wherein a key commandment was, ""Thou shalt not make a machine in the likeness of a human mind."" While I doubt that Luddites shall be able to gain enough influence to bring this into fruition, I cannot rule out such a possibility, especially given some of the darker questions that Dyson asked towards the end of the book. This was one of Dyson's greatest strengths. He cannot see the future. However, he asks the questions that we all need to ask ourselves. He not only asks, How will we accept the influence of machines in the future? But he tells us what these influences are, and asks us which of these we think are good and bad. Hopefully his more upbeat questions will come true, and computers will lead to a better world. I think they already have."
40,0,http://goodreads.com/user/show/5786748-steve,4,"Parts of this book are extremely good, and parts seems to be lost in a rambling recitations of the principal subjects' speculations on the digital universe they were creating. I read this book at the same time as I read ""The Idea Factory"" about Bell Labs, located only a few dozen miles away in New Jersey. It was enlightening to me to realize how very much New Jersey was the center of American innovation from the 1930s to the 1960s. This book focuses on the Institute for Advanced Studies in Princeton, the mission of which was remarkably similar to the research function of Bell Labs--to let the best ""men"" do whatever they wanted to do in the way of speculative research. (At IAS, the key original funders were Jewish entrepreneurs, so an important mission throughout the 1930s became to bring Jewish scientists from Europe to America. No such mission at AT&T, which exhibited the casual anti-Semitism of mainstream culture in the era.) The book focuses is on the effort to build a computer (or ""Turing machine"") under the direction of Hungarian Jewish mathematician John von Neumann, from the late 1930s to the early 1950s. Given the timing, much of the book is devoted to the World War II work of von Neumann and all the others involved in building the computer. We even eventually find out about Alan Turing! The book does make clear how much of the funding for early computer development came from the military budget, whether for breaking German codes or for calculations needed to build the Bomb (A & H). The author was given access to most of the key players' papers, but unfortunately seemed determined to fill a 400+ page book with many verbatim transcripts. If the book had had an good editor, about 100 pages of blather could have been cut out and it would have been much better."
41,0,http://goodreads.com/user/show/644374-william-parsons,1,"I long anticipated reading ""Turing's Cathedral"". My adult life was spent working with computers and I still maintain a keen interest in all things computer related. The early days of computers is of particular interest so I anticipated this book being a great read. However, after reading 100 pages I decided life is too short; I’ll be returning the book to the library tomorrow. While there were several nuggets contained within those first 100 pages those nuggets do not make up for what I thought was a poorly written book, which at times bordered on the unreadable. I put this book down after investing a considerable amount of time and since the subject is one that I have great interest in those factors combined to double my disappointment.It would be unfair to completely dismiss the efforts of George Dyson since there were several nuggets concerning the Princeton Institute of Advanced Studies, the migration of scientists from Europe prior to WWII, the personalities and talents of those pioneers, as well as the drivers behind the construction of the first computers. However, those insights did not make up a book which most of the time focused on the arcane and trivial and failed to draw 'big conclusions' or provide ‘great insights’. George Dyson’s credibility is a product of his being the son of Freeman Dyson, British mathematician and early member of the Institute of Advanced Studies and his being able to recount first hand some of the stories of computer development, these attributes unfortunately do not make Dyson a writer and author.This book was a great disappointment and one I would only recommend to insomniacs. "
42,0,http://goodreads.com/user/show/2090739-gary-greenberg,4,"Huge scope of work. Book begins with a detailed(!) history of the Institute for Advanced Study's origins (financial, geographic, ideological). More than I wanted, thank you.Then, the interaction between computers & nuclear weapon development was discussed in detail. Author does a great job showing their philosophical & conceptual differences, with each needing the other.The personal histories of the theoreticians & engineers who created electronic calculators was fascinating... with the corporate roles following v-e-r-y far behind.The book was longer than I thought it needed to be, but this did allow:- discussion of digital meteorology (& climate control)- examination of digital mgmt of astronomical data- assessment of the know-ability of reality- how well programming (& philosophers) could handle the errors which arise in data processing, esp from hardware glitches. Self-correcting calculators was a new idea to me.Most amazing was the breadth of ideas of the development of a digital universes, the origin of self-replicating life and the evolution of life from digital to physical matter to biological instructions and then back to digital.At first it seemed over-the-top to say that Web 2.0 was an analog engine, replacing the preceding digital computing universe. Then it successfully explained that (eg) Google used arbitrary input (links, queries, content) to grow its mapping of the 'net's universe. By including the non-rational elements from human choices, the self-referential Web 2.0 has become as complex and tangled (& non-systematic) as the organic thinking which directs it.I think I learned a lot. "
43,0,http://goodreads.com/user/show/3030419-fraser-kinnear,3,"I love the stories in this book, and the concepts covered, but I feel like it's impossible to do them all justice in 350 graphic-free pages. Anyone who reads this and is interested in further details can check out these books:Fundamental concepts in computer architecture -> CodeAlan Turing's Universal Machine and solution to the Halting Problem -> The Anotated Turing Shannon & Information Theory -> The InformationKurt Godel/Incompleteness -> IncompletenessKurt Godel and his time at IAC with Einstein -> A World Without TimeThe Manhattan project -> The Making of the Atomic BombAnd while I saw the above topics covered but not given enough attention, I ran across some exciting new ideas that I'd love to learn more about""Worst-case-design"" that allowed the IAC team to build reliabie computers out of unreliable components (e.g., the 6J6)Nils Barricelli's parallels drawn between computing and biologyComputer architectures that differ from von Neumann's by being less time-linear (and, for example, more biological) since digital computation is enormously inefficient component-by-component"
44,0,http://goodreads.com/user/show/18688-cody,3,"It was a motley crew that descended upon the Institute for Advanced Study (IAS) in Princeton, NJ, where John von Neumann’s computer project was based from 1946-1951. There were mathematicians, physicists, engineers, meteorologists, and geneticists, all of whom had excelled in their respective fields prior to joining the IAS faculty. George Dyson awards each of the main players their own chapter, and it’s their personal journeys that make the story of the early days of computers particularly compelling. In this way, Turing’s Cathedral takes the shape of a bicycle wheel, with each chapter a kind of spoke, starting early on in the life of a particular scientist and tracing their development (as well as the forebears and ideas that inspired them), ultimately convening in Princeton with the IAS computer project and John von Neumann as the hub.While rigorous scientific research and discussion form the foundation of the book, Turing’s Cathedral is as much biography and cultural analysis as it is computer history, documenting the clash of ideas and cultures that lead not just to the development of the computer, but also to the breaking down of walls between scientific fields and between theory and real-world application. Ultimately, it’s a reminder that all great scientific advancements are never the work of a single person, but of a group of people who collaborate, experiment, and draw on the rich tradition of ideas and innovation that precedes them in order to change life as we know it."
45,0,http://goodreads.com/user/show/7147893-tom,2,"Man, this book was a slog. The history of the concurrent development of digital computing and the atomic bomb program are Relevant To My Interest, but even that was strained at times on the author's insistence on detailing at length every bit of history or biography related to people and places in the book. Most of the development of the IAS computer, the focus of this book, takes place at Princeton. Is it really necessary to go back two hundred years and learn about the colonization of the town the university was built near?Also, for a book on the history of the development of computers this book seems to assume a level of knowledge that only someone who had already studied this extensively would already have. The same goes for mathematics, computing and electronics in general. If you're not very familiar with all three fields you will often be lost. Even under all the mathematical underpinnings the author seems to have some very romantic ideas about what computers are and our relationship to them. Sometimes he gets quite carried away with himself.I did like learning about some of the people critical to the development of computers, especially Jon and Klara von Neummann, who appear extensively throughout. But overall this was literally too much of a good thing and I only recommend it for the true enthusiast."
46,0,http://goodreads.com/user/show/62334106-cyoce,3,"Turing’s Cathedral by George Dyson chronicles the rise of early computing in the 20th century, specifically the works of Jon von Neumann and Alan Turing leading to the creation of E.N.I.A.C. and M.A.N.I.A.C. These computers were originally meant to simulate nuclear blasts for weapons research, but they revolutionized the general-purpose computer by giving it memory that could store instructions in the same way it stored data. Unfortunately, Turing’s Cathedral turned out to be a dull read. Its writing style was cluttered, leading to long strings of unexplained jargon every few pages. For most of the scientific and mathematical concepts in the book, they are either left with no description or a roundabout explanation that requires re-reading to comprehend. For a book based on the fields of math and science that struggles with introducing too much terminology, the book has a surprisingly low emphasis on the actual mathematical or scientific discoveries. Large portions of the book are about the stories of the various scientists, describing events that are tangential to the content of the book at best. If the book were to focus more on the breakthroughs in computing instead of the characters behind them, it would be a more interesting read for its target audience---the mathematically inclined. "
47,0,http://goodreads.com/user/show/42695975-gargi-punathil,4,"I did expect a lot more science, but this scintillating account of how many greats worked together to build computers is inspiring.That many of these men were Europeans escaping the horrors of war is the wretched part of the story, reminding me to regard the fallacies attributed to their personalities with kindness. This book, heavily centred around John Von Neumann has introduced me to Edward Teller, Oswald Veblen, Stan Ulam, Nils Barricelli, Nicholas Metropolis and the like, names that I have unfortunately never heard of before.  Starting with the minutest details of how the IAS came to be, giving some background on the major contributors, the nearest Dyson gets to technical details is in the couple of chapters where he describes how they chose what devices/materials to use to make the computing machine more efficient. There are many concepts referred to, warranting some more attention and explanation. The timeline gets confusing as he switches between Los Alamos and Princeton occasionally, but it all paints one big picture as we turn the pages.There are brief references to the women, some of who got a chance to work on the machine thanks to their spouses, but that's about it. The parallels drawn between the digital computers and the biological universe make for an interesting read towards the end. It is also peppered with endearing and funny anecdotes."
48,0,http://goodreads.com/user/show/5871553-chris,2,"I read this immediately after The Idea Factory, which is a similar history of Bell Labs (birthplace of the transistor, microwave communications, the cell phone, fibre optics, ...); if you're only going to read one book about research labs in New Jersey, read that one. This book should have been called ""Von Neumann's Cathedral"", given that Turing's at most a peripheral character in what is mostly a history of the Institute for Advanced Study and its residents, most specifically, Johnny von Neumann's quest to build a Universal Turing Machine. It would be nice if Dyson had given a more concrete explanation of how modern computers actually work and described the von Neumann architecture underpinning them. A number of Dyson's more esoteric and dubious passages (e.g. likening search engines to computers dreaming or analog computers) made me wonder how much of the rest of the book was on solid footing; some of them were nearly non-sensical pastiches of ideas that ... as Pauli might say, aren't even wrong.With that said, the discussion of early computing's intertwined development with atomic and nuclear weapons research was quite well done."
49,0,http://goodreads.com/user/show/2579781-paul,3,"I found the biographical and historical aspects of this book very interesting. It covers in at least cursory detail the lives of a lot of the very early computer pioneers; I've read a great deal about the history of the Manhattan project* and about the history of computers and the internet, but surprisingly I have not heard much about the use of computers as part of the effort to build an atomic bomb. This book covers that area nicely.The downsides were that it seemed a bit scattershot - I would have really appreciated an introduction explaining what the book is intended to cover, and a bit sensationalist, taking the time to muse about various techno-futurist topics (in a way that I'm sure will be horribly dated in 30 years).*This is more because it is somewhat sensational and writers love the narrative of technocrats pulling together a major project to solve a massive science and engineering problem to win the war than out of any significant interest on my part.3.5 of 5 stars"
50,0,http://goodreads.com/user/show/3581493-arnie,5,"This is a dense but beautifully written book that really defies a simple definition. Superfluously, it's about the early development of ENIAC and MANIAC and the team of engineers and mathematicians who designed these early computers. Digging deeper, though, what Dyson is doing is examining the evolution of data networks, AI, and the feasibility of ""mechanical intelligence"" (which Turing always considered a better phrase than ""artificial intelligence""). There are some philosophical bombshells along the way, including a mind-boggling chapter 15, which poses the question, ""Are we designing our machines, or are our machines designing us?"" In short, if our computer network was an emergent intelligence, would we even know? Would it even expose itself to us as such? These ideas have resonance, and this book will stay with me for a long time. Highly recommended."
51,0,http://goodreads.com/user/show/21748149-folkert-wierda,4,"At first I tended to agree with the quickly glanced comments that it is a badly written story about an amazing epoch. As I progressed this changed. Firstly the book is loaded with gems, real life stories about real life people who changed the world as we know it. Secondly, the non-linear presentation doesn't make the read easier, but ultimately more meaningful. Especially the description of the different topics and people associated with these topics makes me understand so much better where we come from in scientific modeling and simulation. I cannot comment on the claims that the book contains many technical mistakes, I never worked in machine code. But even if that is the case, the essence of the book remains. Nice read for those interested in a fascinating time around the genesis of computing, with central roles for Von Neumann and Turing."
52,0,http://goodreads.com/user/show/7463675-steve,4,"The book was not exactly what I thought it would be, but it was still very entertaining. A thoroughly researched account of the time when the first modern computer was made, and the people who made it possible. There is also an interesting discussion of the types of programs that were first run on these machines, and a very interesting (if not romanticized) connection between digital and biological evolution. I will warn people that the book can be a bit technical at times, and you should really have a serious interest in the material before jumping in. Oh yeah, I originally thought the book would be exactly about putting Turing's idea of a Universal Computing Machine into practice, but the scope of the book was really more broad than that single topic."
53,0,http://goodreads.com/user/show/7767969-bob-simon,4,"What must it have been like to have grown up in the shade of the Institute For Advanced Studies in Princeton? To have Freeman Dyson as dad, Esther Dyson as sister, John von Neumann at dinner? Lots of details about the formation and internal politics of the time and place, plus extraneous stuff like the price of oysters in the cafeteria. Einstein is here, von Neumann, Norbert Weiner (cybernetics), Oppenheimer, the whole illustrious shebang. It's well researched and compelling. George Dyson is cool beyond description, full of wit and charm, and he can write. It will take about a year to digest it all, but I will re-read it for certain.Alan Touring would have been 100 this year. I only wish he had lived to celebrate it."
54,0,http://goodreads.com/user/show/5052871-writegeist,5,"An incredible journey into the history of the computer. Dyson brings out the people who created the world we live in today (and in some ways we are still working with the systems as they were developed back in the 1930's and 40's): von Neumann, Einstein, Touring, Fermi, Oppenheimer... names I have heard through the years but had never had a chance to get to know. This is a dense read, filled with technical information that I did not understand much of; however, Dyson injects the humanity into this discussion of the machine, so I never was overwhelmed by it. But I do plan on asking my own co-worker engineers some questions to fill in some of the blanks in my understanding. Should be a required read for anyone involved in this amazing world of computers. "
55,0,http://goodreads.com/user/show/30186986-tim-warren,4,"This is a fascinating examination into some of the people and history of the digital computing age. US centric with reference to the British - notably Turing. There is some criticism in various reviews of the technical accuracy - for my part as a student of computer science and maths - it was accurate enough in the general perspective to be worth the time. For the non computer scientist, it probably doesn't matter anyway. The book tells many stories, many histories leading up to the IAS computer. Yes it jumps around, yes it is recursive is parts, and this is a nod to the very workings of a computer program itself. "
56,0,http://goodreads.com/user/show/7579757-charles-dewitt,4,"The book is long and demanding yet is pitch perfect in striking the tone of the Literate Scientist. (Isn't it interesting to note that C.P. Snow's ""two cultures"" are only ever successfully united by Cultured Men of Science, rather than Scienced Men of Culture?) Of course, the nature of the subject requires some occasionally daunting passages on ""shift registers"" and the peculiarities of the vacuum tube, but there is deep learning here, too. My full review is available here."
57,0,http://goodreads.com/user/show/18370648-mishehu,5,"Fascinating, inspiring, sobering history, and a great read as well. Only two criticisms: some of the tangents were done to death (interesting stuff, but roamed too far afield at times); and the author had an annoying habit of overusing the following device:""X (did/did not) [VERB] Y. Y [VERBed] X."" All instances fit their contexts snuggly -- and were quite interesting to boot -- but every one jumped off the page and came to irritate this reader in their numbers.As I say, though, mine are trifling criticisms. This is a terrific book."
58,0,http://goodreads.com/user/show/5291315-marcin-wichary,2,"Messy. There are many great tidbits and anecdotes here, and some observations and turns of phrase are classic, ultra-clever, top-notch Dyson. However, the overall structure is a giant random pile of mess where little unimportant factoids get as much spotlight as crucial events. The book has little to do with Turing, and it’s often hard to know what it really is about, since the introduction is as short as the following narrative is loose and rambling. Overall, quite a bit of a disappointment."
59,0,http://goodreads.com/user/show/6614846-joyce-scrivner,5,Excellent story of the development of computer and computing from the security/government/math side. John Von Neumann is the protagonist and player. George Dyson lived at the Institute for Advanced Study as a child and thus had access to the information both within the culture and with the people who were there while the computer was being developed. The implications of how the demand for memory drives computer development and how the original architecture still holds is wondrous.
60,0,http://goodreads.com/user/show/77320112-scott-delgado,3,"Something that people need to know is that this book is about the history of computing, not Alan Turing. There are a few books on Turing and his life. This book just uses his name and mentions him a tiny bit, so if you are looking for a book on Turing, look elsewhere.This book wasn't for me, and I was rather bored with it. But with that being said, I'm not a very savvy computer user. For someone who loves tech and computers, this book is probably very interesting."
61,0,http://goodreads.com/user/show/5914137-jay,1,"Dyson did a lot of research for this book, and it's all in there. All of it. He seems to have taken his (extensive, thorough, exhaustive) notes, added a few verbs, and hit the ""publish"" key. I got about 100 pages into it before pulling the chain. To be fair, I'd recently read Gleick's _The Information_, which is pretty compelling; it may have spoiled me."
62,0,http://goodreads.com/user/show/1585572-sharon,4,"Insightful and well researched. A true history of how the computer age started. I felt that occasionally the the science was beyond my grasp, but I guess it is hard to dumb down most of these concepts. I enjoyed reading about the personal lives of the mathematicians and physicists, many European Jews who escaped being murdered in the Holocaust."
63,0,http://goodreads.com/user/show/24145676-jim-johnson,3,"It had a lot of interesting information, however it's kind of a dry read and I had trouble keeping my interest as I got later into the book. But that may just me; maybe I'll give it time, then go back and reread some day to see if time makes a difference."
64,0,http://goodreads.com/user/show/27472296-paulina,4,Interesting and overwhelming read (I probably need to take classes and do lots of supplementary reading to actually understand all of this. One of them is this: http://kcoyle.blogspot.co.uk/2012/11/...). 
65,0,http://goodreads.com/user/show/3929037-bill-buhler,4,"This was a fascinating book. It is a complex telling of the origins of electric digital computers. It is well worth reading, my only complaint is that it winds back and forth in time frequently as it tries to introduce the key players and how they interact together."
66,0,http://goodreads.com/user/show/7325556-frank-palardy,3,Not about Turing or even tech so much as a biography about the institute for advanced studies where these people worked.
67,0,http://goodreads.com/user/show/62506065-peter-vanhoutte,4,Provide a great insight in the early development of IT and its origins.
68,0,http://goodreads.com/user/show/3945469-judi,4,"I can't say I understood everything but I can say I enjoyed all of it! Starting in the 1920s and ending with the dawn of the Internet, Dyson covers the evolution of digital computing from its inception. While it takes some time for Turing to make his appearance, the history involving von Neumann and Oppenheimer, the Institute for Advanced Studies, and Los Alamos are important precursors to understanding the concepts of Turing's Universal Machine and its impact on today's digital culture.One thing that I picked up is that Klara von Neumann played a much larger role in the birth of the computer than she's been given credit for. Her initial work with coding should place her at the same level as Grace Hopper as one of the first programmers ever.Dyson does get a tad philosophical and I'm not sure I agree with everything he predicts but I will take it as a warning that there should be some level of oversight with the development of computer programs that have the potential to change our lives."
69,0,http://goodreads.com/user/show/99282765-kyle-jones,0,A history of computers through the lens of the small community at Princeton’s Institute for Advanced Studies and John von Neumann. Follows the development of early computers and the role World War II played in driving technical innovations. Lots of interesting stories.
70,0,http://goodreads.com/user/show/771599-joe-arencibia,4,"I guess the only knock on the book would be Dyson's tendency to jump around loosely by theme, then back again later. So the book's chronology is sometimes hard to follow, and occasionally annoying. But hey, if you subscribe to the notion that time is just an illusion, who cares?The book was wonderful in the way it simultaneously documents the context, execution and aftermath of the development of the first digital computing machines. Within this framework of war, politics, academic conflict and personal conflict comes the individual and institutional stories of the people that brought the first computers to life. It follows the individual stories, complete with all their personal histories, philosophies, shortcomings and strengths. The key individual focuses are Johnny Von Neumann and Alan Turing. But it also includes so much on the contributions of others like Oppenheimer, Ulam, Bigelow, Fuld, Goldstine, Barricelli, Von Karman, Godel, and Einstein (just to name a few). It's also amazing to think of this in terms of today's petty social-conservative politics. These revolutions in science and computing were born at academic institutes funded by government dollars (such as IAS and Los Alamos). The academics were almost entirely immigrants. Many were atheists and some were gay (like Alan Turing, the visionary who first framed the mathematical theories of the digital computing machine). It's pretty sad to think that the context of these same ground-breaking inventions probably couldn't even happen today because of dimwits in congress and on the airwaves who get their knickers in a bind when the government spends money on open-ended research, or funds the work of immigrants, homosexuals or the godless heathens in academia. The inconvenient truth, that the Rick Santorums of the world will never accept, is that so much of America's prosperity in the last century is based on the success of government spending and the tireless efforts of immigrant, heathen academics who brought about the computer age. (Political rant done.)Social politics aside, I was also amazed at how forward-thinking the first computer developers were. They didn't see the development of the computer as an end in itself. Rather, they were just trying to solve complex problems that required more computing speed than a human. They were impatient problem-solvers, yearning for a faster answer. The end games were big questions like nuclear energy (and regrettably bombs), weather prediction, biological evolution, planetary evolution and even the future of self-replicating computation. They didn't care about computing, for computing's sake. Instead, they were making the tools to answer the really big questions. (As opposed to today when programmers often seem more concerned with the ""great questions"" of how to become mayor of your local bar on Foursquare). Of course it got into the details too, including the background mathematical theories and the overall philosophy of computation. It also spent a lot of time on the practical issues of dealing with unreliable hardware and the accuracy and repeatability of simply storing and retrieving bits. Pretty amazing stuff, especially when you consider that the fundamental architecture of computers hasn't changed in 70 years. That said, this is certainly not a book about the details of the architecture itself. If you're looking for a treatise (or even reasonably coherent explanation) of the logic or assembly, this ain't it. In fact, Dyson's explanations of those details are a bit disjointed. This was a great read, not just for how the 1s and 0s came to life, but for why they came to life in the first place. "
71,0,http://goodreads.com/user/show/10287510-steven,2,"Color me disappointed. I expected a history of the construction of ENIAC, one of the first general purpose computers; arguably the most historically and technically important, in that its design lives on in the heart of nearly all modern computers. I also expected something of a history of the Center for Advanced Studies at Princeton, seeing as that's where the machine was built and where the author, George Dyson, son of Institute physicist Freeman Dyson, spent at least part of his childhood as, ""one of a small band of eight- to ten-year-olds who spent our free time exploring the Institute Woods..."" It's not that I didn't get both histories, and more; all the information that I wanted to read is there, in this book. It would seem, however, that Mr. Dyson’s primary goal with this book was not so much historical as it was prophetic. The structure of the book is thus not chronological, but conceptual, with each chapter presenting the history of a single idea, and how each idea was conceived or how it evolved in the new world of digital calculation. Concepts such as shock waves, Monte Carlo method, self-reproducing automata, biology, weather prediction and climate modeling – all of these were wrestled to the ground and cranked through ENIAC and its brethren, MANIC and JOHNNIAC and SILLIAC and many others. Some of these ideas were entirely new; others were suddenly computable.What Dyson tries to do with this book is to show us how these ideas came about, how they changed, how they grew and branched into still other ideas, and how the digital universe was born in the very exploration of these ideas. Whole new dimensions of space and time – measured in bits and nanoseconds – sprang into existence with ENIAC. The physics of these dimensions is defined in the code we execute there; the worlds that float in that universe are built of the data that we inject from outside. It’s a grand concept, as is his suggestion that humanity’s successor is already here, sharing the planet with us. That, with the birth of the internet, the development of cloud computing and other abstractions of computation and data storage , we have created the species that will ultimately, if not replace, then supersede, our own. The machines that allow us to interact with our world, our bank accounts and our friends; that predict our weather or guide our spacecraft – these machines also manipulate the DNA of creatures living in our own universe; in fact must act as our intermediaries, because our minds simply cannot operate at the speeds or scales required to communicate with that microscopic world.He may be right; his arguments are compelling. But it wasn’t what I expected. By following the ideas, rather than the calendar or even geography, every chapter seems to start in the 1920s or 1930s, in Hungary or England, finding its way to New Jersey or Maryland, before crossing the continent to Los Alamos or the Atlantic and back to Europe in the 1950s. With each chapter, I learn a bit more about Von Neumann or Ulam or Turing before being flung back to 1924 Budapest.In the end, maybe the problem is mine, not Dyson’s. Maybe I simply wanted a nice, straightforward history of science and computing that didn’t strain my mind too badly; something that I could pick up and put down easily. Perhaps if I had treated it as a collection of essays with a common theme, I might have been happier.As it is, I am just lost in time and space. And I’m disappointed."
72,0,http://goodreads.com/user/show/6387522-david,3,"Turing's Cathedral is a bit of a chimera: most of the book's pages are devoted to the early history of computing (and most of those pages to the life of John von Neumann), but most of its interest is on the conceptual leaps that lead to the titular ""digital universe"". I'm similarly of two minds in my response to the book: while I'm very interested in the story George Dyson wants to tell, I found the book itself frustrating and often disorienting.Dyson's primary thematic interests throughout are the conceptual shifts involved in the development of Von Neumann architecture computers, specifically the use of numbers to encode instructions as well as data, the mathematical logic of computing, and analogies between digital and biological organisms. Unfortunately, this is also where Dyson is weakest. He repeatedly misuses and overextends analogies to biology and makes egregious technical errors while attempting to describe how computers actually work. In particular, the conclusion of the book descends into fairly outlandish claims about the current state and direction of computing that would be more at home in a Neal Stephenson novel than a factual history.It's almost ironic, considering Dyson's constantly repeated invocation of data as numbers structured in space and instructions as numbers ordered in time, that Turing's Cathedral is as chronologically confused as it is. This is not a history of computing. The actual ordering of the book is loosely thematic, taking us through different phases of development at research labs and corporations across the United States and England. Unfortunately, this choice makes it very difficult to see the whole, as we leap from the Institute for Advanced Study in the mid 1950s to the other side of the country in the mid 1930s. Dyson would have done better to lose the pretense of generality and focus on his obvious primary interest in the IAS.Turing's Cathedral is a misnomer: this book is not about Alan Turing at all. Although we jump from time to time to the scenes of other early computer projects (and for a scant paragraph or two to Bletchley Park), Turing's Cathedral is almost exclusively focused on John von Neumann and the Institute for Advanced Study. While Kurt Goedel and Alan Turing and Stanislaw Ulam and Julian Bigelow play supporting roles, and a scant, unfriendly cameo goes to Mauchly and Eckert of the ENIAC, von Neumann is the main figure on nearly every page.Ultimately, the book suffers mainly from overreaching. If George Dyson had actually set out to write a biography of John von Neumann and the Institute for Advanced Study, a book much like Turing's Cathedral could be considered a rousing success. It's well-written and well-researched, peppered with insight from the author's interviews with the surviving principals and his personal recollections, all couched in lovely prose. The book's failures come from its status as a biography masquerading as a history and its author's as a fine historian attempting to play tech pundit."
73,0,http://goodreads.com/user/show/16930686-jeff-bell,5,"I thoroughly enjoyed Dyson's walk from Turing's Universal Machine to his visit to Google. The story is reverent, well told, and exceptionally well referenced. I referred to the 40 pages of sources and archive references, a 30 page index, and a 6 page guide to ""principal characters"" often. They gave good context, and represent an academic yet (again) personal history of the digital universe. The background Dyson provided and the parallel's he draws to other universes or universal systems were brilliant. Some of the criticism around the path he took and the title I think misunderstand Dyson's larger goal. This isn't meant to be about the origins of the computer... it's meant to be about the origins of the digital universe. Great book.Some favorite quotes:- ""First draft of a report on the EDVAC,"" reproduced by mimeograph and released into limited distribution by the Moore School on June 30, 1945, outlined the design of a high speed stored program electronic digital computer, including the requisite formulation and interpretation of coded instructions - ""which must be given to the device in absolute exhaustive detail.""- The paradox of artificial intelligence is that any system simple enough to be understandable is not complicated enough to behave intelligently, and any system complicated enough to behave intelligently is not simple enough to understand. - The path to artificial intelligence, suggested Turing, is to construct a machine with the curiosity of a child, and let intelligence evolve. - ""Are we searching the search engines, or are the search engines searching us?""- ""Thanks to the computer, the Institute could do applied science without having to build laboratories.""- ""Viewing the problem of self-replication and self-reproduction through the lens of formal logic and self-referential systems, von Neumann applied the results of Godel and Turning to the foundations of biology""- ""It appears that we have thereby exhibited a machine which can reproduce (ie design) itself.""- ""Sixty some years ago, biochemical organisms began to assemble digital computers. Now digital computers are beginning to assemble biochemical organisms. Viewed from a distance, this looks like part of a life cycle.""- ""Von Neumann extended the concept of Turing's Universal Machine to a Universal constructor: a machine that can execute the description of any other machine, including a description of itself. The Universal Constructor can, in turn, be extended to the concept of a machine that, by encoding, and transmitting its own description as a self-extracting archive, reproduces copies of itself somewhere else.""-"" Our time is becoming the prototime for something else.""- Ulam's question ""what kind of infinity do we want?""- ""Virtual machines never sleep. Only one-third of a search engine is devoted to fulfilling search requests. The other two-thirds are divided between crawling (sending a host of single-minded digital organisms out to gather information) and indexing (building data structures from the results).""- ""There must be something about this code you haven't explained yet."""
74,0,http://goodreads.com/user/show/4688703-chavi,3,"This book attempted to be both the story of the first computer and an explanation it, and ended up accomplishing a very confused mix of the two. It's a beautiful effort with a ton of information (and I understand from other reviews, new information) about the building of the first computer, and everything that led up to it, and the lasting impacts today (on the philosophy and structure of computing, not just on the fact of it). But unfortunately, the chronology was confusing, the narrative was unclear, the explanations assumed more knowledge than I had, and the rhapsodizing about search engines as the ultimate -- even possibly thinking -- computers put the other philosophical/prophetic passages in doubt. I don't have another history of computers book to suggest at the moment but I have a feeling that there are far better ones out there depending on your level of previous knowledge and desired depth of knowledge. It's a shame because there's so much great stuff in here. It was just hard to parse most of the time. That being said, there were plenty of themes/questions/ideas/tidbits that got me thinking. Here are a few of those:""The part that is stable we are going to predict. And the part that is unstable we are going to control."" John von Neumann said that, and it's part of a larger theme about how computers were made only once people realized that instead of expecting perfection, they should expect imperfection and account for it. ""There is reason to suspect that our predilection for linear codes, which have a simple, almost temporal sequence, is chiefly a literary habit.""I have no idea if this is true but seems like an interesting thing to think about.""If a machine is expected to be infallible, it cannot also be intelligent.""So like humans then?""Machines will dream first."" Before thinking that is. What a wonderful thought. It brings to mind the silent square nineties computer that sat in my father's office at home, alone in the dark after we all finally went to sleep. Did it dream?""That the resulting human behavior can only be counted on statistically, not deterministically, is... no obstacle to the synthesis of those unreliable human beings into a reliable organism.""This (which comes from the section on the future of computers, or what its first inventors thought of its future back in the fifties) is terrifying. Not only because it suggests a future where humans are controlled by computers, but because it means that now currently, we are already statistics. And the fact that predictions made about us based on human data are predictive but not deterministic is not a consolation. Oh, and the origin story of the Monte Carlo method. It's a great origin story and now I actually understand what it is!"
75,0,http://goodreads.com/user/show/35700497-sheepdog,5,"There are so many reasons to read this book about how we entered the computer age. And there are so many different classes of reader who I think would enjoy it.It is not for the faint hearted. You need an attention span. You need to delight in detail. You need some interest in things computery, but you do not need to be a geek... but if you are, you will get a lot from this that will pass over the heads of others.The writing is superb. In places, it is downright poetic. It really helps you to enjoy what could be pretty dry material. Individual paragraphs are well crafted, and it is clear that the book was ""built"" to a carefully considered bigger plan.Much of it is about ""geek-stuff""... but it also addresses some of the larger issues of what computers have done to society.Much of the book is about electronics and computing from around 1935 to 1955... but the roots are traced, and what's happened since 1955 isn't slighted.Side by side with the story of early computing we also learn about the people behind the revolution. And there is considerable exploration of the Institute for Advanced Studies. What do you know about that? I ""knew the name"", knew it was ""important"", but I must admit my understanding of the IAS was pretty vague! Not so, after reading this book. Worth it, if for that alone! John von Neumann is particularly fully treated... and, having read the book, I can see why. Previously, a bit like the IAS, I ""knew he was important""... I even knew what ""the von Neumann architecture"" is!... but I'm glad that I know know more about this fascinating figure.(Turing has a place in the story, of course, and that is told... but don't imagine from the title that this book spends a lot of time in him and his ideas. The ""cathedral"" is something much more interesting. Read the book to find out what!)A word of warning: If you collect books about the seminal works in any branches of science, then reading Turing's Cathedral will not be good for the length of your wish list.If you are more than usually knowledgeable about things computery, I think you may be amazed from time to time by how long certain things have been a part of the computing world.Reading this book set my brain a-fizz. I have PAGES of notes about things I would like to follow up, try (I do computing and electronics) collect (I collect books), etc, etc.Some books are page turners. This one was so ""rich"" that it went slowly, if only because I was constantly making notes!---P.S: In the months since I read this, while I am no way any less a fan of the book, I have had a growing suspicion that the author places John von Neumann perhaps a little too much at the center of things, and too much on a special pedestal? Now... of course von Neumann made HUGE contributions, and was at the center of things. But perhaps others deserved more mention in the stories? But of course, one book can't tell everything."
76,0,http://goodreads.com/user/show/368522-kim-z,3,"An interesting and highly detailed history of computers. I sometimes found it confusing how Dyson would circle back in time rather than tell the stories chronologically because I would lose perspective of what things were happening in parallel. Still, if you are really interested in origin of computers (and, tangentially, the development of atomic and hydrogen bombs, the mapping of the human genome, biological evolution, or artificial intelligence) this book reveals a lot of intriguing anecdotes."
77,0,http://goodreads.com/user/show/12292735-steve,3,"Like many, I bought this book partly due to the allusion to the role of Alan Turing and partly due to the sleeve notes. Alan Turing turns out to be an apparition that appears at breathless pace in Chapter 13 as we hurtle through his profound conceptual thinking and the inspired work of Tommy Flowers, Freddie Williams and Tom Kilburn. Tommy was the Steve Wozniak of his time, building a robust system based on a efficient and inspired use of readily available components that delivered an extensible and reproducible computing platform, compared to the fragility of ENIAC, yet is dispatched in less than a paragraph. Even in this chapter von Neumann takes precedence and Dyson is unable to provide any insight into the degree of collaboration which may or may not have taken place across an ocean at war. Just as quickly Turing is left in the literary dust as the bromance with John von Neumann is continued. In fact, in the picture section only one image even refers to Turing, a side-face shot of his team operating the Ferranti Mark 1I'll be generous and suggest that the title was proposed by Dyson's editor or literary agent as a rouse to increase sales, for clearly a book about the history of Princeton and the IAS was always likely to garner a smaller readership. As many have said, much of the early history covered in this book is pointless and ill judged in the context of the likely readership, but the rise of academic philanthropy is itself interesting. I simply struggle with the enormous amount of trivia that is worked in enormous detail, given the confinement of Turing to a single chapter. There are parts of this book I enjoyed enormously. I can forgive many of the technical errors but not the failure to correctly define the von Neumann architecture and its limitations. The description of the Williams-Kilburn tube memory is confused and bizarre given its influence on so many early systems. Many 'characters' are sacrificed in the story, including the true role of University of Manchester in developing early von Neumann machines some three years ahead of Princeton, or the role of Australia's CSIRAC, but then this book was never really about the early history of computing or Babbage would surely have warranted a mention, amongst others.The positives for me include the range of characters exposed as powerful contributors to the story that is ENIAC and, less so, Colossus. The character viewpoints as mini biographies would also be interesting if, as many have said, the relative timelines had been pulled together by the author and not left to the reader. I would also stop at the end of Chapter 14. You are done. The balance of the book is beyond the author."
78,0,http://goodreads.com/user/show/2458054-frank,4,"This was a very interesting history, but it sure could benefit from some careful editing. There are the small things, beginning with the title. The book only deals briefly (too briefly!) with Alan Turing. A more representative title would have referred to John von Neumann or, perhaps, a history of the Institute of Advanced Study. Then there is Dyson's annoyingly equanimous treatment of the great (foundations of mathematical logic, beginnings of the computer, fusion inside stars and over the Bikini atoll) with trivia about the effect of new redhead in the Institute typing pool. Dyson's analogies (google telling us what to think; our unrecognised domination by electronic life forms) are provocative, but sometimes seem strained.Nonetheless, Dyson has written a fascinating and very readable account of the times, some of it little known, uncovering a wealth of background and conveying it as if he had been there (to an extent, I guess he had been). About Alan Turing, for instance: his groundbreaking work in cryptography and decision theory are well known, as is his suicide; but his very precocious work in artificial intelligence much less so. And then, even as someone who experienced Princeton physics, I was never aware of the very close connection between the Institute and the effort to build atom and, later, hydrogen bombs. If the account of this book can be credited, Princeton's luck with the institute is only comparable with their obliviousness in recognising what they had in hand. Dyson tells us how a gift from a retailing magnate arrived within months of the Nazi racial laws; how, more or less by accident, the decision was made to establish an institute of mathematics just as Göttingen was closing down, so that the then intellectual backwater of Princeton pulled in Einstein, von Neumann and Weyl as the first IAS fellows. (They had enough funds for just three appointments. You could say they were lucky with their selections!) Take von Neumann, for instance. He can reasonably be credited with giving the US nuclear weapons (=military superiority), game theory (=economic hegemony) and computers (=technological preminence). I'd estimate his contribution to the US economy in the range of ten trillion dollars. But few Americans know his name.We learn about the niggardly treatment of the engineers and scientists (assignment of sleeping quarters on office floors, administrative harassment if you took more than two sugar packets for coffee!) who were at the same time giving the US the technology that let them rule the world. Also, about the IAS turning their back on these men, and abandoning the world's most advanced computers once the war ended."
79,0,http://goodreads.com/user/show/29766869-collin-oswalt,3,"It's called Turing's Cathedral, but Alan turing is only relevant to about 10% (maybe even less) of the book. It really ought to be called ""Von Neumann's Cathedral"" or ""Julian Bigelow's Cathedral"" or ""Stan Ulam's Cathedral"" because these are the three principal characters of the book.The book discusses the early years of the development of computers at the IAS in Princeton, New jersey, as well as the lives of the men and women who contributed. The book isn't broken into any chronological structure, except near the end when things progress past the 1940s into the 1950s. However, each chapter keeps jumping back and forth from the 1930s to the late 1940s, only occasionally foraying into the 1950s. Alan Turing is mentioned only a couple times until the last 100 pages of the book, in which an entire chapter of him is dedicated really to his work on artificial intelligence.Each chapter can be categorized as one of two things: an early application for computers, or the biography of someone who worked on early computer science, or both. For example, one chapter is about how meteorology was revolutionized by computing power, and how previous mathematical attempts to predict the weather lacked both the instrumentation and computational power. Another chapter is a biography of Stan Ulam, and another on Von Neumann. One chapter discusses Nils Baricelli's early attempts at modeling evolution using computers and objects who lived or died based on program parameters and their randomly selected bits (representing genes). This chapter was both discussing how computers revolutionized biology and a biography of Nils Baricelli.The book is, no doubt, informative. The two things I found annoying is that 1. The book had a sticker on it at Barnes and noble saying ""read about it then watch it!"", in reference to ""The Imitation Game"", a biodrama starring Bumblebee Cabbagepatch about the life of Alan Turing, though the book had VERY little directly to do with Alan Turing and 2. that the book took no chronological order at all.A better example for the organization of a book about the history of technology is Jon Gertner's ""The Idea Factory"", which chronicles both the lives of the men who worked at AT&T's Bell Labs and the technologies they created, in an orderly and chronological fashion.I would characterize the writing of this book as chaotic and disorganized. DO NOT buy this book if you are interested in the life and work of Alan Turing. Rated 3 stars for information, but -2 for poor writing skills and a misleading title."
80,0,http://goodreads.com/user/show/9731893-ken-mattes,5,Wonderful account of the work of John Van Neumann and the beginnings of computers in the US and at Princeton University.
81,0,http://goodreads.com/user/show/5241251-andrew,3,"Alan Turing was a brilliant British mathematician who predicted in 1936 that ""It is possible to invent a single machine which can be used to compute any computable sequence."" This prediction laid the foundation for the digital revolution that has blossomed in the last half century, but it would take the events of World War II to drive the efforts to actually build such a 'Turing Machine'. Two disparate mathematical problems required the increased computational power and flexibility of such machines: weather forecasting and the design of nuclear and thermonuclear weapons.This book profiles the mathematicians, engineers and others who participated in building the first truly ""universal computing machine"" at the Institute for Advanced Studies at Princeton, New Jersey, and their efforts to design and build the machine. Coming from a wide range of backgrounds, these men and women worked to build the first programmable computer with an addressable memory. In the process, they broke the distinction between numbers that meant something and numbers that did things, namely the distinction between instructions and values. In the process, they launched the digital age.Every computing system in the world is build upon the same principles underlying that first machine at the IAS. The final chapters of this book explore the implications of these digital computing systems as they continue to evolve into ever more sophisticated systems. The book also explores how the discovery of DNA and the efforts to decode our genetic codes parallels the development of digital computing, and speculates on the symbiotic nature of the evolution of digital evolution and our own future evolution.This book includes a mountain of interesting historical information about the dawn of the digital age, and its roots in the development of atomic weapons, especially. However, the narrative jumps around a lot, hopping backward and forward in time as it explores the contributions of the different people involved, which makes it a bit more difficult to truly grasp the entire picture. In addition, for a book that starts out as a historical examination of the development of digital computing, the later diversions in to speculative explorations of the nature and meaning of the evolving digital universe feel a bit extraneous. Perhaps there was enough material for two different books? All-in-all, I'd have to call this a mixed bag. "
82,0,http://goodreads.com/user/show/3536108-cailan,4,"fascinating, especially towards the second half of the book. The earlier sections require some prior understanding of Turing machines and an engineering background to really make sense of all the information (and truly detailed explanations of the crucial concepts dealt with in this book aren't really present - which I found a little frustrating, but I guess I'll be doing some further research on my own now), and i felt the material about the formation of Princeton was dry. But the later chapters of the book detailing the stories and theories of Ulam, Von Neumann, Turing, Barricelli and Bigelow, and their respective takes on what their work meant and where to take further research in any particular field was amazing. I don't know that I've ever been exposed to such a wide array of intriguing ideas while reading a historical account before. There are musings on automata as represented by genetic sequencing and cellular reproduction, tracing the path from logical theory through early computing implementations and the ensuing microprocessor revolution to how the internet represents analog computing more than its digital underpinnings, a speculation on the nature of extraterrestrial life (One of the most interesting takes I've ever heard. Not sure if the author was taking liberties and weaving his own speculation in with his side material, but still... ).Ulam and Von Neumann's intertwined stories in particular I really enjoyed. I come away with the sense that Von Neumann's sudden succumbing to cancer in the late 50s might have robbed the world of one of its brightest minds way too early, and despite all the technological advances in the years since we have never really expanded computing beyond his ideas. If there is failure in this book, aside from the fact that technical details of the early computers are vague and hand-wavy (and I was practically begging for technical details for most of this read), it might be that it's a little too reverent of some of its subjects.Anyway, aside from me getting maudlin over how awesome of a theoretician and person Johnny Von Neumann was, this book is with the effort and early blandness. Highly recommend. I want to see this material spun into a Ken Burns documentary."
83,0,http://goodreads.com/user/show/4282067-liam,4,"""A window of liberalization had opened following the 1867 Compromise with Austria, and closed with the rise of Bela Kun and the late-1919 counterrevolution, under Admiral Horthy, that brought in the 'Numerus Clausus,' requiring that university enrollment reflect the composition of the general population, effectively returning to a quota against Jews entering academic and professional life."" (43)""Although happy in small, nondescript offices, von Neumann liked large, fast cars. He bought a new one at least once a year, whether he had wrecked the previous one or not. Asked why he always purchased Cadillacs, he answered, 'Because no one would sell me a tank.'"" (54)""Once the bugs were worked out, 'the computation time for a 24-hour forecast was about 24 hours, that is, we were just able to keep pace with the weather.'"" (quoting Jule Charney, 167)""The brain is a statistical, probabilistic system, with logic and mathematics running as higher-level processes. The computer is a logical, mathematical system, upon which higher-level statistical, probabilistic systems, such as human knowledge and intelligence, could possibly be built. 'What makes you so sure,' asked Stan Ulam, 'that mathematical logic corresponds to the way we think?'"" (278)""Thirty years ago, networks developed for communication between people were adapted to communication between machines. We went from transmitting data over a voice network to transmitting voice over a data network in just a few short years. Billions of dollars were sunk into cables spanning six continents and three oceans, and a web of optical fiber engulfed the world. When the operation peaked in 1991, fiber was being rolled out, globally, at over 5,000 miles per hour, or nine times the speed of sound: Mach 9. ... The 'last mile' problem -- how to reach individual devices without individual connection costs -- has evaporated with the appearance of wireless devices, and we are now rolling out cable again. Global production of optical fiber reached Mach 20 (15,000 miles per hour) in 2011, barely keeping up with the demand."" (300)"
84,0,http://goodreads.com/user/show/10361209-dan-downing,5,"Background: Alan Turing was a British mathematician who outlined the possibility and architecture of a Universal machine---in 1936. He also made what can be described as war winning contributions to coding and cryptography during WWII (being awarded the Order of the British Empire --secretly); significant contributions to other areas of mathematics and science and computer building, until 1954 when he was either killed by British agents or committed suicide, either alternative a result of being found out as a homosexual. In 2009 the British government ""apologized' for his mistreatment. I'm sure he was thankful. (As an aside, I think all anti-homosexuals who use or benefit form any computer device, from the internet to FRIDs, are vile hypocrites. Our modern digital world is Turing's Cathedral. To reap the benefits while reviling the benefactor is horrid.)Beyond that biographical note, this book really revolves around the most unsung genius of the 20th Century. In some ways it is a better biography of John von Neumann than his biography is. Besides Johnny, as his friends called him, the book introduces us to most of the greatest mathematicians and scientists of the century. These biographies are necessary to describe how Turing and von Neumann and Einstein, all at Princeton's Institute for Advanced studies before the World War, worked together and led the world to computing, which, among other things, was absolutely needed to make the atomic bomb.For me, this was a romp through computer history with many old friends, which Dyson enlivened with new anecdotes and perspectives, while making the science and engineering (dirty word at IAS) understandable.Definitely an informative book, providing perspective, history and humanity laced with humor to the story of what might someday be our successor as the dominant life form on the planet. Or in the galaxy.Highly Recommended."
85,0,http://goodreads.com/user/show/41132825-dennis-swanger,3,"This is a dense book about the initial development of computers at Princeton University's Institute of Advanced Study (IAS) in the years following WWII. Alan Turing, a minor character in the book, may have had the initial conceptual inspiration for the digital computer, but it was the mathematical genius John von Neumann, and the team he assembled at the IAS, who actually designed and built the first computing machines. These early computers were primitive, grindingly slow, and almost devoid of memory (a few kilobytes). The first problems they worked on, modeling thermonuclear explosions, required running the computers more-or-less continuously for weeks at a time to obtain useful, but highly simplified, models.George Dyson is an excellent writer; his first book Baidarka is one of my all-time favorites, but I found this book to be a mixed bag. Some chapters were fascinating while others were pure drudgery. The wearisome parts of the book focus on descriptions of facilities, the engineering of the machines, and frequent digressions to follow extraneous narratives that only marginally enhance the story. The best parts focussed on the people and the development/programming of the computers, e.g., John von Neumann's penchant for driving his Cadillacs cross-country several times each year to visit scattered colleagues (Los Alamos was a favored destination), Klari von Neumann's single-minded running of the early computers day-and-night to collect data, the inspiration to incorporate Monte Carlo game theory into the early programs enabling them ""to extract meaningful solutions in the face of overwhelming information"" --- all modern-day search engines still utilize this technology. Eventually, this disparate band of people came to recognize that ""meaning resides less in the data at the end points and more in the intervening paths."" "
86,0,http://goodreads.com/user/show/6119641-christina,3,"As a software engineer, I was excited to read Turing's Cathedral which is a comprehensive examination of the beginnings of the computer in the early to mid 20th Century and the digital universe. I was pleasantly surprised by the inclusion of accounts of the contributions made by women. While extensive research is apparent, I do not feel that the facts were organized logically. This is a history of computing but jumps from subject to subject, giving a history of each category and sub-category. This would be OK, but I felt that the author was prone to random ramblings away from the current subject of a chapter. My favorite chapter was 13, Turing's Cathedral (chapter and book name). Here, Mr. Dyson offered a biography of Alan Turing's life. I also enjoyed learning about John von Neumann as well. I would have liked to have learned even more about these two gentlemen and their important roles in WWII. However, their roles may still be classified.I enjoyed some of the questions that were raised in this novel, and asking my own questions. Computers developed considerably during WWII in correlation with the work on the bombs and code breaking. If it was not for WWII, what kind of computers would we have today? Also, both Turing and VonNeumann died relatively young. Turing committed suicide by eating an apple laced with cyanide and von Neumann was taken by cancer in his 50's which may have been a result of attending the atomic bomb tests. If these two men had lived 20 or 30 more years, what kind of advances and contributions would they have made? We may never know the answers to these questions, but Turing's Cathedral brings these questions into the spotlight as well as those who made significant contributions to the early computing community."
87,0,http://goodreads.com/user/show/22906182-kat-steiner,4,"I really enjoyed this book as my first foray into the history and origins of modern computer science. Its style, which is thematic rather than chronological, did make it hard to fit together the various themes in a coherent timeline, but it was great at giving flavour and story to the various people and ventures Dyson discussed. The title is very misleading: the book is much more about Von Neumann than Turing, and although Turing's 1936 paper is much mentioned, the action of the book is really the 1940s and 50s, with the first electronic, digital computers at the Institute for Advanced Study and the Aberdeen Proving Ground.I found the level of detail about the original few computers' architecture fascinating without fully understanding every description (or needing to), and as a maths graduate, the maths and physics weren't too scary. It's a bit intense in places, but really this book is about people's stories, and it's worth it for them, because you almost certainly haven't heard of most of the people discussed.I have no idea how good the discussions of early computer programming were, as I have nothing (yet) to compare it with - at times, I felt Dyson strayed more or less explicitly into science fiction and rather silly pseudo-evolutionary speculation (are machines evolving to control us and replicate themselves, or vice versa). This was fine, I just took it with a pinch of salt as not-quite-philosophy and got back to the history.I really recommend this book for people involved in computer science now who would like to know someone's view of how it all began, but don't want to go back to Babbage and Lovelace. It's moderately technical, so needs some dedication, but the writing is engaging and not too dry, and I very much enjoyed it."
88,0,http://goodreads.com/user/show/4606766-scott-johnson,4,"For the most part, this was very enjoyable. I genuinely did not know a lot of the stories told in this book.Thankfully I was already familiar with things like Godel numbers, because the author does gloss over a few topics like that without any real explanation. At the same time, there was way too much detail about a few things, particularly people.That was the one big flaw with this: There was an enormous amount of unnecessary fluff. The entire second chapter could be deleted and the book loses exactly nothing. The same can be said about a lot of the random anecdotes and backstory about many of the people involved; I don't really need to know about every bowel movement Edward Teller ever had.That said, it was a great summary of a lot of the birth of the computer. It's a complex topic that has a lot of parallel threads to follow, and the author does that well. I very rarely found myself wondering who someone was or what year we were in.I do wish there was more detail regarding the actual science. I'm still on my quest to understand computers, and this did do a little to help that -- I now understand what the hell vacuum tubes were and why/how they were used, I couldn't imagine any way they'd be useful on my own, and the best line in the book described a monitor display as just a visual memory...related to vacuum tubes and old CRTs being used for memory devices -- but I think the book would have benefited from being more technical. The audience this is going to attract in the first place is going to be advanced enough to handle it, you don't really have to worry about scaring off your readers by being too technical.Overall, an enjoyable read, just replace some of the useless fluff about people with more details about the science."
89,0,http://goodreads.com/user/show/28996386-barrett,3,"Dyson's history of the dawn of the computing age, Turing's Cathedral, is undeniably well written and thoroughly researched. Unfortunately, it's almost incomprehensible. This is a book that I wanted to enjoy but I really just couldn't. I enjoy computers, I really do. They're certainly a marvel of engineering, and I would love to understand how they were developed and how they work today and how they might further evolve in the future. Dyson did a pretty good job of explaining the conditions leading to their development, and how the numerous collaborators gathered into place to undertake such a historically significant project. He just didn't really explain how the computer actually worked. He certainly made attempts, devoting many pages to a discussion of the hardware and design features of the computer, and why those specific features were chosen. And maybe a more intelligent, more knowledgeable person than myself would have understood that exposition. Nevertheless, much of the technical descriptions in this book remain incomprehensible to me, and I am pretty certain I am not the only one. Of course, that's not to say the book was all bad. Dyson's description of the events and characters involved in the development of the computer are pretty interesting. I definitely learned a lot more historical knowledge than scientific knowledge.I could certainly imagine this book being an enjoyable read for people who already have extensive knowledge about computer science and electrical engineering, but I imagine most people without much knowledge in these fields will be as much in the dark as I was after reading this book."
90,0,http://goodreads.com/user/show/44711152-carl-svensson,3,"I really appreciate the historical research that went into this. Dyson's look into the personalities around the IAS's project, what people were doing, the importance of the environments they were working in, how they funded it, the personnel issues they had, and all this plethora of background behind the project is just great. It'd be really easy to just look at what von Neumann was up to and what he did and go 'Wow this dude was really on point.' I mean, this book does that, but there was an entire infrastructure which was essential to getting von Neumann and everyone else together to do what they did. I like that aspect of the book. But I echo other reviewers in their charges that Dyson just is not competent at describing the technology. Lord knows I'm not a computer engineer, but I've read books on Boolean algebra and basic computing architecture, so I'm not flying utterly blind here. I'm not sure what the issue is exactly, but it feels like possibly he's explaining things in a heterodox way. Dude really loves the term 'digital universe' and will just sling that around without really explaining the significance. Yeah, the punch cards in that drawer were like 40% of the digital universe in 1947. I get it. There were not a lot of digital computers in 1947. It seems like something like that was stated three times a chapter. All the tech stuff feels, very precise in very specific areas, but other areas are not touched on at all. It just doesn't give the full picture.And I have no clue what the hell is going on in that penultimate chapter. Search engines mean we're going to analog computing? What?So, yeah mixed bag."
91,0,http://goodreads.com/user/show/12186160-melinda,5,"I thought this was an excellent book of huge scope and am frankly puzzled by some of the negative reviews on here - I also wrote code in the 70s but I am still in the field, not writing romance novels ... But anyway, the author brings alive the men and women (yes, there were women) who built the IAS machine to do hydrogen bomb calculations and what their lives were like. It shows us the nimble mind of the Hungarian Wunderkind John Von Neumann and the circle of people around him. It covers the various machines that were all starting at roughly the same time (spurred by war applications) and discusses how JVN came to assimilate all the ideas and came up with the machine that was paid attention to and replicated. The last three chapters are speculative and provocative, pushing the reader to think about where the ""digital universe"" may be going with its pace of ever-increasing sequence, rather than steady-state time, the world that humans live in. But be prepared - this is not an easy read. It is not chronological, and the chapters jump around, with each centered on a theme. However, if you are a serious student of computing and give this book the respect it deserves, it will enrich your perspective. The author had access to amazing new sources, even (if you watch his book talks on line) the personal letters of von Neumann dusty boxes of archives from the Institute for Advanced Study. Kudos to Dyson for saving this unique piece of computer history ""just in time"" while his sources were still around to share their perspectives. Beautifully written. "
92,0,http://goodreads.com/user/show/5138376-margaret,4,"George Dyson's Turing's Cathedral summarizes the history of early computing, especially John von Neumann's involvement with the Institute for Advanced Study in Princeton. While von Neumann was interested in and developed the basis of game theory, his major contribution to the modern world was his leadership of the IAS computing group during the mid-20th century. Shock front calculation to simulate the ignition of a hydrogen bomb was the problem that the first computers were built to solve, but the applications quickly extended to meteorology and biology. The talent of the mathematicians involved in the project was incredible while their collaboration may have been shaky at times. Dyson is very thorough in his treatment of the personal histories of those involved as well as the history of the IAS itself. His treatment is not completely linear at times, though the insights that he provides are interesting and valuable. Dyson interviewed many of the living members of the IAS computation group, including his father, Freeman Dyson. One of my favorite inclusions of Dyson's were passages from the MANIAC computer logs, where the frustrations of programmers are more towards failing vacuum tubes in addition to mistakes in code. While the hardware has changed greatly, the frustration in trying to get a program to run has not changed. Overall, this insightful history of modern computing sheds light on how the modern digital age began and the colorful mathematicians that started it while presenting it all in a highly readable and interesting way. "
93,0,http://goodreads.com/user/show/29963242-william-korn,2,"I fell into my 40-odd year career as a programmer/analyst essentially by accident, way back in 1967 when computers were huge and their capabilities small. Because it was accidental, I did not spend any time to speak of learning about how computers, particularly programmable computers, came to be - I was too busy programming them.So when in my dotage I came upon this book I thought, ""Hot puppies! Turing! von Neumann! The Institute of Advanced Studies at Princeton! Freeman Dyson's son! WOW!!"" I'm sorry to say that this book was a sad disappointment.To be fair, there were some very interesting portions of this book: attempting to explain what Alan Turing accomplished (a difficult task for Dyson, as one thing he isn't is a programmer); how the folks at IAS helped to turn Turing's work into a functioning computer; and the personalities of some of the great names of that age.But far too much of this book deals with the history of the IAS and the academic politics endemic at big university research institutes. These are subjects of interest only to Princeton alums and those folks that enjoy doing academic politics (there must be a lot of them, since there is so much politics in the Academy). For those interested in what this book was supposed to be about, it was a massive waste of time. If this book was cut to one-third of its current size, I might bump it up to a 3-star rating, the only remaining issue being Dyson's infelicity at writing a coherent narrative. Otherwise, I commend myself in a perverse sort of way for finishing it."
94,0,http://goodreads.com/user/show/1868162-nick,3,"Alan Turing is being masticated by the popular culture machine, what with the Cumberbatch movie and various other efforts to elevate this somewhat forgotten mathematical genius whose work in breaking the Nazi Enigma machine code led to an Allied victory. And, for even the most dilettantish computer geek knows that it was Turing who offered the most coherent theory of the programmable computer, first envisioned by Leibnitz, and, in the story told here, actualized for the first time by Von Neumann at Princeton's Institute for Advanced Studies, in large part to provide the sheer number-crunching required to create the hydrogen bomb (Oppenheimer directed both the Manhattan Project and the IAS). This book is quite detailed, and I confess, I drove in a somewhat pleasant haze as I listened to the explication of the computational challenges presented and solved along the way to the creation of various machines. But this was a small price to pay for the richness of the biographical storytelling. Dyson is particularly good in blending primary and secondary sources into a coherent story whose key actors came, literally, from all over the world for the chance to work with the most brilliant computer scientists (and meteorologists, engineers, and others) at a time when the field was being validated as both an academic and applied discipline.BTW: I was amazed to learn that Turing himself was working on the computer problem in the UK and for a brief period in Princeton with Von Neumann. "
95,0,http://goodreads.com/user/show/7226831-josh-friedlander,2,"Story of the invention of the ""digital universe"" - spanning from personal computers to computational methods in the sciences: weather prediction, the development of the A- and H-bombs, and so much more. It's a shame that Dyson's narration is erratic, leaning too heavily into academic politics (the high-minded humanists of the IAS in Princeton resented the presence of engineers [government-affiliated, to boot] in their midst), and then skimming over some of the complex ideas discussed too quickly, at least, for this reader to follow. Some of the ideas are astonishing. For example: modern computers use an archaic address architecture based mainly on the arbitrary specs of early computers. Could a more efficient and sophisticated computing follow nature's methods - such as the RNA system which commands a protein to move a certain number of steps, without being tied down to a location? (At least I think that's the gist of it.) Other ideas are quite airy. Dyson extends the biology-electronics metaphor, suggesting that today's apps form a kind of symbiotic lifeform transferring digital knowledge from humans to the growing digital mind. (I think that the cathedral of the title is a reference to the Googleplex, where, as labradors leap through fountains and unnervingly happy people wander around, our minds are slowly fusing with the Singularity.) This book relates what the greatest minds of the last century were doing during a thrilling period of progress, the ramifications of which are still rippling through our world."
96,0,http://goodreads.com/user/show/13459886-yhc,3,"If you are looking for Alan Turing's story, the title of this book is misleading. I bought the book with great expectation of getting to know Turing, but more than that it's actually surrounding by the scientists of Project Y (Manhattan project ), centered by John von Neumman. Of course it was during the WWII, the unclear bomb was under development but at the same time, somehow computer taking shape. This book is more than a history book with a lot of info provided by Klara Dan, Neumann's wecond wife. It was not an easy book for me because i don't know how computer was built, i am not an engineer as background, but it surely trigger my interest to dig in more about the history and get to study some certain scientists mentioned in the book.Just a thought that if not with the war happening at that time, would we have fully power to develop the computer so that we are already so advanced maybe 50 years or more (since computer power grows exponentially), would we already reach next star? (in the end chapters of the book, we look forward to understand universe...) Or would we even have the pressure to develop whatsoever? Pressure from cold war pushed human to get out of earth into space, now we don't seem to be in a hurry to get back to explore. Wars are destructive, but at the same time, war push human capability beyond boundary. Highly recommended for those who have engineer background to read it, you will get more fun than someone without enough knowledge in this field. "
97,0,http://goodreads.com/user/show/22876109-leland-william,3,"I originally picked this book up thinking it would be a detailed history of Turing's accomplishments and their contributions toward science and society. I probably should have read the book jacket... Turing's Cathedral centers not around Alan Turing, but John Von Neumann, a Hungarian born mathematician who in 25 years rose from emigrant to Washington Elite. He made invaluable contributions to the construction of the first modern computer, the atomic bomb, game theory, the theory of self-replicating automata, and kinds of infinities. Obviously, the man was incredible, and he is largely responsible for creating the environment at the Institute of Advanced Studies in Princeton, New Jersey that produced one of the first computers. The story of Turing's Cathedral is set in woody Princeton, and it follows the many characters who helped contribute ideas and engineering expertise to the design of the MANIAC (the acronym for the Princeton computer). In its grasp of history, this book is very well executed. George Dyson (son of Freeman Dyson) weaves an excellent story. Unfortunately, his artistic flourishes are somewhat distracting when he is describing the engineering and physics behind the construction of the MANIAC. I left this book with a sense of who the players were that contributed to the marvelous machine, but not much technical understanding of what went on. Nonetheless, the book was an enjoyable read. "
98,0,http://goodreads.com/user/show/10404161-alex-shrugged,3,"""Turing's Cathedral"" by George Dyson is a competent biography of several of the leading creators of the digital age, mostly Von Neumann. I enjoyed it but I also wondered how the average reader would receive this book, without the background of having lived through a good portion of the digital age's creation.This book runs through a number of key events that are the foundation of our digital age, but the narrative doesn't give those events enough context. I have context since they were part of my reading (and experience) as I took Computer Science courses in college, but I don't think the average reader would know, or realize the implications of what an ""IBM Verifier"" is, why it would be needed, and the effort it would take to use it... as one of several examples. And even realizing it, would they comprehend why that effort was better than what they had been doing before... employing women to do the calculations using paper and pencil... calling these women ""computers"" since that was what they were doing... computing. These men tried to draw a distinction by calling their electronic devices ""computors,"" but the word never caught on.This book is worth reading if one wants to get the feel of the people involved. The character of the people is revealed, but not the events that they precipitated in. The lack of context means that you will have to seek out that context elsewhere.OK... I doubt I will read this book again but I'm glad I read it."
99,0,http://goodreads.com/user/show/3115431-topherjaynes,5,"I was maddened by the depth of research that went into this book. I mean really? Who has time for that? The detailed narrative of the ""birth"" of the computer around between the two world wars was beyond captivating, but that might be me nerding out. John von Neumann is the center of this amazing world of Mathematicians and Physicists that not only create the atomic bomb, but the computer. The overlapping of world renown people is staggering: Einstein, Richard F Feynman, Bohr, Oppenheimer, Kurt Godel, Claude Shannon, and of course Alan Turing. How does one get into these circles? My favorite antidote is von Neumon chasing his soon to be wife around Europe, but gives her space and stays with Niels Bohr (the guys we basically had to take chemistry because of in HS).Turing actually has little to do with the book, but the one quote that sticks out to me was:""Before Turing, things were done to numbers. After Turing, numbers began to do things. By showing that a machine could be encoded as a number, and a number decoded as a machine, "" On Computable Numbers"" led to numbers (now called software) that were ""computable"" in a way that was entirely new.""Dyson's book makes me very both thankful for my class in ""Computational Theory"" and regretful for not paying more attention. Amazing read that isn't necessarily for the hardcore nerd, but follow a fascinating birth of technology and math that hasn't been seen before or since in history. "
100,0,http://goodreads.com/user/show/87605701-jamin-chavez,4,"Some of the most important reasons why there was a push for advances in computing are in this book. ""Turing's Cathedral"" is a very interesting read. For somebody who is interested in computers and the history behind them, I was surprised throughout the book. I learned some history I never thought was associated with computers. The book gives a historical explanation behind not only advances in computing before and during World War II but also gives insight into the lives and characterizations of the computer and electrical engineers, scientists, and physicists behind computer developments and innovation. The beginning and mid portions of the book paint a good picture of a lot of historical aspects and explanations for occurrences. The reader might wonder why some of what they're reading is pertinent to Allen Turing. However, even though the book's title is ""Turing's Cathedral"" and the reader might think it is solely about him it is important to note that Turing was, basically, at the base of everything anyway. Everybody around him benefited from his intelligence and insights into computing then, artificial intelligence, and computing as we know it today. I believe the author has done excellent work in matching the title of the book with the overall aspects and pictures he paints for the reader. From reading this book I'm sure the reader will see, truly, how Turing really did create a cathedral. "
101,0,http://goodreads.com/user/show/8595498-sam,3,"This is a mighty work from a mighty brain, about some of the mightiest brains of the twentieth century. In spite of the title the central figure is less Alan Turing than it is Jonny Von Neumann, the Hungarian-American mathematical polymath who in addition to playing a central role in inventing computing, also co-invented game theory, and had a major role in the nuclear program at Los Alamos.It works as an intellectual and technology history. In addition there is a philosophical question at the book's heart. The author, whose father worked alongside many of the scientists and mathematicians covered in the book, was inspired to write this history as he sees digital technology as an emerging ecology, and programs and apps as ""numerical symbial organisms"" - as life. That;s a pretty massive question, and no doubt that there are others who would ague the case against Dyson as eloquently as he argues his - something which I for one am completely unqualified to do. It's very readable and accessibly written (luckily for me) but I would still advise people to take this on when they have time to read for an hour or more at a time. The subject matter means it may be pretty heavy-going if you try to get through it 20 minutes at a time while on the subway. That said, I strongly recommend it - it's thought-provoking stuff."
102,0,http://goodreads.com/user/show/2336309-judy,3,"I stand in awe of the geniuses who envisioned and constructed the digital universe--largely because I haven't a prayer of understanding what they did. Although written in plain English, somehow my brain will simply not grasp the concepts. I suspect this comes from not only being math deficient but also math phobic too. If you don't have that problem, this is a weighty account of John von Neumann's group at Princeton's Institute for Advanced Study. The part that was easy for me was the academic rivalry/snobbery that, oddly enough, didn't derail the project in its early stages. Theoreticians and engineers don't mix--literally. The author grew up at IAS. His father was on the faculty which explains why a book with Turing in the title, is really about von Neumann's achievements building on and branching off from the Turing machine. This book is an important contribution to history of digital computing and communication. However, it doesn't tell the story I wanted to hear. During and after WWII, small enclaves of brilliant to genius level individuals devoted themselves to building, developing and advancing the computer. IAS was only one of many. Where is the book that will tell us about all of them and explain what, if anything, emerged from each of them that contributed to the overall effort?"
103,0,http://goodreads.com/user/show/3012739-bob,5,"I was drawn to this book by its title & cover design. ""Turing"" in the title plus the punched cover directly meant (at least in my own mind) that it was about Alan Turing and the Universal machine.Before this book I knew little about Turing's universal machine and the origins of the computer. I knew about Von Neumann only by the name.After reading this book, I got more interested about computers. I now have an awareness about how powerful the computer is (especially in our times) and how inefficient we (or at least me) are using it. Now I know the origins of the ENIAC, MANIAC & their derivatives. Know I know what an ""app"" was like in the 1950's.Turing's Cathedral (still not sure why ""Cathedral""!; maybe referring to the computer as the cathedral?) is an exciting read especially for the computer enthusiast, mathematicians, physicists, and scientists in general. It is eloquently written and describes things in details.What I liked most about it is that it has references to actual scientific papers written the creators of the computer. As a matter of fact I have selected a couple of papers too read.I finished the book without even knowing it. It suddenly stops without prior notice, as if there is a continuation that has been cut.I really enjoyed reading this book and recommend it for everyone, literally everyone."
104,0,http://goodreads.com/user/show/9675792-eugene-miya,3,"This book is not an easy read for some. George does not write in a linear chronological narrative, but he does write from the unique perspective of a person who was present or near present to some of the figures at Princeton at the Institute for Advance Study (IAS). He basically paints a picture of life at the IAS.I know first hand some of the sources Dyson touches upon. George has a tendency to chronologically jump back and forth on his subtopics, and this was a criticism I received from my English teachers growing up. His writing is locally consistent. I did find a few real typos like negative degrees Kelvin (he meant Celsius).The book is a collection of chapters of men (and a few women) responsible for computing at Princeton as we know computing today: mainly John von Neumann, my favorite in Stan Ulam, but also Richardson, Nick Metropolis, and lots of others little written about. All these guys (most have their own other books or written about by others) cover with Turing's Cathedral.But the book has a great quote which I must add from one of his last chapters, which I must add to this review (delay of a day, I forgot to bring the book in): Chapter 9 titled Mach 9, starting quote:No time is there. Sequence is different from time. --Julian Bigelow, 1999.And this explains to me George's writing."
105,0,http://goodreads.com/user/show/17726446-ryan-sloan,3,"This was a good book on the history of the IAS, MANIAC, and the important contributions of John von Neumann and others to computing. I enjoyed reading it, and it was great to read some more humanizing stories of computing legends. That said, there were a few things that l didn't love.* I have a background in Computer Science, so this wasn't usually a problem for me (though for some older technologies I did have to go read up!), but the way Dyson glosses over some important details about the tech could make this hard to understand/appreciate for someone who isn't familiar with computer architecture.* The book sort of jumps around in time in a way that makes it hard to piece together the timeline. Even without adjusting the sequence of chapters, something as simple as a ""Timeline"" page in an appendix could have helped.* Dyson provides some commentary on the future of technology that is interesting on the surface, but shows a pretty serious lack of understanding when you look at it in greater detail. There were lots of statements that were clearly designed to sound profound, but unfortunately not all of them were profound.I'd recommend it if you're a computer geek who's interested in the history of computing, but if you don't have a technical background there are probably better places to start."
106,0,http://goodreads.com/user/show/7609801-kevin-o-brien,4,"This book is for someone who is interested in the the history of computing, and in particular the early days around WWII when many of the basics were set. Although the title references Turing, and he does receive some attention in this book, the focus is more on John von Neumann and the group at the Institute for Advanced Studies at Princeton University. And of course the author is the son of Freeman Dyson, who is a prominent scholar at IAS, and that may have helped in getting access to archival materials.John von Neumann created the basic architecture of computers as they continue to this day, so the decisions he and his colleagues made still inform our use of computers. And along the way they created many techniques. One in particular that I was interested in was the technique of Monte Carlo simulation. As someone who has taught Statistics to college classes, I was familiar with this technique, but didn't know the whole history. It was developed as a way to model, in a computer, the results of atomic bomb explosions. And it is clear from this book that the driving force in the development of computers at this time was nuclear explosions. But the book also looks that artificial life experiments and other excursions, even if they were not the main driver. If you like this sort of thing, it is worth a read."
107,0,http://goodreads.com/user/show/3248359-justin,4,"This was a great book... the history of 'Automatic Computing Machines' as told from the point of view of the Princeton Institute for Advanced Study (the 'Princtitute'). The book mainly focuses on the life of von Neumann, who was the spiritual leader of the advent of computing machines, as well as Alan Turing who showed theoretically that it is possible to create a machine that - according to Wikipedia ""can be adapted to simulate the logic of any computer algorithm"". The book also checks in with E. Teller and S. Ulam, and recounts the invention of the Markov Chain, which the author (G. Dyson - Freeman Dyson's son) claims is one of the greatest 'thought inventions' of the 20th century. At times the book gets bogged down with Dyson's excursions into 'evolutionary algorithms', and doesn't really provide enough detail for a reader to easily follow. There were also some sections on evolution and natural selection - as applied to computer programs - which were a bit strange and probably not directly relatable to biological evolution. Other than that, a fascinating book on a group of men and women (some of the first computer coders were women) who had enormous impacts on the shape of our modern world."
108,0,http://goodreads.com/user/show/4818808-nicholas,5,"As attempts at ""decoding something"" go, decoding the origins of complex computational machines is an interesting task to take on.As attempts at ""decoding something"" go, this one was highly successful. It is helped greatly by the fact that the author grew up at the Institute for Advanced Study at Princeton, his father was Freeman Dyson (of Dyson Sphere fame for Trek nerds). The whole narrative of how and why computers were conceived and created as general purpose calculation engines, and what problems those large-scale calculations could be used to solve, is tremendously interesting even without the author's first-hand insights.Add in the personal touches and the cast of luminaries that drop in and out of the overall story at the whim of history (or the US Government depending on your perspective) and you have a perfect time capsule for technology's rapid development in the 30's, 40's and 50's. Readers with a past professional experience with the US Government and high tech will find this especially interesting as the dance has clearly remained the same for decades with no signs of truly changing.I both enjoyed it and found it fascinating and informative. Everything one could ask of an attempt at decoding the past."
109,0,http://goodreads.com/user/show/1849793-richard,2,"I suppose this book deserves credit for inspiring criticism more thoughtful than itself. It's a dreadful disappointment. Dyson has an exceptional talent for finding the least interesting avenues to explore in his subject and pursues them relentlessly. The first chapters are a study in digression, inflicting on its readers a pointless history of colonial New Jersey; several biographical sketches of obscure and uninteresting figures; and an excruciatingly detailed account of the founding of the Institute of Advance Studies. Had this book been published by Princeton U.P. at least the level of detail would've been somewhat understandable. And it might have then found its true audience: Princeton alumni and IAS faculty, past and present. But Pantheon chose to inflict the work on a wider audience, and one is left wondering if they bothered to have even a single editor go over the manuscript. The shame of it is, there probably is a four-star book buried under all the dross. Dyson does have many colorful and interesting details to offer about the more interesting figures in the story, and he had unprecedented access to IAS records and archives. But none of that is put to good effect here, and he never manages to integrate the better material into the heart of the book. "
110,0,http://goodreads.com/user/show/1554782-mark-lisac,3,"The people in the story are compellingly interesting. So is the subject matter (the origins of digital computing and a look at some of the possible long-run consequences). Dyson does a good job of the writing, too. Computer experts have complained in other review that he botched some of the technical explanations. I'll defer to their knowledge, but suggest the problems do not matter much to non-experts since many of the technical descriptions are barely or not at all within the grasp of a general reader like me. Admittedly, the unclear description of several technical matters isn't a recommendation either. But the book is not really about the technology, it's about the individuals who brought the ""digital universe"" into being. Dyson does a good job of describing them and their interplay. It's no defect that he had plenty of biographical sources to draw from; synthesis is valuable too.Some of the speculation in the final chapters about digital life forms feels like digression and is rather wobbly, as other reviewers have noted. A paradox arises there. Dyson plays with ideas about a new life form made up of codes and circuits. Yet what makes the book is the story of the people, who are invariably more interesting than the machines and methods they created."
111,0,http://goodreads.com/user/show/3856855-uchicagolaw,0,"Recommended by Anup Malani. [2012]I have been doing background reading on computer history for my upcoming law and technology MOOC (massive open online course in the current academic name for these). First read George Dyson’s Turing’s Cathedral, which covers the key early period in the creation of the computer during and after World War II. Computing power was necessary for figuring out how to fire guns at a distance, but also for building atomic and nuclear weapons and so computers and new weapons rose together. And Walter Isaacson, author of the recent biography on Steve Jobs, has returned to the field with his just-out The Innovators. This is wide-sweeping starting with Charles Babbage and Ada Lovelace, but moving forward innovation (and innovator) by innovation, including the creation of the transistor, the microprocessor, and the personal computer. Neither of those are law books, and as lawyers, we should be interested in the intersection of law and technology, and for that, come take my online course on law and tech next summer. —Randal C. Picker [2014]"
112,0,http://goodreads.com/user/show/5918940-barry,3,"I wanted to give this a 4-star review, but the last chapter pushed it under the borderline. And not or the reasons of so many crap GoodReads reviews. No, it's not 'about Turing', who cares? I really like its historical perspective and its concentration on the engineering difficulties and the people who overcame them, rather than the usual mathematical abstraction. And the theory is a nice complement to Gleick's 'The Information'. Annoyingly, though, one is expected to know throughout what happened to Oppenheimer. I simply don't. And indeed technically one can question whether he understands bit-shifting as arithmetic - I was defending him, but then he completely misunderstood the nature of binary addition and the computational complexity of multiplication. Not good. What pushed me over the edge was his nonsense about apps. Also virtual machines. ""Apps are virtual machines"", ""virtual machines never sleep"" - huh? Finally, while he has a point about social network analysis and search ranking being essentially analogue processes, the primary job of Facebook is to faithfully reproduce Unicode statuses and bit mapped images. Nothing could be more digital."
113,0,http://goodreads.com/user/show/4644723-eva,4,"If you leave out Dyson's few 'prophetic' interpretations, then this is an anthology of the Institute for Advanced Study's years when they built the MANIAC-0 (now: IAS machine) - one of the first computers based on the Universal Turing machine. Dyson untangles and entangles the many lives that came through Princeton during those years to help build the machine. We get biographies, quotes, lost documentation from the institute's archives, tales of housing logistics and partying engineers. And the story also visits Los Alamos, where the same people who build computers, work on the atomic and hydrogen bomb. At the center of this giant net of disciplines and ideas you find the genius tying everything together: John von Neumann. Sometimes too detailed, sometimes not didactic enough, but i loved the portraits Dyson painted of these men and women, who dedicated their lives to jumpstart this digital world we live in now. Mainly i come away from the book with this understanding that the invention of the computer is naturally a wide-spread fuzzy collaborative effort where no-one truly stands out, but everyone builds on top of other's ideas. "
114,0,http://goodreads.com/user/show/2939121-sam-diener,4,"I was in the middle of writing a longer review of this book when the page somehow automatically refreshed or something and wiped out what I wrote. Dyson's vaunted symbiotic electronic intelligent partners to human intelligence are still frustratingly prone to such errors.I won't reconstruct everything I wrote. This book's structure is a mess, it's explanations of key technologies uninformative or confusing, and it largely omits key aspects of developments at the Institute for Advanced Studies in Princeton that would have enriched his themes (information theory and game theory). So, why the four stars?Dyson sketches the character of many lesser known figures in the IAS in memorable fashion (e.g. Barricelli, who developed computable cellular automata, if I understand this correctly). In addition, this books meaningfully engages with powerful themes of central importance to our existence as a species: the purposes of science, the definition of life, the meaning of intelligence, the ethics/immorality of nuclear and hydrogen bombs, the importance of intellectual freedom, and the importance of societies remaining open to immigrants and new ideas. "
115,0,http://goodreads.com/user/show/50528687-albert-chaharbakhshi,3,"There were parts of this book that were enlightening to me, even as a computer engineer. How cathode ray tubes were originally used as memory storage and later became displays, and other such tidbits were really interesting, especially in the earlier parts of the book. An overarching issue in this book, however, is that people and names are introduced at a lightning fast pace, and I found myself having to constantly look back to remember who someone was, or who the book is even talking about at any given time. That notwithstanding, in later parts of the book I found some of the history and science on how the atomic bomb was created to be extremely interesting.This book is more of recounting of the history of the Princeton Institute for Advanced Study than a technologically-dense history on early computing. As such, it's interesting, though somewhat scatterbrained. Look elsewhere if you're interested in more of the technological side of things. I would also recommended reading this straight through as much as possible, as I let it languish and read other books in between, which made it hard to pick up where I left off due to the nature of the book."
116,0,http://goodreads.com/user/show/7529193-steve-sarrica,4,"""Turing's Cathedral"" by George Dyson starts out as a review of the early computer work done at the Institute for Advance Study at Princeton and morphs into something much more interesting by the end. With a large cast of characters, two in particular stood out for me: John von Neumann and Nils Barricelli. Von Neumann stood out for his superlative, otherworldly intelligence and tremendous energy and for his untimely death. Barricelli stood out for his seminal work in cellular automata and much of what has become computational biology — his breakthrough work was done with 5K of RAM! Alan Turing and Klári von Neumann are also notable for their tremendous contributions, almost none of which were recognized in their short lifetimes. Edward Teller and the bureaucrats (and several faculty) at Princeton University don't come off especially well — although a late-in-life Teller is partially redemptive. While a chapter or two might have been slogs, the whole is a thoroughly engaging work that finishes very strong and raises interesting questions that are quite relevant to today's Facebook and Russian election-meddling controversies."
117,0,http://goodreads.com/user/show/12700288-paul,2,"Title should read ""Von Neumann machines: The human stories"". You're welcome dear publisher.I hope you've already read and know the history of the creation of the computer because this book dispenses with all that nonsense and instead concentrates on absolutely inconsequential trivia. Parties, social interactions, immigration issues, divorces, building houses, fixing cars and all kinds of irrelevant waffle that is day to day life pushing to the side exploding nuclear bombs.It's nice to see the background to the revolution and I appreciate it but it's like a misfocused photo where the face is completely blurry but by god that concrete wall behind the subject has razor sharp detail. I cannot but feel frustrated every time the author wraps up the technical side with a glib and borderline misleading paragraph (as it's glossing over any and all details) only to waste the rest of the chapter recounting sleeping arrangements and letter writing. I care about those things too but not that much."
118,0,http://goodreads.com/user/show/1140777-rob,4,"A wide-ranging, discursive history of the Institute for Advanced Studies, the beginnings of digital computers, the development of atomic weapons, and related and ancillary topics. The author flitted forward and back in time, going from subject to related subject, but always in an interesting manner. Along the way we learnt the histories of the people involved in this milieu.The book concludes with the author's thoughts on digital evolution, definitions of intelligence, the unknowability of alien life, and the possibility that digital code has already taken on humanity as a symbiotic host... but these mild speculations are done aren't done in the way that most kooks would do it, and are gentle musings forming a small part of the narrative. I mostly mention them because I didn't expect to encounter them.In all, a very interesting history of the people and choices during the 1940s, and a lucid0-enough description of the physical, logical, and institutional challenges faced by those folks. Clear-eyed, not hyperventilating or fawning."
119,0,http://goodreads.com/user/show/76780163-guthrie-c,4,"If you’ve already read George Dyson’s 1998 book “Darwin” than “Turing’s Cathedral” will cover a lot of familiar ground as its focuses on the birth of computing at the Institute of Advanced Studies. That same backdrop provides a rich framework to explore several new areas, of which I found these to be most engaging: (1) expansive exploration of Klari’s von Neumann foundational role in both early software programming and cross-discipline application of population analysis approaches to nuclear fission simulations;(2) the development of the Monte Carlo method and its tie-in with the birth of computing;(3) brief biographies of key, background contributors in the development of ENIAC as well as how this project affected the culture at IAS; and,(4) notable recognition by contemporary minds that industrial-age electricity and information-age computing were heralding a new life form (one of Mr. Dyson’s key assertions from Darwin revisited here).If you enjoy the author’s style and the computing history subject, then you’ll likely enjoy this book."
120,0,http://goodreads.com/user/show/3767458-matthew,1,"The author could not hope for a more sympathetic reader for this subject, but the presentation of the material was too poor for me to wade through. I hate giving up on a book like this because it is chalked full of information of great interest to me, but the writing is so scattered and disjointed that it is just not worth the effort. There can be little doubt that Dyson knows his subject. There can also be little doubt that he does not have the faculty for communicating it. These are fascinating times he is writing about, yet he manages to sap almost everything from them. The effect is rather like listening to a very bright, but painfully dull and hopelessly flight savant ramble drunkenly through a historical subject, its people, its event, fracturing the timeline and meandering about from one thing to the next with no regard for how they are connected. This book needed both a different editor and a different writer."
121,0,http://goodreads.com/user/show/3070349-les,4,"""Computers rose from the mud and code fell from the sky.""Francis Bacon set the stage in 1623 when he proposed that 2 symbols would be sufficient for coding ALL communication. Gottfried Wilhelm Leibniz determined in 1679 that 0 and 1 were sufficient for logic as well as arithmetic.But it wasn't until the 1930's that the first theoretical computer was put forth by Alan TuringTuring's main question / goal:""What would it take for machines to think?'This book has incredible insight into the world of computers a.k.a. our world today.Interesting Facts:- Time in our universe is a continuum, in the digital universe it is a countable # of sequential steps.- Today the digital universe is estimated to be about 10^22 bits- At it's peak (1991) we we're rolling out production fiber at a rate of 5,000 MPH (9x the speed of sound)- In 2011, we reach 15,000 MPH (or 20x the speed of sound)""What if the price of machines that think, are people who don't."""
122,0,http://goodreads.com/user/show/4839599-john-bickelhaupt,5,"This book bent my mind. It is a history of the postwar evolution of computers and the development of the first truly digital machines. Much time is spent on the roles of John von Neumann and the Institute for Advanced Study at Princeton in the process. Many scientists who worked on the Manhattan Project were involved as part of the justification for investment of government funding in computers was the need to model nuclear explosions to aid in the development of new weapons, especially the hydrogen bomb. It's very interesting what users chose to do with the first machines when they weren't being used for modeling shockwaves. It's also amazing what they were able to accomplish on such primvitive computers. One of the first oomputers used for their research was a machine with 5 kilobytes of memory that cost in excess of $100,000. The same amount of memory with 1000 times the processing speed costs about 1/10th of a cent today. I will probably read this again."
123,0,http://goodreads.com/user/show/7848805-andy,3,"An interesting tour through the entwined development of the first electronic computers, nascent computer science, and the atomic bomb. It's always an awkward straddle to write about extremely technical innovation in a way that's comprehensible to the intelligent layman and in this book Mr. Dyson attempts to encompass, or at least encapsulate: nuclear physics, Goedel's completeness theorem, Turing's universal machine, digital logic, evolution, meteorology, and more. He seems to extrapolate from this rich porridge of theory to a future of machine intelligence capable of self-reproduction. Intriguing, entertaining, perhaps interesting. I'm left with the sense that Mr. Dyson took on a bit more than he could accomplish. A comparable but much more successful example of a history of technology is John McPhee's Annals of an Ancient World, one of my favorites of the genre. Or maybe I just didn't invest enough in the reading to get it. "
124,0,http://goodreads.com/user/show/1422008-alison,0,"I really expected to enjoy this more than I did, and more than some other reviewers here seemed to...but now I understand what people were saying. I knew this was not a biography of Turing, as some seemed to think, but even so it was just...not that good. It was disjointed, for one thing - it felt more like a collection of articles that were loosely connected enough to maybe be published in the same journal. There were tangents that were quite unnecessary and way too long. And even when the author was talking about what I wanted to read the book for, it was so dry and boring. And I read a lot of non-fiction on dense and difficult topics, so it's not the genre I have an issue with - it's the execution here. Way too much tiny detail for pages and pages, with no real hook to draw you in. I always feel weird giving a low rating to a book that got such praise from professional critics and reviewers, but...to each their own, I guess."
125,0,http://goodreads.com/user/show/35020173-william-anderson,4,"This is a fantastically detailed tome focused near exclusively on John Von Neumann. At Dyson sidetracks to extremes spending multiple pages on irrelevencies such as the founding of pennsylvania in an attempt to give depth to areas where fact alone would suffice. In other area the personal nature, details, quotes from letter/exchanges and interviews serve to shed excellent light on the character of those whom the book accounts the life of. In short this is a biography of Von Neumann through the lens of his work at Princeton on the IAS machine. Turing in mentioned briefly for about a chapter, but mostly just in reference to inspiration given to Von Neumann. Its a poorly named book that couldve been written in 2/3 the length without losing any detail, but was still a great read overall. You will find yourself reading certain parts almost like a fiction as the historical characters from the birth of digitial computing come to life."
126,0,http://goodreads.com/user/show/37718278-price,4,"I thoroughly enjoyed reading this book. The book provides fascinating insight into the creative and exhaustive work that was done in the at Princeton starting roughly in the late 3os and extending into the 50s. The possibility of creating a computer capable of being programmed to process input led to the integration of highly competent individuals having diverse backgrounds in engineering, mathematics, physics, meteorology, etc.It is a fascinating account of the progression of multiple discoveries and widespread applications in technology and science that continue to progress and expand with each passing day.I am quite certain that other authors have explored this period exhaustively and can provide a far different description and interpretation of the sequence of events and players. More than likely I will research other works broaching this topical area. But, the content was satisfying and informative. It met my expectations. "
127,0,http://goodreads.com/user/show/10716719-william,4,"About seventy years ago in Princeton’s IAS a group of Europe’s best and brightest biochemical organisms initiated work on a digital computer using Alan Turing’s thoughts to enable the calculations necessary for constructing a thermonuclear bomb. Viewed from a distance, this looks like part of a life cycle; but are biochemical organisms the larval phase of digital computers, or are digital computers the larval phase of biochemical organisms? George Dyson believes, as I do, that biological lives have developed in many other locations in the universe and are most likely located within globular clusters in our own galaxy. Within these closely clustered star systems these more evolved and collaborative lives, the most successful for propagating itself will be digital life; which will adopt a form that is independent of the local chemistry and migrate from one place to another as an electromagnetic copy to colonize when it arrives at new locations."
128,0,http://goodreads.com/user/show/83141-jdk1962,1,"Abandoned on p. 91, about 1/4 of the way through. As a software engineer by trade, I'm very interested in the topic, but this book was massively disappointing. It wandered around, occasionally pulling in material of no interest whatsoever (Chapter 2, ""Olden Farm"" covers the Revolutionary War [!] history of the area that the IAS would inhabit ~150 years later. I really--really--couldn't give a rat's ass, and I'm mystified as to why the author thought it in any way significant, other than as a means to six pages to the text). The focus on people rather than on the timeline meant that the focal point of time kept jumping around and the story came in and out of focus. I'd rather read a couple of well-researched, well-constructed books about the period, rather than continuing to wade through this."
129,0,http://goodreads.com/user/show/3250363-alex-long,4,"I really loved Dyson's last book on the project Orion and my career is very tied to scientific computing so I was really excited for this book. There is a lot of great material and great writing but it never really feels like more than a collection of anecdotes about the development of the computer. This may be because the history of the computer is not a clean, straightforward tale like the development of atomic weapons or project Orion. The story jumps around from person to person, from application to application without a clear thread. Some of the early history included in the book also feels very unnecessary and turns the beginning of the book into a slog. Later chapters on early weather prediction algorithms and evolution simulations are great. Dyson also does a fairly good job of waxing philosophical at the end of the book."
130,0,http://goodreads.com/user/show/7477960-james-eastwood,4,The men profiled in this lovingly-researched narrative of the birth of the modern digital universe accomplished a feat nearly unbelievable in today's intellectual climate: they took digital computers from first principles to inception to reality and laid the groundwork for much of the software development that would follow. They did it all in one generation. Why are computers the way they are? Not because of any fundamental law of the nature of computation (though most of the men and women involved in their creation seemed to think they were discovering more than inventing) but because of the peculiar realities of a group of immigrants at a non-teaching academic institution immediately following a globally catastrophic war and during the heady days at the beginning of the nuclear arms race.
131,0,http://goodreads.com/user/show/252811-brian,4,"I constantly come across references to the work of von Neumann in my work as a physicist and it was interesting to learn more about his life from a historical perspective. I also really enjoyed learning more about the founding of the Institute for Advanced study and the various scientists and engineers involved. Overall I think the book did a good job of painting a picture of the people and circumstances involved in the creation of the computer age that we now take for granted. But I think that maybe the author tried too hard to include technical details that should have either been explained in more depth, or far less depth. If you don't care much for the history, and would rather have a better layman explanation of many of the same topics check out 'The Information' by James Gleick."
132,0,http://goodreads.com/user/show/9086776-mat,4,"Ever since I was a child I have marveled at the incomprehensible leap that takes place between not having computers and the computers of the day (at the time, the Amiga). Granted I usually pondered this in terms of how we could come back from a post apocalyptic situation and reinvent computing technology. This text firmly and at times gruelingly answers how mathematical theory was advanced to the point when multiple independent, mathematicians began theorizing and then implementing computing machines to deal with the advanced mathematical problems they were seeking to conquer or concepts they were trying to disprove (98% without reference to the apocalypse I envisioned as a child). I have a little further insight now into the world of early and modern computing and I marvel even further at the other-worldly accomplishments these men and women made that have changed my entire life."
133,0,http://goodreads.com/user/show/15026495-duane,4,"So this was a guilty geek pleasure. The first 9/10s of the book is a history of computing from the very beginnings of the binary system with Leibniz who said in 1679 'This [binary] calculus could be implemented by a machine (without wheels),' he wrote, 'in the following manner, easily to be sure and without effort. A container shall be provided with holes in such away that they can be opened and closed. They are to be open at those places that correspond to a 1 and remain closed at those that correspond to a 0. Through the opened gates small cubes or marbles are to fall into tracks, through the other nothing. It [the gate array] is to be shifted from column to column as required.' all the way to the ENIAC and the Manhattan Project. The last 1/10th asks the question, "" and what are we going to do with this?"" Good question. "
134,0,http://goodreads.com/user/show/10869905-david,3,"A fascinating book about the big-bang of computing. Having spent a couple of beer fueled evenings haunting the hulking husk of the ENIAC at the Moore School of Engineering when I was at Penn, I was excited to learn about the details and personalities of the larger project of moving from adding machine to universal machine. The book delivers great portraits of the various personalities and the basic and big-picture science and philosophy behind the digital universe. My middling performance in calculus and physics left me on the edge of the deep end discussions of vacuum tubes and relays and miles of punched tape and cards. And the book did bog down there. For others, I'm sure those sections and details made the book. Lacking a more general history though, this one will do."
135,0,http://goodreads.com/user/show/174365-kurtbg,3,"A historical look at the development of the computer as it rose out of the ashes of the concept of ""Human Computers"" in world war II and the need for higher computing power in the development and testing of the atomic bomb used in WWII.In it is the rise and proliferation of John von Neumann, and the creation of an institute for a non-teaching academic think tank in the spirit of those found in the early 20th century in east Europe. Thus, the Institute of Advance Study in Princeton New Jersey was created. There one could find Albert Einstein, Benoit Mandelbrot, and Kurt Godel amongst many other contributors to modern science.From the calculations of missile trajectories, weather predictions, and atomic reactions the path the need for the computer is born. "
136,0,http://goodreads.com/user/show/2298410-damon,3,"I probably didn't read enough of the cover description of this book because I thought it was going to be more about Turing but I enjoyed learning about the Institute for Advanced Studies, and von Neumann's role in particular in building some of the first computers. The stand out for me was just how much of early computing came out of government sponsored research and applied mathematics. Also how early thinkers were concerned with machine learning and artificial intelligence when they had such, to us at least, rudimentary tools. I think the author over plays how much early predictions have 'come to pass' - being Turing complete is a long way from creating virtual machines - but it was an interesting read about a lot of people I'd heard of but didn't know much about."
137,0,http://goodreads.com/user/show/2686505-elderberrywine,5,"This is the tale of the origins of our digital age as the computer stumbles into existence. Not an easy read, by any means, but full of enlightening insights, such as the two things a computer has to provide are a location for each bit of stored information and the concept of sequence. Everything else stems from that. Filled with players such as Johnny von Neumann, Alan Turing, and the author's own father, Freeman Dyson, the Institute for Advanced Study at Princeton, New Jersey, presents itself as some sort of fabled summer camp for the gifted, a Nirvana for nerds. Alas, the pressures of WWII and even worse, the Cold War, are just around the corner. Hearts will be broken, and projects will become defunct. But, somehow, along the way, the interwebz as we know them will be born."
138,0,http://goodreads.com/user/show/27678334-eric,4,"A very readable(listenable) telling of the modern history of computer science, although I do give some slight pause to clear religious fervor of the title and some elements of what has come to us a artificial intelligence. I think the fundamental question gets to whether ""code begetting code"" is quite the same thing a self-awareness. It is indeed stunning how the power of the computers in service today grew out of vacuum tubes just a generation or two ago, and today with the internet expose us to mountains of data, some of which we can even comprehend as information. But it is a capture of history without a caution of what to expect next should we turn over ever more control to the machines around us."
139,0,http://goodreads.com/user/show/777050-andrew-martin,4,"one part micro-history of the MANIAC project @ the institute for advanced study/princeton, one part meditation on the reach/speed/scope of the digital universe. a history and philosophy of science indeed; dyson is alternates between reporting back the minutiae (logs, parts sourcing and internecine internal politics) of the the IAS computer and free-associating on the deep symmetry between digital systems and biological life.fascinating; many stars for successfully tackling the past and future history of computing/math; many-1 stars for sometimes perplexing decisions to leap erratically around the chronology of events."
140,0,http://goodreads.com/user/show/2649574-dan,3,"I grabbed this off the shelf at the library because I thought it was a biography of Alan Turing. It's not. Despite the title, the book only briefly covers the life of Turing. But it was his insight into storing instructions side-by-side with data that allowed von Neumann (the central character in the story) and his colleagues at the IAS to build the groundbreaking computer in the late 1940s. Dyson's account of the IAS computer is in-depth, but meandering and often dull. Once I came to terms with the actual focus of the book, I did enjoy it. But I'm still on the lookout for a good biography of Turing."
141,0,http://goodreads.com/user/show/5996952-tim,1,"This insufferably dry (yet obviously well researched with references peppered throughout the text) academic book mostly ignores Alan Turing and focuses on the development of Princeton, from a scruffy patch of land between two unpaved roads to the centre of research excellence into mathematics and computing it is today.The ""narrative"", unfortunately, does not follow such a simple path, and instead meanders backwards and forwards in time. I gave up when I reached the end of the Second World War for the umpteenth time and the author got bogged down in detailing the salaries of the researchers and engineers trying to build the earliest computers."
142,0,http://goodreads.com/user/show/5858246-alan,4,"A nicely written history, with provocative observations, of the computer built at Princeton’s Institute for Advanced Study at the time of WW2. John von Neumann is the central character. While some of the computer science and mathematics were over my head, that didn’t detract from the story, and I enjoyed learning the quirks and accidents of things like who invented the Monte Carlo method, how, and why. I also enjoyed the provocative observations about alternative life forms, artificial intelligence and where the digital universe, which has more analogue characteristics than I thought, might be headed."
143,0,http://goodreads.com/user/show/7119289-brian,3,"Recently, I read the work of history, Turing's Cathedral: The Origins of the Digital Universe (Vintage), which is an interesting book that tells of the development of some of the first computers in the United States. It's particular focus is on the founding of the Institute for Advanced Study (IAS), and then John von Neumann's time there. Now, the von Neumann architecture is something I regularly conceptualize and use in teaching. And it was interesting to read of how this architectural model was developed, and why.Review continues on: http://elegantc.blogspot.com/2013/09/..."
144,0,http://goodreads.com/user/show/9410676-lou,3,"The book was just OK. The cast of characters, overlapping with to those in ""The Making of the Atomic Bomb"", was familiar. Telling the story, however, seemed haphazard - lots of skipping around, with a very jumbled sense of continuity that made the reading more difficult than it had to be. The book's jacket should note ""some assembly required"", as the reader will need to put the pieces in the correct order on his own. FOr me it was enjoyable because I had read ""The Making of the Atomic Bomb"" first, so I had knowledge of the characters. If you read the book cold, with no foreknowledge of its cast, it might be a more difficult read. "
145,0,http://goodreads.com/user/show/7467322-heep,4,"Several other reviews are critical because the descriptions of computer architecture are inadequate. I don't know enough about that to quibble. I found the book interesting because of the biographies and the historical context. It does a good job of describing the circumstances that inspired development of computing technology in the United States and Britain during the 1940s and 1950s. The chief motivation was military research, and more specifically, to develop the atomic and hydrogen bombs. John von Neumann is the focal character in all this. His intellect is staggering, and it is astonishing that he played such a key role in the creation of the Bomb and computers. "
146,0,http://goodreads.com/user/show/912478-susan,3,"This is a rather peculiar book with a portrait of Alan Turing on the cover but a story that is mostly about John von Neumann. The author has done an impressive amount of research, capturing a strand of technological history, much of which might have been lost. He included interviews of some of the participants in the story, some of them in their nineties. He has produced a resource for anyone wanting to study the history not only of the digital universe, but also of the use of technology by the military. However the book contains quite a bit of technical detail that may be a little excessive for some readers."
147,0,http://goodreads.com/user/show/12705473-william-crosby,3,"Historical account of computers in 1940's & 1950's in U.S. focusing on VonNeumann and interlinking our development of computers with World War II.Meanders, giving irrelevant historical and biographical details: too much noise. And does go on and on about personality and bureaucratic problems.Plus the technical details as to the operation of the computers were not adequately described clearly.Still, I enjoyed the various relationships between the people (just wanted the descriptions more terse and less repetitive), their quirkiness and the situations related to U.S. culture at the time."
148,0,http://goodreads.com/user/show/5282959-seth-pierce,3,"While this book is interspersed with some profound stories and observations, they can only be arrived at by wading through profound piles of super technical references that feel like reading the ending credits to a movie.This book could be half the size, and summarized better. The ""history"" is also limited to the 1940s and 1950s with jumps into the present--making this feel incomplete. Finally, the author's organizing by biographical vignettes clutters the narrative and gives the reader the feeling they are reading the same chapter over again.If you can read quick and extract the big ideas, this book has some good things to offer; otherwise keep looking. "
149,0,http://goodreads.com/user/show/4114192-julio,2,"Misleading title/premise. This book romanticizes the early days of Princeton and sets out to create a sort of Homeric epic of Von Neuman's life.It's a shame that the enormous amount of scholarly research that's behind this book was used to create a catalog of trivias and an account of history blinded by nostalgia. I know I'm overreacting, but this book had so much more potential, but it was squandered on the author's delusions of literary grandeur. In this way, the title is actually accurate; an overblown metaphor that mixes the righteousness of religion with the invention and construction of the first computers."
150,0,http://goodreads.com/user/show/85872735-michal-piekarczyk,5,"This book introduced me to my new favorite genre. I don't know the name of the genre, but I think of it as historical narrative non-fiction. This book is the ""technical"" sub-genre of historical narrative non-fiction. It tells the story of the birth of modern computing and many of its key players. There was no need to come up with a clever story arc, because the circumstances that led to the IAS computer at Princeton were interesting enough, with characters like John Von Neumann and Richard Feynman, building the field of Computer Engineering in some boiler room of the Mathematics department at Princeton."
151,0,http://goodreads.com/user/show/3392903-daniel-schulte,3,"I appreciate all of the names that I was introduced to by reading this book, but I get the feeling that this really should have been three or four different books. I'm a software engineer interested in the history of computing. I'm not interested in nuclear weapons, I'm not interested in Von Neumann's love life. I'm also not interested in the author's conspiracies about how modern mobile apps are living, self-replicating, digital organisms that will someday subject us to a digital rule. Just because something is theoretically possible does not mean that it is realistically probably.I'm glad I learned about Julian Bigelow, though. He seems like the kind of guy I would've like to meet."
152,0,http://goodreads.com/user/show/2194759-kevin-furr,4,"Interesting and informative history of some of early computing efforts, but more philosophical than a straight narrative. While the title carries Turing's name, the book is much more about John von Neumann -- practically a biography of him, as well as being an account of the early history of Institute for Advanced Studies in Princeton, and the effort to build an early computer there. Which I appreciated, I've always wanted to read more about von Neumann, who all the other geniuses of the time looked up to as the real genius. (Other books give much more detail about Eckert and Mauchly's ENIAC, and the British work at the time.)"
153,0,http://goodreads.com/user/show/6315060-mike-garrity,3,"This had a lot of nice details about life at IAS during the construction of von Neumann's machine. Maybe too much detail. George often needs a bit more editing. He'll occasionally go haring off on something like the family history of the step father of the group's secretary. I found Tom Green's Bright Boys a more enjoyable read in some ways. That said, the cast of characters in this one is certainly great. Some of the most interesting people in 20th century American history passed through IAS at this time. "
154,0,http://goodreads.com/user/show/5850177-michael,3,"George Dyson does a commendable job of research and provides some interesting stories surrounding the birth and development of the digital age and the great minds behind it.However, I didn't enjoy reading this book; the topic interested me but with it's very dry prose, reading it became tedious and not satisfying. At times, I felt like I was reading code instead of a story; it was an uninspiring read.Poor editing:p. 59: in 1943, Britain's Nautical Almanac Office was ""His"" Majesty's, not ""Her"" Majesty's."
155,0,http://goodreads.com/user/show/856383-ted-tschopp,4,"A wonderful book that covers the history of the creation of the computer and the modern age we live in. The title is a bit off as the main character seems to be Von Neumann who acted as an engineer to Turing's hypothetical machine. The book left me wanting more, while others found the details either too many or too obscure, however I have grown up with computers and find many texts covering the topic to be too pedestrian. I recommend the book to anyone who has a background in computers, math, science, physics, biology, or some of the more statistical leaning social sciences. "
156,0,http://goodreads.com/user/show/4249244-michiel,3,"I find it difficult to rate this book. The history of the development of the computer is an interesting one, with the main characters some of the greatest minds of the first half of the 20th century. You can feel the excitement and wonder in the development of the this technology which the goal of modeling (and controlling) the weather, simulating evolution and, of course, making the atom bomb.The main issue with this book, in my opinion, is that some chapter somewhat lacked focus with respect to the main message. "
157,0,http://goodreads.com/user/show/24141062-rory-hyde,3,"After reading American Prometheus: The Triumph and Tragedy of J. Robert Oppenheimer -- which is utterly compelling, and covers much of the same territory in the creation of the atomic bomb and the early years of Princeton's Institute for Advanced Study -- I have to say I found myself flagging a bit with this one. An incredible period however, (and it was great to meet George briefly a few years back). "
158,0,http://goodreads.com/user/show/7287514-lauri-nikkanen,1,"I was able to fight my way halfway of this horrible embodiment of tedium. I was originally intrigued by the topic - a history of how the digital computer came to be - but holy hell, if your sanity is precious to you, stay away from this one! Unless you're someone who is fascinated by finding out in minute detail what a particular summer trainee in the project has for breakfast on a particular day, which colour of socks she wore and where her uncle went to school. I don't like not finishing books, but...even my patience has its limits. What a torture."
159,0,http://goodreads.com/user/show/3402107-brian,3,"""Turing's Cathedral"" was a long but interesting read about the origins of today's computers in the context of their World War II evolution. Readers of Neal Stevenson's ""Baroque Cycle"" will gain fact-based hints of the fictional story painted by Stephenson. Although Dyson's work bears no intentional relationship to Stephenson's opus, there is definitely some synchronicity in the overlap of the modern evolution of the computer and the origins of natural philosophy and modern cryptology in the 17th century."
160,0,http://goodreads.com/user/show/5270220-nelson-rosario,4,This book is fascinating. The author details the development of computers which coincided with the development of atomic weapons. I was unaware how closely linked the US war effort in WWII was to the rise of computing. There are so many remarkable tidbits in this book that it's hard to pick one. I think the most interesting was the revelation that Google scanned all those library books to be read by an AI one day. Along those lines the author engages in a lot of thought provoking discussion about the possibility of digital life and where evolution is headed. Highly recommended.
161,0,http://goodreads.com/user/show/4178301-glenys,3,"As an account of the early years of computing at the Princeton Institute for Advanced Study, the relationships, the politics, this was very interesting. As an explanation of the evolution of computer technology for the non-specialist, it fails to engage or educate very well. I must also mention that I found his rampant anthropomorphism becomes very irritating. Despite his metaphors, I remain of the opinion that computers don't think, reproduce or manipulate human beings to provide for their well-being. "
162,0,http://goodreads.com/user/show/12785220-stuart-sullivan,5,"Fascinating background on the development of modern computing machines as a legacy of the WW2 Manhattan project research. Intense biographies of the major players, brilliant men who were grouped together in the War effort and beyond, and developed the computer architecture that is still used for the most part today. Not as much on Turing himself, but on his collaborators including Einstein, and some of their quirks and faults as well as their inspiration. Recommended for anyone really interested in the history and biography of the men and women who shaped the computer revolution. "
163,0,http://goodreads.com/user/show/56682622-raymond-goss,4,"The boring title of this book could have been: ""A biography of Johnny von Neumann and the contributions he and his friends made to the development of computer architecture and system programming and related applications including the hydrogen bomb and meteorology."" There is very little about Alan Turning, which might even be the author's point, or perhaps the book's backstory grew to be a book on it's own. This book provided a different perspective on the ENIAC and computer development than the ""The Innovators"" by Walter Isaacson, which was also enjoyable. "
164,0,http://goodreads.com/user/show/3346363-matt-heavner,3,"An amazing look at the beginnings of computing. Primarily a history of the IAS. The book had quite a great scope of history but I felt it could have used an editor. It jumped around concepts - the history of the land are area that IAS was built, then mid-20th century physics, then back to Hegel/Kant and philosophic discussions, then political intrigue with Manhattan project and Teller/Oppie and the ""super debate"". It felt scattered, but was all a fascinating look at the history of compute and the applications of big computing."
165,0,http://goodreads.com/user/show/917269-thomas,4,"This would have been better titled von Nuemann's Kingdom, but I'm sure the question of whether or not to capitalize the V in 'von' would have driven them crazy. There's actually not all that much on Turing himself or his work, really. Still, it's a good book and should be of interest to those of you who enjoy learning more about the history of computing and the circumstances in which much of the work around it began."
166,0,http://goodreads.com/user/show/1664214-thorsten,2,"Covers some good history, and I did enjoy some of it, however my main problem was with any technical descriptions, which I felt the author himself didn't understand; It read awkwardly, as in when someone tries to explain something they have been told without fulling comprehending the material. I started skimming towards the end, after his claim that search engines and social networks were a return to analog computers - I couldn't even fathom that metaphor!"
167,0,http://goodreads.com/user/show/2466848-joab-jackson,3,"  Good if meandering personal history of how Turing and (mostly) von Neumann laid the mathematical and engineering foundations for modern day computing. Dyson indulges in a bit too much personal history of some of the supporting players, and his theories around the biological nature of computer code are, well, something to think about anyway. Still, educational and enjoyable enough to read."
168,0,http://goodreads.com/user/show/916090-luke,4,"Brilliant pop-history of Von Neumann and the first general purpose computers, and the scientific problems opened up by the speed of programming that came with the ability for computers to change their own instructions. Simply enjoyable for the variety of brilliant people overlapping and intersecting through Von Neumann, as well as for the forward-looking sense of all current computing as an organism evolved from this original architecture."
169,0,http://goodreads.com/user/show/5310812-matt-allyn,1,"This book was a major disappointment. There were a few interesting parts about the early nuclear tests, and how computers evolved from single-purpose to programmable machines - but other than that, it was a painful read. There was very little character development or insight into the brilliant minds that were the focus of this work. I kept waiting for it to get better or culminate in some meaningful way, but it just never did!"
170,0,http://goodreads.com/user/show/6255898-david-levine,2,I was so excited to read this book but was ultimately disappointed. I like George Dyson but this felt like one long conversation that went on and on with facts and figures and on and on. I was lost. I work with code in product marketing and I had a hard time following what was going on. I so wanted it to be better because the story is so great and so important for the world that we live in. Dyson is much better as a writer of shorter essays. 
171,0,http://goodreads.com/user/show/1717479-terry,3,"Turing's Cathedral is an unremarkable and rambling view of the people and events the early days of digital computing. There are brief flickers of utter brilliance but the rest is largely historical narrative. Some characters are developed well and others are just kind of a smear. I can't tell how he chose which to be which. Turing, himself, is largely glossed over.This should be called Von Neumann's Cathedral."
172,0,http://goodreads.com/user/show/15515752-to-c,5,A wonderful tour thru the beginnings of the digital universe in the USA. not necessarily a history of the dawn of the computer age because Dyson waxes philosophical about it all. Wonderful insights and it has affected my view what I do. But he also brings to life many of the pioneers of the age. I'll probably read of again in a year or two. But first I'll read Alan Turing and His Contemporaries from the British Computing Society for another take on the whole thing. 
173,0,http://goodreads.com/user/show/1216051-doug-wells,3,"A fascinating history of the intersection of the birth of the computer and the race for atomic and thermonuclear weapons. At it's base, it's a biography of the brilliant John von Neumann, but surrounding that is a rich tapestry of characters. Written by the son of Freeman Dyson who was an active player, the book sometimes veers in to technical minutiae that can be challenging to follow, but overall is approachable and readable."
174,0,http://goodreads.com/user/show/35450686-discospacepanther,3,"Should really have been called 'Von Neumann's Cathedral', as it focuses far more on the IAS at Princeton than on Turing and the development of early computing in the UK.Nonetheless, it is an informative and usually engaging read about the politics and purposes behind these early computers, despite the unnecessary detours into the personal lives of the scientists and engineers.Unfortunately some of the technical details seem to be either poorly explained, or poorly understood by the author."
175,0,http://goodreads.com/user/show/20170415-timothy-volpert,3,"if this book had a main character, it was John Von Neumann, not Alan Turing, so the name is a bit of an odd choice. also some really extraneous background early in the book about the founding of new jersey, William Penn etc. interesting enough info, but not what i was expecting from a book about the birth of the digital universe. was very enlightening to see the way that computers and the atomic bomb grew up together, though."
176,0,http://goodreads.com/user/show/20827109-david-shinabarger,3,"Pretty interesting content but hard to understand a lot of it, and so many characters it was a challenge to keep track of everything going on. Seemed like a book by a historian for historians. I felt that it lacked a lot of key storytelling and narrative elements to really keep you hooked and wanting to continue reading. Helped understand some beginning of the computer history, but I felt it could have been more succinct. "
177,0,http://goodreads.com/user/show/11858843-andrew,4,A historic overview of computers seen from the vantage point of researchers at Princeton. The participation of the computer in the development of the atom bomb is examined in detail. George Dyson dives into careful detail with his personal connections to his celebrated father Freeman Dyson. John von Neumann is the central figure in this book and Turing's universial machine is a key theme of the book. 
178,0,http://goodreads.com/user/show/9327213-vladimir-chupakhin,3,"So, the book is not about the computer invention itself. It's about the people who make it possible. Author going into chaotic description of personal and public life of the inventors. Those details are interesting to understand about particular period of history but add very little to the plot of the book.It's interesting to realize that Weiner was rather strong schauvinist, and hated all Russian."
179,0,http://goodreads.com/user/show/69970939-matt-reno,4,"Growing up immersed in computer science, I heard all these names and what these people contributed but I didn't really have a sense for who they were. Thinking about reading a dozen biographies was just daunting though. This really gave me a good survey/overview on what the people who made computing possible were really like. If you are interested in an in depth history lesson on the roots of computing this is a great place to start."
180,0,http://goodreads.com/user/show/5890695-bill-glover,5,"Amazing accomplishments are documented here that we can’t afford to forget. This is an important part of our story, something Americans can be proud of aside from the bit about how Jewish European intellectuals, scientists, and professors still had to be smuggled into American academia due to domestic Antisemitism.Plenty of great anecdotes. Hear Dylan Thomas is in town? Call his hotel, wake him up, find someone sober enough to drive into the city and get a party going."
181,0,http://goodreads.com/user/show/2077405-tara,3,"An interesting read that ultimately struggles with poor and inconsistent organization. The author introduces ideas that he doesn't explain for pages or chapters, assuming that the reader will be able to hold all the strings in their hand. The most interesting parts were how the origins of the original computers was connected to more modern issues and consequences."
182,0,http://goodreads.com/user/show/8240451-mike,3,"This is a tedious book to read. Having read many books about the pioneers in the development of the computer, there was really nothing new in this book except the excruciating details of the origins of the different people, and their family trees.It was a disappointment to me, but for someone less familiar with the history, it may be very interesting."
183,0,http://goodreads.com/user/show/10041011-dennis,4,"An incredibly detailed history of John Von Neumann's scientific collaborations. Basically a very entertaining read for anecdotes and historical detail about familiar scientific figures. However it's a bit weak on the technical sides, and could have done without some chapters on speculative sci-fi type ideas."
184,0,http://goodreads.com/user/show/3700843-tom,3,"Excellent description of the early history of digital computing... particularly the 1940s, how it was used at the time, what other interests drove the innovations, how it was funded, and the interactions of the key people.I thought that the author digressed in places... like a description of the Indian tribes and Washington's trek through the Princeton area; like the concept of digital life. "
185,0,http://goodreads.com/user/show/10089162-matt,3,"Should be titled Von Neumann's Cathedral as he is the protagonist. Turing hardly makes a cameo. It's a story more about the people involved with the IAS computer at Princeton rather than a comprehensive discussion of early computing. Still it would be of interest to anyone who wants to know where the ""digital universe"" got it's start and where it might be heading."
186,0,http://goodreads.com/user/show/9781535-chris,4,"It starts with a rather long (but still interesting) historical background. The book gets much more interesting in the second half, especially the final third.While some of the claims about self-replicating codes may be a bit over-the-top, they do give some great material to think about. Are we in control of our machines, or are we so dependant on them that they controlling us unwittingly?"
187,0,http://goodreads.com/user/show/2935560-drew,3,"interesting but uneven. Should have been called Von Neumann's Cathedral, IMO, since he is - not undeservedly - the primary character in the book. The story of early computing is fascinating in itself, but always serves for me as a touchstone to remind me of how awesome and incredible computing devices are... all while they become ever more invisible and taken utterly for granted."
188,0,http://goodreads.com/user/show/11505335-otto-emanuelsson,3,Somehow a masterpiece and a disappointment at the same time. I expected a somewhat more philosophical account on Turing's contribution to the history of digital computing but found a very elaborate description of the life and work of von Neumann. Still worth reading and I especially enjoyed the the last few chapters. 
189,0,http://goodreads.com/user/show/8638809-vernon-smith,4,"Well researched. Stories told from nearly a first-person perspective. Gets a little dry about 2/3 of the way through. Appears disjointed from the first half which was amazing. The end is all closures with so many names of actors that you are uncertain who they are talking about. In short, brilliant beginning, dull ending; overall, good but wanted better."
190,0,http://goodreads.com/user/show/6079817-rich-bergmann,5,"I'm sure my high rating is biased by the fact that I love this subject and George Dyson is Freeman Dyson's son. But that notwithstanding, I found this to be well written, thorough, and evenhanded in its treatment of the many characters who contributed to the genesis of digital computing. If you are both a fan of history and of digital computing then this is a must read."
191,0,http://goodreads.com/user/show/4822244-ronan-o-driscoll,4,"Really it should be called Von Neumann's Cathedral, especially as Turing himself only makes a small appearance. Recommend it for the history of computer science and as a near biography of Von Neumann. Dyson has the occasional unfortunate foray into speculation about the singularity which is mostly fine but occasionally overblown."
192,0,http://goodreads.com/user/show/9144929-thomas,5,"I very much recommend ""Turing's Cathedral"" to my friends who are fellow engineers and/or who are interested in how the pioneers like Von Neumann booted up the digital universe.Full of interesting stuff - For example, the first RAM was implemented on 5"" surplus oscilloscope CRTs, now THERE'S a hack!"
193,0,http://goodreads.com/user/show/10838825-russ,3,It's not really about Alan Turing or Princeton's Institute of Advanced Studies. It's more about all of the personalities involved with the Manhattan Project and early computer development that had some relationship to IAS. THis is still an entertaining and enlightening read. There are tolerable sections of raw scientific descriptions but skimming them doesn't impact the book.
194,0,http://goodreads.com/user/show/34117068-jamie-hannaford,3,"Apart from all the technical detail surrounding the first computer (how the Williams Tube or Selectron worked, the von Neuman architecture, etc.), what I liked most was the attention given to the individual biographies of the people who helped create it. It's very easy to take for granted the genius that drove their work, but Dyson always brings it to the fore. "
195,0,http://goodreads.com/user/show/31294222-christian-dietrich,3,"Besides the name, the title of the book is misleading. Turing himself is only of importance in a single character. The book should be named ""von Neuman and the MANIAC"". But still, it is a good book, that describes the development of the first computer in the US an the Insititude of Advanced Studies in detail.It is sad, that almost no reference to Konrad Zuse is drawn."
196,0,http://goodreads.com/user/show/3476595-simsian,3,"Another research book that didn't quite fit the topic at hand. An interesting presentation of the history of the digital computer but I'm not sure I like quite how much it diverged from the subject matter to give the history of the individuals and the math behind the development. I'd have preferred a book half the length and just as technical.Also, Von Neumann seems like kind of a jerk. "
197,0,http://goodreads.com/user/show/40758507-robert-cerefice,3,"Parts of this story were excellent. But there were long intervals which were pretty dull. Some of the biographical information I thought led to my criticism. While I am not a scientist or mathematician I found the technical easy to follow. All in all the history as to how we got to where we are today was a true ""eye opener."""
198,0,http://goodreads.com/user/show/8782986-ian,3,"It was great to get a comprehensive history of the beginning of the computing age, of which I knew some, but not all put together like this. Only problem is it is too comprehensive which gets in the way of readability. Dyson had connections through his father to many of these people so that helps tell the tale. I learnt many things I had not known despite working in this field for decades."
199,0,http://goodreads.com/user/show/47361993-shawnfuryan,3,"The subject matter is incredibly compelling, but the book suffers from an aggravated lack of focus that saps the proceedings of their intrinsic drama in the retelling. There is a lot of worth while information in this book, and so I think it's a worthwhile read for those interested in John von Neumann and his exploits. It is something of a dreadful slog, however."
200,0,http://goodreads.com/user/show/61756782-eli-pollack,3,"the historical information is interesting and I have to conclude that it is impossible to write a book on this subject that I would understand. Probably anyone who reads Popular Mechanics or has a similar level of interest, but my warning is to be ready for a lot of technical information on, for example, how the first computers were wired and why this method worked but that way wouldn't. "
201,0,http://goodreads.com/user/show/4926442-dave-jones,3,"Interesting series of historical vignettes about the development of the first electronic computers. Would have liked it to make a point but all it seemed to do is provide a series of antecdotes. If you are interested in the history of computing, this is worthwhile but it is a slog."
202,0,http://goodreads.com/user/show/1415049-david,4,"I guess Turing's name sells more books than von Neumann, which is who the book is really about. Still, it's a fascinating look at a crucial time in the development of the modern world, when bombs, Boole, and biology came together."
203,0,http://goodreads.com/user/show/7261646-kevin,4,"This is at times a difficult read -- expect many references to chemistry, physics, biology and math -- but ultimately quite rewarding. It's fascinating to see where the modern computer ultimately sprung up from and all the exciting results that came about through the discovery of the computer."
204,0,http://goodreads.com/user/show/5447407-christer-karlsson,5,"If you hope for a exciting reading with tons of entertainment value, walk right by it. If you interested in the development of the first machine, the people that built it and the problems and programs they tried to use this machine for, then read it."
205,0,http://goodreads.com/user/show/9603853-pablos,5,"Nobody interested in computers should miss this extraordinary history by George Dyson, a true historian. George got access to Von Neumann's notes and to most of the living people from the Institute for Advanced Study when researching this book. Incredible stuff."
206,0,http://goodreads.com/user/show/3012146-megan-richardson,2,"I wanted to like it, but after about 100 pages (that took me almost a week) I had to give up. It's very dry. And the author went so deep into the mathematical aspects of developing Turing's Universal Machine that my eyes kind of glazed over. hmm"
207,0,http://goodreads.com/user/show/81564-sarah,2,"I wanted to like this book, particularly a this is a historical period that I've been particularly interested in of late. But I found the first hundred pages too dry to hold my attention and couldn't finish."
208,0,http://goodreads.com/user/show/3314747-denise,3,  Plethora of interesting information. Poorly assembled. Great for those wishing to learn more about John von Neumann and the many contributors to computer development. Highly recommend the TED talk by Dyson as it is better organized.
209,0,http://goodreads.com/user/show/5937919-daniel-farabaugh,1,I could not finish this book. At times it was too technical to follow and at other times it boiled down into committee meeting lists. The book just seemed to fail to capture the excitement of the creation of the computer and it development.
210,0,http://goodreads.com/user/show/2678453-scott-thrift,2,"This book doesn't know what it wants to be. I didn't need a history of the founding of Pennsylvania. Or Princeton. Yikes. Quaker theology? The first chapter was fascinating, but then it stopped being about Turing or the cathedral or even about computers. I gave up."
211,0,http://goodreads.com/user/show/3761150-lily,4,"Good book for listening -- especially if you like technology, particularly computing. Spans the trivial to the profound. I recommended heartily to friends and family in the computer industry. Having been there myself, I enjoyed the book very much, but am glad I used the audio version."
212,0,http://goodreads.com/user/show/16207566-jean,3,"Interesting background story of the mathematicians and engineers at Princeton who designed ENIAC, MANIAC, JONNIAC. Should have been named ""Newman's Cathedral"". Good reading if you enjoy history as it does go back to Liebniz"
213,0,http://goodreads.com/user/show/16376152-bill-wells,5,"I thought this was a fascinating look into the beginnings of the digital world, especially for someone who is not very aware of the history. The writing I found to be clear and not overly technical, but more about the people behind the ideas."
214,0,http://goodreads.com/user/show/634204-debra,2,"I am finding the nonlinear organization difficult to.follow, probably because I am not well versed in the subject. Having come this far I will finish the book, just not sure I will take much away with me."
215,0,http://goodreads.com/user/show/19653201-joe-rim,4,Really great science writing focusing on the micro-politics between the researchers at the Institute for Advanced Studies at Princeton. It was both fun and difficult to follow who was who doing what where. 
216,0,http://goodreads.com/user/show/21811329-katie,3,"While the book had some interesting information on the first computers, sometimes it got bogged down by having too many details. In addition, Chapters 14-17 could have been removed, and the book would have been better for it. All in all, an okay book."
217,0,http://goodreads.com/user/show/18250619-chris-ingram,5,I really enjoyed this book. It is a broad brush look at both the personalities and the technical achievements of the people who were present at the dawn of the computer age - primarily von Neumann and others at the IAS in Princeton during and shortly after WWII.
218,0,http://goodreads.com/user/show/5687721-cypress-butane,4,"Good book. Even if it jumps over my head a bit even from the start. A lot of information on the birth of computing, told in parallel with the birth of atomic weapons. A lot of people involved and a lot of stories."
219,0,http://goodreads.com/user/show/11126556-angelo,3,"Interesting story of most facts you probably already now, but still fun to read. I didn't care much for the 'moralizing' afterthoughts to each sections, where the author tries to put the awesomeness you just read into its modern-day context."
220,0,http://goodreads.com/user/show/1971043-rich-brown,2,"The ratings are high because it's being rated by people willing to eat their impossibly dense broccoli. There're great insights and historical insights and personal anecdotes here... But a tough, dense read. "
221,0,http://goodreads.com/user/show/2827908-john-e,3,"Maybe I'm not smart enough to follow all the logic, but the narrative flow sure left a lot to be desired. This is a wonderful series of glimpses of the men and women who lead the world into computers and hydrogen bombs. "
222,0,http://goodreads.com/user/show/12999628-neal-w,3,"Have to agree with another reviewer that this book is really about Von Newman. Very little about Turing. Very strange structure to the book. Interesting, detailed, but not very reviewing on explaining the tech. "
223,0,http://goodreads.com/user/show/27619244-ant,3,"As an historical account of computing, with reference to it's place on the world stage, I really enjoyed this. The digressions are entertaining and the biographical details add weight to the story. Not so much with the technical details though, which was a bit of a shame."
224,0,http://goodreads.com/user/show/2410100-mrklingon,5,"What Marvels They Wrought!Excellent survey of the prehistory and growth of the digital age. Dyson does a tremendous job here, providing insight into the roots of the world we now live in, spanning time, space and cyberspace."
225,0,http://goodreads.com/user/show/7143706-troy,5,"""Everything that is visible must grow beyond itself and extend into the realm of the invisible."" Top-notch, deep dive & detailed story of the trials & tribulations of some of the brightest minds of 20th century. "
226,0,http://goodreads.com/user/show/39580273-farooq-mahmud,3,Personal accounts from the people who pioneered digital computing. Set against the backdrop of WWII the development of atomic/nuclear weapons accelerated the development of computing. The title is a bit misleading since it focuses on von Neumann and his circle. 
227,0,http://goodreads.com/user/show/30519456-emil-petersen,3,"This is more about Princeton's Institute for Advanced Study and John Von Neumann than the digital universe, Turing or whatever the titles might suggest. I did not mind, because there is a really interesting string of events linked to IAS and JVN, much of which is dealt with here. "
228,0,http://goodreads.com/user/show/7453859-shayne,4,"This book chronicles the work of some of the giants in computer science – people like von Neumann and Turing. It's a good read, and the last third takes the discussion places I hadn't foreseen. (No spoilers from me, though!)"
229,0,http://goodreads.com/user/show/4481939-cario-lam,5, A detailed account of the building of the foundations of digital computers mainly at the IAS and all the men and women involved. The foundations are mathematical and the book would even make the most math anxious person appreciate maths. 
230,0,http://goodreads.com/user/show/5085972-jes-s-navarrete,4,"The origin of the first computers is part of the history and it is a good read to understand was going on, how the different versions were developed and the places. All of this mixed with the II World War makes this book a really nice story."
231,0,http://goodreads.com/user/show/78256306-ishaan-gandhi,5,Great stuff
232,0,http://goodreads.com/user/show/4719410-douglas,4,"Interesting history of the tie between mathematics, engineering and computer science. Not quite what I expected but I did learn a number of things and got a few ideas for further reading. "
233,0,http://goodreads.com/user/show/64295986-simeon,3,Lots of interesting math and technical details about Von Neumann and early development of computers
234,0,http://goodreads.com/user/show/26739219-william-sedlack,3,Had such promise but Dyson was too distracted by the sometimes wonderful stories of his father's generation at the IAS and the book was ultimately killed by bloat. No clear thesis.
235,0,http://goodreads.com/user/show/36205036-steve-harvey,4,A great book if you're an old guy who's been programming computers since the 1960's and if you love history and if you're interested in the theoretical basis of computers!
236,0,http://goodreads.com/user/show/14240252-dendi,4,Story jumping but still nicely written
237,0,http://goodreads.com/user/show/11764068-bob,4,Interesting bit of history and profiles of some brilliant people doing amazing things. 
238,0,http://goodreads.com/user/show/17794099-emily,4,"Are we searching the search engines, or are the search engines searching us?I rate books based on two main factors: did it make me think, or did it make me feel?The degree to which a book achieves one of those two aims (and very, very rarely both) influences a good chunk of my rating. I knew a tiny amount about Turing. Mostly his work on decoding the Enigma machines and as the father of modern computing. I saw this book and it intrigued me. So I picked it up.It was not, in many respects, the read I had anticipated. For one thing, Turing is referenced very minimally until near the end of the book, and for another, the book goes into the lives of dozens of people surrounding the creation of the first computers. This is not a bad thing, but it made this a difficult read to keep track of, at points.This was also technically a challenge for me to understand. I got the basics, like how we've gone from numbers that mean things, to numbers that do things, but much of the technical discussion of the code and mathematical or engineering principles went way over my head. But, this book had a few truly interesting insights that I found really worthwhile. The quote at the beginning, and the discussions of the future of AI, discussions of how we are essentially building a new lifeform that will soon outlive and outpace us, and of course the connections between DNA, Nuclear bombs, computers, and the weather were interesting.So. Did this book make me think? Yes, it did. It posed interesting theories of where we're going, and how we're likely to get there. I fear I am not smart enough to understand all the knowledge this book has to offer, but for the most part, I enjoyed what I did understand."
239,0,http://goodreads.com/user/show/2400984-greg-stearns,5,"This is where it all began.Despite being the bishop of the titular cathedral, Alan Turing makes a rather brief appearance in this book. Instead, this is mostly a biography of one of the most incredible minds of the 2oth Century: John von Neumann. He was arguably the smartest man of his age (and he shared a building with Einstein, Gödel, Pauli, etc). This book covers the birth of Princeton's Institute for Advanced Study by its benefactors and administrators and sweeps us right into the Manhattan Project. This book shows that the twins of the Computer and Atomic Bomb relied on each other in so many ways. While the physics necessary to create the bomb helped shape science for coming generations the invention itself was kept by US military. If not for some very persuasive scientists and mathematicians, the computer would have faced the same fate. Once freed to be studied, the modern world can begin in earnest.This is an fantastic story of incredibly brilliant people, told excellently. Turing's Cathedral is a fantastic prequel to Hackers: Heroes of the Computer Revolution. I think it is a fantastic overview of early computing history, especially through the early Mainframe era. If you're interested in this slice of history, you can't go wrong with this book."
240,0,http://goodreads.com/user/show/49575307-rodney-harvill,3,"I think this book might have been more interesting had I not listened to it as an audiobook while commuting, and there are several reasons for this assessment. First, Mr. Dyson does not organize the book chronologically; instead, he picks a topic and follows it chronologically and then moves on to another topic. Having a table of contents might have actually helped me to better follow the book. Second, while I am an engineer, I am no expert on the nuts and bolts of computers, and this book discusses in detail the underlying theory and engineering of the early computers.On the other hand, there were some positives to the book. First, because much of the early computer work described in the book took place at Princeton’s Institute for Advanced Study, there was substantial tension between pure mathematicians and scientists, on the one hand, and the engineers who implemented the design, on the other. An engineer myself, I found these dynamics an intriguing supplement to the jokes I heard years ago in college explaining the differences between engineers, physicists and mathematicians. Second, as an engineer who does computer modeling, I was fascinated by the accounts of the early use of computers for modeling purposes. The Monte Carlo techniques used today for radiation shielding, criticality safety and radiation heat transfer were pioneered for nuclear weapons development and were one of the first uses of digital computers. Furthermore, another early use of computer modeling was weather prediction. Because I live in the Carolinas, I occasionally have to pay attention to tropical storm and hurricane forecasts that are based on computer models. The science of computer-based meteorology started with the earliest computer at Princeton. Fascinating stuff."
241,0,http://goodreads.com/user/show/14734538-roberto-rigolin-f-lopes,0,"This is an entertaining account of how the digital computer was developed in the US mixing bits of history, biography, and science. The adventure started with Leibniz, was defined by Hilbert, solved by Turing and implemented by Von Neuman, a Jew who had to move from Germany to the US. In this book, the whole thing orbits around John von Neuman and his friends; a bunch of brilliant refugees engaged in war research (say Ulam, Fermi and Teller). George also reminds us of the intense interplay among Mathematics, Physics, and Biology pushing the development of computers, which is mostly forgotten these days because computers are everywhere. There are also some striking definitions that may force you to rethink your view of universal computers. Here goes my favorite: memory is a set of bits varying in space and invariant across time and code are bits varying in time but invariant across space. Takeaway message: it is edifying to know the origin of both physical and digital universes and the isomorphisms among them."
242,0,http://goodreads.com/user/show/68029956-paul,4,"3.5 stars. An interesting example of an author being chained to their research; it's clear that Dyson spent quite a bit of time in the IAS archives researching Von Neumann's team, but the problem is that he seems to feel obligated to focus on this to the detriment of everything else, i.e., Turing himself (who only appears briefly near the end), computing in general, or the wider historical context. Dyson stuffs in so many random factoids about hiring dates of IAS staff and buildings at Princeton (did you know that the Cleveland Tower was commissioned by the class of 1892 and has a bell that weighs 12,880 pounds? well, now you do) that he never really gets around to explaining his topic especially well. With that said, the story of Von Neumann and the origins of digital computing is incredibly interesting, so Dyson's shortcomings as a writer don't get in the way too much."
243,0,http://goodreads.com/user/show/5599380-alex,2,"A sort of strange book. It’s called Turing’s Cathedral, but it’s almost entirely about John von Neumann. It’s about one of the most logical of subjects, but the last 30-40 pages skips before and after von Neumann’s death in very confusing ways. In general, the book has a deep lack of context and an aversion to technicality — instead describing computer operations in bizarre ways trying to make them sound like biological autonoma. That said, the subject matter is fascinating. The deep interconnection between the race for the H-bomb and the creation of the first Turing Machines is worth understanding for anyone who lives surrounded by them."
244,0,http://goodreads.com/user/show/23553598-rodney-norris,2,"Interesting for the historical research and biographical content for the people involved in building early computers. But lacking in technical understanding. I found some of the explanations and pontificating on computer architectures and code laughable.Overall I liked the content, but found the book hard to follow. He is covering a large cast of characters and jumps around giving biographical information for each of them. The effect is the author talks about overlapping time periods through the book, while also jumping backwards and forwards in time depending one who he is focused on at the moment. Which can make the book feel like spaghetti at times."
245,0,http://goodreads.com/user/show/37469192-glen,1,"My smart friends who I look up to read this book and loved it. I couldn't get through it. The author laboriously summarizes the lives of seemingly all of Turing's ancestors, which just wasn't interesting to me at all. I probably should have skipped ahead, but instead I just skipped the book. I felt similarly about Code Girls: The Untold Story of the American Women Code Breakers Who Helped Win World War II."
246,0,http://goodreads.com/user/show/113879395-john-tobler,5,"An Essential Computer Historical Magnum OpusGeorge Dyson has created, with great love and effort, an unsurpassable group portrait of the brilliant and amazing people who invented modern computing and many other things, both good and bad. This is a deep analysis of a time and place and people and events that could only have happened once upon a time.A magnificent work of literary and historical art!"
247,0,http://goodreads.com/user/show/3437934-courtney,2,This book is really more about von Neumann than Turing. Von Neumann seemed to be the personality around which most computing activity at the Institute for Advanced Study revolved. Turing shows up as a prudish and demanding visitor.This book has a tedious amount of detail. I don't really care about salaries and costs in terms of 1940/1950s dollars because that doesn't mean anything to me. The narrative also jumps around in time.
248,0,http://goodreads.com/user/show/995651-nick,2,"George Dyson may be a really smart guy, but in this book, he either does a poor job of explaining the technical concepts at the core of the early days of digital computing, or he allows his preference for flowery language to obscure them. If you don't already know how this stuff works, this book isn't going to teach it to you. And given the title, Alan Turing comes off as a bit player, with roughly 75% of the book focused on John Von Neumann. "
249,0,http://goodreads.com/user/show/106296401-serhan-yalciner,3,"""...Computers are designed to be problem solvers, whereas the politicians have inherited the stone age syndrome of the tribal chieftains, who take for granted that they can rule their people only by making them hate and fight all other tribes,” Alfvén continued. “If we have the choice of being governed by problem generating trouble makers, or by problem solvers, every sensible man of course would prefer the latter."""
250,0,http://goodreads.com/user/show/100575742-frank,2,"The book ends with the quote ""There must be something about this code that you haven't explained yet"", from Theodor W Lancaster to Mr Barricelli. Coincidentally, that's how I feel about the book as a whole. It seems that there was very little information in it except regarding the arrangement and composition of buildings, and rooms within those buildings."
251,0,http://goodreads.com/user/show/30414650-bill-yancey,5,"Great history of the origins of the digital world (from Turing to the present), which includes the history of mathematics, the making of the atomic and hydrogen bombs, the influx of scientists from Europe to the USA prior to WWII and how they helped win the war, and of the history of the Institute of Advanced Study in Princeton. Dyson's best so far. "
252,0,http://goodreads.com/user/show/81651835-porter,1,This book was insanely boring and focused almost entirely on unimportant extraneous details about famous scientists lives. Like what cars they bought and what years. Where they moved to with no relevance to their scientific or mathematical efforts. 
253,0,http://goodreads.com/user/show/1775057-alaina,1,"This book is incomprehensible. I give up. I rarely give up on a book, but I'm understanding, at best, 2/3 of the content. None of what I'm understanding is computer-related, and that was the entire reason I picked up this book. "
254,0,http://goodreads.com/user/show/8304337-david,3,"I really enjoyed a lot of the contents of this book, and perhaps its just a reflection on how I listened to it, but as it went on, I found it harder and harder to follow the chronology/main ideas. "
255,0,http://goodreads.com/user/show/10101439-norah-s,1,Just couldn't get interested.
256,0,http://goodreads.com/user/show/68369797-ricardo,1,The way this book is written is too boring
257,0,http://goodreads.com/user/show/3386213-brian-kovesci,2,Given the title I was expecting the story of Alan Turing. He was barely discussed in this history of computers. Snoozefest 2017.
258,0,http://goodreads.com/user/show/36523169-laurie-brooks,2,"It was too hard to read, I never really finished it."
259,0,http://goodreads.com/user/show/1392158-kyle,2,"Turned into a slog by the end, but had some interesting bits. Some good research and stories, but you'd better be very, very interested in early computer history. Much more than I was."
260,0,http://goodreads.com/user/show/85828020-colin,2,"Alan Turing is my hero, but I just couldn't get into it. Too many names too quickly too haphazardly."
261,0,http://goodreads.com/user/show/5145024-victoria-grace,2,"We have extremely different ideas about what computers are, and are good for. "
262,0,http://goodreads.com/user/show/32221231-abrandon,2,"The copy I read was marked up & under-lined, meaning I hadn’t any option to resell the book. "
263,0,http://goodreads.com/user/show/16191221-daniel-calder-n,4,Good account of origin of computers. Great testimonies and huge reference readings.
264,0,http://goodreads.com/user/show/14759531-earl-veale,3,A very very detailed accounting of the birth of computers. Lots of information. 
265,0,http://goodreads.com/user/show/102568976-jeremy-bernick,4,Deeply moving and inspiring
266,0,http://goodreads.com/user/show/82931574-emily,3,"I read this in one day because it was overdue lmao. Considering I'm computer illiterate, most of this went over my head, but it was interesting and made me felt smart as I read it."
267,0,http://goodreads.com/user/show/15304811-william,5,"The Institute for Advanced Study and the beginning of the digital world. Features Turing, von Neumann, Julian Bigelow, and others."
268,0,http://goodreads.com/user/show/89404830-qin-zhong,3,I think this book is not clear enough in explaining how a computer works at a low level.
269,0,http://goodreads.com/user/show/67046655-rustyshack,3,"Tiring to read, DNF at 20%. Hard to combine and make compelling or coherent a collection of highly detailed personal, geographic, institutional, political, and multiple disciplinary histories."
270,0,http://goodreads.com/user/show/504424-bup,3,"Pretty interesting book about the first computers and the people who built them, and what problems they were first used for."
271,0,http://goodreads.com/user/show/5564421-kyle,3,Too much biography; the last few chapters about artificial life are the best.
272,0,http://goodreads.com/user/show/115339407-raakhi-chotai,4,Skip the first 50 pages if you hated Physics/Maths at school. 
273,0,http://goodreads.com/user/show/100813945-sue-chant,1,Couldn't get into it - the style of writing was very irritating. Gave up after 3 chapters.
274,0,http://goodreads.com/user/show/26982704-steve,4,Cool history. Gave me some useful tidbits on the history behind modern-day computer architecture. Some parts were a breeze to read. Others require a strong will and patience. 
275,0,http://goodreads.com/user/show/77893263-brittany-henke,2,DNF because I couldn’t get into it. This book was just not for me!
276,0,http://goodreads.com/user/show/46070845-chris-plaisier,5,So amazing. History and technology together are awesome.
277,0,http://goodreads.com/user/show/35438505-edward-b,1,"UPON FINISHING THE BOOK:tl;dr - DON'T BOTHER.I finally made myself finish it only through sheer bloody-mindedness.This is nothing but incoherent biographies of everyone who had anything *at all* to do with the development of the computer *at the Institute for Advanced Studies*. If you weren't in New Jersey then you don't even rate a mention - Charles Babbage, Ada Lovelace, Konrad Zuse, for example.And despite the title, it's mostly about John von Neumann, not Alan Turing.The only non-biography bits are a handful of imaginative but wrong hand-wavy paragraphs about the ""meaning"" of computers and the future of humanity, in the last couple chapters.I had originally tagged this as ""comp-sci"", but I'm dropping that as misleading, and calling it strictly ""biography"".UPON BARELY STARTING THE BOOK:I am on page 28 (of 338, excluding end-notes and index) and I need to vent.I do not need to know the history - back to pre-European times! - of the piece of land on which Princeton's Institute for Advanced Studies now stands. I am not particularly interested in the family histories, going back centuries, of the people who donated the funds that allowed for founding of the Institute.Get a grip, George (Dyson; the author; also son of physicist Freeman Dyson, although that's neither here nor there). You said in the Foreword that writing this book took you eight years, but that doesn't mean that you have to include every single little detail that you learned. I get that you're trying to tell a story, and not just present a dry list of facts. But too much is too much, eh?I am incurring an opportunity cost to read this, after all - I could be reading things that would be much more interesting/useful/relevant to me, or even just amusing.Most people who already know who Turing is, or are interested in the history of computing, are techies - we don't find other people to be inherently endlessly fascinating...Now, all that being vented, I am going to continue reading this book, because I am hopeful that there is some good meat under all this fat.END_RANT"
278,0,http://goodreads.com/user/show/111355073-andrew-judson,5,"Review copied from my website: https://andrewjudson.com/book%20revie...I recently read “Turing’s Cathedral” by George Dyson. The book presents the history of the computer at the Institute for Advanced Study, interwoven with the stories of the different people involved in both the founding of the Institute as well as the history of computing itself. I found the book very enjoyable; more technical than some average pop sci book, but high enough level that I could skip over some technical details and still get the gist of things. In particular I was struck by how closely entwined the history of the computer was with the building of the atomic and hydrogen bombs. I had always had a sort of general feeling about the correlation of the invention of the computer and war, but it was always on the level of “Oh, Turing used it to crack codes!”. There wasn’t that explicit direct link between the computer and atomic fire. The book describes how all of the early computers in the US were explicitly military and primarily used for bomb calculations - from ENIAC on to the IAS machine, and then the commercial IBM machines that came after that. The book does touch on some scientific application, but that seemed to only emphasize the military origin more. For example, some weather researchers got to use the machine one hour a day in the middle of the night, when the bomb calculations had completed for the day.The other thing I really enjoyed in this book was learning more about Von Neumann’s life. I’ve always been interested in Von Neumann due to his famous intelligence, his broad contributions across so many topics, and his sense of duty and dedication to his adopted country. The book gives a detailed biography of the man, and is fairly even handed in both its praise and its criticism. One alleged flaw I had not been aware of was how Von Neumann published the technical details of the computer designs, denying the patent rights to the designers of ENIAC and later on similarly to his own engineering team at IAS, despite gaining lucrative consulting fees for himself at IBM. Despite this, I came away from the book even more impressed with him than I had been before. Von Neumann essentially stopped focusing on pure mathematics at the end of the Manhattan Project, besides the occasional recreational pursuit, in order to focus on what had to be done to survive the Cold War. He correctly foresaw the importance of developing the hydrogen bomb, whereas others - e.g. Oppenheimer - had either been naive about Soviet intentions or the US’s capabilities. This was also true for the development of the ICBM, which Von Neumann also helped gain political backing for. It is sobering to think of a world where the US had only begun hydrogen bomb and ICBM research after the Soviets had capability in both. The Great Man Theory of History may not be universally applicable but I do believe that Von Neumann saved us from a much darker path.Underneath this military reality is the sense of tragedy, of what could of been. Both Von Neumann and the IAS were derailed by the political realities of the time. His tragic death prevented us from seeing what he could have contributed to biology, and the war prevented us from seeing what else he could have contributed to pure mathematics. This is compounded by the other, also tragic, death of Turing, and of lead computer engineer Julian Bigelow’s ostracization from IAS by the jealous academics there. At the same time, that war was the crucible that brought the scientists together and forged all of these new concepts in physics and engineering, and enabled the physical realization of Turing’s dream. All in all, this was a profoundly human book, despite its mechanical topic."
279,0,http://goodreads.com/user/show/7199476-david-sogge,1,"Origins of the Digital Universe? That intriguing subtitle, and the book’s prestigious publisher (Penguin), were bait enough for me to grab this book, at an airport, with no time to sample some bits before buying it. Buy in haste, regret at leisure. For it is a mediocre book.For one thing, the story could have been told as a chronicle. But the writer elected instead to hang most of his chapters around certain personages. They walk into and out of the main theater, Princeton, often accompanied by sundry others with no relevance to the story. Hence successive chapters oblige the reader to wander back and forth across roughly the same decades, often without timelines, trying to pick up threads of meaning and connections among the players. The writing is tiresome, and also badly organised. The preface introduces a word, Entscheidungsproblem, referring to something or other that “makes the digital universe so interesting and that’s what brings us here.” This proved a case of pump and dump, since the meaning of this German mouthful is not explained until we are three-quarters of the way through the book. At that point we are finally told (in a side remark put in parentheses) that the practical result of resolving the Entscheidungsproblem was … software. Now he tells us! Another arresting statement, also well past the middle of the book, is that these advances in computer science “made their creators fabulously wealthy, securing contracts with the national laboratories and fortunes for Remmington Rand and IBM.” Now there’s a juicy and relevant topic. But it gets no follow-up whatever. It was just a throwaway line. This book's path is potholed with missed opportunities.  As a stranger to the history of science and technology I can't say for sure, but would be surprised if there are not several good alternatives to this book. My own suggestion: Jon Agar’s Science in the Twentieth Century and Beyond; in 14 pages of clear prose and logical narrative he synthesizes the story that Turing’s Cathedral purports to tell, but can’t, in its ± 350 baggy, confusing pages. "
280,0,http://goodreads.com/user/show/7809864-richard,2,"The book is overly broad and lacking in focus. Too much detail is provided on a number of players in the relatively recent history of Mathematics and the development of digital computers. Some of the events, and the names of some of the players, have been lost to history, and the author apparently felt that it was his duty to tell all of their stories in a single book. Some of the conclusions drawn by Dyson are just plain wrong, not to mention wacky. It was very difficult to determine the goal of the book. The title suggests Turing, but the story describes von Neumann in great detail. It also describes a number of other Mathematicians, Engineers, Economists, Physicists and Theoreticians. The organization of the book seems very confused. The frame of reference is constantly shifting between the historical, the geographical, the chronological, and the biographical aspects of computing. At times, it was riveting. At other times it was so boring and filled with minutiae that I would skip several consecutive pages before resuming my reading. Unfortunately, there is ample evidence in the book that the author does not really understand the subject about which he chose to write: digital computers and how they were developed. He makes absurd comparisons between machines and life forms. He believes that analog computing will be resurrected (HUH??). On the last page of Chapter 14, he goes so far as to say: ""Analog is back, and is here to stay."" That's just nonsense! He even makes ridiculous predictions about the Internet and search engines being self-aware and having dreams. As the book progressed, he began to confuse Philosophical and Metaphysical notions with straightforward computer technology. Eventually, I gave up trying to slog through this chaotic work and abandoned it about two-thirds of the way through. The author should have made a better effort to familiarize himself with the technology about which he chose to write. I have a computer background, and I found this book to be very disappointing. 2 Stars."
281,0,http://goodreads.com/user/show/3612009-ben,2,"I don't mind the rambling ""scenic detour"" nature of this book. It has a vaguely Renaissance–man air about it, and I'd like to think I can find something of interest in a wide range of topics. That wasn't the case with the tangential topics in this book, unfortunately. I also don't mind the misleading title, as Turing does eventually show up a dozen or so chapters in. I could have done without the blow–by–blow administrative detail and admit I began skimming the protracted background stories when I realised the author wasn't going to relate it to the story or really, draw any conclusions at all. These are forgiveable flaws though. I suspect the author did a great deal of research, but left the chaff with the wheat to show something for the labour—the editing may not be commendable, but the initial effort, at least, is.My real problem with the book is at some point in the latter third, it ceases to be a historical account, and steps boldly into lunatic prophecy. The author takes the digital and biological worlds in one hand each, claps them together, binds them with a tape of tortured analogy, and like a cat with a dead bird, leaves this gift for the reader to enjoy. There are powerfully vague parallels drawn between the digital and biological, but they have no justification, and offer little insight. I have never put down a book so often in simple disbelief at what I'm reading. The author particularly likes the venerable A-B, B-A form of pith:Are biochemical organisms the larval phase of digital computers? Or are digital computers the larval phase of biochemical organisms?This sounds profound, but is largely drivel.During the historical section of the book I enjoyed the scope, and in parts, the thorough detail. I would not, however, recommend that it be read."
282,0,http://goodreads.com/user/show/18942380-william-schram,2,"This book was okay. The most terrible thing about it was how the author seemed to jump from one subject to another. He talks about how everything that was computable was attempted to be computed on early computers. George Dyson seems to want to do too much with his limited space in this book. He goes from the problem of Nuclear Weapons development to the creation of digital life.Although this book is called Turing's Cathedral, it's mostly about John von Neumann and how he went flitting about and making advances in early computer technology. Even if he didn't make the advances himself, von Neumann always found a way to be involved. There's nothing really wrong about this, but, it is slightly misleading. Turing doesn't even come into the picture until the thirteenth chapter, causing me to wonder why it was called this at all. I mean, I guess the point of the entire book is to convey the enormity of Turing's Universal Machine, and the influences it has on the present.Due to the meandering nature of the author's attention, this book is comparable to Gravity's Rainbow by Thomas Pynchon in some ways. It also made it so I wanted to drop this book numerous times. The author starts out conventionally enough, by talking about 1953 and the first Hydrogen Bomb. He then wanders over to the creation of New Jersey...? But wait no, this matters because New Jersey is where Princeton is, and Princeton is where the IAS (Institute for Advanced Study) is located.So we get to find out that William Penn, the famous Quaker that founded Pennsylvania also had a hand in founding New Jersey.*Sigh* In any case, this book is not really that well organized, though I suppose it does contain a number of neat old photographs.In any case, I might read this book again but it would have to be for a pretty good reason."
283,0,http://goodreads.com/user/show/1286115-vanessa,2,"Not history, not journalism, not popular science. More than anything it comes off as an attempt to write a history book by someone who knows nothing about how to do so. It was brutally difficult to finish.The author spends an inordinate amount of time relating facts that have no significance to anything. He's especially fond of detailing people's travel arrangements: they drove themselves in this kind of car, they got a lift with so and so in that kind of car, they took the train... Also salaries, how much everyone was paid. How much they paid for their houses. Exactly which numbered offices were occupied by which mathematicians. The life story of the woman responsible for Einstein's menu at IAS. And on and on, a horrifically boring and irrelevant list of facts. The important stuff, like what the Von Neumann computer architecture is, why it was important, and why it has endured, is barely touched upon.On the other hand, a few things such as the Hilbert program, the work of Gödel and Turing, is explained to some decent extent, but sort of requires you to know something about it all in the first place. So it's a nice reminder for those of us who either have a background in mathematics and computation theory, or have read popular accounts of it. But for people new to it, I expect it will be as opaque as the purpose behind this book.Just give it a pass. There must be a better book on this topic somewhere."
284,0,http://goodreads.com/user/show/1279153-michael-berman,1,"I really wanted to like this book, but it turns out that I can't bring myself to finish it. The main flaws that I've found (as I've worked my way through about half of it) are:--I don't understand the basics of the computer technology. I don't know if that's my limitation or the author's failure to explain things well enough. I was hoping to get at least a basic grasp of how the first computers worked, and I'm more mystified than when I started. Some other reviewers suggest that it might be because the author doesn't understand. I don't know enough to know if this is true or not. At any rate, without being able to understand how the earliest computers did what they did, the rest of the book is less interesting.--The level of irrelevant detail is excruciating. I don't really care (for example) about the menu items or prices at the dining room in the Institute for Advanced Studies. Just because your research team gathered the information doesn't mean you have to include it in the book.--The chronology in the book seemed to jump around a bit. I sometimes couldn't tell if we were before WWII, during the war, or after.In theory, a history of how our digital world got started seems fascinating to me, and maybe it will would be, but in a different book."
285,0,http://goodreads.com/user/show/870226-andrew-damian,2,"The subject is quite interesting but the way in which the book is written is very confusing. There are countless digressions along the way and although this could be interesting, the author has no gift in really bringing to life these various characters and their interactions. He also quotes so much from his research that it led me to believe that he's simply published the results of his extensive research without polishing them into a coherent story. The title is also misleading. Turing is really the focus of one chapter. The ""Cathedral"" he's talking about is in my opinion the wrong metaphor for the computer, especially since he goes on to talk about the rise of the ""digital universe"". How a cathedral and an universe are related is unclear. The concept of the digital universe is really only talked about at the end of the book and should have been a theme explored in more detail and with more thoughtfulness throughout the book. Overall, I find the book to be a chronological telling of events without much thought given to their meaning. I was disappointed."
286,0,http://goodreads.com/user/show/33015077-jacob-simmering,2,"I have complicated thoughts about this book. I want to like it, but so much of it was not something I wanted. First, as a history of IAS, von Neumann, the atomic bomb and early computing, I enjoyed the book throughly. It was definitely disjointed and many times wrong or very vague (as other reviewers point out). I don't think Dyson fully understood a lot of the tech that he was trying to write and ended up just blowing that task. But as a biography and the parts about people were more than a good balance. That said, from the mid point about the hydrogen bomb on, the book goes down a crazy rabbit hole. At one point, in all honesty, Dyson attempts to argue that digital things, like the Internet, are a form of ET style alien life. Computer programs, from Google to Angry Birds, are sentient living and conscious organisms. It was like reading the writings of an insane person. My take away: read the first half and stop at the h bomb."
287,0,http://goodreads.com/user/show/3014030-vivian,2,"This could have been such a wonderful book, and I stuck with it hoping the wonderfulness would somehow emerge in between the interesting anecdotal stories about the contributors to the origins of computer science; unfortunately, it didn't quite happen...When the author is talking about the personalities involved, Turing, Von Neumann, Bigelow, Veblen, and all the rest, there are amusing tidbits and snippets of conversations. But there is also much veering off into unneeded detail; worse, the fundamental structure of the earliest computers and the ""words"" and ""numbers"" needed to make them work - we'd call them ""programs"" today - is annoying dense and incomprehensible. Terms are rattled off without explanation, and concepts are ""explained"" in a way which is anything but clarifying. Not recommended, unless you want to jump around for the interesting bits. "
288,0,http://goodreads.com/user/show/46181430-bart,2,"(I've read this book partially. I've made it a little past the halfway point before loosing interest. The book itself is OK, just not my cup of tea.)The book provides a rich account of the history of the computer, centered around the Institute of Advanced Study and approached from numerous different angles. Every significant player or location is discussed in great detail, with the author clearly intent on leaving no stone unturned. Unfortunately, this vast amount of detail also works against the clarity of the story. The author talks with equal enthousiasm about the smallest of details as he does about the most significant elements of the story, and leaves it up to the reader to pay attention and descern between the two. While this is not a totally unreasonable expectation, some help from the author in this regard would have been appreciated."
289,0,http://goodreads.com/user/show/12924633-addo99,2,"Okay history of the first functional electronic computers. You get a good amount of atmosphere of the times (late 30s to late 50s). Dyson spends a lot of time talking about the bomb - maybe more even than the computers. He certainly spends very little time on Turing the man. He gets maybe half a chapter. The central character is actually Johnny von Nuemann, another proto computer scientist. I wish Dyson had spent more time on how computers work, and what makes the binary system so key. Instead the book focuses on how the computers were funded - their construction and early use as war simulation tools. I'm sure you can find better books about either the atomic bomb or Alan Turing or early computer design."
290,0,http://goodreads.com/user/show/7827662-sivaram-velauthapillai,3,"The title is sort of misleading. It has nothing to do with Turing per se; however, it does cover the beginnings of the computer age. In particular, it focuses on the very first supercomputers built in the 1930's and 1940's. The book is an ode to von Neumann's life and his impact on the development of the first numerical computation machines.The book is mostly biographical, covering the key characters that influenced early computing. If there is one flaw in the book, it is that it didn't spend more time on some of the technical aspects of the problems that were being solved. Yes, they were described, but only in a casual sense that didn't provide any insight into why computers became what they were."
291,0,http://goodreads.com/user/show/6243880-gary-beauregard-bottomley,1,"""Too much history not enough Turing""One of the few books where I did not read it all. I generally love any book about Turing or information theory, but he delved too much in to the history. I really didn't need to know that the Indian tribe was on the site of the think tank before the think tank was built on it and so on. Not enough on Turing and his theory and too much history for my taste. (If you like history more than information theory, the book can work for you and go ahead and give it a try). "
292,0,http://goodreads.com/user/show/6260082-sean-martin,3,"More about von Neumann than Turing really, but it's a really nifty look into the development of computing and explains a lot about why certain things developed the way they did. In particular, I liked the focus on the very human history of the computer. I also had a really happy nerd moment when he was talking about the development of Monte Carlo simulations.The bits where the writer is speculating about the future are interesting but didn't feel as strong as the rest of the book. Occasionally rambling/randomly tangential bits were also a bit annoying, but overall, I really liked this book."
293,0,http://goodreads.com/user/show/8715497-esther,0,"This book takes me back in my own life to my troubleshooting days with the machine code print-out, an oscilloscape, and a schematic of the circuit. And even further back to card-punch input at MIT's Physics Department for the development of computerized typesetting. And with the origins so well described in this book, I see the impact of the military and the events of the world on the computers that we take for granted today. It's a fascinating story full of the personalities of the players. A great read!"
294,0,http://goodreads.com/user/show/4126687-vinod-khare,2,This book is a history of technology. The technology part is not very well understood by the author (or at least he fails to communicate it to the reader in any useful way). The history part is bogged down by minutiae. He actually has an entire paragraph listing who at IAS worked out of what room number!It does give a good overview of the atmosphere in which early computing developed and the motivations of the pioneers. But that could very well have been a magazine length article rather than a whole book.
295,0,http://goodreads.com/user/show/7229239-ali-lafferty,1,"Well THAT was boring. I don't think you're supposed to understand what they're saying about computers unless you actually KNOW computers so perhaps I shouldn't have even tried to read this. There didn't seem to be a huge plot build up, which is a bit more common in nonfiction, but even good nonfiction authors try to tell a story since humans respond to stories. The philosophical parts, as few as they were, were better when you could find them. So that's a plus. But otherwise I couldn't get into it."
296,0,http://goodreads.com/user/show/2122767-leah,2,"I wanted to like this one more than I did. I'm mildly obsessed with all things Turing-related, but this book was barely one of them. It might have more acutely been titled 'An Ode to John von Neumann: A piecemeal history of the IAS computer project.'Some points are awarded for recognizing contributions made by all people involved (e.g., calling out Franklin, Watson, and Crick instead of just Watson & Crick).The writing style didn't grab me, and it was such a disjointed way of telling a story that I finished it unsure of exactly what that story was supposed to be."
297,0,http://goodreads.com/user/show/33942373-david-bakker,2,"Some anecdotes from this book were interesting, but I was frustrated at the lack of technical detail about how early computers worked. It was mostly historical tales about the people and events surrounding the early computers and not about the computers themselves. It also focussed entirely on a period in the early 20th century when valves and tapes were being converted from analogue to digital instruments.However, it was well written and opened my eyes to an extremely influential epoch in history."
298,0,http://goodreads.com/user/show/9401123-chris,2,"Have you ever had someone explain something to you and then figure out that they kind of understand what they're talking about, but not really? If you'd like to experience that feeling then read this book. It's a mess of almost-but-not-entirely-incorrect explanations of computer architecture and almost-but-not-entirely-incorrect interpretations of scientific evidence and technological advancements. It's ambitious, but overly so. I recommend reading a different book, but I'm not sure which one yet. I'll get back to you on that."
299,0,http://goodreads.com/user/show/19030923-greg,2,"The topic, subject and concept of this book are great. However, it is all seriously hampered by poor writing, digressions, odd choices in organization and editing that is sorely lacking. Certain chapters have good pace and coherency while others are painfully difficult to get through. More than once I found myself wondering why it was taking so long to read this book and why such an interesting topic is so tedious to read. A real shame given the potential of the subject matter. Not recommended unless you have serious patience to slog through the clunky bits. "
