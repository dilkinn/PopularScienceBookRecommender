,isbn,user_link,ranking,review
0,0553418815,http://goodreads.com/user/show/32466179-arshias,1,"This was such a Malcolm Gladwell take on data science. I think this book touches on an important subject, and people should be aware of the issues O'Neil discusses. But instead of doing a deep dive into the subject, it just felt like a list of bad algorithms with instances of the people they hurt. It didn't contain many examples of ""WMDs"" that I had not already heard of, and that might be because she cited the New York Times for *like* all her case studies.As someone who works in the field, I don't think this book was geared towards me. It wasn't technical or specific enough. Knowing her background, I really wanted her to get into the nitty gritty of some of the mathematics behind the algorithms. I know she is capable of doing this, but I think she instead chose to appeal to a wider audience. That's cool... except I don't think she did a great job of that either because this book lacked the context necessary to give people unfamiliar with the field a view of how machine learning and analytics typically work.Ultimately, I thought the book felt unfocused, and it showed in the conclusion where proposed a series of pretty ridiculous recommendations within the span of about 15 pages. She has strong opinions on the topics she covered in the book. Although I agree with almost all of them, that isn't the point. The book is supposed to be about the algorithms, and instead she takes us on a tour of a collection of business and public policy malpractice, stating that the solution is to ""encode values into our algorithms."" Wut. Leaving the logistics of this aside... if the people doing the coding don't share your values (and they clearly don't), why would they do that? O'Neil herself noted a misalignment of incentives in many cases, particularly where the data work has been contracted out to other parties. Telling them to have values and trash their contracts is obviously not gonna fly! You can't fear-monger the whole book and then think everything will be happy sunshine rainbows once we take the data science Hippocratic oath. I don't think there are easy answers, and I think it is ok to admit that. The takeaway should have been that data science isn't better than humans... because it is humans. We made the algorithms, so they run the gamut of use cases and demonstrate all the shades of gray we exhibit ourselves (yes, all fifty of them.)TL;DR - Boo. I expected more from you, O'Neil."
1,0553418815,http://goodreads.com/user/show/1826682-trish,4,"O’Neil deserves some credit right off the bat for not waiting until her retirement from the hedge fund where she worked to tell us the secrets of how corporations use big data (our data). Underlying the collection and use of big data is an attempt to utilize efficiencies in the market place for goods, money, and talent. Big data ostensibly can also “set us free” from time constraints and uneven knowledge dispersal. Conversely the opposite is often true. We are at the mercy of how our own data is shredded and packaged, and errors in the model can mean mutually assured destruction—for the school, corporation, family.The book starts with examples any readers who actually picked up this book to read might recognize: the chances of getting into a major university. O’Neil doesn’t go into the actual algorithms but just explains the variables chosen to populate the algorithms. Just when I was wondering who this book is targeted at, since after all, we kind of know how to get into university already, she comes up with examples of big data messing with aspirations that are still (hopefully not) in our futures.She addresses the real pain-in-the-ass nature of minimum wage jobs where the inadequate part-time hours are constantly changing to maximize profits for owners and to screw with employees ability to plan their life, their children’s lives, and the children’s caretaker’s lives. O’Neil addressed the situation in 2009 when Amex decided to reduce the risk of credit card nonpayment by reducing the credit ceilings on users who shopped at certain stores, like Walmart. She shows us the way micro-targeting ends up using data to perpetuate inequities in opportunity and “social capital.”The hardest part of reading this book (there is no actual math), was keeping my mind on what O’Neil was saying. Every time she'd mention another example of the ways big data was screwing us over, my mind would wander to experiences of my own, or ones I’d heard from friends, family, or others. This is real stuff, and just when I thought that it would be an excellent book for those with skills and interest in social justice to take to an interview with Google, Amazon, or a big bank, in she comes with another example of how the “fixes” are almost worse than the disease (Facebook’s method of who your friends are determining your credit risk).But O’Neil reminds us big data, mathematics, algorithms, etc. aren’t going to go away.""Data is not going away. Nor are computers—much less mathematics. Predictive models are, increasingly, the tools we will be relying on to run our institutions, deploy our resources, and manage our lives. But as I’ve tried to show throughout this book, these models are constructed not just from data but from the choices we make about which data to pay attention to—and which to leave out. Those choices are not just about logistics, profits, and efficiency. They are fundamentally moral.""Exactly. We still have to use our brains, not just our computers. It is critical that we inject morality into the process or it will always be fundamentally unfair in some way or another, especially if the intent is to increase profits for one entity at the expense of another. One simply can’t include enough variables or specifics. Some universities have begun to audit the algorithms—like Princeton’s Transparency and Accountability Project—by masquerading as people of differing backgrounds and seeing what kind of treatment these individuals receive from online marketers. O’Neil suggests that sometimes data might be used to good effect by targeting frustrated online commenters with solutions to their issues: i.e., affordable housing info, or by searching out possible areas of workplace or child abuse and targeting that area with resources. She wades into national election data and notes that only swing states get candidates attention, suggesting, by the way, that the electoral college has outlived its usefulness to the citizenry. Algorithms are not going to administer justice or democracy unless we find a way to use them as a tool to root out inequities and try to find ways to deliver needed services where they are deficient.When I look at the totality of what O’Neil has discussed, I am inclined to think this book is best targeted to thoughtful high schoolers and college-aged students who are thinking about planning their careers, who have a penchant for mathematical and computer modeling, and who think their dream job might be with an online giant. I’d be happy to be disabused of this notion if someone wants to challenge my thought that much of this information is known to many of us who have been out of school for awhile and who have been paying attention to our online experiences and junk mail solicitations. But it is always interesting to read someone as coherent and on the side of social justice as Ms. O’Neil. It might be noted that Jaron Lanier in Who Owns the Future? (2013) also talks about the use of big data to steer our thinking and makes a preliminary suggestion that individuals should be paid for their data—for data that is collected about them, for profit. It is an interesting discussion as well. Love these intersections of technology and humanity."
2,0553418815,http://goodreads.com/user/show/2118933-leo-walsh,5,"Captivating. Insightful. And important. A 50,000 foot view of how automated big-data is a great tool for understanding human nature. How it has great promise to make our lives easy. And yet, a very real takedown of how systems engineers -- and corrupt power-seekers, like corporate executives and for-profit universities -- misuse this powerful tool. And the even worse cases where people start with good intentions, like ridding school systems of bad teachers, only to toss out ""false negatives.""I found Cathy O'Neil's ""Weapons of Math Destruction"" a very important book that highlights a lot of what's been going on in America over the past 30 or 40 years. For instance, I just finished ""The New Jim Crow,"" and wondered how the Supreme Court would continue to rule in favor of crime policing tactics that 1) target poor urban areas -- populated by mostly black men, and 2) allow the police the ability to stop people, willy-nilly, that they found ""suspicious"" -- despite the fact that minorities are caught in stop-and-frisk situations while most are innocently going about their lives. Problem is, many of these people going to jail for nuisance crimes -- possessing small amounts of marijuana, open containers, driving on expired tags, etc. Things that seem the exact definition of ""systematic racism."" But when O'Neil lays out how systems engineers have written algorithms that send police to ""hot spots,"" those rulings make sense. For instance, large crimes that we all want to stop -- car thefts, burglaries, rapes, assault and murder -- are rare. While petty and nuisance crimes -- jaywalking, possessing weed, noise violations, vandalism, etc. -- common. Based on the law of large numbers, a program trying to optimize ""broken windows"" policing would send more officers to a district with a higher concentration of people, typically ghettos. Which leads to more petty arrests in those areas for crap that doesn't matter much. Which sends even more police into these areas, making more arrests for petty offenses... and so on and so on. In short, these ghettos are caught in a negative feedback loop. And the residents more likely to end up in jail for something stupid. Like smoking dope which, based on most evidence, is just as high among whites than blacks. So a white 19-year-old frat boy at Ohio State can smoke-up at will, with little possibility of being caught. While a nearby ghetto-dweller, who works maintenance or in the office at the university, or attends the university while living at home, will have a greater likelihood of being arrested. Just based on where they live.Same crime -- use of narcotics. Two different outcomes. That is what O'Neill terms as a ""weapon of math destruction."" Since it is pervasive, destructive, and opaque. And things get worse if the two hypothetical dope-smokers get arrested. Odds are, the white frat boy's parents live in a safe suburban neighborhood and thus knows zero convicted felons. So his court-appointed ""recidivism"" score -- attained by another destructive math weapon -- will be lower than the ghetto dweller's who lives near many felons. Thus, the courts may lessen the frat boys charge to a misdemeanor while charging the ghetto dweller with a felony. Due to that recidivism score. Talk about kicking someone when they're down.And when both released on parole, and ordered to ""steer clear of felons."" the frat boy will have no problems following this. While the ghetto kid, thanks in part to the policing software noted above, will be surrounded by them. Which, of course, adds to the already disadvantaged the further risk of being pegged as a parole violator. Smack. O'Neill lays out other ways that system engineers perpetuate injustice. She takes-down for-profit universities -- like the University of Phoenix -- who actively target poor people with aspirations. Not to help, since a University of Phoenix degree costs tens of times more than a community college degree, while adding less salary. Instead, University of Phoenix and their ilk exist to cash in the student loans guaranteed by the government. Yep. Poor people, who don't know any better, are targeted by companies that give them less while charging them more. Using data-driven web advertising, another Weapon of Math Destruction, all for a quick buck. These are just a few WMD's that O'Neill examines. She looks at others credit scoring, e-scoring (a sort of electronic ""credit score"" derived on you based on your social media friends and activities), the USA Today college rankings, which have lead to spiraling tuition costs while providing questionable value. All of these WMD's lead to increasing social stratification. And, in many ways, drive the ""winner take all"" nonsense that gives big money to a handful of developers who program an app.But the nice thing is that O'Neill ends by providing valuable insight into how, when properly used, deep-dive statistics can actually help people. For instance, O'Neill was part of a task-force that examined New York City's homeless. They uncovered the single unequivocal variable that would keep people off the streets -- access to Section 8 housing. And once these people were housed, they'd move on to get jobs --since having a stable residence makes it easier to gain employment. And thus, less likely to end up on the streets. And all this research came at a time when Mayor Bloomberg was contemplating reducing Section 8 housing. So it's prove important.She also points to other positive uses of algorithms -- all of which point to putting our compassionate, human-based morality ahead of the appearance of objective, measurable efficiency -- with ""appearance"" being the operative word. Since O'Neill makes the cost of following these algorithms clear. All-in-all, ""Weapons of Math Destruction"" is the best science book I've read over 2016. Since it focuses not only what we can do -- the science and how it makes things more efficient -- but forces us to focus on the ethics, the ""why"" we may choose a less efficient alternative as it may be more just. Especially when blindly accepting a model often degenerate to ""pseudoscience."" And that anti-scientific narrative can be amplified if the person wielding the WMD is either greed or malicious. 5-stars.That said... YAY!!! This is my 80th book of the year. So I've just completed my goal."
3,0553418815,http://goodreads.com/user/show/793473-clif-hostetler,5,"""Welcome to the dark side of big data."" Thus the author concludes the Introduction section of this book. Computers and the internet have enabled us to advance into the new world of algorithms and big data with ramifications that most people are unaware of. Surfing the web, clicking ""like"" in Facebook, Googling (i.e.searching on line), and making online purchases are common examples where big data is either tracking and potentially impacting our lives. Some of these are benign and can be helpful. But this book focuses sharply in the other direction, on the ""harmful examples that effect people at critical life moments, going to college, borrowing money, getting sentenced to prison, finding and holding a job. All these life domains are increasingly controlled by secret models wielding arbitrary punishments."" One of the most shocking pieces of information provided by this book are the examples of how big data contributes to economic inequality. It is devastating how efficiently private colleges and payday loan companies target the economically stressed portion of the population. It's also astounding how frequently limitations and flaws of big data are ignored and substituted for the truth. It's almost comical how big data systems can be manipulated (a.k.a gaming the system) by clever institutions and companies. Whether we like it or not this is the new world in which we all live. Citizens of this new environment need to become knowledgeable about how big data works. Otherwise we will all be its clueless victims. The following are some quotations from the book.The following are the closing sentences of the Introduction:Big data has plenty of evangelists, but I'm not one of them. This book will focus sharply in the other direction, on the damage inflicted by WMDs and the injustice they perpetuate. We will explore harmful examples that effect people at critical life moments, going to college, borrowing money, getting sentenced to prison, finding and holding a job. All these life domains are increasingly controlled by secret models wielding arbitrary punishments. Welcome to the dark side of big data.In the following the author defines some of the shortcomings she observed in how big data was being used.More and more I was worried about the separation between technical models and real people and about the moral repercussions of that separation. In fact I saw the same pattern emerging that I had witnessed in finance, a false sense of security was leading to widespread use of imperfect models, self-serving definitions of success, and growing feedback loops. Those who objected were regarded as nostalgic Luddites. The author worked at a hedge fund during the 2008 financial collapse. Thus when she moved into the field of consumer data modeling she looked for flaws in the use of data that were similar to what led to the credit crisis. I wondered what the analog to the credit crisis might be in big data. Instead of a bust I saw a growing dystopia with inequality rising. The algorithms would make sure that those deemed losers would remain that way. A lucky minority would gain evermore control over the data economy raking in outrageous fortunes and convincing themselves all the while that they deserved it.  The following is the author's summary near the end of the book. In this march through a virtual lifetime we've visited school and college, courts and the work place, even the voting booth. Along the way we have witnessed the destruction caused by WMDs. Promising efficiency and fairness, they distort higher education, drive up debt, spur mass incarceration, pummel the poor at nearly every juncture, and undermine democracy. The author, Cathy O'Neil is highly qualified (she got her Ph.D. in math at Harvard), had work experience from inside the system (a “quant” at D.E. Shaw—a major hedge fund), and has evolved into a ""Occupy Wall Street"" activist. Thus she has experience and qualifications needed to explain how big data effects our lives. It's needed information.Author's Ted Talk:https://embed.ted.com/talks/cathy_o_n...Cathy O'Neil was the winner of the 2019 MAA Eurler Book Prize:https://www.maa.org/programs-and-comm..."
4,0553418815,http://goodreads.com/user/show/78485297-mario-the-lone-bookwolf,4,"Is it legitimate to reduce people to the data that can be extracted from them?Please note that I put the original German text at the end of this review. Just if you might be interested.Especially the predictions made possible by big data are a frightening aspect so that behavior and personal development can be predicted with increasing probability. Also, like any artificial intelligence, the algorithms and programs become more and more efficient, both in parallel with the growing amount of data about the individual and through the optimization of the functioning. Whether habits, buying behavior, Facebook Likes, movement patterns, writing style, illnesses, search machine inputs, political activities, professional aspects or social contacts, everything flows together in a large memory.Particularly explosive is the ability to accurately predict the electoral behavior of specific groups of the population and thereby be able to address election advertising even more targeted. It also opens up the option of manipulating groups, which are more likely to be responsible for the competition camp, utilizing disinformation and misinformation in their awareness-raising and decision-making. Even outside of elections, the data volumes offer promising perspectives for influencing opinions, facts, trends and mood barometers.The subtle, digital prompters are hardly recognized as such. The information is automatically trimmed in the form of auto-completion and suggestion functions, the individually compiled search results and purchase recommendations based on the previous buying behavior. Moreover, directed to the individual interests or the opinion to be shaped. Even what you type in and then delete again or also revise and change, is saved.This can be considered optimistic as practical, time-saving and relieve and helper at work. Alternatively, as an indirect kind of influencing, over which only the compulsory consumption is promoted as optimized as possible. As an alternative to the offer of tailor-made products, there is the option of deliberately focusing just on specific information and news. So that unpopular and annoying reports, opinions and activists are intentionally not even displayed in the higher result ranges. To merely skip the scissors in the head and defamation immediately and proceed directly to censorship.Perfide is the playful way in which free offers lure the consumer. For example, a health application for smartphones, with which one competes in real time with friends, in which insurance and health insurance should also be interested in. These hands-on free programs provide a fun and competitive and user-driven option for businesses to access users' location, interest, and behavioral data. The book is an excellent appetizer, with the benefit of awakened interest in possible deepening of the study of the topic by further and detailed, related literature. Given the overall situation a worthwhile endeavor. Contrary to the apathy and disinterest of large populations owed the fact that it could come too far. So without long-awaited outcry across all layers of civil society, much more could go on. In Western countries, social networks, search engines, and Internet merchants are only interested in manipulating buying behavior and collecting data to monetize it. The direction of China with the Citizen Score is likely to find imitators. Then, the arbitrary and negligently disclosed data would not anymore only decide the price of a product or the next banner ad. However, about your future and maybe your survival or death. For example, if you criticize the government.Ist es legitim, Menschen auf die Daten zu reduzieren, die sich aus ihnen extrahieren lassen?Gerade die durch Big Data möglich werdenden Prognosen stellen einen erschreckenden Aspekt dar, lässt sich Verhalten und persönliche Entwicklung auf diese Weise doch mit immer höherer Wahrscheinlichkeit voraussagen. Und wie jede künstliche Intelligenz werden die Algorithmen und Programme sowohl parallel zu den wachsenden Datenmengen über die jeweilige Person als auch durch die Optimierung der Funktionsweise immer effizienter. Ob Gewohnheiten, Kaufverhalten, Facebook Likes, Bewegungsmuster, Schreibstil, Krankheiten, Suchmaschineneingaben, politische Aktivitäten, berufliche Aspekte oder Sozialkontakte, alles fließt in einem großen Speicher zusammen.Besonders brisant ist die Möglichkeit, Wahlverhalten bestimmter Bevölkerungsgruppen präzise vorhersagen zu können und dadurch Wahlwerbung noch zielgerichteter adressieren zu können. Es öffnet auch die Option, eher dem Konkurrenzlager anzurechnende Gruppen gezielt mittels Des- und Falschinformation in ihrer Bewusstseinsbildung und Entscheidungsfindung zu manipulieren. Auch abseits von Wahlen bieten die Datenmengen verheißungsvolle Perspektiven für die Beeinflussung von Meinungen, Fakten, Trends und Stimmungsbarometern.Dabei werden die subtilen, digitalen Souffleure nur schwerlich als solche erkannt. In Form von Autovervöllständigungs- und Vorschlagfunktionen, den individuell zusammengestellten Suchergebnissen und auf das bisherige Kaufverhalten angelehnte Kaufempfehlungen werden die Informationen automatisch beschnitten. Und auf die individuellen Interessen oder die zu prägende Meinung hin gelenkt. Selbst was man eintippt und dann wieder löscht oder noch überarbeitet und verändert, wird gespeichert. Das kann optimistisch als praktisch, zeitsparend und die Arbeit erleichternd betrachtet werden. Oder als eine indirekte Art der Beeinflussung, über die vorerst nur der Konsumzwang möglichst optimiert gefördert wird. Alternativ zur Offerierung der auf den Leib geschneiderten Produkte bietet sich die Option, gezielt nur bestimmte Informationen und Nachrichten in den Fokus kommen zu lassen. Damit missliebige und lästige Berichte, Meinungen und Aktivisten gezielt gar nicht erst bis in den höheren Ergebnisrängen angezeigt werden. Sondern die Schere im Kopf und Diffamierung gleich zu überspringen und direkt zur Zensur voranzuschreiten.Perfide ist die spielerische Art und Weise, mit der der Konsument durch Gratisangebote geködert wird. Zum Beispiel eine Gesundheitsapplikation für Smartphones, mit der man in Echtzeit mit Freunden konkurriert, was Versicherungen und Krankenkassen auch interessieren dürfte. Diese praktischen kostenlosen Programme ermöglichen eine spielerische und auf Wettbewerb und den Ehrgeiz der Anwender zielende Option für Unternehmen, an Standort-, Interessens- und Verhaltensdaten der Benutzer zu kommen.Das Buch ist ein guter Appetitmacher, mit dem Interesse an einer etwaigen Vertiefung der Beschäftigung mit dem Thema durch weiterführende und auf einzelne Aspekte detaillierte eingehende Literatur geweckt wird. Angesichts der Gesamtsituation ein erstrebenswertes Unterfangen. Entgegen der, der Apathie und Desinteresse breiter Bevölkerungsgruppen geschuldeten Tatsache, dass es so weit kommen konnte.Und, ohne längst fälligen quer durch alle Schichten der Zivilgesellschaft gehenden Aufschrei, noch viel weiter gehen könnte.In den westlichen Ländern sind die sozialen Netzwerke, Suchmaschinen und Internethändler nur an der Manipulation des Kaufverhaltens und dem Sammeln von Daten zu dessen Monetarisierung interessiert. Die Richtung, die etwa China mit dem Citizen Score geht, dürfte Nachahmer finden. Dann entscheiden die willkürlich und fahrlässig preisgegebenen Daten nicht nur mehr über den Preis eines Produkts oder das nächste Werbebanner. Sondern über die eigene Zukunft und vielleicht auch das eigene Überleben oder Sterben. Wenn man etwa die Regierung kritisiert."
5,0553418815,http://goodreads.com/user/show/155663-david,4,"The subtitle of this book, How Big Data Increases Inequality and Threatens Democracy really says it all. Big data has come into our lives in numerous ways, and many of them are a scourge on our lives. Big data, in and of itself, is not to blame, but the uses to which it is put are often outrageous. Take the case of automated teacher evaluations. These are often based on the improvement of students' scores. It seems like a no-brainer, and since the scores take into account the improvement rather than the absolute scores, they seem to be very fair. However, one New York teacher received an abysmal score of 6 (out of 100) one year, and the following year received a wonderful score of 96. Obviously the teacher did not suddenly improve his teaching methods.If a mathematician were to analyze the scores from such evaluations, their random scatter would be instantly recognized as meaningless. Yet, these automated evaluations are used for hiring/firing decisions, as well as compensation decisions. The worst aspect, is that there is no attempt to improve the algorithms. The algorithms used in these evaluations are opaque, and no attempt is made to apply feedback to tweak the scores to make them more accurate and fair.Such algorithms are used in many avenues of life. Credit scores are used by loan companies--and even by auto insurance companies! Police departments use algorithms to plan targeting of police activity, while courts use algorithms to predict recidivism. Colleges, especially for-profit colleges, use algorithms to target potential students.While this book comes across as very preachy, and very liberal, it does make some very good points. Big data is often used--intentionally or not--to punish the down-trodden and to increase inequality. Profitability is usually the goal, and while fairness is often the purported goal, these algorithms rarely turn out to have the effect of being fair.I recommend this book to people who are curious about the effects of big data on their lives and on democracy."
6,0553418815,http://goodreads.com/user/show/5096743-suzanne,4,"This book did a nice job describing large-scale data modeling and its pitfalls in a very accessible manner. It is so easy to think of computer algorithms as unbiased; however, the author demonstrates how they really do discriminate. Next time I teach a class involving statistics, I may use this book to show students how it is dangerous to blindly believe the numbers."
7,0553418815,http://goodreads.com/user/show/2431873-emily,3,"Big Data is opaque, complicated, managed by profit-seeking corporations, and is more and more dictating certain societal conditions: from getting a job to applying to college to receiving healthcare. ""Data,"" on its own, seems amoral, a way to implement systems that are more fair. But O'Neil's point in this book is that all algorithms include basic assumptions, and sometimes those basic assumptions are full of bias and not grounded in fact. If the algorithms aren't regularly inspected, they create their own feedback loops. As O'Neil says, ""In each case, we must ask not only who designed the model but also what that person or company is trying to accomplish.""This book touches lightly on a number of topics to give an overview of the algorithms that O'Neil finds the most objectionable. Each of the different sections is interesting, but they don't delve particularly deeply into the topic; for example, if you're interested in how poor feedback loops on policing and prisoner recidivism work together to create our prison industrial complex, you're better off reading The New Jim Crow. But if you're interested in an introduction to the ways in which data runs our lives, this is a good place to start. I was particularly interested in the section on college admissions, because O'Neil ties skyrocketing college tuition prices to the US World News ranking (the first ranking for universities in the US). In 1988, journalists used a number of proxies to create a way to rank colleges against each other. This example hits on a few of the points that O'Neil brings up again and again: the journalists wanted proxies that would reaffirm their own biases, so Stanford and Harvard had to be at the top of the list. But they weren't able to fully factor in the things that mattered, like the quality of education, so they relied on proxies like admissions rate and freshman retention rate. Cost was conspicuously excluded, and thus - or so O'Neil says - college tuition has increased 500 percent between 1985 and 2013. Colleges are gaming the system to get themselves back up to the top of the list, but then are passing those costs directly on to the students who attend.While this book is nothing groundbreaking, it's another reminder that tech HAS to do better if we are going to put more of our personal data - our lives - into its hands. I personally believe we're all being swindled by the ""sharing economy"" (heavy quotes there), and the example about Lending Club hammers that home. O'Neil uses Lending Club as an example of a service that was supposed to ""democratize"" loans, but it's quickly devolved into 80% institutional money: that is, money from big banks. Why? Banking has regulations, and banks aren't legally allowed to discriminate against consumers on the basis of e-scores and other algorithmically created scoring systems that take into account factors like zip code, punctuation on applications, and social networks. These institutions have found a way to get outside of the law, apply their sketchily drawn ""data"" models (that have no statistical basis), and reap the rewards, while further exacerbating class inequality."
8,0553418815,http://goodreads.com/user/show/175635-trevor,5,"We like to think of mathematics as basically pure and free from the nastier side-effects of human nature. And this purity rubs off – so that the closer a science is to being able to be described in numbers, the more highly we regard it – so, physic is seen as somehow higher than biology, and economics than anthropology. That is, if you can predict behaviour on the basis of an algorithm – whether that be the behaviour of a billiard ball or a homeless person – then this is proper science and it has a claim to a kind of objective truth that sets it aside from being challenged.And that is the point of this book – it seeks to help shatter this illusion, particularly in relation to the human sciences, but also, and perhaps more importantly, in relation to marketing, insurance, policing, education and other social activities that are increasingly being modelled and even normalised by algorithms. She refers to these algorithms as the weapons of math destruction in the title (the title is better in English, of course, where we say maths, rather than math, but her point stands). The destruction such algorithms can cause became clear to her while she was working in finance just before the 2008 crash. It certainly isn’t that she sees maths itself as being the problem, she refers to herself almost immediately as a kind of math nerd and proud of that designation. Her point is that these algorithms are dangerous because we tend to think of them as being purely objective and therefore the results they provide as being beyond question. And this state is helped along by more than just the fact we hold mathematics in such high regard. It is furthered by the fact that often the algorithms that spit out these assessments of us are obscure, unfair and grow exponentially. These are the three conditions that the author believes makes an algorithm a likely WMD. So, in looking at these in turn, is the algorithm is obscure? – and most of them tend to be, as she says at one point, they are the ‘special sauce’ for many companies and so they need to remain hidden from the competition. For instance, if you have an algorithm that allows you to predict who is going to make an ideal partner for someone else, then your dating site is going to make you lots of money. You are hardly going to want to let your competition know you secret. But the problem is that by keeping your algorithm obscure and hidden from outside analysis, you can say nearly anything you like about its effectiveness, if no one is then able to check. You know, this is a version of the ‘they can vote any way they like, as long as I get to count’ story.But it isn’t just outright fraud that is the problem here – although, that doesn’t mean fraud isn’t a problem. Just as bad is the idea that often these models bury their false negatives. That is, if the algorithm says ‘never employ anyone with green eyes, they are under-performers’ the company that follows this advice is unlikely to ever find out if this is true or not. That’s because they won’t have employed anyone with green eyes to test the model – so the model will be confirmed by default. The problem is that many of the algorithms used – say in policing – can encourage over-policing of certain populations (this is set in the US, so let’s just call those certain black, Hispanic and Muslim populations to save time) and this over-policing, by defining certain populations in these ways are also likely to create the monster they were supposed to be eliminating. A nice example is an algorithm that denies jobs to people according to their credit scores, which then means these people are less able to pay off their debts, which gives them a worse credit score, which then confirms the risk – vicious cycle anyone?Other examples focus on the use of psychometric tests for all manner of things, but increasingly as a pre-employment test. You might think these ought to be relatively easy to game – you know, how stupid would you have to be to answer ‘very true’ to the prompt, ‘I sometimes fly off the handle for no real reason’. You know, unless you are going for a job in as a wrestler, your employer is probably not going to be overly impressed with that answer. But as she points out, sometimes applicants, for example, for a job at McDonalds, are asked to choose between one of two alternatives: “It is difficult to be cheerful when there are many problems to take care of – or – Sometimes, I need a push to get started on my work”. I have no idea what the ‘right’ answer to that choice is. I’m not even sure which of the two really applies to me – although, there have been times in my life when both of them have, and in your life too, I suspect. That is, it isn’t at all clear what this is seeking to achieve, but it is clear that there is likely to be a ‘wrong’ answer, in the sense that making that ‘choice’ might leave you without a job. But even this level of obscurity isn’t her main worry (at least you know you’ve been asked a question here that might be used against you) – but it is too often easy to correlate factors to ensure that certain people are excluded due to their sex, sexuality, race, social class and so on, merely by the underlying assumptions of those programming these algorithms – and the algorithms aren’t ‘objective’ examples of the purity of mathematics, but rather socially produced ephemera that are likely to have been shaped by the social stereotypes of the society and the people who produce them. The author mentions that curious fact that orchestras now appoint five times as many female players since auditions have been held with the player behind a curtain. Who’d have thought women would play so much better when they were hidden from sight? Such shy and retiring little things, bless them… The solution is to ensure that algorithms used to judge us are open and available for anyone to check – a condition that will become increasingly unlikely as these algorithms are proprietary.The question of fairness isn’t at all easy to address and for many of the reasons already mentioned. One of the examples given is teacher scores being used to determine who should get pay increases and who should be removed from the profession. Basically, the idea is to compare student outcomes so that the teachers who do not increase student scores enough should be shown the door. But, as is pointed out here, student scores aren’t only affected by teacher performance. And worse, if you are teaching students who are either very far behind or very far in front you are unlikely to affect as big a movement in their scores as you might if you are teaching kids in the middle. If you are going to be assessed on the basis of a score, that score ought to be related to something you have control over, rather than merely something that is relatively easy to measure. The author points out that far too often the kinds of assessments made of teachers based on student attainment produce virtually random results year on year. Since it is very unlikely that an exceptional teacher will become a very poor teacher from one year to the next, then any assessment that produces such a result ought to be suspect. And this is also true of loan application algorithms that judge you based upon the people you associate with. For instance, I’ve read a few times lately that your credit rating can be impacted by your ‘friends’ on FaceBook, but even if this is not literally true, algorithms are shown here to judge you by many factors that assume associations between you and other people more or less likely to be a credit risk. For instance, if you buy things in certain stores, as American Express admitted recently they restricted access to credit to people based on them frequenting particular stores. The last condition of a WMD is that the algorithm can be scaled up to cover large populations. An algorithm that works well in one set of circumstances might be both transparent and fair within its limited application, but because it is then being used across a larger population it might suddenly stop being fair or reasonable. This is because it creates a new norm in the population at large and that might well have seriously disadvantageous impacts on populations beyond the one it was originally intended to be used upon. Again, the examples I jump to tend to be associated with education, where large scale testing programs have a disproportionate impact on poor communities which are defined as failing and then ‘success’ becomes defined as ‘doing well on the test’ – so that the tail starts wagging the dog. And this then has impacts on how kids get taught – if you are only going to be measured by test results, then we should just prime you for taking tests. Which then makes education as boring as it is possible to make it for kids that were already struggling to see the point of education in the first place. But the communities that do well on these tests are normally the already advantaged and they suffer none of these negative impacts, because, well, they do well on these tests, so no point priming them on more of them…Let them do art.I really would recommend this book – she gives lots and lots of examples and it is vitally important that we understand that this is the world we are increasingly moving towards. More and more of our lives are going to be influenced by algorithms and big data – and yet too many of us are so terrified of mathematics that we will blame ourselves when these algorithms punish us. It isn’t at all clear to me how we might go about making these algorithms transparent, fair or limited to a scale that keeps them safe – but these are questions we really ought to think about and act upon."
9,0553418815,http://goodreads.com/user/show/5917740-nilesh,2,"For the most part, WMD is a rant with only impractical statements as solutions. The author is onto something critically important when one reads the title of the book and goes through the first few pages. However, it is a tragedy to see the author falling in love with her own phrase WMD and completely losing the plot. The examples used are good in the beginning but soon turn ridiculous (they would be laughable if not so lamentable for the people involved). In the process, the author loses her credibility as a champion of the topic.Consider the following example: a person is unable to find a job as he claims he fails all pre-interview personality tests. The author debunks the objectivity of tests in favor of the subjectivity of interviewers with the claim that in another world, one or the other interviewer would have given the job to this person. The author never thinks about the interviewers' non-transparent and more difficult to fathom biases wreaking greater damage in that world versus the current one where the biases are at worst in models that are far easier to scrutinize and amend. The author failts to realize that every world with fewer desirable jobs compared to applicants will leave some unhappy - the fairness is perhaps far better where the selection process is more equal for all.In a way, the author wishes ""we"" to stand up and help reduce the use of math. The author is clear in the ""reduction"" goal without ever realizing how ""we"" would ever decide to what extent the reduction should take place and whether the constituents of ""we"" could ever agree on the replacement. The repeated call for subjective biases assumes that the biases come in only one form and are acceptable to all. The absurdities reach a peak when the author declare that at least subjective biases evolve over time and hence should be preferred while the mathematical models are static!The author's hatred for anything involving models not only creates a highly one-sided discourse but also leads to inadequate treatment of any important points within. The only mathematical concept discussed somewhat is that of the circularity caused by the self-reinforcing feedback in multi-equilibrium equations leading to solutions that might be locally optimum but not globally (all my words). Otherwise, the book almost exclusively focuses on the unsuitability of results arrived through statistical sampling on any individual case.To summarize, the author seems to yearn for insurance companies to abandon actuarial methods, schools to move away from test scores for admissions and financial institutions/tech companies/governments to reduce their use of numbers. The author wants humans to take decisions that these models might be taking using their value judgments. Or the world to simply not take those decisions. There is so much sensible that needs to be discussed on the damage created by the excessive use of math in the modern world but this book is not the best place to explore it."
10,0553418815,http://goodreads.com/user/show/30018093-scott,3,"Book reviews are all about expectations, and honestly I, as someone doing data science and grappling with issues, expected more. With a data scientist writing a full length book inditing data science one expects a deep dive revealing real points. Instead it ends up being a very surface level essay without the deeper exploration and meaning one expects from a full length work. Perhaps more worrisomely, her own definition of a WMD it introduces is often worked around to bring in arguments she wants to make.I do agree with her main argument/definition of a Weapon of Math Destruction(WMD), though the name is far too punny for real use. More or less the author argues that bad algorithms are secret, and expand out rapidly without checks on actual results. This is hardly a novel problem statement, but it is certainly a real problem. However several times she applies shortcuts of assumed results to actual results in evaluating presumed WMDs which is the very same problem she is complaining about. She also seems to have added an unspoken fourth criteria being anything that offends her liberal (and she is very liberal, and interventionist) ideals. This fourth criteria is apparently enough to give a pass to what are clearly bad algorithms that fail her own tests. This is needless to say, not a great way to prevent issues with unethical use of algorithms, unless you happen to have the same ideals and goals as Cathy O'Neil. Somewhat surprisingly, this is a data science book without any data. It doesn't even have in text footnote (at least in my kindle edition, though they do sometimes update that), and the notes section at the end shows the citations to be really quite poor and sparse. We are talking college freshman ""The professor said 10 citations so I better cite a bunch of random webpages"" level of citations. To be fair her arguments are only weakly data driven, but I still expected something with a deeper academic backbone.This is also a data science book that won't teach you anything about data science. If you are looking for that you should look elsewhere. The author has co-written another book Doing Data Science, and it shows her quite capable of handling the topic. I do wish she had brought more of that into this book, which will be many people's first book in data science. The structure of the book plays out more as a series of related blog posts than a book that builds on themes and has progression. Partly this may be an unfortunate result of the author being a blogger, but it leaves one wanting more. "
11,0553418815,http://goodreads.com/user/show/6816078-maru-kun,4,"Forget those cute pastel illustrations from the fifties with their flying cars, robot servants and dreams of unlimited leisure. Our future has finally arrived and for most of us, especially for the less rich and less privileged who won’t qualify for individualized attention, the computer says “No”.‘Weapons of Math Destruction’ is a timely book about the increasing influence of algorithms to control the news we see, the jobs we can get and the politicians we vote for; algorithms working tirelessly on someone’s behalf (not yours), unseen and unaccountable.The book explains statistical and methodological problems with these algorithms and illustrates how these same problems manifest themselves when they are applied to real world situations.An example gives a flavor of the issues that recur throughout the book:An algorithm which compared changes in student performance year-on-year was used to decide which teachers were a poor performers who would be let go. In one terminated teacher’s case subsequent evidence suggested that student’s scores had been tampered with the previous year and as a result she had inherited a class whose performance had been overstated, so was bound to deteriorate with the algorithm marking her out as a poor teacher.The teacher was not told how the algorithm applied nor allowed to appeal the decision; the algorithm was a black box that produced a result the school system wanted – terminations. And, notwithstanding the lack of transparency in this decision, a sample of scores from one class alone - perhaps twenty to thirty students - was not sufficient to produce a statistically valid result in any event. One more reason why being an American public school teacher must be one of the worst teaching jobs in the developed world.Please read the book for more egregious examples of “algorithm abuse”. Below is my summary are some of the key problems the book outlines. It is helpful to think about these problems in two categories, problems which come from poor application of the technical aspects of the algorithms (bad programming, misapplication of statistical or machine learning methods) and problems which come from how the algorithms are developed and used, how they incorporate hidden biases or value judgments, poorly thought out objectives and other ‘human factors’ beyond just poor maths:Some of the common issues were:	Lack of transparency in how the models operate and how they make a decision, often leading to no means of appeal against a clearly unjust outcome. Related problems include over reliance on models in the face of contradictory data or, where people do understand the models but are benefitting from them, a lack of integrity in applying them. Models used to price mortgage backed securities during the financial crisis are a leading example.	The use of “proxy data” for model input or output because the real data desired is unavailable, too expensive to obtain or cannot be objectively measured. Does sending out more e-mails with “creative phrases” mean that you are really a more creative and innovative person? FICO scores are a relatively good model for use in predicting credit risk, but not as a proxy when used for a whole host of other unrelated things such as predicting future job performance when used in the hiring process.	Feedback loops, whereby a model increasingly encourages non optimal outcomes by rewarding certain behaviors at the cost of intended benefits. An example given is US college rankings which increasingly reward “user experience” and “research citations” rather than the actual educational outcomes for students.	Algorithms that, mainly for efficiency purposes, use data from arbitrarily selected groups when individual data is available. Why should an algorithm price insurance for a driver based on the experience of other drivers that live near him or are in a similar economic position instead of on his own individual driving record?	Optimization that is good for an algorithm’s owners but not for society as a whole. Monetary return is the most common priority for an algorithm used in the private sector, but is this what society wants when it results in micro-targeting the poor with the marketing of for-profit colleges that provide a below average education at great cost?	Hidden biases and unfairness when assumptions are built into an algorithm that are reflective of social factors rather than individual experience. Algorithms used by the police to forecast crime are first used to predict easy to identify nuisance crimes which, unsurprisingly, occur mainly in deprived neighborhoods. The police have yet to develop an algorithm that forecasts where white collar crime takes place, although if they did Wall Street would surely light up red.	Incorrect use of statistics; lack of feedback of model results into predicting outcomes. Baseball is a good field for prediction because outcomes – homes runs, strikes, batting averages - can be objectively measured and predictions can be fed back into the model to improve it. Using FICO scores in recruitment is not what the developers of FICO intended; there is no study suggesting statistical correlation between FICO and subsquent good job performance; good or bad job performance is also never used to assess HR models or improve their reliability.The last chapter of the book looks at the micro targetting of voters with political messages through Facebook and similar social media sites. This was written in September 2016, a respectable amount of time before the world had a glimpse into the dizzying vortex of fake news, Russian hacking and tweeting Presidents. This is an excellent chapter and may well be seen as prophetic once we look back over our current period of political chaos, if it ever ends.I resolved to write notes on the books I read as I get so much more out of them if I do. This resolution will last about one and a half books I’m sure, but if anyone is interested here they are:Introduction: Mathematical models are being increasingly used to make decisions that have real world impacts. However the algorithms they use are opaque except to a limited number of mathematicians or computer scientists and may, sometimes unknowingly, encode human prejudices, misunderstandings and biases.Assumptions may be camouflaged by the maths and go untested and cannot be questioned by the persons to whom they are applied. These algorithms are often used in inappropriate contexts where there is insufficient objective data to properly apply the underlying statistical theory; algorithms may work for baseball with a many tens if not hundreds of thousands of objective, constantly updated data points but not to evaluate a teacher with a class of thirty. The algorithms produce “feedback loops” under whereby they produce output that eliminates particular classes of result based on false assumptions, but as a result reinforce the weighting given the assumption in the model. Injustice is reinforced as these algorithms are applied to ever larger numbers of people, although the rich and privileged may still be assessed on an individual basis rather than as one of the masses assessed by machine. People unfairly denied opportunities as a result of this software are “collateral damage”.The algorithms optimize a payoff designed by the developers of the model - in the case of many private sector models, monetary profit - but who is to say that this is the optimum payoff for society as a whole?Chapter 1, Bomb Parts - What is a Model? The author explains how her managing of her family cooking is a form of “data model”: inputs are family preferences, appetite that day; available food, special cases such as cooking on a birthday, the output is “family satisfaction” and the model determines menu for that day The model could be pre-programmed with a set of rules to determine its menu or could be trained through observing many examples. In either case mistakes may be made, perhaps through forgetting a rule or not including a rare case in the training data. The key point is that the model can incorporate personal biases that are not visible, in the case of the menu model towards healthy food and away from ice-cream.Models don’t have to be complicated to be effective; a smoke alarm is a model intended to identify fire that operates on only a single input, the concentration of smoke particles. Three questions are posited to evaluate models: Firstly, is the model understandable - or even visible to - by the people to whom it is applied? Secondly, does the model work in the subjects’ interests? Is it fair or can it cause unjust damage? Thirdly, does the model scale? May it be used in ever wider circumstances, at the same time reinforcing its hidden biases as it is applied in ever wider circumstances?An example is the model used to determine sentence length in US courts based on a model about the risk of recidivism. The model takes into account factors, such as previous criminal record or age of first contact with the police, which would not be admissible as evidence in court and which may unfairly discriminate against certain sections of the population.Chapter 2 talks about the role of algorithms in the financial crisis, noting two key issues that lead to the collapse in the mortgaged backed securities market. Firstly the assumption that models had been subject to proper mathematical vetting; in reality few people understood the mathematical and statistical issues and many that did lacked the integrity to speak up, especially as initial success had created its own feedback loop encouraging growth in the market. Secondly modern computing power had allowed a massive secondary infrastructure to grow around the market - credit default swaps, CDOs etc - that instead of diversifying the risk masked, magnified and concentrated it.A very interesting point was made based on the author's experience in a risk assessment firm, that many traders are remunerated based on their Sharpe ratios, the ratio of revenues to risks taken, and accordingly are motivated to “...actively seek to underestimate…” risk in order to effectively manipulate the Sharpe ratio and hence their bonuses. This contrasts with the approach of hedge funds, which genuinely care about risks (given their own money is at risk) and traders at large financial institutions which don’t have their own financial capital at stake.Chapter 3 looks at the impact of the algorithm developed by US News to rank colleges in the US. 75% of the ranking was based on proxy items intended to measure college success and 25% on subjective evaluation. The proxies selected, for example admission ratios, SAT scores, invited gaming that distorted applications; colleges would invest in sports in order to encourage applications that could be rejected; in an extreme case a new Saudi college required part time professors with large numbers of citations to change their site reference to the college in order to move up the rankings. The system does not measure the key success of education - what the students learnt at each school. The ranking generates its own feedback loop; colleges that rank high attract more applicants, generating more rejections thus moving them further up the rankings. Wealthy applicants pay consultants to game the system.Crucially the original algorithm did not take into account college tuition costs. This guaranteed the early rankings being in line with existing “common sense” with Yale, Harvard and other wealthy colleges ranking high but thereby excluded an issue critical to students, value for money from an education, while encouraging colleges to spend excessively in order to improve student experience and hence ranking without regard to cost, ultimately leading to more student debt.US govt has now made available data on schools allowing students to check directly.Chapter 4 looks at targeted online advertising, taking for profit colleges that target poor and vulnerable people as a particularly nefarious case. The internet gives instant feedback for targeted marketing campaigns through Facebook or Google in particular where successful or failing ads can be evaluated in real time. Bayesian analysis is used to evaluate success.20-30% of a for profit college’s budget may go on lead generation with more spent on recruitment than on education itself. Specialist lead generation firms exist targeting particular communities, posting fake job ads or promising health coverage.Chapter 5 looks at “Predpol” an algorithm to predict crime which has a key input the geographic location of crime but which excludes data on race. The algorithm is successful in forecasting where crimes can occur, but mainly because it includes low level crimes - public drunkenness, jaywalking - which are better correlated with geographic location. However these crimes are also often associated with poverty and indirectly with race; more serious crimes are more difficult to detect while the system ignores some crimes altogether, such as white collar fraud. On the surface the algorithm is objective but under the hood it reflects value judgments around where the police direct their attention.Issues of probable cause are also raised by the use of algorithms that predict whether a person will commit crime based on proxy data such as location of residence, whether or not employed or similar. These algorithms raise questions around to what extent the public is prepared to balance efficiency in police work against fairness, but without any public debate.Chapter 6 looks at the use of algorithms in employee hiring decisions.. Personality tests are often used for screening job applicants, but these may be “run arounds” laws preventing discrimination against people with disabilities and, a key consideration for the correct use of algorithms, are rarely updated through monitoring their actual success in predicting good job performance. This may not be a problem for a system evaluating baseball stars, whose performance can be objectively statistically measured and where the individuals may be paid millions of dollars, rather the burden falls on lower paid staff in the collateral damage suffered by individuals that are screened out by the algorithms but are capable of doing the job.These algorithms include negative feedback - those discriminated against fail to get good jobs, thus justifying the discrimination - and may be based on data that reflects historically discriminated hiring practices; an example is given of a system used in a British hospital that after many years of use was held to discriminate against women and immigrants, repeating discrimination reflected in the original data on which the system was based.Chapter 7 examines the impact of algorithms in the workplace.Job scheduling software uses data to schedule peaks and troughs of staffing, dehumanizing employees in the process who are unsure of their work schedule until only a day or two before being called on. Another algorithm - ‘Cataphora’ - attempted to identify the most creative and innovative employees through tracking the flow of e-mail including certain key phrases through the e-mail system. There is little evidence that this approach worked, but employees who were not among those identified as the most creative may be first in line for termination. This algorithm suffers from the two classic problems identified early in the book, the difficulty of finding measurable proxy data for the items (soft skills such as creativity) that you want to measure and the lack of any feedback on success and failure of those measured to help the algorithm learn.The chapter explained an egregious error in the evaluation of teacher performance through the use of SAT scores. The ‘Nation at Risk’ report issued by the Reagan administration was intended to address the decline in teaching standards as measured by a gradual decline in SAT scores of graduating students over the years. In fact the data illustrated an example of ‘Simpson’s Paradox’ in which the whole body of data showed one trend but when examined on a segmented trend the opposite trend was apparent. In this particular case the scope of people taking SAT tests had expanded over time including enough people at the lower end of the scoring range to lower the overall average (i.e. elite students had been sitting the test for many more years, so there was little room for scope to increase at the higher end of the scoring range). When the data was segmented in narrow SAT ranges SAT scores had increased in all segments over time. The whole premise of the report was false.Chapter 8 looked at FICO scores. FICO scores themselves have some good features as data; they are based on an individual’s history (in contrast to being based on aggregate data inferred from people who resemble the individual), default on a loan is relatively objectively measured and feedback is used to improve scoring.Problems arise when FICO scores are used as proxies for other data that is not so easily or objectively identified, for example future job success when used as screening in the hiring process, or when combined with other data in developing proprietary “e-numbers” used by firms for purposes such as marketing. These metrics are not transparent to consumers and risk being a backdoor for discrimination because they may use factors that are indirectly linked to poverty or race such as residence.Chapter 9 considered how the whole insurance business model may be undermined by data algorithms. Insurance relies on the pooling of a wide range of risks, good and bad, to allow the pricing of the risk in aggregate. Algorithms allow such risks to be segmented in a non-transparent way which, in addition to undermining the insurance model itself by pricing out high risks, permits price gouging of other segments. The use of non risk related indicators, for example credit scores or location of residence, in pricing motor insurance also allows price gouging. Why should a drop in a driver’s credit rating impact his premium when the risk that he has an accident is unchanged? Drivers are being assessed on the consumer patterns of their friends and neighbors rather than on their accident record.Chapter 10 looks at the application of algorithms to civic life, in particular the use of micro targeting of political messages through Facebook. Facebook has experimented with manipulating emotional responses of users through manipulating their newsfeeds. Facebook’s approach to disseminating news is contrasted to a conventional newspaper in which the Editor makes a decision on what to put on the front page but that decision is seen by everyone and open to public debate whereas the criteria for selection of articles for a newsfeed by Facebook are opaque and unique to the viewer. Facebook micro targeting may be one of the reasons many Republican voters still believe Obama ‘birther’ and other conspiracies.Micro targeting of voters risks disenfranchising everyone to the benefit of those paying for it. Voters in swing states are subject to more focused and intensive campaigns which are relevant only to themselves to the detriment of the democratic process for all."
12,0553418815,http://goodreads.com/user/show/41236164-jack-teng,5,"I can't stress how important of a book this is. I don't think people really know how the obsession with Big Data and algorithms is about to control/influence our lives. I suppose I sound paranoid, but I really don't think I am. There are too many tinkerers out there who have some degree of competence at math and who think they'll solve all the world's problems with the next greatest optimization formula, and yet they lack even the most basic experience in asking proper research questions and understanding the major limitations of data. My own experience in theoretical ecology exposed me to empiricists wanting to get in on the modeling action and just produced garbage papers. Beware, people. Beware."
13,0553418815,http://goodreads.com/user/show/5861086-margaret-sankey,5,"I was really delighted to see this book on the list of nominations for the National Book Award--O'Neil was a math professor, then hedge fund analyst, then Occupy activist, now excellent non-fiction explicator of the power that we've given algorithms in daily life. Certainly, the old way, in which a human bank manager or college admissions officer selected people for loans or Harvard reflected significant prejudices--but have we gone too far in the other direction, thinking that hard data will erase biases? O'Neil offers vivid examples: microtargeted ads, teacher evaluations, police stats for predictive patrols, insurance, credit scores and the warping of higher ed around the US News and World Reports college rankings to demonstrate the potential for toxic grips of big data as opaque, unexplained, unchallenged and dictatorial, as well as the possibilities for using its power to help rather than exclude."
14,0553418815,http://goodreads.com/user/show/9656259-c-p-cabaniss,2,"*I received a copy of this book through Netgalley. All thoughts are my own.*This book did not turn out to be what I was expecting. I expected O'Neil to go more in depth about the math behind data she was discussing, to explore the algorithms in greater detail. That was not what I got, however. This book turned out to be more of a superficial look at some of the ways big data can/has impacted society in the author's opinion. And while I found some of the material presented interesting and informative, I was overall not impressed by the arguments the author made. She didn't seem to have a clear idea for a resolution. More thoughts will be up on my blog: www.courtneysreads.blogspot.com"
15,0553418815,http://goodreads.com/user/show/3079724-chad-kohalyk,3,"Solid overview of the various mathematical models that govern our education, labour, wealth, and commerce. O'Neil packs in many examples and unpacks how simplistic, unfair and damaging to already disadvantaged people these models can be. As someone who worked on the front lines of developing models for predictive internet shopping, I was familiar with many of the tactics mentioned in this book, and their ethical shortcomings (which finally led to me leaving the business). What she says is entirely true, and it makes a small amount of people a lot of money. More people should be outraged. This book could help.There are two shortcomings, though, both possibly due to its short length. First, there is no discussion at all of government abuses of all the data collection detailed within its pages. Maybe in the age of Snowden this is just assumed, but I think it is an important outcome of the big data revolution that should at least be addressed, even in passing. Secondly, I was hoping for more imaginative solutions. The ""hippocratic oath"" for data scientists she refers to is a nice idea... from twenty years and two big economic recessions ago. Advocating for stronger regulation is certainly prudent. However, I was hoping for something new.Weapons of Math Destruction is a well documented tour of the standard examples of the misuse of math and big data, and concludes with the standard solutions. A good book to recommend to friends who need a primer on these issues that we have been facing for the last decade or so."
16,0553418815,http://goodreads.com/user/show/2704424-nick-klagge,4,"Cathy O'Neil was my professor for number theory in college, I think in 2006. I thought she was a great teacher, but didn't keep in touch at all after the class. I was somewhat aware that she was involved with Occupy Wall Street's financial policy arm, and after I heard about this book, also learned that she had been co-hosting a podcast on Slate (which she is now about to leave!--but I still have a lot of back-episodes to listen to).I'm broadly in agreement with the thrust of her argument in this book, and only have quibbles with regard to ways I think it necessarily simplifies things to be a popular-consumption nonfiction book. The main thrust of the book is that people frequently conflate ""algorithmic decision-making"" with ""neutral decision-making"", and that this is a fallacy (one that algorithm-purveyors are happy to perpetuate, for the most part). As ""big data"" and quantitative models get applied to more and more aspects of everyday life, it's incumbent on us to understand this, and to consider ways in which algorithmic decision-making can be problematic, biased, or dangerous. O'Neil describes three features that characterize a ""weapon of math destruction"": scale, secrecy, and destructiveness. We mostly only need to focus on algorithms that have all three of these features, at least to some degree--a model that is in limited use, transparent, or harmless is not much of a cause for concern. She gives examples of algorithms in many fields that she sees as meeting all three of these criteria. One example is recidivism risk modeling, which is now used in many states to determine, at least in part, criminal sentencing. Errors or bias in these algorithms may result in additional years behind bars for the individuals they apply to, and they are both widespread and not publicly disclosed. There are many other interesting (/troubling) examples in the book, such as teacher value-added models.An emergent property of many such algorithms is that they may engender undesirable feedback loops. For example, a recidivism risk model will be biased against black people if it is trained on historical data that cover an environment characterized by bias against black people. (If black people are more heavily prosecuted in general, they are likely to appear as higher recidivism risks, and even if the algorithm doesn't use race directly, it will pick up on correlated factors and amount to the same thing.) This is largely a function of the opacity characteristic--if an algorithm is publicly disclosed, people can bring scrutiny to it and highlight flaws.The issue of model opacity is an especially interesting one to me, as someone who works on regulatory financial models that are intentionally not disclosed. There are strong reasons for not disclosing any models used for high-stakes decisions (in my case, setting minimum capital levels for banks). A primary concern is that a transparent algorithm will be ""gamed"", in the sense that those subject to it will figure out ways to make themselves ""look good"" to the algorithm that are driven more by the details and limitations of the model rather than by the underlying substance. A second concern is that a transparent regulatory model can encourage a ""monoculture"" in which those subject to it will simply adopt the model for themselves, rather than developing their own models that, while still flawed, will have different flaws than the regulatory model.I don't think there is an obvious solution to this transparency problem. One solution that I definitely don't agree with is to eschew quantitative decision-making altogether. As O'Neil clearly states in the book, we shouldn't assume that pre-algorithmic decision-making was unbiased either--it seems quite apparent that, for example, there is bias in judgmental sentencing, perhaps more than in algorithmic sentencing. O'Neil herself has one proposed solution to this (which I don't think she really discusses in the book)--she has started a consulting company whose intent is to audit existing algorithms for potential biases or other damaging impacts. This would allow some degree of independent assessment while not disclosing the model generally. I think this is an interesting idea and I hope it takes off, but there are limitations. Especially in the private sector, it's not clear why a company would voluntarily request such an audit, especially if an algorithm is making them lots of money. Fear of regulatory penalties could be one motivation, but we're clearly entering an era of deregulation. Regulators themselves might force audits, but again that requires a strong regulator (also, who watches the watchers?). One approach that I think could make sense in at least some contexts is a hybrid algorithmic-judgmental process. (At least right now, hybrid processes can be most effective in many fields--for example, the best chess player is neither a computer nor a human, but a human assisted by a computer.) To take the example of recidivism risk, we might have an algorithm that is publicly disclosed that produces a publicly-disclosed outcome to the judge (perhaps a recommended range). The judge may then choose to depart from the recommendation, but needs to give a written description of her reasoning for doing so. In this way, the algorithm can be audited by any outside party for potential biases, but the final judgmental step serves as insurance against flagrant cases of gaming the system, or cases with significant factors that are not considered by the algorithm.As I said earlier, my only real quibbles with the book are around simplifications that I think are a reality of publishing a non-fiction book for popular consumption. For example, the terminology of ""WMD"" encourages us to think in a binary way (is it or isn't it one?), rather than seeing a continuum from OK to troubling, which I think is a better reflection of reality.Finally, I'll add that this book, published in September 2016, proved to be quite prescient. The public controversy about ""fake news"" and the Facebook newsfeed algorithm arose shortly after its publication. Interestingly, I think before this happened, few would have identified the newsfeed algorithm as a potential WMD, because the vector for ""destructiveness"" was non-obvious. O'Neil has written some articles on this topic since the publication of the book, which are worth looking up."
17,0553418815,http://goodreads.com/user/show/1444762-jill,4,"This book shows the hidden ways in which the use of ""Big Data"" is much more far-reaching and harmful than expected. Big data refers to the massive amount of information now available because of computers that is collected and analyzed and sold to third parties.In particular, as the author demonstrates convincingly, applications of Big Data “punish the poor and the oppressed in our society, while making the rich richer.” She paints a sobering picture.The author calls the mathematical models employing Big Data and used to such harmful effect “Weapons of Math Destruction” or WMDs. In WMDs, she explains, “poisonous assumptions . . . camouflaged by math go largely untested and unquestioned.” They create their own toxic feedback loops, and, to an extent which shocked me, guide decisions in a large variety of areas ranging from advertising to prisons to healthcare to hiring and firing decisions. Most importantly, because they rely on esoteric mathematical models, no matter that they are many times based on biased and/or erroneous premises:“They’re opaque, unquestioned, and unaccountable, and they operate at a scale to sort, target, or ‘optimize’ millions of people.”The goal is always profit, but what is lost is fairness, the recognition of individual exceptions, and simple compassion and humanity, adding to the inequality gap, not to mention downward spirals for some unfortunate victims from which it is almost impossible to escape.I am not at all well-versed in math, but the author manages to explain how all this works without requiring that one understand specific algorithms. She provides examples from the worlds of teacher evaluations, hiring decisions generally, advertising, insurance, police programs, college admissions, lending and credit evaluation, and political targeting. One of the saddest chapters (and they are all sad, unfortunately) is about the many for-profit universities (Trump University comes to mind) that specifically target people in great need, selling them overpriced promises of success. Her quotes from the marketing materials of these places are horrifying. They look for individuals who are “isolated,” with “low self esteem” who have “few people in their lives who care about them” and who feel “stuck.” She shows how they use google searches, residential data, and Facebook posts, inter alia,  to find “the most desperate among us at enormous scale”:“In education, they promise what’s usually a false road to prosperity, while also calculating how to maximize the dollars they draw from each prospect. Their operations cause immense and nefarious feedback loops and leave their customers buried under mountains of debt.”The chapter on the way the “stop and frisk” policing operates is also very depressing; and in truth we have seen the tragic results in city after city.The fact is, the whole book is rather a downer, albeit an important one. Although O’Neil cites a few programs that have used Big Data to help people rather than to enrich a few and oppress the rest, can one really think that “moral imagination” can take precedence over prejudice and greed? Personally, I’m not so sure. The author provides ideas about how to change (and importantly, regulate) uses of Big Data, but she is more optimistic than I am, ending on a positive note:“We must come together to police these WMDs, to tame and disarm them. My hope is that they’ll be remembered, like the deadly coal mines of a century ago, as relics of the early days of this new revolution, before we learned how to bring fairness and accountability to the age of data. Math deserves much better than WMDs, and democracy does too.”Evaluation: I hope this important book gets a lot of attention. My husband always makes the argument about privacy concerns that what do we care if we’ve done nothing wrong? This book shows how, astoundingly, that isn’t enough to stop Big Data from hurting us in many aspects of our lives. It is a critical lesson for today’s world, and the world of our children."
18,0553418815,http://goodreads.com/user/show/3938672-kurt-pankau,5,"My father, in one of his grouchier get-off-my-lawn moments, complained to me about computers in the workplace. Specifically, it bothered him that people were so trusting in the software that they were unwilling to gut-check outputs. This conversation happened maybe twenty years ago, but it stuck with me, and part of the reason is because he was absolutely right. We've all heard the phrase ""garbage in, garbage out"" to describe this phenomenon.O'Neil takes the idea of ""garbage in, garbage out"" and compounds it, and this book is an exploration of the pitfalls, both real and potential, involved in letting algorithms dictate our lives, in everything from teacher retention to prison recidivism to workplace wellness programs. I'm a huge fan of O'Neil's contributions to the Slate Money podcast, which is how I learned of this book in the first place. And if you're at all interested in the Big Data economy, you should go read it right now. Even if you're afraid of math--in fact, especially if you're afraid of math. O'Neil's writing is not overly technical. This book is a very easy read and it is dense with ideas.Many of these ideas resonated the instant they hit my brain, making me recognize things that had been around me for months or years. To give an example, O'Neil has a chapter on insurance and describes how more insurers are using e-scores based on ""birds-of-a-feather"" logic and demographic information. She points out the extent to which these things can become ridiculous, where having a drunk driving conviction can result in LOWER car insurance premiums than having a bad credit rating. While reading this, I remembered the ""Sorta Marge"" commercial Esurance put out and thought ""Oh, that's what that was about.""There's a big social justice component to this book that hinges on the fact that algorithms sacrifice individuals in favor of trends, and in doing so they create feedback effects. Another example: some states include the defendant's zip code in sentencing models. This would be inadmissible in a court case, because it's a proxy for race and/or income and is irrelevant to the individuals guilt or innocence. But sentencing modelers argue that it accurately predicts how likely they are to be a repeat offender. So a person from the ""wrong"" zip code gets a longer sentence, then it's harder for him/her to find a place in society after release, and they end up going back to jail. The model doesn't just predict, it reinforces, and this is a problem. Because inherent in these models is an idea that O'Neil circles back on again and again: since we use these models to help shape the future, they have to reflect the kind of future we want to live in rather than just codifying the past.It's another book I desperately want everyone I know to go out and read so we can have lengthy discussions about it in bars. Here's my only real gripe: I hate the phrase ""Weapons of Math Destruction."" It's an excellent title and a key concept in the book, but O'Neil uses ""WMD"" as shorthand throughout. Every time I saw it, I had to remind myself that it meant ""Math"" destruction instead of ""Mass"" destruction, and by about Chapter 5 the joke had stopped being funny to me. End rant.I loved this book and I will no doubt be reading it again. I could talk about it for a long time, so I'm just going to stop. But, as O'Neil points out, we're in the early days of Big Data. These mathematical models are affecting your life whether you realize it or not, and they don't have to be pernicious. But now is the time to start informing yourself if you want to help shape the conversation tomorrow."
19,0553418815,http://goodreads.com/user/show/2199404-andy,2,"It's good to critique what the author calls WMDs, but for me this book missed the mark. For example, the entire example of teacher-rating in DC is a red herring. The real question is: What is proven to deliver success in an urban US school system? (Answer: Hope and Despair in the American City: Why There Are No Bad Schools in Raleigh) So it's valid to point out the silliness of a silly GIGO teacher-rating algorithm, but it's a distraction to fall down the vortex of dissecting that silliness in detail. We need to focus instead on facts about relevant outcomes. This is sorta kinda vaguely the theme the author seems to be developing but never fully formulates: the afterword delivers a pretty wishy-washy conclusion about the nature of objective truth. Trying to say something nice... The flip side of the superficiality of this book is that it alights on numerous interesting factoids as it skims along. Also, it is clearly written.





"
20,0553418815,http://goodreads.com/user/show/5935491-tanja-berg,4,"Much of our lives are now influenced by algorithms. These are deemed to be neutral, but in fact many are based on models that are glaringly discriminatory. Zip codes stand in for race, for example. The poor are being particularly targeted for pay-day loans. Teacher are measured on opaque proxies that do no reflect their skills at all. Your credit score is as much a result of the group of people you are thrown in with, as with your own behavior. These algorithms are what the author calls ""weapons of math destruction"". Every on-line decision you take, every click you make, is being monitoried. Your ads are tailored, and so is your Facebook news feed. ""Big brother sees you"" has a whole new meaning since ""1984"" was written 50 years ago. We willingly leave a trail of information across a range of sites. We take stupid tests on FB that sells information on ourselves and our friends to the highest bidder. Some times it seems that you only need to consider making an investment for the ads to start trailing your feed and your online existence.Turns out that in making hiring decisions, intelligence tests are prohibited - that is why personality tests flourish. Personality is being used as a proxy for intelligence. This despite the fact that there is little correlation between personality and job performance and that only one trait - conscientiousness - bears any correlation to work performance at all. Job seekers aren't told when their personality test red-flags them even for minimum wage jobs. This book reinforces my notion that we're all screwed. Still, awareness that the internet bubble we all live in does not represent the ultimate truth, would be beneficial. "
21,0553418815,http://goodreads.com/user/show/1459884-krysten,4,"""Big Data processes codify the past. They do not invent the future. Doing that requires moral imagination, and that's something only humans can provide."" So, for work, I read a lot about big data. A lot. And it's all basically *jackoff motion* uhhnnnggg big dataaaa unnnnnnngggghhh yeah. And that makes me want to die.This book is refreshingly critical of big data and algorithms, from a blessedly human approach. You might expect a lot of statistics and dryness, but it's a lot of real life stories to illustrate O'Neil's arguments, which makes it easy to read and care about. It's also a fairly quick read and doesn't waste a lot of time defining concept. O'Neil gets right to it, and her arguments are powerful. If I ever wonder why I can't seem to get my credit card APR down to a reasonable level even though my credit score is excellent, I have to think - maybe it's my zip code! So many pieces of data are taken into account for so many things that end up affecting poor people and people of color in wildly disproportionate ways. You might pay 5x more for auto insurance because you drive through ""bad"" zip codes at certain times of the day. This book is pretty amazing and I'm so, so glad I read it. "
22,0553418815,http://goodreads.com/user/show/21223898-roxana-chiril,4,"Good stuff. Scary stuff. A book about how badly created algorithms are screwing people over in all sorts of ways, all for the sake of ""simplifying"" processes - also a book about part of what's wrong with the US, because it isn't always obvious when you're living in Europe and noticing that people there are acting crazy about their credit scores and zip codes.I have some issues with her interpretation of things (like crying out ""racism"" all over the place, and making contradictory points which amount pretty much to ""things should be good, because now they're bad and that's not good""), but the info is interesting and eye-opening in ways she perhaps didn't intend. "
23,0553418815,http://goodreads.com/user/show/7494007-aaron,3,"We've all seen the Big Data books: the future is now! A/B testing forever! AlphaGo crushed it! OkCupid says you shouldn't have a shirtless fish pic, you adorably dull redneck!But Big Data has a darkside, and O'Neil goes through each segment of our life to show how these ""models"" can be used against us, to extract goods from us, and to keep us poor. Unfortunately, she also loses her argumentative power that could come with nuance, and she has to disregard nuance in order to make it understandable to the layperson (in other words, I don't think she's very charitable to the layperson). The first big issue she brings up and has lots of evidence for is: Transparency. Lots of Big Data models that we use (and are fed into) on a daily basis, are not transparent. They're opaque equations sitting on a server farm. A teacher being graded on a value added model doesn't know where their score is coming from. A potential hire doesn't know why he failed his psyche evaluation. A criminal standing before judge doesn't know why the score said he'd be more like to return to jail. This is honestly one of my biggest takeaways, and it hits close to home. When you apply for a credit card, they tell you why you failed to get one. You can access your ""data point"" and know why you have the FICO score you do. Google and Facebook tell you the metrics they use to serve you ads in their settings. I know that Amazon is trying to get me to buy another wallet even though I just bought a wallet because they showed me five hundred ads for another wallet. But what about those ads that Forbes tries to serve me? The cookies sitting on my computer watching me? I have no ideas what they're doing and not readily known way to find out. What about non-regulated credit systems that exist out there in Web 2.0 land? Bank of America controls for race and tries to stop redlining when they make a new policy, but will Peter Thiel try to do that when he invests in E-Corp? Probably not.The second big problem with some Big Data systems is that they create feedback loops that increase inequality. Here, O'Neil is super weak except when she brings up the criminal justice example- we could be using big data to help keep people out of prison and make programs that lower recidivism, but instead we're using it as a way to keep white people out of prison.... but am I really supposed to believe that ads help make poor people poorer? She brings up for-profit schools using targeted ads to lure immigrants and poor people into massive student debt to make a profit, and, while I admit that's super shady, it's not the targeted ads' fault, is it? The third problem with these ""Weapons of Math destruction"" is that they often have skewed data. This is the old line ""garbage in, garbage out"" except now it's ""racist/sexist garbage in, racist/sexist garbage out."" For example, if you make a employment system that filters out resumes, than teach it on a bunch of older resumes, you're inputting the bias of those older resumes. So if the guy that was reading those resumes was racist, you might be teaching a racist model.Technically O'Neil has two other ""bad points"" about ""WMDs"" but they're just about scale.Now, there are a lot of problems in this book and O'Neil kind of goes on tangents. For one thing, the WMDs she brings up are less ""weapons"" than they are symptoms of a bigger societal problem. Take ""democracy"": our current political system allows a few people- those that live in Orlando, Florida and Pennsylvania, basically- to chose who will be the President. This is messed up, but it means that the Democratic Party could build a powerful machine learning system that most efficiently spent money in locations to help change hearts and minds and win. She really dislikes this Big Data system, and says it's a threat to democracy...... but the electoral system itself is giant problem and threat to democracy (see: election of 2000)! Big Data has nothing to do with it!The book ought to have been longer, and it ought to have included more counterexamples of positive data models (I can recall only two, FICO and some housing model). I think that she should've, if not had hand written equations or step-by-step instructions, at least given some background on actual data science. The way it is written makes it seem like she's a magician-mathematician that wandered down from the Ivory tower and realized that bankers were using magic for evil and now she wants to raise hell. But I guess if I wanted authors to stop writing popular non-fiction books that they A/B tested on their blogs and turned into TED talks, I should stop reading popular non-fiction. "
24,0553418815,http://goodreads.com/user/show/8316749-katiemc,5,"This is not a math book, it's a book about how math is used and misused. Anyone interested in public policy, management theories, or scoring systems should read this. Math is wonderful but it only does what you tell it to. The thesis of this book is that algorithms used in many facets of our lives can be poorly designed, measure the wrong things, or are just plain misapplied. Just because a computer spit out a number or some fancy looking statistics doesn't mean it's fair or should be blindly trusted. As someone who builds predictive models from data, I get this, but I honestly never thought about how models could be destructive. Spoiler - here are some criteria that makes an algorithm or model a weapon of math destruction:1) It is widely used2) It is opaque - those that are being measured or evaluated don't know how algorithms are applied or what criteria are used to make decisions, and in some cases may not even know what metrics are being used to feed the algorithm3) They can be destructive to people's lives - jobs can be lost, arrests can be made, loans can be denied, etc.4) They often don't have a feedback loop to verify if they are working I will say that without a doubt, the author leans left politically. If you are sensitive to that, you may decide that you don't like this book. Regulation of algorithms is a good idea, but not one that will get lots of traction in the current 2016 political climate. Maybe other nations can provide some leadership in this area.2016 reading challenge checks the box for 12. A book recommended by someone you just met . This was recommended to me by a colleague with whom I have co-authored several papers with, but I had never met him person until a few months ago. "
25,0553418815,http://goodreads.com/user/show/3035194-lubinka-dimitrova,5,"I'm rather sorry that I only got around to reading this book in December, otherwise I would have voted for it on Goodreads Choice Awards. A concise, clear-written and eye-opening book, it is even more frightful, for the fact that is it in fact science fiction of the harrowing kind becoming reality as we speak. I belong to the lucky (?) part of the world where technology is still lagging behind what's the norm now in America, but sadly it's unavoidable that sooner or later we'll all catch up with them. And then more and more aspects of our lives will fall under the purview of automated data analysis. Contrary to popular opinion that algorithms are purely objective, this book shows us that models are opinions embedded in mathematics. The author identifies how algorithms integral to both data collection and data interpretation can be manipulated, even if initially they claim the concept of using ""best practices"". Thanks to the feedback loop mathematical models can virtually prove anything their creators want them to prove, depending on their objectives and financial interests. An algorithm is not any less racist than a human, and ultimately algorithms, according to O’Neil, reinforce discrimination and widen inequality, “using people’s fear and trust of mathematics to prevent them from asking questions”. The system is rigged so that it perpetuates the status quo - poor people will stay poor or will get even poorer, and the rich - -well, they'll just buy their way out of being profiled by math, and will stay rich. Or, more probably, will become richer. We live in a sad, sad world..."
26,0553418815,http://goodreads.com/user/show/13716226-murtaza,3,"A short and concise overview of the problems being wrought by the algorithms that are now quietly governing our society. While the overview is useful, I was a bit surprised at how little new information that there was in here. The issues that she raises should be familiar to anyone who has been following the impact of Big Data in even a cursory way. Facebook news bubbles, insurance profiles and other common phenomena are the main examples she uses. I did appreciate the social justice focus of the book, which makes sense given that the author is a former hedge fund analyst disenchanted by the malign behavior that she witnessed on Wall Street. One key takeaway is that it there is a drive for poor and middle class people to have their lives processed by algorithms, while the rich will continue to depend on real-life personal connections to get by. In effect the masses are being turned into raw numbers with scores attached, while a small elite will continue to be authentic ""personalities"" who live in the more nuanced way. This book lays out some of the ways to fight back against opaque algorithmic power and make these algorithms more fair. A solid overview but nothing groundbreaking."
27,0553418815,http://goodreads.com/user/show/28781546-ana,4,"I started this book with an ambivalent outlook on its premise, but by the end, I was a believer and my skepticism level towards algoritms increased."
28,0553418815,http://goodreads.com/user/show/5015851-ms-pegasus,4,"Fear not, mathophobes! Despite a professional background in academe, in finance as a hedge-fund quant, and in online marketing as an algorithm designer, O'Neil avoids mathematical theory and formulas in this highly readable overview of expanding Big Data applications. The examples she uses are in familiar areas: teacher evaluations, college rankings, law enforcement, corporate hiring, and micro-targeted marketing. Because of their familiarity, they warrant close examination.Her first observation is that sometimes Big Data is not so big. Should teaching effectiveness, pay and job security be determined by a metric based on a classroom of 30 students? She aims this critique at both the underlying assumption of value-added modeling as well as its impact on teachers in Washington D.C. Her discussion of “proxies” is particularly thought-provoking. A teacher's effectiveness, college excellence, probability of recidivism — these are all complex, multi-dimensional, and highly subjective areas. We all know that. Yet, the simplicity of an algorithm churning out an absolute ranking is beguiling. O'Neil points out that not only are these proxy variables unreliable but they can easily be gamed once the basic data points are understood. Heavy investments in amenities and publicity-garnering sports teams can and are used to boost a ranking. Moreover, the key consideration of cost is omitted in the algorithms. (Chapter 3: “Arms Race; Going to College”)Another area she effectively explores is an expanding use of personality tests in hiring (Chapter 6: “Ineligible to Serve; Getting a Job). A common theme which she cites here is that such tests are never accountable for their predictions. No one ever knows if a candidate eliminated by a test score would have been a great fit for the job. (An irony that O'Neil does not mention is that a 2010 study found that 3% of a sampling of 203 leadership track candidates exhibited psychopathic traits in their assessments (https://www.forbes.com/sites/victorli...). The entire chapter supports her introductory assertion:  “They [big data models] specialize in bulk, and they're cheap. That's part of their appeal...The privileged, we'll see time and again, are processed more by people, the masses by machines.” (p.8)For profit colleges are the subject of current public policy debate. O'Neil once again makes extremely troubling disclosures. She details their aggressive use of personalized market-segmentation. (Chapter 4: “Propaganda Machine; Online Advertising”). A trail of on-line lead generation techniques narrows the targets for the recruitment staff, armed with persuasive scripts and a list of pressure points. Their targets are not merely the unwary, but the most vulnerable.  “According to the ProPublica report, between 20 and 30 percent of the promotional budgets at for-profit colleges go to lead generation.”Big Data has developed a benign reputation, in part from the book and subsequent film, MONEYBALL. O'Neil contrasts the examples in her book with the use of algorithms in baseball. She highlights glaring differences. She also demands that as citizens we weigh the issue of fairness against the promise of efficiency. Fairness is the overlooked side of the equation. Given the scale of its impact, as a society we must question who and how these big data models impact, and we must demand on-going audits of accountability. It's a sobering declaration."
29,0553418815,http://goodreads.com/user/show/86305081-greg-watson,3,"Weapons of Math Destruction makes some good points about the use and abuse of math models and big data. The most persuasive arguments focus on the use of predictive modeling and its use in criminal sentencing, monitoring driving habits to determine auto insurance rates, and monitoring physical fitness as part of health insurance coverage.The book goes awry though in its focus on inequality along race and class lines. The inequality problem with big data should focus on the gap between the population in general and the small number of people understanding the math going into the models. As a solution to the problem posed by big data modeling, O'Neil lauds the work of the Web Transparency and Accountability Project at Princeton. This project creates fake web profiles to determine the ""treatment those robots receive"" along the lines of race, gender, income, and disability categories. However well-intentioned, the project sounds more like a dystopian novel with an elite benevolent group acting as web bias monitors. "
30,0553418815,http://goodreads.com/user/show/8325737-philipp,4,"This is an angry book, but its got a point! She summarises many 'weapons of math destruction', WMDs, algorithms that encode human biases, are opaque, which give recommendations or orders that no-one questions, while making things worse. For example, this summary:Poor people are more likely to have bad credit and live in high-crime neighborhoods, surrounded by other poor people. Once the dark universe of WMDs digests that data, it showers them with predatory ads for subprime loans or for-profit schools. It sends more police to arrest them, and when they’re convicted it sentences them to longer terms. This data feeds into other WMDs, which score the same people as high risks or easy targets and proceed to block them from jobs, while jacking up their rates for mortgages, car loans, and every kind of insurance imaginable. This drives their credit rating down further, creating nothing less than a death spiral of modeling. Being poor in a world of WMDs is getting more and more dangerous and expensive.O'Neil goes through many, many of examples like this where an automated approach made things worse, where people didn't bother to critically look at the output, where the model doesn't have a way to incorporate feedback, it's stuck in time and cannot evolve like humans. The majority of the book is spent on case studies and great, angry discussion of what went wrong. There's one short chapter on what could be done - mostly she proposes more regulation (auditing of algorithms), but also to 'dumb' down models by law, force them to treat people equally and not bin them in groups. That chapter on solutions is extremely short - I expect a second book from her on that topic.P.S.: I've been reading a lot about adversarial perturbations in deep learning (see this great paper). One idea could be to counteract some of those models to become an adversarial entity. For example, you have one of those shopping points cards. Instead of feeding their model with your shopping data, you mix it up - write down your shopping list for a month, shuffle it, and buy one part with your points card and the other in cash. Keep mixing it up and add random stuff. Your data should be garbage to the model as it's not consistent, easily removable as an outlier, i.e., worthless.You could also make the model work for you - find out which groups of customers the market sends special offers to, game your shopping so that you fall into that group, reap rewards.Of course you'd need to know in detail what the model is looking for, which is highly unlikely."
31,0553418815,http://goodreads.com/user/show/115473-siria,4,"In the two years since Cathy O'Neil published this study of the danger posed by mathematical models—how their supposed neutrality merely reflects the biases of those who create them while creating closed-system feedback loops that warp society in unjust ways—it hasn't become any less relevant. In fact, not only does the book remain relevant, but subsequent revelations—most prominently those about Facebook's involvement with the 2016 US elections—show that O'Neil may even have been too cautious in what is a trenchant and prescient critique. "
32,0553418815,http://goodreads.com/user/show/25178051-izzat-radzi,5,"""Mathematical models are supposed to be our tools, not our masters"" Statisticians, quants, actuaries : take heed. This book argues that the new 'industrial revolution' of big data is bringing destruction. To whom (?) , one might be tempted to asked. Humans, normal masses who sometimes makes errors of course.And due to that, they got red in their ledgers, which in these new century means, that they'll most likely subjected to the 'vicious loop' of rejection and manipulation, and will felt the usual 'life is not fair' feeling. To start off, I claim that this book is the best (so far) compliment to Taleb's Black Swan book. Whilst Taleb as arguably he as a trader and academia revolves his discussion around some financial models & it's discontent (whilst in most part slanted his discussion into the philosophical realm); this author meanwhile goes into a more broad discussion. She, whom has background in chronologically in academia, hedge funds, a short stint in risk management before focusing work as a data scientist, argues that given the presence of these Gargantuan Big Data in almost every aspect of life, it has tarnished if not destroyed people's life. The problem-before I proceed case by case- lies in two reasons. First is the data collected & recorded by the data mining by corporate companies, mostly are left uncrunched, meaning they're not subjected to updated information about those people data taken or they're mistakenly (yet significantly affect) people who are thought 'one', in this case people with the same name and birthday. This, argued by the author, because the model used isn't not tinkered time to time according to feedback; unlike those used in baseball & basketball where opponents and managers subject each individual players to new & paternalistic habits of players -thus a better use of correlation to causation-, they are taken as face value. And feedback that is not taken -later I'll mentioned in job application survey- left those aspiring people hoping to get some good news, baffled as they themselves don't know why their application was rejected. Turns out, it was their 'behaviour', recorded in the subjective and highly interpretable application questions. The second problem here, if not wrong should I put it, is that the usage of the data is, as author's claimed, taken as God's word, the highest possible theological blasphemy, if said to those in denial data miners. This is due to the reasons that they basically use the data available, let it be credit scores, health scores, SAT (or generally educational) scores, and even resume applications -to name a few- as the bestowed revelation, thus dismissing proven good teachers, potential shining employees and laying off 'unhealthy' current workers; all based on the Big Data, either snooping privately on their life, or in other cases, a take-it-or-leave option wether they'll provide their credit score to get an insurance or their health progress for work employment group insurance practiced in firms & companies. The dismissal action has become so autonomous directly from the data, that frightenedly, that one does not care of others, it's just life. Coincidentally, I read Adorno's Culture Industry directly before this, and it does personally provides a more enlightened discourse on the systemic issue. I personally first came across to this topic back a few years ago, that with this newly published book kindles my interest. The first was back around late 2013, where I was taking a UK driving licence. In the course, similarly argued by the author, you'll be getting some discount given that you choose to put a 'Black Box' in your car. This instrument will record your driving habits & feed back to the insurance company. Although initially used to monitor those big truckers -who tends to be sleep deprived due to late night & long driving- to reduce the risks of death of others due to reckless driving; here argued that it's afraid to be used by others to manipulate the privacy of car drivers, at other instances flawed due to normal human behaviour, taking short cuts at night through 'dangerous neighbourhood' thus ended up one marked to be subjected to 'handcuffed' by auto insurance companies. The second one I encountered was in early 2015, in a student organised event to explain & showcase the structure & function of Big Data by companies. At that time, I was furious given the fact of the the then environment of whistle-blowers expose on companies manipulating & snooping normal civilians life. In here meanwhile, as argued in The Impulse Society by Paul Roberts, politicians backed by big companies are microtargeting voters up to the point of what they read and hear are tailor made. Given to the rise of fascism and 'fear of the normal neighbour' one known differently pre-election in America which now turns into something totally unexpected, I humbly agreed to some point. The third encounter is with regard to behavioural test subjected to work or scholarship applications. Exposed initially by my parents, whom my dad was at times hired to be an interviewer. He's the first to enlighten me on the presence of attitude test taken by applicants, which is imperative for career climbing ambitions. Here written an illustration. An applicant answered promptly the normal application survey on personal attitudes. Obviously one is subjected to child education for example that one is unique bla3; yet answering unique now instead of other answers) will be perceived as narcissist by the possible employer, which now is not possible anymore. Unfortunately, this data, is shared and this unfortunate applicant will ended up in this continuous 'dangerous loop'. The author coined Recidivism in regard to this loop. The poor guy, living in a certain 'bad' neighbourhood, want to apply a credit card, or just simply living as a normal civilian. Now, dues to this, of course one might have some background of knowing some in-behind bars people. This data, used by the security enforcement, will cause the police to routinely knocks on doors to tell them they have eyes on them. A more idiosyncratic example is those trying to move houses. Property firms with big data on their very own hand, easily exclude those with a bad tag, even when they've changed, or for unfortunate some, mistakenly recorded in. As said before, no feedback can be given by the victims, unless one has some wealth to pursue on a legal lawsuit on this issue, which of course deteriorate middle class let alone the poor condition. Obviously the rich bankers and markets manipulators escaped from this loop. As a matter of fact, they're invulnerable and invincible. She suggested a few suggestions in the concluding chapter. First for example, oath taking by those finance practitioners, whom impact upon their ignorance is highly significant on the whole. Nevertheless she noted the downside that when in work bureaucratic structure, manager will want answers for certain issues, this will put those people in corners. Second is, as Taleb suggested, ditching those backward minded model. Obviously, she argued, one have to be specialised in the field to help recognise and build a better one. She illustrated a new model on preventing child abuse. There's similarity between the model to 'possible criminals' as in the Minority Report movie in reducing - if not preventing- crime she criticised, the hidden outcomes has not - at least she didn't yet- been analysed. The third is the involvement of human judgement into the model. Yes, initially the algorithm of the system is used to remove human bias, yet seeing how things turned out, the machine as expected cannot understand how human (cognitive) work. The problem of language and rational judgement will always triumph human above machine, I claimed this ambiguous argument. Hats off to this book!"
33,0553418815,http://goodreads.com/user/show/11951948-paul,3,"One of the funniest bits of Little Britain was where David Walliams as Carol Beer would type a request into the computer, before turning to the customer and saying ‘Computer says no’… Whilst it is funny, it is not so funny when it happens to you. In this book, O’Neil, a former Wall Street quant raises alarm bells on the way that these mathematical models have infiltrated our lives. We don’t see them, but these algorithms that help us with our searches online and finding books, films and other items on online sites are now being used to determine just how much of a risk you are. Next time you want a loan, to renew insurance or just need to get another job O’Neil thinks that some of us may have a problem.She calls them ‘Weapons of Maths Destruction’; these are incontestable, unregulated and opaque algorithms. They are being used by companies to decide the tiniest details. They are used because they make larger profits for corporations and most worryingly for us is that they are frequently conclude the wrong thing having made incorrect assumptions about individuals. As the saying goes ‘crap in; crap out’… Having worried the life out of the reader, O’Neil goes onto suggest a variety of things that could help; more regulation, better design of the code and us being aware of their use. The writing is clear, if a little dry and technical at times. The examples are a little American centric, but you can see the way that it is going in the UK. Even though the title mentions the dreaded word maths, it really isn’t that mathematical. Worthwhile reading."
34,0553418815,http://goodreads.com/user/show/16212136-peter-pete-mcloughlin,4,"Very depressing book on how computer algorithms are becoming a powerful tool of the exploitation of workers, the poor, minorities and ordinary people mostly in the service of profits. It goes deep and the problem is much worse than you think. This book is a downer as it covers relevant developments in politics and business and that always is not pleasant to think about in these times in general and are especially dispiriting in how they affect ordinary people."
35,0553418815,http://goodreads.com/user/show/5550611-hallie,2,"Just ok. I like the idea more than the execution of this book. While O'Neil uncovers a great, dark place in our society that needs more light shone on it, much of her conclusions she gives away in the introduction to the book. This would have been an excellent piece in the New Yorker or Atlantic but as a book began to feel redundant far too early. "
36,0553418815,http://goodreads.com/user/show/11345492-kate,4,"3.5, bumped up for likeable clarity-without-condescension and consistent up-front political engagement."
37,0553418815,http://goodreads.com/user/show/761281-mike,4,"Reading this book I could not help but wish the author had used a quote from Paul Goodman as the epigraph of the book: ""Technology is a branch of moral philosophy, not of science."" (Every technology ultimately is making a value judgement about what is important or worth doing, and what aspects of a task or reality are important.)This book is a fascinating look at how 'big data' is used and abused by colleges, banks, insurance agencies, in sentencing and parole decisions, by employers screening applicants, and more. Collecting vast amounts of data and using to guide decisions can be a very useful technology, but the author -- who herself had a career as a 'quant' or data analyst for a large firm -- points out the downside of basing decisions on data and algorithms. Of course it should be obvious, but our society's information fetishism can make us overlook that the data we collect and the algorithms we use to analyze it can be deeply flawed by omissions, prejudices, fallacies that the designers may not be aware of. A couple of the many examples she sues help illustrate this. We read about a school teacher whose performance is measured by an algorithm that takes into account his students' past performance, current performance on standardized tests, and various other factors. One year he is scored 6/100, and the next year 94/100 -- after making no changes to his syllabus, assignments, or anything else he was doing. Anyone with common sense can see there is something wrong with the algorithm if someone could get such wildly varying scores, but the scoring algorithm is a 'black box' that not event the school administrators understand. Among the many problems with the system here is that there is no allowance for the designers of the algorithm to get feedback on the scoring and adjust the model -- instead, the 'failing' teachers are fired and the algorithm rolls on. Contrast this with the use of 'big data' algorithms in commercial enterprises: when Amazon.com collects data on your browsing and purchasing habits, they use this recommendations to you for future purchases. If their models are bad, they suggest things you don't want, and they don't increase sales. But they take this feedback and change their algorithms, because there is a financial incentive to do so.So a huge blind spot in some of the models and algorithms policy makes use is the lack of feedback. Another is huge downside of the rise of big data modeling is that private companies can use opaque models to discriminate in ways that punish the poor particularly. The author explains how car insurance companies use data about where their customers live, drive, and work to assess risks -- which is largely legitimate -- but also abuse this information to determine which customers are likely to be able to shop around for better deals, and which are less likely to have the time, resources, or access to information to do so, and they are charged much more. Indeed affluent customers with DUI convictions may be better rates than safe drivers who live in areas with limited internet access and higher poverty. Likewise employers may screen applicants with big data algorithms that identify 'risks' which rule out the poor, people with past mental illness, or immigrants. Many other problems are discussed, and the final chapters look at how political parties have begun to utilize big data too -- an ominous prospect in light of the rest of the book.The author offers some solutions, ranging from the admittedly ineffective (a code of ethics for 'quants' who create the models) to the more daunting but effective measure of demanding transparency and regulation (which has made some progress in the insurance industry, but is far from complete).*Full disclosure -- I read a free advanced reader's copy that I won in a Goodreads giveaway.*"
38,0553418815,http://goodreads.com/user/show/3502690-graeme-roberts,2," Cathy O'Neil writes well and provides much good information, but I found the fundamental thesis of  Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy to be foolish and irresponsible. The title, condensed inevitably to WMD, is used promiscuously to describe any application of data science, statistics, or even information technology that she considers to be unfair or discriminatory. In most cases, she is right; some applications are used by the greedy and the uncaring with insufficient thought of the consequences, but she rarely considers the positive attributes of the applications. Once branded as a WMD, a term indelibly attached to the evil trifecta of George W. Bush, Saddam Hussein, and the Iraq War, how are readers meant to consider their many positive aspects and what can be done to make them better?At first, I thought that Ms. O'Neil had been the victim of a foolish PR or marketing person in her publishing company, who had come up with this catchy name, but her ardent use of the term convinced me that she had invented it.Ms. O'Neil seemed to follow a downward trajectory from her Harvard PhD in mathematics and a teaching position at Barnard to a leading hedge fund and a series of other morally disappointing positions that left her convinced that data science, and even mathematics, are roots of much evil. I don't buy it. They are tools that can be used or misused, and her diatribe does nothing to help.She asserts, on page 210:If we find (as studies have already shown) that the recidivism models codify prejudice and penalize the poor, then it's time to have a look at the inputs. In this case, they include loads of birds-of-a-feather connections. They predict an individual's behavior on the basis of the people he knows, his job, and his credit rating—details that would be inadmissible in court. The fairness fix is to throw out that data.But wait, many would say. Are we going to sacrifice the accuracy of the model for fairness? Do we have to dumb down our algorithms?In some cases, yes. If we're going to be equal before the law, or be treated equally as voters, we cannot stand for systems that drop us into different castes and treat us differently.I disagree strongly. Some data is better than none, though we should be looking to constantly improve our data science systems. I would like to know that the recently paroled criminal moving in next door had been released by a judge in full possession of whatever facts are available. So, I suspect, would Ms. O'Neil."
39,0553418815,http://goodreads.com/user/show/19993-robin,4,"If you've been listening to Cathy O'Neil on Slate Money for some time, you probably expected this book to emerge in exactly this form and with exactly this content. She's smart, progressive, focused, and clear, just as she was on the podcast. I saw her speak at the Mechanic's Institute Library a few months ago, before I'd read the book, and was very impressed by her ability to be articulate about complex situations with a decisiveness that belies their complexity, or at least certainly never allows for it as an excuse. The argument I found most challenging was (heavily paraphrasing here) that we should be privileging fairness because it is otherwise so easy to be blind to the unfairness that creeps in when we pursue other goals. If I'm honest with my own tendencies, I recognize my longing for accuracy might indeed cause me to overlook fairness, and I resist the idea that I need to choose one as a priority over the other. Nevertheless, O'Neil convinces me to question assumed neutrality much more closely, and be much more vigilant for it in my own industry. As she points out, it's easy for people who write the algorithms to absolve themselves of responsibility for how they are used, but if we pay attention to the real outcomes we may find that we need to do more to train our algorithms to learn from externalities and flaws. Focusing ruthlessly on measuring only what you are trying to measure and ignoring all other factors is a good start; evaluating the results to make sure that's actually what happened is as necessary a practice. The book is full of examples of failed algorithms in wide use, actively and detrimentally affecting at-risk populations today, as well as potential future areas where algorithms could be used, perhaps even inadvertently, to consolidate and perpetuate the power of the already powerful. The free market will not solve this problem. This is an excellent example of where good regulation is imperative. As constituents with the ability to influence legislators to put their attention here, we can demand more transparency, more awareness and education, and a more good, old-fashioned compassion.Further reading:https://www.ted.com/talks/joy_buolamw...https://www.propublica.org/article/ma...https://standards.ieee.org/develop/pr..."
40,0553418815,http://goodreads.com/user/show/7139041-dorin,2,"Disappointing - such a rich topic, yet the author decided to be racist, Democrat and to induce panic instead of having a cool headed approach. She ignores her own advice, contradicts herself time and again, and she makes her Democrat leaning so obvious it makes me puke.The book could have contained real data, real information, but instead she goes on to explain that statistics are bad because statistics don't catch statistical errors. She pretends she's a professional in data processing, but she sounds like she doesn't understand how statistics work, and she can barely go beyond her mathematical training to understand what went wrong from a human point of view.My point: „a machine is as useful as the person who uses it”. Her point: ""big data is bad unless it's used by the Democrats”."
41,0553418815,http://goodreads.com/user/show/7837611-greg-fanoe,2,"I think that on the whole reading this book will do people more good than harm but I had several problems with it:1. The only section which I have personal expertise on is the section on ""insurance"" - I am an actuary - and I can personally verify that this chapter was filled with inaccurate, misleading, and/or totally speculative statements.2. The correlation between credit score and income is nowhere near as strong as she makes it appear in this book.3. Many of the problems she identifies have nothing to do with the models in question. Advertising models are not the issue with the payday loan or for-profit college industry those industries need to be totally upended/destroyed at the source. Letting them operate as is but kneecapping their ability to advertise would be nonsensical.4. The solutions she presents are ridiculously vague and/or impractical."
42,0553418815,http://goodreads.com/user/show/1045774-mehrsa,5,This is the book I was waiting to read! The rejoinder to our data-obsessed culture. We got drunk on algorithms and forgot to focus on their limitations. are they asking the right questions? are they recycling inequality? Great book
43,0553418815,http://goodreads.com/user/show/3126915-imi,2,"Sadly, a rather superficial look at big data, algorithms and how they've impacted on society. You probably know/have guessed all this already. The correlations O'Neil point out are not very convincing, and it felt like she just wanted to rant about how unfair the world is. Not that I disagree with that conclusion; society is unfair and unequal. But O'Neil really doesn't state much more than that, and, worse, some of her examples felt misleading or simply speculative. Where was her evidence? And if she'd explored this issues in more depth I'd have wanted clear ideas for resolving these issues. Instead, any suggestions she made seemed vague and impractical. I admitedly feel out of my depth with this topic, but am most definitely willing to learn more and expand my understanding about it. Disappointingly, this book didn't offer that."
44,0553418815,http://goodreads.com/user/show/777176-dave,5,"I learned more from this book than I have from any book I've read in years. O'Neil is a former math professor and spent time as a hedge-fund quant, so she understands the theory and experience of the effect of out-of-control algorithms on our society.As corporations make more and more efficient use of algorithms, they are starting to show dangerous tendencies, according to O'Neil. She calls them ""weapons of math destruction"" or WMDs. WMDs present the following grave problems for society:--They usually lack transparency, such that it is difficult or impossible to know what goes into them and what factors sway or change their results. When they are opaque, WMDs are patently unfair (O'Neil cites the algorithms used to evaluate teachers in many districts, highlighting their absurdity as they attempt to quantify the unquantifiable).--They make broad generalizations and reduce human beings to ""buckets."" One of the worst examples is the criminal justice system, which is increasingly reliant on algorithms to deploy police patrols and even to sentence criminal defendants. These algorithms punish the innocent by assuming that those who live in poor neighborhoods are necessarily more likely to commit crimes, thereby subjecting them to frequent ""stop and frisk"" type searches that have been found to be unconstitutional repeatedly.--They can use information in an unfair way that tends to perpetuate further injustice. A prime example of this is the increasing tendency to identify individuals by their credit score, on job applications and even in online shopping. When job applicants are screened by credit score, those who are already poor (and therefore much more likely to have a low credit score already) will find it even harder to get a job and thereby earn enough money to help improve their credit score. This self-perpetuating cycle of poverty is completely unnecessary, because credit scores are an exceedingly poor proxy for job-worthiness of applicants.--Many WMDs are not very good at learning. They are created as models of human behavior (with all the flaws and assumptions of the humans who programmed them), and when they miss (when for example they screen out a potential hire who would have actually been a first-rate employee), they never learn that they have missed. The ""false positive"" never presents itself (the rejected applicant goes on to success elsewhere), and the oblivious WMD continues to chug along with its assumptions and blind spots unchallenged.And WMDs are like racism:""Racism, at the individual level, can be seen as a predictive model whirring away in billions of human minds around the world. It is built from faulty, incomplete, or generalized data. Whether it comes from experience or hearsay, the data indicates that certain types of people have behaved badly. That generates a binary prediction that all people of that race will behave that same way....Racists don't spend a lot of time hunting down reliable data to train their twisted models. And once their model morphs into a belief, it becomes hardwired. It generates poisonous assumptions, yet rarely tests them, settling instead for data that seems to confirm and fortify them. Consequently, racism is the most slovenly of predictive models. It is powered by haphazard data gathering and spurious correlations, reinforced by institutional inequities, and polluted by confirmation bias. In this way, oddly enough, racism operates like many of the WMDs I'll be describing in this book."" (p. 22-3)And finally:""Models like this will abound in coming years....Many of these models, like some of the WMDs we've discussed, will arrive with the best intentions. But they must also deliver transparency, disclosing the input data they're using as well as the results of their targeting. And they must be open to audits. These are powerful engines, after all. We must keep our eyes on them.... Data is not going away....Predictive models are, increasingly, the tools we will be relying on to run our institutions, deploy our resources, and manage our lives. But...these models are constructed not just from data but from the choices we make about which data to pay attention to--and which to leave out. Those choices are not just about logistics, profits, and efficiency. They are fundamentally moral. If we back away from them and treat mathematical models as a neutral and inevitable force, like the weather or the tides, we abdicate our responsibility....We must come together to police these WMDs, to tame and disarm them."" (p.218)"
45,0553418815,http://goodreads.com/user/show/3029658-doug-cornelius,5,"With big data, comes formulas to parse through the data trying to make sense of it. Those algorithms can help make sense of the data and help filter through the noise to find trends. But those algorithms can also be easily misused and have harmful, if unintended consequences. Cathy O'Neil explores these problems in 
Weapons of Math Destruction
.Ms. O'Neil is a former academic, hedge fund quant, and data scientist for startups. At the hedge fund she was looking at ways to make money by predicting financial trends. As a data scientist she was looking to predict people's purchases and browsing habits to monetize those habits. Those are good algorithms where the people using them understand them and update the algorithms as they see problems and can check to see if they are working properly by verifying outcomes.Many data driven outcomes fail. Those are the focus of 
Weapons of Math Destruction
.I'm a big fan of the Slate Money podcast with Cathy O'Neil, Felix Salmon, and Jordan Weissmann. When the publisher offered me a review copy of Ms. O'Neil's book, I jumped at the chance.It's a short book and even though the title makes it sound like it's full of math, it's not. It's more about social policy. She points to the danger of decision makers blindingly following algorithmic output that they do not understand.Take the US News and World Report listing of best colleges. This is preeminent guide for high school students trying to figure out which college to attend. The rankings do not look at actual student outcomes. The publisher does not look at happiness, learning or improvements. It looks at proxies for the outcomes like SAT scores, alumni contributions, and graduation percentages. Colleges started making decisions to improve their rankings in the algorithm by improving those measured proxies. As anyone writing checks for their college bound children, affordability is not one of the measured proxies.As you might expect there are big chunks of the book dedicated to failings in the financial sector by relying on poorly designed algorithms. The biggest problems often being false assumptions plugged into the data running through the algorithm.With the election season upon us, the chapter on political algorithms is fascinating. The data scientists of commerce were brought into political campaigns. They are able to micro-target potential voters sending different messages to different groups highlighting the positions that would make the candidate more favorable to that segment of the population. Civic engagement and the political process is increasingly being algorithm driven.Since our world is increasingly being driven by big data, it's important to understand what is happening behind the decision-making. Weapons of Math Destruction is an excellent tool to help you understand data driven decision-making."
46,0553418815,http://goodreads.com/user/show/23020614-germ-n,3,"A fascinating must-read guide on how to navigate and interpret a world increasingly ruled by materialistic prejudices and oversimplifications that are repackaged as ""science"" and ""tech"" through poorly designed computer algorithms.Reading this gives one the feeling that the ""evil androids ruling over mankind"" dystopia has already been unrolling. Except in a very sneaky way: this time we don't get to face or even understand the robots and their actions which are nonetheless already reshaping our reality and the course of our lives through ridiculous university rankings, credit scores, social media ideological bubbles, etc.Unfortunately O'Neil ruins an otherwise brilliant book by an extreme leftist bias that seeks to absolve her personal heroes in the US democratic party and their political agendas, and attempts to attribute most blame on a specific other side. I assume this is why a book which raises such important points gets to be published and become mainstream after all.For example, she portrays Obama as a saint who just wants the best for immigrants and fiercely battles the evil rich. Yet for any unbiased observer, it's quite clear that Obama and democrats are just serving the wealthy who finance their own political campaigns (duh) when they set to ruin nations overseas and bring in thousands of immigrants, ideally illegal ones, so the price of labor can go down and corporations can continue to reap political advantages.This skewed perspective is damaging to the bottom argument and distracts readers from what's really the main issue here: the constant loss of truth and distortion of increasingly astroturfed public debate. No attempts are made, for example, at addressing how these WMDs are influencing the toxic foreign policy of the US and the press - another entity that passes as a saint in this book, while being the number one responsible for war and destruction of nations around the world."
47,0553418815,http://goodreads.com/user/show/20465079-tam,4,"A quick and highly useful read - I would recommend it to all people not just ones working with predictive models and Big Data.The title is sensational enough, but at one point I put that aside and decided to read this book. So many have been praising Big Data and Machine Learning, few cautions have been put forward but not enough, mostly dealing with ethics, security, personal data issues. This book, on the other hand, discusses the large scale tragedies that Big Data can invoke on societies as a whole, if continuing to go unchecked. 10 chapters deal with examples about the misuses of data and predictive models in many aspects of lives, especially the lives of the most unfortunate.O'Neil has a strong opinion and at certain point I thought she might be too pessimistic (saying that from a self-proclaimed pessimistic person as I am), and O'Neil freely claims how angry she is with the whole thing. Yet at the very end of the book, she makes constructive comments, ideas for improvement, for regulations. Those are the most important, and perhaps the most useful passages for data scientists. I've been quite fascinated by the tool and sometimes forget the darker side that the tool can invoke, and I'd like to thank O'Neil for the reminder, for the clarification about issues that sometimes I feel uneasy but not so clear about. No, the tool is neither inherently good nor bad, the people who make them and wield them can choose do what they want. And we can choose to do good things.Ehh, not so optimistic though, there are tons of challenges and... But I should shut my mouth. But at least we should try."
48,0553418815,http://goodreads.com/user/show/15409024-ahalya-vijay,5,"I found this to be a fascinating read on the importance of using technology and data in an ethical way. I was shocked to learn how pervasive irresponsible uses of data are throughout the developed world and how widespread it is for people to place their faith in ""black box"" models as Cathy O'Neil refers to them. It must be frustrating, for example, for teachers to be evaluated (and fired!) based on a model they have no knowledge of, without knowing how to improve their rating. And it must be even more so if they are getting anecdotally strong and positive feedback from school leadership and parents.Another shock was how liberally companies and 'data scientists' use proxies without thought to the consequences of the model not capturing the 'error term' sufficiently. The danger of making decisions based on things that are modeled (or estimated) using proxies was drilled into my head throughout my entire schooling. How would you feel if the fact that you were late to one meeting at work was used to brand you as an irresponsible or unreliable worker? Ms O'Neil made a clear link between such models being used to trap people in a cycle of poverty, using example after example across a variety of industries (e.g., education, finance, insurance). It is clear that reform is needed, but what left me worried at the end is that these models are not hurting the rich and famous, but the poor and under-resourced. Will our current political and justice system listen to their needs? I'm skeptical."
49,0553418815,http://goodreads.com/user/show/48433695-alexis-tremblay,5,"A must read for all data scientist, mathematicians, modellers, statisticians,... There is a dark side to big data and she exposes it masterfully. I always had an uneasy feeling when I started my career as an actuary and later on as data scientist. My moral compass was always off but I didn't have the words for it. Now she has given me a framework to think about my job and what to look out for. Will read again. "
50,0553418815,http://goodreads.com/user/show/16204614-bing-gordon,4,"Good overview of modern analytics, but too much of a guilt trip laid on top. "
51,0553418815,http://goodreads.com/user/show/6100646-brian-clegg,4,"As a poacher-turned-gamekeeper of the big data world, Cathy O'Neil is ideally placed to take us on a voyage of horrible discovery into the world of systems making decisions based on big data that can have a negative influence on lives - what she refers to as 'Weapons of Math Destruction' or WMDs. After working as a 'quant' in a hedge fund and on big data crunching systems for startups, she has developed a horror for the misuse of the technology and sets out to show us how unfair it can be.It's not that O'Neil is against big data per se. She points out examples where it can be useful and effective - but this requires the systems to be transparent and to be capable of learning from their mistakes. In the examples we discover, from systems that rate school teachers to those that decide whether or not to issue a payday loan, the system is opaque, secretive and based on a set of rules that aren't tested against reality and regularly updated to produce a fair outcome.The teacher grading system is probably the most dramatically inaccurate example, where the system is trying to measure how well a teacher has performed, based on data that only has a very vague link to actual outcomes - so, for instance, O'Neil tells of a teacher who scored 6% one year and 96% the next year for doing the same job. The factors being measured are almost entirely outside the teacher's control with no linkage to performance and the interpretation of the data is simply garbage.Other systems, such as those used to rank universities, are ruthlessly gamed by the participants, making them far more about how good an organisation is at coming up with the right answers to metrics than it is to the quality of that organisation. And all of us will come across targeted advertising and social media messages/search results prioritised according to secret algorithms which we know nothing about and that attempt to control our behaviour.For O'Neil, the worst aspects of big data misuse are where a system - perhaps with the best intentions - ends up penalising people for being poor of being from certain ethnic backgrounds. This is often a result of an indirect piece of data - for instance the place they live might have implications on their financial state or ethnicity. She vividly portrays the way that systems dealing with everything from police presence in an area to fixing insurance premiums can produce a downward spiral of negative feedback.Although the book is often very effective, it is heavily US-oriented, which is a shame when many of these issues are as significant, say, in Europe, as they are in the US. There is probably also not enough nuance in the author's binary good/bad opinion of systems. For example, she tells us that someone shouldn't be penalised by having to pay more for insurance because they live in a high risk neighbourhood - but doesn't think about the contrary aspect that if insurance companies don't do this, those of us who live in low risk neighbourhoods are being penalised by paying much higher premiums than we need to in order to cover our insurance. O'Neil makes a simplistic linkage between high risk = poor, low risk = rich - yet those of us, for instance, who live in the country are often in quite poor areas that are nonetheless low risk. For O'Neil, fairness means everyone pays the same. But is that truly fair? Here in Europe, we've had car insurance for young female drivers doubled in cost to make it the same as young males - even though the young males are far more likely to have accidents. This is fair by O'Neil's standards, because it doesn't discriminate on gender, but is not fair in the real world away from labels.There's a lot here that we should be picking up on, and even if you don't agree with all of O'Neil's assessments, it certainly makes you think about the rights and wrongs of decisions based on automated assessment of indirect data."
52,0553418815,http://goodreads.com/user/show/7019761-cindy,4,"While I don't necessarily agree with all of the conclusions, this was a very thought-provoking book. The author is a mathematician who has worked on Wall Street and now takes on some of the math models that rule our lives. Some of these were, at least originally, developed with good intentions - intending to make fairer some of the decisions that affect our lives: where and who gets into college; what the schedule for our job looks like; how much we pay to get a loan; even how you are judged in your chosen profession. But the author poses that many of these types of decisions are actually less than fair. The models are in her terms WMDs - weapons of math destruction - they are hidden, unregulated, and virtually uncontestable, even when they are blatantly wrong. One of the more troubling examples from the book concerned the use of testing to judge teacher performance. She traced how an exemplary teacher with outstanding evaluations was judged as poor on the basis of one set of tests and fired. The teacher showed that the tests which showed that her ""value-added"" score was sub-par were flawed due to a high number of erasures on the previous year which indicated cheating. Indeed, the system said that there *might* have been cheating but they would not re-instate this excellent teacher. (And please don't get me started on ""value-added"" - my daughters' school was always excellent in all the testing but this score was just average. So if your kids are already high achievers, how do you continue to score excellent in this measure? The answer is it's almost impossible.)Other examples showed that even when you removed race or ethnicity or economics as an indicator in these WMDs, they were there. If a zip code puts you in a risky category for a student loan, then it's hard to get the education that can get you out of poverty; or if a work schedule makes it hard for a single mother to get the child care she needs so that she can get the money to go to school to make a better life for herself and her child...well, I can see how some of this sets up a self-fulfilling prophecy. It makes it harder, not impossible, to break this cycle. That's why this book is important. If this is being used, then we need to know and like credit scores have the ability to correct errors and inaccuracies that work against the average person.Quotes to remember:....This would all be wonderful for students and might enhance their college experience, if they weren't the ones paying for it in the form of student loans that would burden them for decades. We cannot place the blame for this trend entirely on the US News rankings. Our entire society has embraced not only the idea that a college education is essential, but the idea that a degree from a highly-ranked school can catapult a student into a life of power and privilege. As I write this, the entire voting population that matters lives in a handful of counties in Florida, Ohio, Nevada, and a few other swing states. Within those counties is a small number of voters whose opinions weigh in the balance. I might point out here that while many of the WMDs we've been looking at, from predatory ads to policing models, deliver most of their punishment to the struggling classes, political microtargeting harms voters of every economic class, from Manhattan to San Francisco, rich and poor alike find themselves disenfranchised, though the truly affluent of course can more than compensate for this with campaign contributions."
53,0553418815,http://goodreads.com/user/show/6894205-allie,4,"This was by far the most interesting book about math I have ever read (admittedly a low bar to set), possibly because it contains no actual math formulas. Instead, former mathematics professor and hedge fund analyst Cathy O'Neil tells stories about the applications of math - how ""ill-conceived mathematical models now micromanage the economy from advertising to prisons."" Among other things, mathematical models determine which news you see on social media, your Google search results, the cost of your auto insurance and credit card rates, which neighborhoods police are sent to patrol, and how your child's schools are evaluated. In one staggering statistic about the reach of these models, O'Neil notes that 72 percent of resumes are never seen by humans, but are instead screened and scored by software programs looking for employer keywords. While math itself is neutral, the models are based on assumptions that reflect human biases and limitations. Often, the models are designed to process data from large numbers of people efficiently, but not necessarily fairly. She distinguishes between ""good"" models built on transparent assumptions that are constantly corrected with new data (e.g., those used in professional baseball) to opaque models that use proxy data such as race or zip codes and do not have a corrective feedback mechanism (e.g., predictive policing software). Typically, there is no method of recourse when given a poor score by a computer (hello, Hal) since companies consider their algorithms proprietary and won't disclose details of how scores are developed. This was an issue for teachers evaluated - and sometimes fired - based on inaccurate models of student test scores. The book's second main argument is that the built-in biases in the models are especially putative towards the poor, who are steered to high interest loans, targeted by ""stop and frisk"" policing based on their neighborhoods, and less likely to be personally interviewed for jobs.I found many of O'Neil's arguments persuasive and somewhat alarming, although she tries to end the book on a positive note. Worth reading, even if you don't like math. Or weapons. Or destruction. "
54,0553418815,http://goodreads.com/user/show/70315594-enoughtohold,4,"A nice, clear overview of the problems of big data for laypeople. Not sure why other reviewers expected this to be deeply technical, or why they think O’Neil thinks there are easy answers — she plainly doesn’t.I for one am glad a book this accessible exists on this topic. After all, the vast majority of the people affected by these harmful models are not data scientists, and they deserve to know what’s going on.(Also, the author did a great job reading for the audiobook.)"
55,0553418815,http://goodreads.com/user/show/4430576-miri,5,"I've said it before, and I'm sure I'll say it again, because I make a point of seeking out books like this. But if there's one book I could get everyone to read right now, it would be this one. Even next to Ta-Nehisi Coates's We Were Eight Years in Power, which I'm reading concurrently and which is staggering in its importance, this book feels urgent—because what it explains is how our new economy of Big Data is codifying, concealing, and amplifying inequality of every kind, on an enormous scale. Every injustice I'm reading about in Coates's book is made worse by what I read about here. We think of math as inherently neutral, the way we think of technology and logic and science. But these things are like any tool, dependent on the person using them, capable of being used for good or evil. (Remember the way science was abused throughout American history, fabricating the entire concept of biological race to provide justification for white humans to enslave black ones.) When we start using math to make decisions instead of people, assuming that this will decrease human bias and make things more fair, what we can actually end up doing is obscuring that prejudice and magnifying the scale of its effect. Cathy O'Neil was a mathematician working in the finance industry in 2008. She left after a few years, disgusted with the way the system was clearly rigged, and has since created the concept of Weapons of Math Destruction: mathematical models that codify human prejudice, are opaque and impervious to feedback, and affect large numbers of people. In this book, O'Neil takes us through several WMDs, showing how they're used in different areas of our lives. There's the disastrous school reform attempt in Washington D.C., which tried to evaluate teachers in a way math simply cannot be used, and which fired hundreds of teachers based on results no one in the district could explain or defend. There's the way many employers now use credit scores to screen applications, assuming that making regular payments means you'll be a dependable employee, ignoring the facts that (1) responsible people experience financial difficulty too and (2) preventing people with bad credit from getting jobs is only going to make their situation worse.There are the computerized risk models that judges use in sentencing, trying to gauge from prisoners' background information—where they grew up, whether their friends and family have criminal records; information which would be inadmissible in court—how likely they are to return to prison after being released, and how long their sentence should be. There's the way U.S. News & World Report helped cause an arms race when it decided to start ranking universities on everything except their cost, driving up tuition over 500 percent (nearly four times the rate of inflation) in just the 30-ish years that I've been alive.There's political advertising which is now so individually personalized that even people who support the same candidate, even people visiting the candidate's website, will see totally different things based on the profile the campaign has created for them.There's predatory advertising like that done by for-profit colleges, which specifically targets vulnerable people's ""pain points"" looking for those who are ""stuck,"" who have ""few people in their lives who care about them,"" who have ""low self-esteem"" and are ""unable to see and plan well for the future"" (these quotes, shared by O'Neil, come from the California Attorney General's office when it was investigating one of these colleges). (And if you're wondering how they would know this about people, the point is that they know this about all of us now. Just try to imagine what someone could learn about you from seeing everything you do online.)It might seem like the logical response is to disarm these weapons, one by one. The problem is that they’re feeding on each other. Poor people are more likely to have bad credit and live in high-crime neighborhoods, surrounded by other poor people. Once the dark universe of WMDs digests that data, it showers them with predatory ads for subprime loans or for-profit schools. It sends more police to arrest them, and when they're convicted it sentences them to longer terms. This data feeds into other WMDs, which score the same people as high-risk or easy targets and proceed to block them from jobs, while jacking up their rates for mortgages, car loans, and every kind of insurance imaginable. This drives their credit rating down further, creating nothing less than a death spiral of modeling. Being poor in a world of WMDs is getting more and more dangerous and expensive.The same WMDs that abuse the poor also place the comfortable classes of society in their own marketing silos . . . For many of them it can feel as though the world is getting smarter and easier. Models highlight bargains on prosciutto and chianti, recommend a great movie on Amazon Prime, or lead them, turn by turn, to a cafe in what used to be a ""sketchy"" neighborhood. The quiet and personal nature of this targeting keeps society's winners from seeing how the very same models are destroying lives, sometimes just a few blocks away.Though we're all affected by them in ways we can't see, right now the majority of the damage is being done to the same groups who are always hurt by flaws in the system—minorities and the poor. The same models could easily be used to help people instead of prey upon them, but that won't happen in the glorious free market of capitalism. We need regulation and oversight in the data industry. We need accountability. When statistics itself, and the public’s trust in statistics, is being actively undermined by politicians across the globe, how can we possibly expect the Big Data industry to clarify rather than contribute to the noise? We can because we must. Right now, mammoth companies like Google, Amazon, and Facebook exert incredible control over society because they control the data. They reap enormous profits while somehow offloading fact-checking responsibilities to others. It’s not a coincidence that, even as he works to undermine the public’s trust in science and in scientific fact, Steve Bannon sits on the board of Cambridge Analytica, a political data company that has claimed credit for Trump’s victory while bragging about secret ""voter suppression"" campaigns. It’s part of a more general trend in which data is privately owned and privately used to private ends of profit and influence, while the public is shut out of the process and told to behave well and trust the algorithms. It’s time to start misbehaving. Algorithms are only going to become more ubiquitous in the coming years. We must demand that systems that hold algorithms accountable become ubiquitous as well."
56,0553418815,http://goodreads.com/user/show/17187084-sean-goh,4,"Very readable book warning of the downside of amoral models. With great power comes great responsibility.___Math-powered applications were based on choices made by fallible human beings. Many models encoded human prejudice, misunderstanding, and bias into the software systems that increasingly managed our lives. Like gods, these mathematical models were opaque, their workings invisible to all but the highest priests in their domain: mathematicians and computer scientists.Models are opinions embedded in mathematics.Statistical engines without feedback can continue to spin our faulty and damaging analysis while never learning from its mistakes. E.g. teachers mistaken labelled as failures are fired, and no one is the wiser. The model's output becomes unquestioned reality.The parallels between finance and Big Data: The productivity of the field's talents indicates that they are on the right track, and it translates into dollars. This leads to the fallacious conclusion that whatever they are doing to bring in more money is good. It ""adds value"". Otherwise why would the market reward it?In both cultures Wealth is no longer a means to get by. It becomes directly tied to personal worth.On the rise of university rankings: from the perspective of a university president, it's quite sad. Here they are at the summit of their careers dedicating enormous energy towards boosting performance in fifteen areas defined by a group of journalists at a second-tier magazine (U.S. News). The ranking had become destiny. When you create a model from proxies, it is far simpler for people to game it. This is because proxies are easier to manipulate than the complicated reality they represent. (e.g. trying to evaluate influence by number of twitter followers)In a system in which cheating is the norm, following the rules amounts to a handicap. So preventing the students in Zhongxiang from cheating was unfair.In our largely segregated cities, geography is a proxy for race.Patrolling particular areas creates a pernicious feedback loop. The policing itself spawns new data, which justifies more policing. Eventually the zeroing in on ghetto districts ends up criminalising poverty, believing all the while that our tools are not only scientific but fair.Instead of simply trying to eradicate crimes, police should be attempting to build relationships in the neighbourhood. This was one of the pillars of the original ""broken windows"" study. The cops were on foot, talking to people, trying to get them to uphold their own community standards. But that objective, in many cases, has been lost, steamrollered by models that equate arrests with safety.Mathematical models can sift through data to locate people who are likely to face great challenges, whether from crime, poverty, or education. It's up to society whether to use that intelligence to reject and punish them - or to reach out to them with the resources they need. It all depends on the objectives we choose.Computing systems have trouble finding digital proxies for soft skills. The relevant data simply isn't collected, and anyway it's hard to put a value to them. They're usually easier to just leave out of the model.The modelers of e-scores (unofficial credit ratings) have to make do with trying to answer the question: ""How have people like you behaved in the past?"" when ideally the question would be ""How have you behaved in the past?Even with the Affordable Care Act, which reduced the ranks of the uninsured, medical expenses remain the single biggest cause of bankruptcies in America.A sterling credit rating is not just a proxy for responsibility and smart decisions. It is also a proxy for wealth. And wealth is highly correlated with race.Mistakes made by models are learning opportunities - as long as the system receives feedback on the error. When automatic systems sift through our data to size us up for an e-score, they naturally project the past into the future. The poor are expected to remain poor forever and are treated accordingly. It's inexorable, often hidden and beyond appeal, and unfair.Yet machines cannot make adjustments for fairness by themselves. That is where the human overseer comes in.Human decision making, while often flawed, has one chief virtue. It can evolve.Big data processes codify the past. They do not invent the future. Doing that requires moral imagination, and that's something only humans can provide. We have to explicitly embed better values into our algorithms, creating Big Data models that follow our ethical lead. Sometimes that will mean putting fairness ahead of profit."
57,0553418815,http://goodreads.com/user/show/6991670-bob,4,"Summary: An insider account of the algorithms that affect our lives, from going to college, to the ads we see online, to our chances of getting a job, being arrested, getting credit and insurance.Big Data is indeed BIG. Mathematical algorithms shape who will see this post on their Facebook newsfeed. If you go to Amazon or another online bookseller, algorithms will suggest other books like this one you might be interested in. Have you seen all those ads about credit scores? They are more important than you might imagine. Algorithms used by employers and insurance companies determine your employability and insurability in part through these scores. Far more than another credit card (bad idea, by the way) or a mortgage are on the line. These algorithms seem objective, but how they are formulated, and the assumptions made in doing so mean the difference between useful tools that benefit people, and ""black boxes"" that thwart the flourishing of others, often unknown to them.Cathy O'Neil should know. A tenure track math professor, she made the jump to Wall Street and became a ""quant"" who helped develop mathematical algorithms and witnessed, in the crash of 2008, the harm some of these caused. And she began to notice how algorithms often painfully impacted the lives of many others.  She describes how a teacher was fired because of the weighting of performance scores of a single class, despite other evaluations finding her an excellent teacher (afterwards it was found that there were a high number of erasures on tests for students who would have been in her class the previous year, suggesting these had been altered to improve scores).As she looked at the algorithms responsible for such injustices, she came to dub them ""Weapons of Math Destruction"" or WMDs and she identified three characteristics of these WMDs:Opacity: those whose lives are affected by them have no idea of the factors and weighting of those factors that contributed to their ""score"".Scale: how widely an algorithm is applied across industries and sectors of life can affect how much of one's life is touched by a single formula. For example, the FICO scores mentioned above affect not only credit, but the ability to get a job, the cost of auto insurance, and your ability to rent an apartment.Damage: WMDs can reinforce other factors perpetuating a cycle of poverty, or incarceration.She also shows that what makes these algorithms destructive is the use of proxy measurements. For example an employer may not know directly how savvy someone is as a marketer, and so they use a ""proxy"" measurement of how many Twitter followers that person has. Or age is used as a proxy for how safe a driver one is. For a group, the proxy may work well, and be utterly inaccurate for an individual that falls within that proxy group.Then in successive chapters, she chronicles some of the ways WMDs operate in different parts of life. She discusses the U.S. News & World Report college rankings, and the use of algorithms in admissions processes. Social media uses algorithms to target advertising, which means some will see ads for for-profit schools and payday lenders, and others for upscale furnishing or Viagra, based on clicks, likes, searches, and comments. Policing strategies, including locations for intensified ""stop and frisk"" policing are shaped by another set of algorithms. Algorithms to filter resumes may use scoring algorithms that discriminate by address and psych exam algorithms may render others unemployable in a certain industry. Scheduling algorithms may promote efficiency at the expense of the ability of workers to sleep on a regular schedule, or arrange childcare, or work enough hours to qualify for health insurance. Algorithms sometimes shut people out from credit or low cost insurance when in fact they are good risks. She concludes by showing how algorithms determine ads and news we see (and don't see). In an afterword she explores the flaws in algorithms revealed on the election of Donald Trump (algorithms, for example predicted Clinton would easily win Michigan and Wisconsin, where consequently she did not campaign, and lost by small margins).In her conclusion, she makes the case not only for a code of ethics for mathematicians but also argues that regulation and audits of these algorithms are necessary. The value assumptions, as well as the mathematical methods of many algorithms are flawed, and yet opacity means those whose lives are most affected don't even know what hit them.She helps us see both the sinister and useful side of these algorithms. They may reveal where a pro-active intervention may save a family from descending into family violence, or provide extra assistance to a child in danger of falling behind in a key subject. Or they may be used to invade personal rights, or even to perpetuate socio-economic divides in a society. The reality is that the problem is not the math but the old GIGO problem (garbage in, garbage out). The values and assumptions of the humans who devise the formulas and weightings of values and the use of proxies determine what may be destructive outcomes for some people. Yet it can be hidden behind an app, a program, an algorithm.The massive explosion in storage capacities, processing speeds, and the way our interests, health status, travel patterns, spending patterns, fitness, diet and sleep habits, our political inclinations and more may be tracked via our online and smartphone usage makes O'Neil's warning an urgent one. We create mountains of data that may be increasingly mined by government and private interests. Perhaps as important as asking whether this will be governed in ways that contribute to our flourishing, is whether we will be alert enough to care.____________________________Disclosure of Material Connection: I received this book free from the publisher. I was not required to write a positive review. The opinions I have expressed are my own."
58,0553418815,http://goodreads.com/user/show/51162416-max,5,fast pleasant primer on what’s wrong with big data. Fun. I love using a VPN turning everything off and being practically nobody to these freaks 
59,0553418815,http://goodreads.com/user/show/4061006-angela,5,"Cathy O'Neil is one of my heroes; I love listening to her on Slate Money, and her less pop/more technical book, Doing Data Science, occupies a prominent place on my desk at work - and a special place in my heart. Going beyond the usual data science manuals of what a random forest is, or how to account for large, sparse data, O'Neil and her coauthor - in that book - make an almost anthropological survey of the data science ""field""/profession/whatever. It's enlightening, and it's very real; less a textbook, more a professional guide.So if Doing Data Science is the ""how to do your job"" manual, and Slate Money is the occasional soapbox, Weapons of Math Destruction is the manifesto. With clarity, conviction and intelligence, Cathy O'Neil scalpels away at the utopian, misty-eyed tech worship that has contributed to the data science/big data bubble. She identifies the ways in which algorithms encode and perpetuate existing prejudices, and are hidden from plain sight by the obscuring (both intentional and not) use of Fancy Math.She notes how anti-human ""market efficiency"" is, and how it's often been short-hand for unjust and unequal systems, and big data is just scaling those dumb systems way, way up. For example, the use of social network data to ""enrich"" lending platforms; or recidivism models that ask about where a former convict grew up, whether his/her friends are in jail, how many encounters with the police he/she has had. These algorithms - because they chase correlations to make predictions (rather than identifying causal factors) - create big-data-driven poverty traps. They are also beyond the purview of the government (and, given the new administration, will probably be for quite some time): just like your FICO score is ONLY about how often you pay your credit card bills, and NOT your demographic group, so too should these algorithms be scrutinized and regulated.Increasingly, O'Neil notes, the ""human touch"" comes at a price; in upper income groups, you get more real, live service - hearing about a job through connections, informational interviews with alumni, blah - while lower income groups are serviced by increasingly mechanized, machine learning code. The hiring and HR practices of CVS, McDonald's, and other low wage places were especially striking (and severe): mindless, well-meaning algorithms deployed to effectively punish workers whose BMIs (itself a stupid measure of health) are too high, or who seem ""anti-social"" according to a creepy questionnaire, or whatever. I think my favorite chapter - and, of course, the most relevant one - was about the toxic brew that can be politics and data science. It was especially powerful to think through the way that, as opposed to the past, where we had ""agreed upon facts"" given by carpet bombing-style hegemonic culture (e.g. four news channels that everyone watched), now we have hyper-tailored DARE I SAY ""alternative facts"" where, by the power of marketers (who prey on confirmation bias), your neighbor's political beliefs are the result of a very specific and private relationship between them, their web browser cookies, and some political marketer. This means you may have NO IDEA why your swingy neighbor in swingy state X thinks Candidate Y is running an underground child sex ring literally under the ground of a DC pizza shop. O'Neil (and this book was written Before The Madness) notes that these sorts of micro-targeted ads that follow you around the web are very smartly tailored; the marketers know who would be repulsed by such a stupid conspiracy theory, and who would be intrigued. And that's just terrifying. Anywayyyyy. From a data perspective, my (cold dead) economist heart reminds me that OF COURSE we're going to scale up injustice when we mindlessly maximize profit, instead of humans. We need to go back to our roots: to the philosophy and debates of Jeremy Bentham, and welfare economics, and what ""utility"" really means. Efficiency is not the end all, be all, despite what our current economic system seems to diktat. O'Neil also makes a good point about the repeated erroneous conflating of correlation and causation; something even the fanciest of linear algebra stochastic process meddlers fall prey to. That's something that I see a LOT in data science: the field is built on the premise of perfect prediction - and prediction doesn't need causation, it just needs correlation. OK, I will step off my soapbox. Highly recommended.edited to add: OMG and I just read her delightful 2017 resolutions post, and had much LOLs. Cathy O'Neil, you da best."
60,0553418815,http://goodreads.com/user/show/8325737-philipp,4,"This is an angry book, but its got a point! She summarises many 'weapons of math destruction', WMDs, algorithms that encode human biases, are opaque, which give recommendations or orders that no-one questions, while making things worse. For example, this summary:Poor people are more likely to have bad credit and live in high-crime neighborhoods, surrounded by other poor people. Once the dark universe of WMDs digests that data, it showers them with predatory ads for subprime loans or for-profit schools. It sends more police to arrest them, and when they’re convicted it sentences them to longer terms. This data feeds into other WMDs, which score the same people as high risks or easy targets and proceed to block them from jobs, while jacking up their rates for mortgages, car loans, and every kind of insurance imaginable. This drives their credit rating down further, creating nothing less than a death spiral of modeling. Being poor in a world of WMDs is getting more and more dangerous and expensive.O'Neil goes through many, many of examples like this where an automated approach made things worse, where people didn't bother to critically look at the output, where the model doesn't have a way to incorporate feedback, it's stuck in time and cannot evolve like humans. The majority of the book is spent on case studies and great, angry discussion of what went wrong. There's one short chapter on what could be done - mostly she proposes more regulation (auditing of algorithms), but also to 'dumb' down models by law, force them to treat people equally and not bin them in groups. That chapter on solutions is extremely short - I expect a second book from her on that topic.P.S.: I've been reading a lot about adversarial perturbations in deep learning (see this great paper). One idea could be to counteract some of those models to become an adversarial entity. For example, you have one of those shopping points cards. Instead of feeding their model with your shopping data, you mix it up - write down your shopping list for a month, shuffle it, and buy one part with your points card and the other in cash. Keep mixing it up and add random stuff. Your data should be garbage to the model as it's not consistent, easily removable as an outlier, i.e., worthless.You could also make the model work for you - find out which groups of customers the market sends special offers to, game your shopping so that you fall into that group, reap rewards.Of course you'd need to know in detail what the model is looking for, which is highly unlikely."
61,0553418815,http://goodreads.com/user/show/115473-siria,4,"In the two years since Cathy O'Neil published this study of the danger posed by mathematical models—how their supposed neutrality merely reflects the biases of those who create them while creating closed-system feedback loops that warp society in unjust ways—it hasn't become any less relevant. In fact, not only does the book remain relevant, but subsequent revelations—most prominently those about Facebook's involvement with the 2016 US elections—show that O'Neil may even have been too cautious in what is a trenchant and prescient critique. "
62,0553418815,http://goodreads.com/user/show/25178051-izzat-radzi,5,"""Mathematical models are supposed to be our tools, not our masters"" Statisticians, quants, actuaries : take heed. This book argues that the new 'industrial revolution' of big data is bringing destruction. To whom (?) , one might be tempted to asked. Humans, normal masses who sometimes makes errors of course.And due to that, they got red in their ledgers, which in these new century means, that they'll most likely subjected to the 'vicious loop' of rejection and manipulation, and will felt the usual 'life is not fair' feeling. To start off, I claim that this book is the best (so far) compliment to Taleb's Black Swan book. Whilst Taleb as arguably he as a trader and academia revolves his discussion around some financial models & it's discontent (whilst in most part slanted his discussion into the philosophical realm); this author meanwhile goes into a more broad discussion. She, whom has background in chronologically in academia, hedge funds, a short stint in risk management before focusing work as a data scientist, argues that given the presence of these Gargantuan Big Data in almost every aspect of life, it has tarnished if not destroyed people's life. The problem-before I proceed case by case- lies in two reasons. First is the data collected & recorded by the data mining by corporate companies, mostly are left uncrunched, meaning they're not subjected to updated information about those people data taken or they're mistakenly (yet significantly affect) people who are thought 'one', in this case people with the same name and birthday. This, argued by the author, because the model used isn't not tinkered time to time according to feedback; unlike those used in baseball & basketball where opponents and managers subject each individual players to new & paternalistic habits of players -thus a better use of correlation to causation-, they are taken as face value. And feedback that is not taken -later I'll mentioned in job application survey- left those aspiring people hoping to get some good news, baffled as they themselves don't know why their application was rejected. Turns out, it was their 'behaviour', recorded in the subjective and highly interpretable application questions. The second problem here, if not wrong should I put it, is that the usage of the data is, as author's claimed, taken as God's word, the highest possible theological blasphemy, if said to those in denial data miners. This is due to the reasons that they basically use the data available, let it be credit scores, health scores, SAT (or generally educational) scores, and even resume applications -to name a few- as the bestowed revelation, thus dismissing proven good teachers, potential shining employees and laying off 'unhealthy' current workers; all based on the Big Data, either snooping privately on their life, or in other cases, a take-it-or-leave option wether they'll provide their credit score to get an insurance or their health progress for work employment group insurance practiced in firms & companies. The dismissal action has become so autonomous directly from the data, that frightenedly, that one does not care of others, it's just life. Coincidentally, I read Adorno's Culture Industry directly before this, and it does personally provides a more enlightened discourse on the systemic issue. I personally first came across to this topic back a few years ago, that with this newly published book kindles my interest. The first was back around late 2013, where I was taking a UK driving licence. In the course, similarly argued by the author, you'll be getting some discount given that you choose to put a 'Black Box' in your car. This instrument will record your driving habits & feed back to the insurance company. Although initially used to monitor those big truckers -who tends to be sleep deprived due to late night & long driving- to reduce the risks of death of others due to reckless driving; here argued that it's afraid to be used by others to manipulate the privacy of car drivers, at other instances flawed due to normal human behaviour, taking short cuts at night through 'dangerous neighbourhood' thus ended up one marked to be subjected to 'handcuffed' by auto insurance companies. The second one I encountered was in early 2015, in a student organised event to explain & showcase the structure & function of Big Data by companies. At that time, I was furious given the fact of the the then environment of whistle-blowers expose on companies manipulating & snooping normal civilians life. In here meanwhile, as argued in The Impulse Society by Paul Roberts, politicians backed by big companies are microtargeting voters up to the point of what they read and hear are tailor made. Given to the rise of fascism and 'fear of the normal neighbour' one known differently pre-election in America which now turns into something totally unexpected, I humbly agreed to some point. The third encounter is with regard to behavioural test subjected to work or scholarship applications. Exposed initially by my parents, whom my dad was at times hired to be an interviewer. He's the first to enlighten me on the presence of attitude test taken by applicants, which is imperative for career climbing ambitions. Here written an illustration. An applicant answered promptly the normal application survey on personal attitudes. Obviously one is subjected to child education for example that one is unique bla3; yet answering unique now instead of other answers) will be perceived as narcissist by the possible employer, which now is not possible anymore. Unfortunately, this data, is shared and this unfortunate applicant will ended up in this continuous 'dangerous loop'. The author coined Recidivism in regard to this loop. The poor guy, living in a certain 'bad' neighbourhood, want to apply a credit card, or just simply living as a normal civilian. Now, dues to this, of course one might have some background of knowing some in-behind bars people. This data, used by the security enforcement, will cause the police to routinely knocks on doors to tell them they have eyes on them. A more idiosyncratic example is those trying to move houses. Property firms with big data on their very own hand, easily exclude those with a bad tag, even when they've changed, or for unfortunate some, mistakenly recorded in. As said before, no feedback can be given by the victims, unless one has some wealth to pursue on a legal lawsuit on this issue, which of course deteriorate middle class let alone the poor condition. Obviously the rich bankers and markets manipulators escaped from this loop. As a matter of fact, they're invulnerable and invincible. She suggested a few suggestions in the concluding chapter. First for example, oath taking by those finance practitioners, whom impact upon their ignorance is highly significant on the whole. Nevertheless she noted the downside that when in work bureaucratic structure, manager will want answers for certain issues, this will put those people in corners. Second is, as Taleb suggested, ditching those backward minded model. Obviously, she argued, one have to be specialised in the field to help recognise and build a better one. She illustrated a new model on preventing child abuse. There's similarity between the model to 'possible criminals' as in the Minority Report movie in reducing - if not preventing- crime she criticised, the hidden outcomes has not - at least she didn't yet- been analysed. The third is the involvement of human judgement into the model. Yes, initially the algorithm of the system is used to remove human bias, yet seeing how things turned out, the machine as expected cannot understand how human (cognitive) work. The problem of language and rational judgement will always triumph human above machine, I claimed this ambiguous argument. Hats off to this book!"
63,0553418815,http://goodreads.com/user/show/11951948-paul,3,"One of the funniest bits of Little Britain was where David Walliams as Carol Beer would type a request into the computer, before turning to the customer and saying ‘Computer says no’… Whilst it is funny, it is not so funny when it happens to you. In this book, O’Neil, a former Wall Street quant raises alarm bells on the way that these mathematical models have infiltrated our lives. We don’t see them, but these algorithms that help us with our searches online and finding books, films and other items on online sites are now being used to determine just how much of a risk you are. Next time you want a loan, to renew insurance or just need to get another job O’Neil thinks that some of us may have a problem.She calls them ‘Weapons of Maths Destruction’; these are incontestable, unregulated and opaque algorithms. They are being used by companies to decide the tiniest details. They are used because they make larger profits for corporations and most worryingly for us is that they are frequently conclude the wrong thing having made incorrect assumptions about individuals. As the saying goes ‘crap in; crap out’… Having worried the life out of the reader, O’Neil goes onto suggest a variety of things that could help; more regulation, better design of the code and us being aware of their use. The writing is clear, if a little dry and technical at times. The examples are a little American centric, but you can see the way that it is going in the UK. Even though the title mentions the dreaded word maths, it really isn’t that mathematical. Worthwhile reading."
64,0553418815,http://goodreads.com/user/show/16212136-peter-pete-mcloughlin,4,"Very depressing book on how computer algorithms are becoming a powerful tool of the exploitation of workers, the poor, minorities and ordinary people mostly in the service of profits. It goes deep and the problem is much worse than you think. This book is a downer as it covers relevant developments in politics and business and that always is not pleasant to think about in these times in general and are especially dispiriting in how they affect ordinary people."
65,0553418815,http://goodreads.com/user/show/5550611-hallie,2,"Just ok. I like the idea more than the execution of this book. While O'Neil uncovers a great, dark place in our society that needs more light shone on it, much of her conclusions she gives away in the introduction to the book. This would have been an excellent piece in the New Yorker or Atlantic but as a book began to feel redundant far too early. "
66,0553418815,http://goodreads.com/user/show/11345492-kate,4,"3.5, bumped up for likeable clarity-without-condescension and consistent up-front political engagement."
67,0553418815,http://goodreads.com/user/show/761281-mike,4,"Reading this book I could not help but wish the author had used a quote from Paul Goodman as the epigraph of the book: ""Technology is a branch of moral philosophy, not of science."" (Every technology ultimately is making a value judgement about what is important or worth doing, and what aspects of a task or reality are important.)This book is a fascinating look at how 'big data' is used and abused by colleges, banks, insurance agencies, in sentencing and parole decisions, by employers screening applicants, and more. Collecting vast amounts of data and using to guide decisions can be a very useful technology, but the author -- who herself had a career as a 'quant' or data analyst for a large firm -- points out the downside of basing decisions on data and algorithms. Of course it should be obvious, but our society's information fetishism can make us overlook that the data we collect and the algorithms we use to analyze it can be deeply flawed by omissions, prejudices, fallacies that the designers may not be aware of. A couple of the many examples she sues help illustrate this. We read about a school teacher whose performance is measured by an algorithm that takes into account his students' past performance, current performance on standardized tests, and various other factors. One year he is scored 6/100, and the next year 94/100 -- after making no changes to his syllabus, assignments, or anything else he was doing. Anyone with common sense can see there is something wrong with the algorithm if someone could get such wildly varying scores, but the scoring algorithm is a 'black box' that not event the school administrators understand. Among the many problems with the system here is that there is no allowance for the designers of the algorithm to get feedback on the scoring and adjust the model -- instead, the 'failing' teachers are fired and the algorithm rolls on. Contrast this with the use of 'big data' algorithms in commercial enterprises: when Amazon.com collects data on your browsing and purchasing habits, they use this recommendations to you for future purchases. If their models are bad, they suggest things you don't want, and they don't increase sales. But they take this feedback and change their algorithms, because there is a financial incentive to do so.So a huge blind spot in some of the models and algorithms policy makes use is the lack of feedback. Another is huge downside of the rise of big data modeling is that private companies can use opaque models to discriminate in ways that punish the poor particularly. The author explains how car insurance companies use data about where their customers live, drive, and work to assess risks -- which is largely legitimate -- but also abuse this information to determine which customers are likely to be able to shop around for better deals, and which are less likely to have the time, resources, or access to information to do so, and they are charged much more. Indeed affluent customers with DUI convictions may be better rates than safe drivers who live in areas with limited internet access and higher poverty. Likewise employers may screen applicants with big data algorithms that identify 'risks' which rule out the poor, people with past mental illness, or immigrants. Many other problems are discussed, and the final chapters look at how political parties have begun to utilize big data too -- an ominous prospect in light of the rest of the book.The author offers some solutions, ranging from the admittedly ineffective (a code of ethics for 'quants' who create the models) to the more daunting but effective measure of demanding transparency and regulation (which has made some progress in the insurance industry, but is far from complete).*Full disclosure -- I read a free advanced reader's copy that I won in a Goodreads giveaway.*"
68,0553418815,http://goodreads.com/user/show/3502690-graeme-roberts,2," Cathy O'Neil writes well and provides much good information, but I found the fundamental thesis of  Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy to be foolish and irresponsible. The title, condensed inevitably to WMD, is used promiscuously to describe any application of data science, statistics, or even information technology that she considers to be unfair or discriminatory. In most cases, she is right; some applications are used by the greedy and the uncaring with insufficient thought of the consequences, but she rarely considers the positive attributes of the applications. Once branded as a WMD, a term indelibly attached to the evil trifecta of George W. Bush, Saddam Hussein, and the Iraq War, how are readers meant to consider their many positive aspects and what can be done to make them better?At first, I thought that Ms. O'Neil had been the victim of a foolish PR or marketing person in her publishing company, who had come up with this catchy name, but her ardent use of the term convinced me that she had invented it.Ms. O'Neil seemed to follow a downward trajectory from her Harvard PhD in mathematics and a teaching position at Barnard to a leading hedge fund and a series of other morally disappointing positions that left her convinced that data science, and even mathematics, are roots of much evil. I don't buy it. They are tools that can be used or misused, and her diatribe does nothing to help.She asserts, on page 210:If we find (as studies have already shown) that the recidivism models codify prejudice and penalize the poor, then it's time to have a look at the inputs. In this case, they include loads of birds-of-a-feather connections. They predict an individual's behavior on the basis of the people he knows, his job, and his credit rating—details that would be inadmissible in court. The fairness fix is to throw out that data.But wait, many would say. Are we going to sacrifice the accuracy of the model for fairness? Do we have to dumb down our algorithms?In some cases, yes. If we're going to be equal before the law, or be treated equally as voters, we cannot stand for systems that drop us into different castes and treat us differently.I disagree strongly. Some data is better than none, though we should be looking to constantly improve our data science systems. I would like to know that the recently paroled criminal moving in next door had been released by a judge in full possession of whatever facts are available. So, I suspect, would Ms. O'Neil."
69,0553418815,http://goodreads.com/user/show/19993-robin,4,"If you've been listening to Cathy O'Neil on Slate Money for some time, you probably expected this book to emerge in exactly this form and with exactly this content. She's smart, progressive, focused, and clear, just as she was on the podcast. I saw her speak at the Mechanic's Institute Library a few months ago, before I'd read the book, and was very impressed by her ability to be articulate about complex situations with a decisiveness that belies their complexity, or at least certainly never allows for it as an excuse. The argument I found most challenging was (heavily paraphrasing here) that we should be privileging fairness because it is otherwise so easy to be blind to the unfairness that creeps in when we pursue other goals. If I'm honest with my own tendencies, I recognize my longing for accuracy might indeed cause me to overlook fairness, and I resist the idea that I need to choose one as a priority over the other. Nevertheless, O'Neil convinces me to question assumed neutrality much more closely, and be much more vigilant for it in my own industry. As she points out, it's easy for people who write the algorithms to absolve themselves of responsibility for how they are used, but if we pay attention to the real outcomes we may find that we need to do more to train our algorithms to learn from externalities and flaws. Focusing ruthlessly on measuring only what you are trying to measure and ignoring all other factors is a good start; evaluating the results to make sure that's actually what happened is as necessary a practice. The book is full of examples of failed algorithms in wide use, actively and detrimentally affecting at-risk populations today, as well as potential future areas where algorithms could be used, perhaps even inadvertently, to consolidate and perpetuate the power of the already powerful. The free market will not solve this problem. This is an excellent example of where good regulation is imperative. As constituents with the ability to influence legislators to put their attention here, we can demand more transparency, more awareness and education, and a more good, old-fashioned compassion.Further reading:https://www.ted.com/talks/joy_buolamw...https://www.propublica.org/article/ma...https://standards.ieee.org/develop/pr..."
70,0553418815,http://goodreads.com/user/show/7139041-dorin,2,"Disappointing - such a rich topic, yet the author decided to be racist, Democrat and to induce panic instead of having a cool headed approach. She ignores her own advice, contradicts herself time and again, and she makes her Democrat leaning so obvious it makes me puke.The book could have contained real data, real information, but instead she goes on to explain that statistics are bad because statistics don't catch statistical errors. She pretends she's a professional in data processing, but she sounds like she doesn't understand how statistics work, and she can barely go beyond her mathematical training to understand what went wrong from a human point of view.My point: „a machine is as useful as the person who uses it”. Her point: ""big data is bad unless it's used by the Democrats”."
71,0553418815,http://goodreads.com/user/show/7837611-greg-fanoe,2,"I think that on the whole reading this book will do people more good than harm but I had several problems with it:1. The only section which I have personal expertise on is the section on ""insurance"" - I am an actuary - and I can personally verify that this chapter was filled with inaccurate, misleading, and/or totally speculative statements.2. The correlation between credit score and income is nowhere near as strong as she makes it appear in this book.3. Many of the problems she identifies have nothing to do with the models in question. Advertising models are not the issue with the payday loan or for-profit college industry those industries need to be totally upended/destroyed at the source. Letting them operate as is but kneecapping their ability to advertise would be nonsensical.4. The solutions she presents are ridiculously vague and/or impractical."
72,0553418815,http://goodreads.com/user/show/1045774-mehrsa,5,This is the book I was waiting to read! The rejoinder to our data-obsessed culture. We got drunk on algorithms and forgot to focus on their limitations. are they asking the right questions? are they recycling inequality? Great book
73,0553418815,http://goodreads.com/user/show/3126915-imi,2,"Sadly, a rather superficial look at big data, algorithms and how they've impacted on society. You probably know/have guessed all this already. The correlations O'Neil point out are not very convincing, and it felt like she just wanted to rant about how unfair the world is. Not that I disagree with that conclusion; society is unfair and unequal. But O'Neil really doesn't state much more than that, and, worse, some of her examples felt misleading or simply speculative. Where was her evidence? And if she'd explored this issues in more depth I'd have wanted clear ideas for resolving these issues. Instead, any suggestions she made seemed vague and impractical. I admitedly feel out of my depth with this topic, but am most definitely willing to learn more and expand my understanding about it. Disappointingly, this book didn't offer that."
74,0553418815,http://goodreads.com/user/show/777176-dave,5,"I learned more from this book than I have from any book I've read in years. O'Neil is a former math professor and spent time as a hedge-fund quant, so she understands the theory and experience of the effect of out-of-control algorithms on our society.As corporations make more and more efficient use of algorithms, they are starting to show dangerous tendencies, according to O'Neil. She calls them ""weapons of math destruction"" or WMDs. WMDs present the following grave problems for society:--They usually lack transparency, such that it is difficult or impossible to know what goes into them and what factors sway or change their results. When they are opaque, WMDs are patently unfair (O'Neil cites the algorithms used to evaluate teachers in many districts, highlighting their absurdity as they attempt to quantify the unquantifiable).--They make broad generalizations and reduce human beings to ""buckets."" One of the worst examples is the criminal justice system, which is increasingly reliant on algorithms to deploy police patrols and even to sentence criminal defendants. These algorithms punish the innocent by assuming that those who live in poor neighborhoods are necessarily more likely to commit crimes, thereby subjecting them to frequent ""stop and frisk"" type searches that have been found to be unconstitutional repeatedly.--They can use information in an unfair way that tends to perpetuate further injustice. A prime example of this is the increasing tendency to identify individuals by their credit score, on job applications and even in online shopping. When job applicants are screened by credit score, those who are already poor (and therefore much more likely to have a low credit score already) will find it even harder to get a job and thereby earn enough money to help improve their credit score. This self-perpetuating cycle of poverty is completely unnecessary, because credit scores are an exceedingly poor proxy for job-worthiness of applicants.--Many WMDs are not very good at learning. They are created as models of human behavior (with all the flaws and assumptions of the humans who programmed them), and when they miss (when for example they screen out a potential hire who would have actually been a first-rate employee), they never learn that they have missed. The ""false positive"" never presents itself (the rejected applicant goes on to success elsewhere), and the oblivious WMD continues to chug along with its assumptions and blind spots unchallenged.And WMDs are like racism:""Racism, at the individual level, can be seen as a predictive model whirring away in billions of human minds around the world. It is built from faulty, incomplete, or generalized data. Whether it comes from experience or hearsay, the data indicates that certain types of people have behaved badly. That generates a binary prediction that all people of that race will behave that same way....Racists don't spend a lot of time hunting down reliable data to train their twisted models. And once their model morphs into a belief, it becomes hardwired. It generates poisonous assumptions, yet rarely tests them, settling instead for data that seems to confirm and fortify them. Consequently, racism is the most slovenly of predictive models. It is powered by haphazard data gathering and spurious correlations, reinforced by institutional inequities, and polluted by confirmation bias. In this way, oddly enough, racism operates like many of the WMDs I'll be describing in this book."" (p. 22-3)And finally:""Models like this will abound in coming years....Many of these models, like some of the WMDs we've discussed, will arrive with the best intentions. But they must also deliver transparency, disclosing the input data they're using as well as the results of their targeting. And they must be open to audits. These are powerful engines, after all. We must keep our eyes on them.... Data is not going away....Predictive models are, increasingly, the tools we will be relying on to run our institutions, deploy our resources, and manage our lives. But...these models are constructed not just from data but from the choices we make about which data to pay attention to--and which to leave out. Those choices are not just about logistics, profits, and efficiency. They are fundamentally moral. If we back away from them and treat mathematical models as a neutral and inevitable force, like the weather or the tides, we abdicate our responsibility....We must come together to police these WMDs, to tame and disarm them."" (p.218)"
75,0553418815,http://goodreads.com/user/show/3029658-doug-cornelius,5,"With big data, comes formulas to parse through the data trying to make sense of it. Those algorithms can help make sense of the data and help filter through the noise to find trends. But those algorithms can also be easily misused and have harmful, if unintended consequences. Cathy O'Neil explores these problems in 
Weapons of Math Destruction
.Ms. O'Neil is a former academic, hedge fund quant, and data scientist for startups. At the hedge fund she was looking at ways to make money by predicting financial trends. As a data scientist she was looking to predict people's purchases and browsing habits to monetize those habits. Those are good algorithms where the people using them understand them and update the algorithms as they see problems and can check to see if they are working properly by verifying outcomes.Many data driven outcomes fail. Those are the focus of 
Weapons of Math Destruction
.I'm a big fan of the Slate Money podcast with Cathy O'Neil, Felix Salmon, and Jordan Weissmann. When the publisher offered me a review copy of Ms. O'Neil's book, I jumped at the chance.It's a short book and even though the title makes it sound like it's full of math, it's not. It's more about social policy. She points to the danger of decision makers blindingly following algorithmic output that they do not understand.Take the US News and World Report listing of best colleges. This is preeminent guide for high school students trying to figure out which college to attend. The rankings do not look at actual student outcomes. The publisher does not look at happiness, learning or improvements. It looks at proxies for the outcomes like SAT scores, alumni contributions, and graduation percentages. Colleges started making decisions to improve their rankings in the algorithm by improving those measured proxies. As anyone writing checks for their college bound children, affordability is not one of the measured proxies.As you might expect there are big chunks of the book dedicated to failings in the financial sector by relying on poorly designed algorithms. The biggest problems often being false assumptions plugged into the data running through the algorithm.With the election season upon us, the chapter on political algorithms is fascinating. The data scientists of commerce were brought into political campaigns. They are able to micro-target potential voters sending different messages to different groups highlighting the positions that would make the candidate more favorable to that segment of the population. Civic engagement and the political process is increasingly being algorithm driven.Since our world is increasingly being driven by big data, it's important to understand what is happening behind the decision-making. Weapons of Math Destruction is an excellent tool to help you understand data driven decision-making."
76,0553418815,http://goodreads.com/user/show/23020614-germ-n,3,"A fascinating must-read guide on how to navigate and interpret a world increasingly ruled by materialistic prejudices and oversimplifications that are repackaged as ""science"" and ""tech"" through poorly designed computer algorithms.Reading this gives one the feeling that the ""evil androids ruling over mankind"" dystopia has already been unrolling. Except in a very sneaky way: this time we don't get to face or even understand the robots and their actions which are nonetheless already reshaping our reality and the course of our lives through ridiculous university rankings, credit scores, social media ideological bubbles, etc.Unfortunately O'Neil ruins an otherwise brilliant book by an extreme leftist bias that seeks to absolve her personal heroes in the US democratic party and their political agendas, and attempts to attribute most blame on a specific other side. I assume this is why a book which raises such important points gets to be published and become mainstream after all.For example, she portrays Obama as a saint who just wants the best for immigrants and fiercely battles the evil rich. Yet for any unbiased observer, it's quite clear that Obama and democrats are just serving the wealthy who finance their own political campaigns (duh) when they set to ruin nations overseas and bring in thousands of immigrants, ideally illegal ones, so the price of labor can go down and corporations can continue to reap political advantages.This skewed perspective is damaging to the bottom argument and distracts readers from what's really the main issue here: the constant loss of truth and distortion of increasingly astroturfed public debate. No attempts are made, for example, at addressing how these WMDs are influencing the toxic foreign policy of the US and the press - another entity that passes as a saint in this book, while being the number one responsible for war and destruction of nations around the world."
77,0553418815,http://goodreads.com/user/show/20465079-tam,4,"A quick and highly useful read - I would recommend it to all people not just ones working with predictive models and Big Data.The title is sensational enough, but at one point I put that aside and decided to read this book. So many have been praising Big Data and Machine Learning, few cautions have been put forward but not enough, mostly dealing with ethics, security, personal data issues. This book, on the other hand, discusses the large scale tragedies that Big Data can invoke on societies as a whole, if continuing to go unchecked. 10 chapters deal with examples about the misuses of data and predictive models in many aspects of lives, especially the lives of the most unfortunate.O'Neil has a strong opinion and at certain point I thought she might be too pessimistic (saying that from a self-proclaimed pessimistic person as I am), and O'Neil freely claims how angry she is with the whole thing. Yet at the very end of the book, she makes constructive comments, ideas for improvement, for regulations. Those are the most important, and perhaps the most useful passages for data scientists. I've been quite fascinated by the tool and sometimes forget the darker side that the tool can invoke, and I'd like to thank O'Neil for the reminder, for the clarification about issues that sometimes I feel uneasy but not so clear about. No, the tool is neither inherently good nor bad, the people who make them and wield them can choose do what they want. And we can choose to do good things.Ehh, not so optimistic though, there are tons of challenges and... But I should shut my mouth. But at least we should try."
78,0553418815,http://goodreads.com/user/show/15409024-ahalya-vijay,5,"I found this to be a fascinating read on the importance of using technology and data in an ethical way. I was shocked to learn how pervasive irresponsible uses of data are throughout the developed world and how widespread it is for people to place their faith in ""black box"" models as Cathy O'Neil refers to them. It must be frustrating, for example, for teachers to be evaluated (and fired!) based on a model they have no knowledge of, without knowing how to improve their rating. And it must be even more so if they are getting anecdotally strong and positive feedback from school leadership and parents.Another shock was how liberally companies and 'data scientists' use proxies without thought to the consequences of the model not capturing the 'error term' sufficiently. The danger of making decisions based on things that are modeled (or estimated) using proxies was drilled into my head throughout my entire schooling. How would you feel if the fact that you were late to one meeting at work was used to brand you as an irresponsible or unreliable worker? Ms O'Neil made a clear link between such models being used to trap people in a cycle of poverty, using example after example across a variety of industries (e.g., education, finance, insurance). It is clear that reform is needed, but what left me worried at the end is that these models are not hurting the rich and famous, but the poor and under-resourced. Will our current political and justice system listen to their needs? I'm skeptical."
79,0553418815,http://goodreads.com/user/show/48433695-alexis-tremblay,5,"A must read for all data scientist, mathematicians, modellers, statisticians,... There is a dark side to big data and she exposes it masterfully. I always had an uneasy feeling when I started my career as an actuary and later on as data scientist. My moral compass was always off but I didn't have the words for it. Now she has given me a framework to think about my job and what to look out for. Will read again. "
80,0553418815,http://goodreads.com/user/show/16204614-bing-gordon,4,"Good overview of modern analytics, but too much of a guilt trip laid on top. "
81,0553418815,http://goodreads.com/user/show/6100646-brian-clegg,4,"As a poacher-turned-gamekeeper of the big data world, Cathy O'Neil is ideally placed to take us on a voyage of horrible discovery into the world of systems making decisions based on big data that can have a negative influence on lives - what she refers to as 'Weapons of Math Destruction' or WMDs. After working as a 'quant' in a hedge fund and on big data crunching systems for startups, she has developed a horror for the misuse of the technology and sets out to show us how unfair it can be.It's not that O'Neil is against big data per se. She points out examples where it can be useful and effective - but this requires the systems to be transparent and to be capable of learning from their mistakes. In the examples we discover, from systems that rate school teachers to those that decide whether or not to issue a payday loan, the system is opaque, secretive and based on a set of rules that aren't tested against reality and regularly updated to produce a fair outcome.The teacher grading system is probably the most dramatically inaccurate example, where the system is trying to measure how well a teacher has performed, based on data that only has a very vague link to actual outcomes - so, for instance, O'Neil tells of a teacher who scored 6% one year and 96% the next year for doing the same job. The factors being measured are almost entirely outside the teacher's control with no linkage to performance and the interpretation of the data is simply garbage.Other systems, such as those used to rank universities, are ruthlessly gamed by the participants, making them far more about how good an organisation is at coming up with the right answers to metrics than it is to the quality of that organisation. And all of us will come across targeted advertising and social media messages/search results prioritised according to secret algorithms which we know nothing about and that attempt to control our behaviour.For O'Neil, the worst aspects of big data misuse are where a system - perhaps with the best intentions - ends up penalising people for being poor of being from certain ethnic backgrounds. This is often a result of an indirect piece of data - for instance the place they live might have implications on their financial state or ethnicity. She vividly portrays the way that systems dealing with everything from police presence in an area to fixing insurance premiums can produce a downward spiral of negative feedback.Although the book is often very effective, it is heavily US-oriented, which is a shame when many of these issues are as significant, say, in Europe, as they are in the US. There is probably also not enough nuance in the author's binary good/bad opinion of systems. For example, she tells us that someone shouldn't be penalised by having to pay more for insurance because they live in a high risk neighbourhood - but doesn't think about the contrary aspect that if insurance companies don't do this, those of us who live in low risk neighbourhoods are being penalised by paying much higher premiums than we need to in order to cover our insurance. O'Neil makes a simplistic linkage between high risk = poor, low risk = rich - yet those of us, for instance, who live in the country are often in quite poor areas that are nonetheless low risk. For O'Neil, fairness means everyone pays the same. But is that truly fair? Here in Europe, we've had car insurance for young female drivers doubled in cost to make it the same as young males - even though the young males are far more likely to have accidents. This is fair by O'Neil's standards, because it doesn't discriminate on gender, but is not fair in the real world away from labels.There's a lot here that we should be picking up on, and even if you don't agree with all of O'Neil's assessments, it certainly makes you think about the rights and wrongs of decisions based on automated assessment of indirect data."
82,0553418815,http://goodreads.com/user/show/7019761-cindy,4,"While I don't necessarily agree with all of the conclusions, this was a very thought-provoking book. The author is a mathematician who has worked on Wall Street and now takes on some of the math models that rule our lives. Some of these were, at least originally, developed with good intentions - intending to make fairer some of the decisions that affect our lives: where and who gets into college; what the schedule for our job looks like; how much we pay to get a loan; even how you are judged in your chosen profession. But the author poses that many of these types of decisions are actually less than fair. The models are in her terms WMDs - weapons of math destruction - they are hidden, unregulated, and virtually uncontestable, even when they are blatantly wrong. One of the more troubling examples from the book concerned the use of testing to judge teacher performance. She traced how an exemplary teacher with outstanding evaluations was judged as poor on the basis of one set of tests and fired. The teacher showed that the tests which showed that her ""value-added"" score was sub-par were flawed due to a high number of erasures on the previous year which indicated cheating. Indeed, the system said that there *might* have been cheating but they would not re-instate this excellent teacher. (And please don't get me started on ""value-added"" - my daughters' school was always excellent in all the testing but this score was just average. So if your kids are already high achievers, how do you continue to score excellent in this measure? The answer is it's almost impossible.)Other examples showed that even when you removed race or ethnicity or economics as an indicator in these WMDs, they were there. If a zip code puts you in a risky category for a student loan, then it's hard to get the education that can get you out of poverty; or if a work schedule makes it hard for a single mother to get the child care she needs so that she can get the money to go to school to make a better life for herself and her child...well, I can see how some of this sets up a self-fulfilling prophecy. It makes it harder, not impossible, to break this cycle. That's why this book is important. If this is being used, then we need to know and like credit scores have the ability to correct errors and inaccuracies that work against the average person.Quotes to remember:....This would all be wonderful for students and might enhance their college experience, if they weren't the ones paying for it in the form of student loans that would burden them for decades. We cannot place the blame for this trend entirely on the US News rankings. Our entire society has embraced not only the idea that a college education is essential, but the idea that a degree from a highly-ranked school can catapult a student into a life of power and privilege. As I write this, the entire voting population that matters lives in a handful of counties in Florida, Ohio, Nevada, and a few other swing states. Within those counties is a small number of voters whose opinions weigh in the balance. I might point out here that while many of the WMDs we've been looking at, from predatory ads to policing models, deliver most of their punishment to the struggling classes, political microtargeting harms voters of every economic class, from Manhattan to San Francisco, rich and poor alike find themselves disenfranchised, though the truly affluent of course can more than compensate for this with campaign contributions."
83,0553418815,http://goodreads.com/user/show/6894205-allie,4,"This was by far the most interesting book about math I have ever read (admittedly a low bar to set), possibly because it contains no actual math formulas. Instead, former mathematics professor and hedge fund analyst Cathy O'Neil tells stories about the applications of math - how ""ill-conceived mathematical models now micromanage the economy from advertising to prisons."" Among other things, mathematical models determine which news you see on social media, your Google search results, the cost of your auto insurance and credit card rates, which neighborhoods police are sent to patrol, and how your child's schools are evaluated. In one staggering statistic about the reach of these models, O'Neil notes that 72 percent of resumes are never seen by humans, but are instead screened and scored by software programs looking for employer keywords. While math itself is neutral, the models are based on assumptions that reflect human biases and limitations. Often, the models are designed to process data from large numbers of people efficiently, but not necessarily fairly. She distinguishes between ""good"" models built on transparent assumptions that are constantly corrected with new data (e.g., those used in professional baseball) to opaque models that use proxy data such as race or zip codes and do not have a corrective feedback mechanism (e.g., predictive policing software). Typically, there is no method of recourse when given a poor score by a computer (hello, Hal) since companies consider their algorithms proprietary and won't disclose details of how scores are developed. This was an issue for teachers evaluated - and sometimes fired - based on inaccurate models of student test scores. The book's second main argument is that the built-in biases in the models are especially putative towards the poor, who are steered to high interest loans, targeted by ""stop and frisk"" policing based on their neighborhoods, and less likely to be personally interviewed for jobs.I found many of O'Neil's arguments persuasive and somewhat alarming, although she tries to end the book on a positive note. Worth reading, even if you don't like math. Or weapons. Or destruction. "
84,0553418815,http://goodreads.com/user/show/70315594-enoughtohold,4,"A nice, clear overview of the problems of big data for laypeople. Not sure why other reviewers expected this to be deeply technical, or why they think O’Neil thinks there are easy answers — she plainly doesn’t.I for one am glad a book this accessible exists on this topic. After all, the vast majority of the people affected by these harmful models are not data scientists, and they deserve to know what’s going on.(Also, the author did a great job reading for the audiobook.)"
85,0553418815,http://goodreads.com/user/show/4430576-miri,5,"I've said it before, and I'm sure I'll say it again, because I make a point of seeking out books like this. But if there's one book I could get everyone to read right now, it would be this one. Even next to Ta-Nehisi Coates's We Were Eight Years in Power, which I'm reading concurrently and which is staggering in its importance, this book feels urgent—because what it explains is how our new economy of Big Data is codifying, concealing, and amplifying inequality of every kind, on an enormous scale. Every injustice I'm reading about in Coates's book is made worse by what I read about here. We think of math as inherently neutral, the way we think of technology and logic and science. But these things are like any tool, dependent on the person using them, capable of being used for good or evil. (Remember the way science was abused throughout American history, fabricating the entire concept of biological race to provide justification for white humans to enslave black ones.) When we start using math to make decisions instead of people, assuming that this will decrease human bias and make things more fair, what we can actually end up doing is obscuring that prejudice and magnifying the scale of its effect. Cathy O'Neil was a mathematician working in the finance industry in 2008. She left after a few years, disgusted with the way the system was clearly rigged, and has since created the concept of Weapons of Math Destruction: mathematical models that codify human prejudice, are opaque and impervious to feedback, and affect large numbers of people. In this book, O'Neil takes us through several WMDs, showing how they're used in different areas of our lives. There's the disastrous school reform attempt in Washington D.C., which tried to evaluate teachers in a way math simply cannot be used, and which fired hundreds of teachers based on results no one in the district could explain or defend. There's the way many employers now use credit scores to screen applications, assuming that making regular payments means you'll be a dependable employee, ignoring the facts that (1) responsible people experience financial difficulty too and (2) preventing people with bad credit from getting jobs is only going to make their situation worse.There are the computerized risk models that judges use in sentencing, trying to gauge from prisoners' background information—where they grew up, whether their friends and family have criminal records; information which would be inadmissible in court—how likely they are to return to prison after being released, and how long their sentence should be. There's the way U.S. News & World Report helped cause an arms race when it decided to start ranking universities on everything except their cost, driving up tuition over 500 percent (nearly four times the rate of inflation) in just the 30-ish years that I've been alive.There's political advertising which is now so individually personalized that even people who support the same candidate, even people visiting the candidate's website, will see totally different things based on the profile the campaign has created for them.There's predatory advertising like that done by for-profit colleges, which specifically targets vulnerable people's ""pain points"" looking for those who are ""stuck,"" who have ""few people in their lives who care about them,"" who have ""low self-esteem"" and are ""unable to see and plan well for the future"" (these quotes, shared by O'Neil, come from the California Attorney General's office when it was investigating one of these colleges). (And if you're wondering how they would know this about people, the point is that they know this about all of us now. Just try to imagine what someone could learn about you from seeing everything you do online.)It might seem like the logical response is to disarm these weapons, one by one. The problem is that they’re feeding on each other. Poor people are more likely to have bad credit and live in high-crime neighborhoods, surrounded by other poor people. Once the dark universe of WMDs digests that data, it showers them with predatory ads for subprime loans or for-profit schools. It sends more police to arrest them, and when they're convicted it sentences them to longer terms. This data feeds into other WMDs, which score the same people as high-risk or easy targets and proceed to block them from jobs, while jacking up their rates for mortgages, car loans, and every kind of insurance imaginable. This drives their credit rating down further, creating nothing less than a death spiral of modeling. Being poor in a world of WMDs is getting more and more dangerous and expensive.The same WMDs that abuse the poor also place the comfortable classes of society in their own marketing silos . . . For many of them it can feel as though the world is getting smarter and easier. Models highlight bargains on prosciutto and chianti, recommend a great movie on Amazon Prime, or lead them, turn by turn, to a cafe in what used to be a ""sketchy"" neighborhood. The quiet and personal nature of this targeting keeps society's winners from seeing how the very same models are destroying lives, sometimes just a few blocks away.Though we're all affected by them in ways we can't see, right now the majority of the damage is being done to the same groups who are always hurt by flaws in the system—minorities and the poor. The same models could easily be used to help people instead of prey upon them, but that won't happen in the glorious free market of capitalism. We need regulation and oversight in the data industry. We need accountability. When statistics itself, and the public’s trust in statistics, is being actively undermined by politicians across the globe, how can we possibly expect the Big Data industry to clarify rather than contribute to the noise? We can because we must. Right now, mammoth companies like Google, Amazon, and Facebook exert incredible control over society because they control the data. They reap enormous profits while somehow offloading fact-checking responsibilities to others. It’s not a coincidence that, even as he works to undermine the public’s trust in science and in scientific fact, Steve Bannon sits on the board of Cambridge Analytica, a political data company that has claimed credit for Trump’s victory while bragging about secret ""voter suppression"" campaigns. It’s part of a more general trend in which data is privately owned and privately used to private ends of profit and influence, while the public is shut out of the process and told to behave well and trust the algorithms. It’s time to start misbehaving. Algorithms are only going to become more ubiquitous in the coming years. We must demand that systems that hold algorithms accountable become ubiquitous as well."
86,0553418815,http://goodreads.com/user/show/17187084-sean-goh,4,"Very readable book warning of the downside of amoral models. With great power comes great responsibility.___Math-powered applications were based on choices made by fallible human beings. Many models encoded human prejudice, misunderstanding, and bias into the software systems that increasingly managed our lives. Like gods, these mathematical models were opaque, their workings invisible to all but the highest priests in their domain: mathematicians and computer scientists.Models are opinions embedded in mathematics.Statistical engines without feedback can continue to spin our faulty and damaging analysis while never learning from its mistakes. E.g. teachers mistaken labelled as failures are fired, and no one is the wiser. The model's output becomes unquestioned reality.The parallels between finance and Big Data: The productivity of the field's talents indicates that they are on the right track, and it translates into dollars. This leads to the fallacious conclusion that whatever they are doing to bring in more money is good. It ""adds value"". Otherwise why would the market reward it?In both cultures Wealth is no longer a means to get by. It becomes directly tied to personal worth.On the rise of university rankings: from the perspective of a university president, it's quite sad. Here they are at the summit of their careers dedicating enormous energy towards boosting performance in fifteen areas defined by a group of journalists at a second-tier magazine (U.S. News). The ranking had become destiny. When you create a model from proxies, it is far simpler for people to game it. This is because proxies are easier to manipulate than the complicated reality they represent. (e.g. trying to evaluate influence by number of twitter followers)In a system in which cheating is the norm, following the rules amounts to a handicap. So preventing the students in Zhongxiang from cheating was unfair.In our largely segregated cities, geography is a proxy for race.Patrolling particular areas creates a pernicious feedback loop. The policing itself spawns new data, which justifies more policing. Eventually the zeroing in on ghetto districts ends up criminalising poverty, believing all the while that our tools are not only scientific but fair.Instead of simply trying to eradicate crimes, police should be attempting to build relationships in the neighbourhood. This was one of the pillars of the original ""broken windows"" study. The cops were on foot, talking to people, trying to get them to uphold their own community standards. But that objective, in many cases, has been lost, steamrollered by models that equate arrests with safety.Mathematical models can sift through data to locate people who are likely to face great challenges, whether from crime, poverty, or education. It's up to society whether to use that intelligence to reject and punish them - or to reach out to them with the resources they need. It all depends on the objectives we choose.Computing systems have trouble finding digital proxies for soft skills. The relevant data simply isn't collected, and anyway it's hard to put a value to them. They're usually easier to just leave out of the model.The modelers of e-scores (unofficial credit ratings) have to make do with trying to answer the question: ""How have people like you behaved in the past?"" when ideally the question would be ""How have you behaved in the past?Even with the Affordable Care Act, which reduced the ranks of the uninsured, medical expenses remain the single biggest cause of bankruptcies in America.A sterling credit rating is not just a proxy for responsibility and smart decisions. It is also a proxy for wealth. And wealth is highly correlated with race.Mistakes made by models are learning opportunities - as long as the system receives feedback on the error. When automatic systems sift through our data to size us up for an e-score, they naturally project the past into the future. The poor are expected to remain poor forever and are treated accordingly. It's inexorable, often hidden and beyond appeal, and unfair.Yet machines cannot make adjustments for fairness by themselves. That is where the human overseer comes in.Human decision making, while often flawed, has one chief virtue. It can evolve.Big data processes codify the past. They do not invent the future. Doing that requires moral imagination, and that's something only humans can provide. We have to explicitly embed better values into our algorithms, creating Big Data models that follow our ethical lead. Sometimes that will mean putting fairness ahead of profit."
87,0553418815,http://goodreads.com/user/show/6991670-bob,4,"Summary: An insider account of the algorithms that affect our lives, from going to college, to the ads we see online, to our chances of getting a job, being arrested, getting credit and insurance.Big Data is indeed BIG. Mathematical algorithms shape who will see this post on their Facebook newsfeed. If you go to Amazon or another online bookseller, algorithms will suggest other books like this one you might be interested in. Have you seen all those ads about credit scores? They are more important than you might imagine. Algorithms used by employers and insurance companies determine your employability and insurability in part through these scores. Far more than another credit card (bad idea, by the way) or a mortgage are on the line. These algorithms seem objective, but how they are formulated, and the assumptions made in doing so mean the difference between useful tools that benefit people, and ""black boxes"" that thwart the flourishing of others, often unknown to them.Cathy O'Neil should know. A tenure track math professor, she made the jump to Wall Street and became a ""quant"" who helped develop mathematical algorithms and witnessed, in the crash of 2008, the harm some of these caused. And she began to notice how algorithms often painfully impacted the lives of many others.  She describes how a teacher was fired because of the weighting of performance scores of a single class, despite other evaluations finding her an excellent teacher (afterwards it was found that there were a high number of erasures on tests for students who would have been in her class the previous year, suggesting these had been altered to improve scores).As she looked at the algorithms responsible for such injustices, she came to dub them ""Weapons of Math Destruction"" or WMDs and she identified three characteristics of these WMDs:Opacity: those whose lives are affected by them have no idea of the factors and weighting of those factors that contributed to their ""score"".Scale: how widely an algorithm is applied across industries and sectors of life can affect how much of one's life is touched by a single formula. For example, the FICO scores mentioned above affect not only credit, but the ability to get a job, the cost of auto insurance, and your ability to rent an apartment.Damage: WMDs can reinforce other factors perpetuating a cycle of poverty, or incarceration.She also shows that what makes these algorithms destructive is the use of proxy measurements. For example an employer may not know directly how savvy someone is as a marketer, and so they use a ""proxy"" measurement of how many Twitter followers that person has. Or age is used as a proxy for how safe a driver one is. For a group, the proxy may work well, and be utterly inaccurate for an individual that falls within that proxy group.Then in successive chapters, she chronicles some of the ways WMDs operate in different parts of life. She discusses the U.S. News & World Report college rankings, and the use of algorithms in admissions processes. Social media uses algorithms to target advertising, which means some will see ads for for-profit schools and payday lenders, and others for upscale furnishing or Viagra, based on clicks, likes, searches, and comments. Policing strategies, including locations for intensified ""stop and frisk"" policing are shaped by another set of algorithms. Algorithms to filter resumes may use scoring algorithms that discriminate by address and psych exam algorithms may render others unemployable in a certain industry. Scheduling algorithms may promote efficiency at the expense of the ability of workers to sleep on a regular schedule, or arrange childcare, or work enough hours to qualify for health insurance. Algorithms sometimes shut people out from credit or low cost insurance when in fact they are good risks. She concludes by showing how algorithms determine ads and news we see (and don't see). In an afterword she explores the flaws in algorithms revealed on the election of Donald Trump (algorithms, for example predicted Clinton would easily win Michigan and Wisconsin, where consequently she did not campaign, and lost by small margins).In her conclusion, she makes the case not only for a code of ethics for mathematicians but also argues that regulation and audits of these algorithms are necessary. The value assumptions, as well as the mathematical methods of many algorithms are flawed, and yet opacity means those whose lives are most affected don't even know what hit them.She helps us see both the sinister and useful side of these algorithms. They may reveal where a pro-active intervention may save a family from descending into family violence, or provide extra assistance to a child in danger of falling behind in a key subject. Or they may be used to invade personal rights, or even to perpetuate socio-economic divides in a society. The reality is that the problem is not the math but the old GIGO problem (garbage in, garbage out). The values and assumptions of the humans who devise the formulas and weightings of values and the use of proxies determine what may be destructive outcomes for some people. Yet it can be hidden behind an app, a program, an algorithm.The massive explosion in storage capacities, processing speeds, and the way our interests, health status, travel patterns, spending patterns, fitness, diet and sleep habits, our political inclinations and more may be tracked via our online and smartphone usage makes O'Neil's warning an urgent one. We create mountains of data that may be increasingly mined by government and private interests. Perhaps as important as asking whether this will be governed in ways that contribute to our flourishing, is whether we will be alert enough to care.____________________________Disclosure of Material Connection: I received this book free from the publisher. I was not required to write a positive review. The opinions I have expressed are my own."
88,0553418815,http://goodreads.com/user/show/51162416-max,5,fast pleasant primer on what’s wrong with big data. Fun. I love using a VPN turning everything off and being practically nobody to these freaks 
89,0553418815,http://goodreads.com/user/show/4061006-angela,5,"Cathy O'Neil is one of my heroes; I love listening to her on Slate Money, and her less pop/more technical book, Doing Data Science, occupies a prominent place on my desk at work - and a special place in my heart. Going beyond the usual data science manuals of what a random forest is, or how to account for large, sparse data, O'Neil and her coauthor - in that book - make an almost anthropological survey of the data science ""field""/profession/whatever. It's enlightening, and it's very real; less a textbook, more a professional guide.So if Doing Data Science is the ""how to do your job"" manual, and Slate Money is the occasional soapbox, Weapons of Math Destruction is the manifesto. With clarity, conviction and intelligence, Cathy O'Neil scalpels away at the utopian, misty-eyed tech worship that has contributed to the data science/big data bubble. She identifies the ways in which algorithms encode and perpetuate existing prejudices, and are hidden from plain sight by the obscuring (both intentional and not) use of Fancy Math.She notes how anti-human ""market efficiency"" is, and how it's often been short-hand for unjust and unequal systems, and big data is just scaling those dumb systems way, way up. For example, the use of social network data to ""enrich"" lending platforms; or recidivism models that ask about where a former convict grew up, whether his/her friends are in jail, how many encounters with the police he/she has had. These algorithms - because they chase correlations to make predictions (rather than identifying causal factors) - create big-data-driven poverty traps. They are also beyond the purview of the government (and, given the new administration, will probably be for quite some time): just like your FICO score is ONLY about how often you pay your credit card bills, and NOT your demographic group, so too should these algorithms be scrutinized and regulated.Increasingly, O'Neil notes, the ""human touch"" comes at a price; in upper income groups, you get more real, live service - hearing about a job through connections, informational interviews with alumni, blah - while lower income groups are serviced by increasingly mechanized, machine learning code. The hiring and HR practices of CVS, McDonald's, and other low wage places were especially striking (and severe): mindless, well-meaning algorithms deployed to effectively punish workers whose BMIs (itself a stupid measure of health) are too high, or who seem ""anti-social"" according to a creepy questionnaire, or whatever. I think my favorite chapter - and, of course, the most relevant one - was about the toxic brew that can be politics and data science. It was especially powerful to think through the way that, as opposed to the past, where we had ""agreed upon facts"" given by carpet bombing-style hegemonic culture (e.g. four news channels that everyone watched), now we have hyper-tailored DARE I SAY ""alternative facts"" where, by the power of marketers (who prey on confirmation bias), your neighbor's political beliefs are the result of a very specific and private relationship between them, their web browser cookies, and some political marketer. This means you may have NO IDEA why your swingy neighbor in swingy state X thinks Candidate Y is running an underground child sex ring literally under the ground of a DC pizza shop. O'Neil (and this book was written Before The Madness) notes that these sorts of micro-targeted ads that follow you around the web are very smartly tailored; the marketers know who would be repulsed by such a stupid conspiracy theory, and who would be intrigued. And that's just terrifying. Anywayyyyy. From a data perspective, my (cold dead) economist heart reminds me that OF COURSE we're going to scale up injustice when we mindlessly maximize profit, instead of humans. We need to go back to our roots: to the philosophy and debates of Jeremy Bentham, and welfare economics, and what ""utility"" really means. Efficiency is not the end all, be all, despite what our current economic system seems to diktat. O'Neil also makes a good point about the repeated erroneous conflating of correlation and causation; something even the fanciest of linear algebra stochastic process meddlers fall prey to. That's something that I see a LOT in data science: the field is built on the premise of perfect prediction - and prediction doesn't need causation, it just needs correlation. OK, I will step off my soapbox. Highly recommended.edited to add: OMG and I just read her delightful 2017 resolutions post, and had much LOLs. Cathy O'Neil, you da best."
90,0553418815,http://goodreads.com/user/show/8325737-philipp,4,"This is an angry book, but its got a point! She summarises many 'weapons of math destruction', WMDs, algorithms that encode human biases, are opaque, which give recommendations or orders that no-one questions, while making things worse. For example, this summary:Poor people are more likely to have bad credit and live in high-crime neighborhoods, surrounded by other poor people. Once the dark universe of WMDs digests that data, it showers them with predatory ads for subprime loans or for-profit schools. It sends more police to arrest them, and when they’re convicted it sentences them to longer terms. This data feeds into other WMDs, which score the same people as high risks or easy targets and proceed to block them from jobs, while jacking up their rates for mortgages, car loans, and every kind of insurance imaginable. This drives their credit rating down further, creating nothing less than a death spiral of modeling. Being poor in a world of WMDs is getting more and more dangerous and expensive.O'Neil goes through many, many of examples like this where an automated approach made things worse, where people didn't bother to critically look at the output, where the model doesn't have a way to incorporate feedback, it's stuck in time and cannot evolve like humans. The majority of the book is spent on case studies and great, angry discussion of what went wrong. There's one short chapter on what could be done - mostly she proposes more regulation (auditing of algorithms), but also to 'dumb' down models by law, force them to treat people equally and not bin them in groups. That chapter on solutions is extremely short - I expect a second book from her on that topic.P.S.: I've been reading a lot about adversarial perturbations in deep learning (see this great paper). One idea could be to counteract some of those models to become an adversarial entity. For example, you have one of those shopping points cards. Instead of feeding their model with your shopping data, you mix it up - write down your shopping list for a month, shuffle it, and buy one part with your points card and the other in cash. Keep mixing it up and add random stuff. Your data should be garbage to the model as it's not consistent, easily removable as an outlier, i.e., worthless.You could also make the model work for you - find out which groups of customers the market sends special offers to, game your shopping so that you fall into that group, reap rewards.Of course you'd need to know in detail what the model is looking for, which is highly unlikely."
91,0553418815,http://goodreads.com/user/show/115473-siria,4,"In the two years since Cathy O'Neil published this study of the danger posed by mathematical models—how their supposed neutrality merely reflects the biases of those who create them while creating closed-system feedback loops that warp society in unjust ways—it hasn't become any less relevant. In fact, not only does the book remain relevant, but subsequent revelations—most prominently those about Facebook's involvement with the 2016 US elections—show that O'Neil may even have been too cautious in what is a trenchant and prescient critique. "
92,0553418815,http://goodreads.com/user/show/25178051-izzat-radzi,5,"""Mathematical models are supposed to be our tools, not our masters"" Statisticians, quants, actuaries : take heed. This book argues that the new 'industrial revolution' of big data is bringing destruction. To whom (?) , one might be tempted to asked. Humans, normal masses who sometimes makes errors of course.And due to that, they got red in their ledgers, which in these new century means, that they'll most likely subjected to the 'vicious loop' of rejection and manipulation, and will felt the usual 'life is not fair' feeling. To start off, I claim that this book is the best (so far) compliment to Taleb's Black Swan book. Whilst Taleb as arguably he as a trader and academia revolves his discussion around some financial models & it's discontent (whilst in most part slanted his discussion into the philosophical realm); this author meanwhile goes into a more broad discussion. She, whom has background in chronologically in academia, hedge funds, a short stint in risk management before focusing work as a data scientist, argues that given the presence of these Gargantuan Big Data in almost every aspect of life, it has tarnished if not destroyed people's life. The problem-before I proceed case by case- lies in two reasons. First is the data collected & recorded by the data mining by corporate companies, mostly are left uncrunched, meaning they're not subjected to updated information about those people data taken or they're mistakenly (yet significantly affect) people who are thought 'one', in this case people with the same name and birthday. This, argued by the author, because the model used isn't not tinkered time to time according to feedback; unlike those used in baseball & basketball where opponents and managers subject each individual players to new & paternalistic habits of players -thus a better use of correlation to causation-, they are taken as face value. And feedback that is not taken -later I'll mentioned in job application survey- left those aspiring people hoping to get some good news, baffled as they themselves don't know why their application was rejected. Turns out, it was their 'behaviour', recorded in the subjective and highly interpretable application questions. The second problem here, if not wrong should I put it, is that the usage of the data is, as author's claimed, taken as God's word, the highest possible theological blasphemy, if said to those in denial data miners. This is due to the reasons that they basically use the data available, let it be credit scores, health scores, SAT (or generally educational) scores, and even resume applications -to name a few- as the bestowed revelation, thus dismissing proven good teachers, potential shining employees and laying off 'unhealthy' current workers; all based on the Big Data, either snooping privately on their life, or in other cases, a take-it-or-leave option wether they'll provide their credit score to get an insurance or their health progress for work employment group insurance practiced in firms & companies. The dismissal action has become so autonomous directly from the data, that frightenedly, that one does not care of others, it's just life. Coincidentally, I read Adorno's Culture Industry directly before this, and it does personally provides a more enlightened discourse on the systemic issue. I personally first came across to this topic back a few years ago, that with this newly published book kindles my interest. The first was back around late 2013, where I was taking a UK driving licence. In the course, similarly argued by the author, you'll be getting some discount given that you choose to put a 'Black Box' in your car. This instrument will record your driving habits & feed back to the insurance company. Although initially used to monitor those big truckers -who tends to be sleep deprived due to late night & long driving- to reduce the risks of death of others due to reckless driving; here argued that it's afraid to be used by others to manipulate the privacy of car drivers, at other instances flawed due to normal human behaviour, taking short cuts at night through 'dangerous neighbourhood' thus ended up one marked to be subjected to 'handcuffed' by auto insurance companies. The second one I encountered was in early 2015, in a student organised event to explain & showcase the structure & function of Big Data by companies. At that time, I was furious given the fact of the the then environment of whistle-blowers expose on companies manipulating & snooping normal civilians life. In here meanwhile, as argued in The Impulse Society by Paul Roberts, politicians backed by big companies are microtargeting voters up to the point of what they read and hear are tailor made. Given to the rise of fascism and 'fear of the normal neighbour' one known differently pre-election in America which now turns into something totally unexpected, I humbly agreed to some point. The third encounter is with regard to behavioural test subjected to work or scholarship applications. Exposed initially by my parents, whom my dad was at times hired to be an interviewer. He's the first to enlighten me on the presence of attitude test taken by applicants, which is imperative for career climbing ambitions. Here written an illustration. An applicant answered promptly the normal application survey on personal attitudes. Obviously one is subjected to child education for example that one is unique bla3; yet answering unique now instead of other answers) will be perceived as narcissist by the possible employer, which now is not possible anymore. Unfortunately, this data, is shared and this unfortunate applicant will ended up in this continuous 'dangerous loop'. The author coined Recidivism in regard to this loop. The poor guy, living in a certain 'bad' neighbourhood, want to apply a credit card, or just simply living as a normal civilian. Now, dues to this, of course one might have some background of knowing some in-behind bars people. This data, used by the security enforcement, will cause the police to routinely knocks on doors to tell them they have eyes on them. A more idiosyncratic example is those trying to move houses. Property firms with big data on their very own hand, easily exclude those with a bad tag, even when they've changed, or for unfortunate some, mistakenly recorded in. As said before, no feedback can be given by the victims, unless one has some wealth to pursue on a legal lawsuit on this issue, which of course deteriorate middle class let alone the poor condition. Obviously the rich bankers and markets manipulators escaped from this loop. As a matter of fact, they're invulnerable and invincible. She suggested a few suggestions in the concluding chapter. First for example, oath taking by those finance practitioners, whom impact upon their ignorance is highly significant on the whole. Nevertheless she noted the downside that when in work bureaucratic structure, manager will want answers for certain issues, this will put those people in corners. Second is, as Taleb suggested, ditching those backward minded model. Obviously, she argued, one have to be specialised in the field to help recognise and build a better one. She illustrated a new model on preventing child abuse. There's similarity between the model to 'possible criminals' as in the Minority Report movie in reducing - if not preventing- crime she criticised, the hidden outcomes has not - at least she didn't yet- been analysed. The third is the involvement of human judgement into the model. Yes, initially the algorithm of the system is used to remove human bias, yet seeing how things turned out, the machine as expected cannot understand how human (cognitive) work. The problem of language and rational judgement will always triumph human above machine, I claimed this ambiguous argument. Hats off to this book!"
93,0553418815,http://goodreads.com/user/show/11951948-paul,3,"One of the funniest bits of Little Britain was where David Walliams as Carol Beer would type a request into the computer, before turning to the customer and saying ‘Computer says no’… Whilst it is funny, it is not so funny when it happens to you. In this book, O’Neil, a former Wall Street quant raises alarm bells on the way that these mathematical models have infiltrated our lives. We don’t see them, but these algorithms that help us with our searches online and finding books, films and other items on online sites are now being used to determine just how much of a risk you are. Next time you want a loan, to renew insurance or just need to get another job O’Neil thinks that some of us may have a problem.She calls them ‘Weapons of Maths Destruction’; these are incontestable, unregulated and opaque algorithms. They are being used by companies to decide the tiniest details. They are used because they make larger profits for corporations and most worryingly for us is that they are frequently conclude the wrong thing having made incorrect assumptions about individuals. As the saying goes ‘crap in; crap out’… Having worried the life out of the reader, O’Neil goes onto suggest a variety of things that could help; more regulation, better design of the code and us being aware of their use. The writing is clear, if a little dry and technical at times. The examples are a little American centric, but you can see the way that it is going in the UK. Even though the title mentions the dreaded word maths, it really isn’t that mathematical. Worthwhile reading."
94,0553418815,http://goodreads.com/user/show/16212136-peter-pete-mcloughlin,4,"Very depressing book on how computer algorithms are becoming a powerful tool of the exploitation of workers, the poor, minorities and ordinary people mostly in the service of profits. It goes deep and the problem is much worse than you think. This book is a downer as it covers relevant developments in politics and business and that always is not pleasant to think about in these times in general and are especially dispiriting in how they affect ordinary people."
95,0553418815,http://goodreads.com/user/show/5550611-hallie,2,"Just ok. I like the idea more than the execution of this book. While O'Neil uncovers a great, dark place in our society that needs more light shone on it, much of her conclusions she gives away in the introduction to the book. This would have been an excellent piece in the New Yorker or Atlantic but as a book began to feel redundant far too early. "
96,0553418815,http://goodreads.com/user/show/11345492-kate,4,"3.5, bumped up for likeable clarity-without-condescension and consistent up-front political engagement."
97,0553418815,http://goodreads.com/user/show/761281-mike,4,"Reading this book I could not help but wish the author had used a quote from Paul Goodman as the epigraph of the book: ""Technology is a branch of moral philosophy, not of science."" (Every technology ultimately is making a value judgement about what is important or worth doing, and what aspects of a task or reality are important.)This book is a fascinating look at how 'big data' is used and abused by colleges, banks, insurance agencies, in sentencing and parole decisions, by employers screening applicants, and more. Collecting vast amounts of data and using to guide decisions can be a very useful technology, but the author -- who herself had a career as a 'quant' or data analyst for a large firm -- points out the downside of basing decisions on data and algorithms. Of course it should be obvious, but our society's information fetishism can make us overlook that the data we collect and the algorithms we use to analyze it can be deeply flawed by omissions, prejudices, fallacies that the designers may not be aware of. A couple of the many examples she sues help illustrate this. We read about a school teacher whose performance is measured by an algorithm that takes into account his students' past performance, current performance on standardized tests, and various other factors. One year he is scored 6/100, and the next year 94/100 -- after making no changes to his syllabus, assignments, or anything else he was doing. Anyone with common sense can see there is something wrong with the algorithm if someone could get such wildly varying scores, but the scoring algorithm is a 'black box' that not event the school administrators understand. Among the many problems with the system here is that there is no allowance for the designers of the algorithm to get feedback on the scoring and adjust the model -- instead, the 'failing' teachers are fired and the algorithm rolls on. Contrast this with the use of 'big data' algorithms in commercial enterprises: when Amazon.com collects data on your browsing and purchasing habits, they use this recommendations to you for future purchases. If their models are bad, they suggest things you don't want, and they don't increase sales. But they take this feedback and change their algorithms, because there is a financial incentive to do so.So a huge blind spot in some of the models and algorithms policy makes use is the lack of feedback. Another is huge downside of the rise of big data modeling is that private companies can use opaque models to discriminate in ways that punish the poor particularly. The author explains how car insurance companies use data about where their customers live, drive, and work to assess risks -- which is largely legitimate -- but also abuse this information to determine which customers are likely to be able to shop around for better deals, and which are less likely to have the time, resources, or access to information to do so, and they are charged much more. Indeed affluent customers with DUI convictions may be better rates than safe drivers who live in areas with limited internet access and higher poverty. Likewise employers may screen applicants with big data algorithms that identify 'risks' which rule out the poor, people with past mental illness, or immigrants. Many other problems are discussed, and the final chapters look at how political parties have begun to utilize big data too -- an ominous prospect in light of the rest of the book.The author offers some solutions, ranging from the admittedly ineffective (a code of ethics for 'quants' who create the models) to the more daunting but effective measure of demanding transparency and regulation (which has made some progress in the insurance industry, but is far from complete).*Full disclosure -- I read a free advanced reader's copy that I won in a Goodreads giveaway.*"
98,0553418815,http://goodreads.com/user/show/3502690-graeme-roberts,2," Cathy O'Neil writes well and provides much good information, but I found the fundamental thesis of  Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy to be foolish and irresponsible. The title, condensed inevitably to WMD, is used promiscuously to describe any application of data science, statistics, or even information technology that she considers to be unfair or discriminatory. In most cases, she is right; some applications are used by the greedy and the uncaring with insufficient thought of the consequences, but she rarely considers the positive attributes of the applications. Once branded as a WMD, a term indelibly attached to the evil trifecta of George W. Bush, Saddam Hussein, and the Iraq War, how are readers meant to consider their many positive aspects and what can be done to make them better?At first, I thought that Ms. O'Neil had been the victim of a foolish PR or marketing person in her publishing company, who had come up with this catchy name, but her ardent use of the term convinced me that she had invented it.Ms. O'Neil seemed to follow a downward trajectory from her Harvard PhD in mathematics and a teaching position at Barnard to a leading hedge fund and a series of other morally disappointing positions that left her convinced that data science, and even mathematics, are roots of much evil. I don't buy it. They are tools that can be used or misused, and her diatribe does nothing to help.She asserts, on page 210:If we find (as studies have already shown) that the recidivism models codify prejudice and penalize the poor, then it's time to have a look at the inputs. In this case, they include loads of birds-of-a-feather connections. They predict an individual's behavior on the basis of the people he knows, his job, and his credit rating—details that would be inadmissible in court. The fairness fix is to throw out that data.But wait, many would say. Are we going to sacrifice the accuracy of the model for fairness? Do we have to dumb down our algorithms?In some cases, yes. If we're going to be equal before the law, or be treated equally as voters, we cannot stand for systems that drop us into different castes and treat us differently.I disagree strongly. Some data is better than none, though we should be looking to constantly improve our data science systems. I would like to know that the recently paroled criminal moving in next door had been released by a judge in full possession of whatever facts are available. So, I suspect, would Ms. O'Neil."
99,0553418815,http://goodreads.com/user/show/19993-robin,4,"If you've been listening to Cathy O'Neil on Slate Money for some time, you probably expected this book to emerge in exactly this form and with exactly this content. She's smart, progressive, focused, and clear, just as she was on the podcast. I saw her speak at the Mechanic's Institute Library a few months ago, before I'd read the book, and was very impressed by her ability to be articulate about complex situations with a decisiveness that belies their complexity, or at least certainly never allows for it as an excuse. The argument I found most challenging was (heavily paraphrasing here) that we should be privileging fairness because it is otherwise so easy to be blind to the unfairness that creeps in when we pursue other goals. If I'm honest with my own tendencies, I recognize my longing for accuracy might indeed cause me to overlook fairness, and I resist the idea that I need to choose one as a priority over the other. Nevertheless, O'Neil convinces me to question assumed neutrality much more closely, and be much more vigilant for it in my own industry. As she points out, it's easy for people who write the algorithms to absolve themselves of responsibility for how they are used, but if we pay attention to the real outcomes we may find that we need to do more to train our algorithms to learn from externalities and flaws. Focusing ruthlessly on measuring only what you are trying to measure and ignoring all other factors is a good start; evaluating the results to make sure that's actually what happened is as necessary a practice. The book is full of examples of failed algorithms in wide use, actively and detrimentally affecting at-risk populations today, as well as potential future areas where algorithms could be used, perhaps even inadvertently, to consolidate and perpetuate the power of the already powerful. The free market will not solve this problem. This is an excellent example of where good regulation is imperative. As constituents with the ability to influence legislators to put their attention here, we can demand more transparency, more awareness and education, and a more good, old-fashioned compassion.Further reading:https://www.ted.com/talks/joy_buolamw...https://www.propublica.org/article/ma...https://standards.ieee.org/develop/pr..."
100,0553418815,http://goodreads.com/user/show/7139041-dorin,2,"Disappointing - such a rich topic, yet the author decided to be racist, Democrat and to induce panic instead of having a cool headed approach. She ignores her own advice, contradicts herself time and again, and she makes her Democrat leaning so obvious it makes me puke.The book could have contained real data, real information, but instead she goes on to explain that statistics are bad because statistics don't catch statistical errors. She pretends she's a professional in data processing, but she sounds like she doesn't understand how statistics work, and she can barely go beyond her mathematical training to understand what went wrong from a human point of view.My point: „a machine is as useful as the person who uses it”. Her point: ""big data is bad unless it's used by the Democrats”."
101,0553418815,http://goodreads.com/user/show/7837611-greg-fanoe,2,"I think that on the whole reading this book will do people more good than harm but I had several problems with it:1. The only section which I have personal expertise on is the section on ""insurance"" - I am an actuary - and I can personally verify that this chapter was filled with inaccurate, misleading, and/or totally speculative statements.2. The correlation between credit score and income is nowhere near as strong as she makes it appear in this book.3. Many of the problems she identifies have nothing to do with the models in question. Advertising models are not the issue with the payday loan or for-profit college industry those industries need to be totally upended/destroyed at the source. Letting them operate as is but kneecapping their ability to advertise would be nonsensical.4. The solutions she presents are ridiculously vague and/or impractical."
102,0553418815,http://goodreads.com/user/show/1045774-mehrsa,5,This is the book I was waiting to read! The rejoinder to our data-obsessed culture. We got drunk on algorithms and forgot to focus on their limitations. are they asking the right questions? are they recycling inequality? Great book
103,0553418815,http://goodreads.com/user/show/3126915-imi,2,"Sadly, a rather superficial look at big data, algorithms and how they've impacted on society. You probably know/have guessed all this already. The correlations O'Neil point out are not very convincing, and it felt like she just wanted to rant about how unfair the world is. Not that I disagree with that conclusion; society is unfair and unequal. But O'Neil really doesn't state much more than that, and, worse, some of her examples felt misleading or simply speculative. Where was her evidence? And if she'd explored this issues in more depth I'd have wanted clear ideas for resolving these issues. Instead, any suggestions she made seemed vague and impractical. I admitedly feel out of my depth with this topic, but am most definitely willing to learn more and expand my understanding about it. Disappointingly, this book didn't offer that."
104,0553418815,http://goodreads.com/user/show/777176-dave,5,"I learned more from this book than I have from any book I've read in years. O'Neil is a former math professor and spent time as a hedge-fund quant, so she understands the theory and experience of the effect of out-of-control algorithms on our society.As corporations make more and more efficient use of algorithms, they are starting to show dangerous tendencies, according to O'Neil. She calls them ""weapons of math destruction"" or WMDs. WMDs present the following grave problems for society:--They usually lack transparency, such that it is difficult or impossible to know what goes into them and what factors sway or change their results. When they are opaque, WMDs are patently unfair (O'Neil cites the algorithms used to evaluate teachers in many districts, highlighting their absurdity as they attempt to quantify the unquantifiable).--They make broad generalizations and reduce human beings to ""buckets."" One of the worst examples is the criminal justice system, which is increasingly reliant on algorithms to deploy police patrols and even to sentence criminal defendants. These algorithms punish the innocent by assuming that those who live in poor neighborhoods are necessarily more likely to commit crimes, thereby subjecting them to frequent ""stop and frisk"" type searches that have been found to be unconstitutional repeatedly.--They can use information in an unfair way that tends to perpetuate further injustice. A prime example of this is the increasing tendency to identify individuals by their credit score, on job applications and even in online shopping. When job applicants are screened by credit score, those who are already poor (and therefore much more likely to have a low credit score already) will find it even harder to get a job and thereby earn enough money to help improve their credit score. This self-perpetuating cycle of poverty is completely unnecessary, because credit scores are an exceedingly poor proxy for job-worthiness of applicants.--Many WMDs are not very good at learning. They are created as models of human behavior (with all the flaws and assumptions of the humans who programmed them), and when they miss (when for example they screen out a potential hire who would have actually been a first-rate employee), they never learn that they have missed. The ""false positive"" never presents itself (the rejected applicant goes on to success elsewhere), and the oblivious WMD continues to chug along with its assumptions and blind spots unchallenged.And WMDs are like racism:""Racism, at the individual level, can be seen as a predictive model whirring away in billions of human minds around the world. It is built from faulty, incomplete, or generalized data. Whether it comes from experience or hearsay, the data indicates that certain types of people have behaved badly. That generates a binary prediction that all people of that race will behave that same way....Racists don't spend a lot of time hunting down reliable data to train their twisted models. And once their model morphs into a belief, it becomes hardwired. It generates poisonous assumptions, yet rarely tests them, settling instead for data that seems to confirm and fortify them. Consequently, racism is the most slovenly of predictive models. It is powered by haphazard data gathering and spurious correlations, reinforced by institutional inequities, and polluted by confirmation bias. In this way, oddly enough, racism operates like many of the WMDs I'll be describing in this book."" (p. 22-3)And finally:""Models like this will abound in coming years....Many of these models, like some of the WMDs we've discussed, will arrive with the best intentions. But they must also deliver transparency, disclosing the input data they're using as well as the results of their targeting. And they must be open to audits. These are powerful engines, after all. We must keep our eyes on them.... Data is not going away....Predictive models are, increasingly, the tools we will be relying on to run our institutions, deploy our resources, and manage our lives. But...these models are constructed not just from data but from the choices we make about which data to pay attention to--and which to leave out. Those choices are not just about logistics, profits, and efficiency. They are fundamentally moral. If we back away from them and treat mathematical models as a neutral and inevitable force, like the weather or the tides, we abdicate our responsibility....We must come together to police these WMDs, to tame and disarm them."" (p.218)"
105,0553418815,http://goodreads.com/user/show/3029658-doug-cornelius,5,"With big data, comes formulas to parse through the data trying to make sense of it. Those algorithms can help make sense of the data and help filter through the noise to find trends. But those algorithms can also be easily misused and have harmful, if unintended consequences. Cathy O'Neil explores these problems in 
Weapons of Math Destruction
.Ms. O'Neil is a former academic, hedge fund quant, and data scientist for startups. At the hedge fund she was looking at ways to make money by predicting financial trends. As a data scientist she was looking to predict people's purchases and browsing habits to monetize those habits. Those are good algorithms where the people using them understand them and update the algorithms as they see problems and can check to see if they are working properly by verifying outcomes.Many data driven outcomes fail. Those are the focus of 
Weapons of Math Destruction
.I'm a big fan of the Slate Money podcast with Cathy O'Neil, Felix Salmon, and Jordan Weissmann. When the publisher offered me a review copy of Ms. O'Neil's book, I jumped at the chance.It's a short book and even though the title makes it sound like it's full of math, it's not. It's more about social policy. She points to the danger of decision makers blindingly following algorithmic output that they do not understand.Take the US News and World Report listing of best colleges. This is preeminent guide for high school students trying to figure out which college to attend. The rankings do not look at actual student outcomes. The publisher does not look at happiness, learning or improvements. It looks at proxies for the outcomes like SAT scores, alumni contributions, and graduation percentages. Colleges started making decisions to improve their rankings in the algorithm by improving those measured proxies. As anyone writing checks for their college bound children, affordability is not one of the measured proxies.As you might expect there are big chunks of the book dedicated to failings in the financial sector by relying on poorly designed algorithms. The biggest problems often being false assumptions plugged into the data running through the algorithm.With the election season upon us, the chapter on political algorithms is fascinating. The data scientists of commerce were brought into political campaigns. They are able to micro-target potential voters sending different messages to different groups highlighting the positions that would make the candidate more favorable to that segment of the population. Civic engagement and the political process is increasingly being algorithm driven.Since our world is increasingly being driven by big data, it's important to understand what is happening behind the decision-making. Weapons of Math Destruction is an excellent tool to help you understand data driven decision-making."
106,0553418815,http://goodreads.com/user/show/23020614-germ-n,3,"A fascinating must-read guide on how to navigate and interpret a world increasingly ruled by materialistic prejudices and oversimplifications that are repackaged as ""science"" and ""tech"" through poorly designed computer algorithms.Reading this gives one the feeling that the ""evil androids ruling over mankind"" dystopia has already been unrolling. Except in a very sneaky way: this time we don't get to face or even understand the robots and their actions which are nonetheless already reshaping our reality and the course of our lives through ridiculous university rankings, credit scores, social media ideological bubbles, etc.Unfortunately O'Neil ruins an otherwise brilliant book by an extreme leftist bias that seeks to absolve her personal heroes in the US democratic party and their political agendas, and attempts to attribute most blame on a specific other side. I assume this is why a book which raises such important points gets to be published and become mainstream after all.For example, she portrays Obama as a saint who just wants the best for immigrants and fiercely battles the evil rich. Yet for any unbiased observer, it's quite clear that Obama and democrats are just serving the wealthy who finance their own political campaigns (duh) when they set to ruin nations overseas and bring in thousands of immigrants, ideally illegal ones, so the price of labor can go down and corporations can continue to reap political advantages.This skewed perspective is damaging to the bottom argument and distracts readers from what's really the main issue here: the constant loss of truth and distortion of increasingly astroturfed public debate. No attempts are made, for example, at addressing how these WMDs are influencing the toxic foreign policy of the US and the press - another entity that passes as a saint in this book, while being the number one responsible for war and destruction of nations around the world."
107,0553418815,http://goodreads.com/user/show/20465079-tam,4,"A quick and highly useful read - I would recommend it to all people not just ones working with predictive models and Big Data.The title is sensational enough, but at one point I put that aside and decided to read this book. So many have been praising Big Data and Machine Learning, few cautions have been put forward but not enough, mostly dealing with ethics, security, personal data issues. This book, on the other hand, discusses the large scale tragedies that Big Data can invoke on societies as a whole, if continuing to go unchecked. 10 chapters deal with examples about the misuses of data and predictive models in many aspects of lives, especially the lives of the most unfortunate.O'Neil has a strong opinion and at certain point I thought she might be too pessimistic (saying that from a self-proclaimed pessimistic person as I am), and O'Neil freely claims how angry she is with the whole thing. Yet at the very end of the book, she makes constructive comments, ideas for improvement, for regulations. Those are the most important, and perhaps the most useful passages for data scientists. I've been quite fascinated by the tool and sometimes forget the darker side that the tool can invoke, and I'd like to thank O'Neil for the reminder, for the clarification about issues that sometimes I feel uneasy but not so clear about. No, the tool is neither inherently good nor bad, the people who make them and wield them can choose do what they want. And we can choose to do good things.Ehh, not so optimistic though, there are tons of challenges and... But I should shut my mouth. But at least we should try."
108,0553418815,http://goodreads.com/user/show/15409024-ahalya-vijay,5,"I found this to be a fascinating read on the importance of using technology and data in an ethical way. I was shocked to learn how pervasive irresponsible uses of data are throughout the developed world and how widespread it is for people to place their faith in ""black box"" models as Cathy O'Neil refers to them. It must be frustrating, for example, for teachers to be evaluated (and fired!) based on a model they have no knowledge of, without knowing how to improve their rating. And it must be even more so if they are getting anecdotally strong and positive feedback from school leadership and parents.Another shock was how liberally companies and 'data scientists' use proxies without thought to the consequences of the model not capturing the 'error term' sufficiently. The danger of making decisions based on things that are modeled (or estimated) using proxies was drilled into my head throughout my entire schooling. How would you feel if the fact that you were late to one meeting at work was used to brand you as an irresponsible or unreliable worker? Ms O'Neil made a clear link between such models being used to trap people in a cycle of poverty, using example after example across a variety of industries (e.g., education, finance, insurance). It is clear that reform is needed, but what left me worried at the end is that these models are not hurting the rich and famous, but the poor and under-resourced. Will our current political and justice system listen to their needs? I'm skeptical."
109,0553418815,http://goodreads.com/user/show/48433695-alexis-tremblay,5,"A must read for all data scientist, mathematicians, modellers, statisticians,... There is a dark side to big data and she exposes it masterfully. I always had an uneasy feeling when I started my career as an actuary and later on as data scientist. My moral compass was always off but I didn't have the words for it. Now she has given me a framework to think about my job and what to look out for. Will read again. "
110,0553418815,http://goodreads.com/user/show/16204614-bing-gordon,4,"Good overview of modern analytics, but too much of a guilt trip laid on top. "
111,0553418815,http://goodreads.com/user/show/6100646-brian-clegg,4,"As a poacher-turned-gamekeeper of the big data world, Cathy O'Neil is ideally placed to take us on a voyage of horrible discovery into the world of systems making decisions based on big data that can have a negative influence on lives - what she refers to as 'Weapons of Math Destruction' or WMDs. After working as a 'quant' in a hedge fund and on big data crunching systems for startups, she has developed a horror for the misuse of the technology and sets out to show us how unfair it can be.It's not that O'Neil is against big data per se. She points out examples where it can be useful and effective - but this requires the systems to be transparent and to be capable of learning from their mistakes. In the examples we discover, from systems that rate school teachers to those that decide whether or not to issue a payday loan, the system is opaque, secretive and based on a set of rules that aren't tested against reality and regularly updated to produce a fair outcome.The teacher grading system is probably the most dramatically inaccurate example, where the system is trying to measure how well a teacher has performed, based on data that only has a very vague link to actual outcomes - so, for instance, O'Neil tells of a teacher who scored 6% one year and 96% the next year for doing the same job. The factors being measured are almost entirely outside the teacher's control with no linkage to performance and the interpretation of the data is simply garbage.Other systems, such as those used to rank universities, are ruthlessly gamed by the participants, making them far more about how good an organisation is at coming up with the right answers to metrics than it is to the quality of that organisation. And all of us will come across targeted advertising and social media messages/search results prioritised according to secret algorithms which we know nothing about and that attempt to control our behaviour.For O'Neil, the worst aspects of big data misuse are where a system - perhaps with the best intentions - ends up penalising people for being poor of being from certain ethnic backgrounds. This is often a result of an indirect piece of data - for instance the place they live might have implications on their financial state or ethnicity. She vividly portrays the way that systems dealing with everything from police presence in an area to fixing insurance premiums can produce a downward spiral of negative feedback.Although the book is often very effective, it is heavily US-oriented, which is a shame when many of these issues are as significant, say, in Europe, as they are in the US. There is probably also not enough nuance in the author's binary good/bad opinion of systems. For example, she tells us that someone shouldn't be penalised by having to pay more for insurance because they live in a high risk neighbourhood - but doesn't think about the contrary aspect that if insurance companies don't do this, those of us who live in low risk neighbourhoods are being penalised by paying much higher premiums than we need to in order to cover our insurance. O'Neil makes a simplistic linkage between high risk = poor, low risk = rich - yet those of us, for instance, who live in the country are often in quite poor areas that are nonetheless low risk. For O'Neil, fairness means everyone pays the same. But is that truly fair? Here in Europe, we've had car insurance for young female drivers doubled in cost to make it the same as young males - even though the young males are far more likely to have accidents. This is fair by O'Neil's standards, because it doesn't discriminate on gender, but is not fair in the real world away from labels.There's a lot here that we should be picking up on, and even if you don't agree with all of O'Neil's assessments, it certainly makes you think about the rights and wrongs of decisions based on automated assessment of indirect data."
112,0553418815,http://goodreads.com/user/show/7019761-cindy,4,"While I don't necessarily agree with all of the conclusions, this was a very thought-provoking book. The author is a mathematician who has worked on Wall Street and now takes on some of the math models that rule our lives. Some of these were, at least originally, developed with good intentions - intending to make fairer some of the decisions that affect our lives: where and who gets into college; what the schedule for our job looks like; how much we pay to get a loan; even how you are judged in your chosen profession. But the author poses that many of these types of decisions are actually less than fair. The models are in her terms WMDs - weapons of math destruction - they are hidden, unregulated, and virtually uncontestable, even when they are blatantly wrong. One of the more troubling examples from the book concerned the use of testing to judge teacher performance. She traced how an exemplary teacher with outstanding evaluations was judged as poor on the basis of one set of tests and fired. The teacher showed that the tests which showed that her ""value-added"" score was sub-par were flawed due to a high number of erasures on the previous year which indicated cheating. Indeed, the system said that there *might* have been cheating but they would not re-instate this excellent teacher. (And please don't get me started on ""value-added"" - my daughters' school was always excellent in all the testing but this score was just average. So if your kids are already high achievers, how do you continue to score excellent in this measure? The answer is it's almost impossible.)Other examples showed that even when you removed race or ethnicity or economics as an indicator in these WMDs, they were there. If a zip code puts you in a risky category for a student loan, then it's hard to get the education that can get you out of poverty; or if a work schedule makes it hard for a single mother to get the child care she needs so that she can get the money to go to school to make a better life for herself and her child...well, I can see how some of this sets up a self-fulfilling prophecy. It makes it harder, not impossible, to break this cycle. That's why this book is important. If this is being used, then we need to know and like credit scores have the ability to correct errors and inaccuracies that work against the average person.Quotes to remember:....This would all be wonderful for students and might enhance their college experience, if they weren't the ones paying for it in the form of student loans that would burden them for decades. We cannot place the blame for this trend entirely on the US News rankings. Our entire society has embraced not only the idea that a college education is essential, but the idea that a degree from a highly-ranked school can catapult a student into a life of power and privilege. As I write this, the entire voting population that matters lives in a handful of counties in Florida, Ohio, Nevada, and a few other swing states. Within those counties is a small number of voters whose opinions weigh in the balance. I might point out here that while many of the WMDs we've been looking at, from predatory ads to policing models, deliver most of their punishment to the struggling classes, political microtargeting harms voters of every economic class, from Manhattan to San Francisco, rich and poor alike find themselves disenfranchised, though the truly affluent of course can more than compensate for this with campaign contributions."
113,0553418815,http://goodreads.com/user/show/6894205-allie,4,"This was by far the most interesting book about math I have ever read (admittedly a low bar to set), possibly because it contains no actual math formulas. Instead, former mathematics professor and hedge fund analyst Cathy O'Neil tells stories about the applications of math - how ""ill-conceived mathematical models now micromanage the economy from advertising to prisons."" Among other things, mathematical models determine which news you see on social media, your Google search results, the cost of your auto insurance and credit card rates, which neighborhoods police are sent to patrol, and how your child's schools are evaluated. In one staggering statistic about the reach of these models, O'Neil notes that 72 percent of resumes are never seen by humans, but are instead screened and scored by software programs looking for employer keywords. While math itself is neutral, the models are based on assumptions that reflect human biases and limitations. Often, the models are designed to process data from large numbers of people efficiently, but not necessarily fairly. She distinguishes between ""good"" models built on transparent assumptions that are constantly corrected with new data (e.g., those used in professional baseball) to opaque models that use proxy data such as race or zip codes and do not have a corrective feedback mechanism (e.g., predictive policing software). Typically, there is no method of recourse when given a poor score by a computer (hello, Hal) since companies consider their algorithms proprietary and won't disclose details of how scores are developed. This was an issue for teachers evaluated - and sometimes fired - based on inaccurate models of student test scores. The book's second main argument is that the built-in biases in the models are especially putative towards the poor, who are steered to high interest loans, targeted by ""stop and frisk"" policing based on their neighborhoods, and less likely to be personally interviewed for jobs.I found many of O'Neil's arguments persuasive and somewhat alarming, although she tries to end the book on a positive note. Worth reading, even if you don't like math. Or weapons. Or destruction. "
114,0553418815,http://goodreads.com/user/show/70315594-enoughtohold,4,"A nice, clear overview of the problems of big data for laypeople. Not sure why other reviewers expected this to be deeply technical, or why they think O’Neil thinks there are easy answers — she plainly doesn’t.I for one am glad a book this accessible exists on this topic. After all, the vast majority of the people affected by these harmful models are not data scientists, and they deserve to know what’s going on.(Also, the author did a great job reading for the audiobook.)"
115,0553418815,http://goodreads.com/user/show/4430576-miri,5,"I've said it before, and I'm sure I'll say it again, because I make a point of seeking out books like this. But if there's one book I could get everyone to read right now, it would be this one. Even next to Ta-Nehisi Coates's We Were Eight Years in Power, which I'm reading concurrently and which is staggering in its importance, this book feels urgent—because what it explains is how our new economy of Big Data is codifying, concealing, and amplifying inequality of every kind, on an enormous scale. Every injustice I'm reading about in Coates's book is made worse by what I read about here. We think of math as inherently neutral, the way we think of technology and logic and science. But these things are like any tool, dependent on the person using them, capable of being used for good or evil. (Remember the way science was abused throughout American history, fabricating the entire concept of biological race to provide justification for white humans to enslave black ones.) When we start using math to make decisions instead of people, assuming that this will decrease human bias and make things more fair, what we can actually end up doing is obscuring that prejudice and magnifying the scale of its effect. Cathy O'Neil was a mathematician working in the finance industry in 2008. She left after a few years, disgusted with the way the system was clearly rigged, and has since created the concept of Weapons of Math Destruction: mathematical models that codify human prejudice, are opaque and impervious to feedback, and affect large numbers of people. In this book, O'Neil takes us through several WMDs, showing how they're used in different areas of our lives. There's the disastrous school reform attempt in Washington D.C., which tried to evaluate teachers in a way math simply cannot be used, and which fired hundreds of teachers based on results no one in the district could explain or defend. There's the way many employers now use credit scores to screen applications, assuming that making regular payments means you'll be a dependable employee, ignoring the facts that (1) responsible people experience financial difficulty too and (2) preventing people with bad credit from getting jobs is only going to make their situation worse.There are the computerized risk models that judges use in sentencing, trying to gauge from prisoners' background information—where they grew up, whether their friends and family have criminal records; information which would be inadmissible in court—how likely they are to return to prison after being released, and how long their sentence should be. There's the way U.S. News & World Report helped cause an arms race when it decided to start ranking universities on everything except their cost, driving up tuition over 500 percent (nearly four times the rate of inflation) in just the 30-ish years that I've been alive.There's political advertising which is now so individually personalized that even people who support the same candidate, even people visiting the candidate's website, will see totally different things based on the profile the campaign has created for them.There's predatory advertising like that done by for-profit colleges, which specifically targets vulnerable people's ""pain points"" looking for those who are ""stuck,"" who have ""few people in their lives who care about them,"" who have ""low self-esteem"" and are ""unable to see and plan well for the future"" (these quotes, shared by O'Neil, come from the California Attorney General's office when it was investigating one of these colleges). (And if you're wondering how they would know this about people, the point is that they know this about all of us now. Just try to imagine what someone could learn about you from seeing everything you do online.)It might seem like the logical response is to disarm these weapons, one by one. The problem is that they’re feeding on each other. Poor people are more likely to have bad credit and live in high-crime neighborhoods, surrounded by other poor people. Once the dark universe of WMDs digests that data, it showers them with predatory ads for subprime loans or for-profit schools. It sends more police to arrest them, and when they're convicted it sentences them to longer terms. This data feeds into other WMDs, which score the same people as high-risk or easy targets and proceed to block them from jobs, while jacking up their rates for mortgages, car loans, and every kind of insurance imaginable. This drives their credit rating down further, creating nothing less than a death spiral of modeling. Being poor in a world of WMDs is getting more and more dangerous and expensive.The same WMDs that abuse the poor also place the comfortable classes of society in their own marketing silos . . . For many of them it can feel as though the world is getting smarter and easier. Models highlight bargains on prosciutto and chianti, recommend a great movie on Amazon Prime, or lead them, turn by turn, to a cafe in what used to be a ""sketchy"" neighborhood. The quiet and personal nature of this targeting keeps society's winners from seeing how the very same models are destroying lives, sometimes just a few blocks away.Though we're all affected by them in ways we can't see, right now the majority of the damage is being done to the same groups who are always hurt by flaws in the system—minorities and the poor. The same models could easily be used to help people instead of prey upon them, but that won't happen in the glorious free market of capitalism. We need regulation and oversight in the data industry. We need accountability. When statistics itself, and the public’s trust in statistics, is being actively undermined by politicians across the globe, how can we possibly expect the Big Data industry to clarify rather than contribute to the noise? We can because we must. Right now, mammoth companies like Google, Amazon, and Facebook exert incredible control over society because they control the data. They reap enormous profits while somehow offloading fact-checking responsibilities to others. It’s not a coincidence that, even as he works to undermine the public’s trust in science and in scientific fact, Steve Bannon sits on the board of Cambridge Analytica, a political data company that has claimed credit for Trump’s victory while bragging about secret ""voter suppression"" campaigns. It’s part of a more general trend in which data is privately owned and privately used to private ends of profit and influence, while the public is shut out of the process and told to behave well and trust the algorithms. It’s time to start misbehaving. Algorithms are only going to become more ubiquitous in the coming years. We must demand that systems that hold algorithms accountable become ubiquitous as well."
116,0553418815,http://goodreads.com/user/show/17187084-sean-goh,4,"Very readable book warning of the downside of amoral models. With great power comes great responsibility.___Math-powered applications were based on choices made by fallible human beings. Many models encoded human prejudice, misunderstanding, and bias into the software systems that increasingly managed our lives. Like gods, these mathematical models were opaque, their workings invisible to all but the highest priests in their domain: mathematicians and computer scientists.Models are opinions embedded in mathematics.Statistical engines without feedback can continue to spin our faulty and damaging analysis while never learning from its mistakes. E.g. teachers mistaken labelled as failures are fired, and no one is the wiser. The model's output becomes unquestioned reality.The parallels between finance and Big Data: The productivity of the field's talents indicates that they are on the right track, and it translates into dollars. This leads to the fallacious conclusion that whatever they are doing to bring in more money is good. It ""adds value"". Otherwise why would the market reward it?In both cultures Wealth is no longer a means to get by. It becomes directly tied to personal worth.On the rise of university rankings: from the perspective of a university president, it's quite sad. Here they are at the summit of their careers dedicating enormous energy towards boosting performance in fifteen areas defined by a group of journalists at a second-tier magazine (U.S. News). The ranking had become destiny. When you create a model from proxies, it is far simpler for people to game it. This is because proxies are easier to manipulate than the complicated reality they represent. (e.g. trying to evaluate influence by number of twitter followers)In a system in which cheating is the norm, following the rules amounts to a handicap. So preventing the students in Zhongxiang from cheating was unfair.In our largely segregated cities, geography is a proxy for race.Patrolling particular areas creates a pernicious feedback loop. The policing itself spawns new data, which justifies more policing. Eventually the zeroing in on ghetto districts ends up criminalising poverty, believing all the while that our tools are not only scientific but fair.Instead of simply trying to eradicate crimes, police should be attempting to build relationships in the neighbourhood. This was one of the pillars of the original ""broken windows"" study. The cops were on foot, talking to people, trying to get them to uphold their own community standards. But that objective, in many cases, has been lost, steamrollered by models that equate arrests with safety.Mathematical models can sift through data to locate people who are likely to face great challenges, whether from crime, poverty, or education. It's up to society whether to use that intelligence to reject and punish them - or to reach out to them with the resources they need. It all depends on the objectives we choose.Computing systems have trouble finding digital proxies for soft skills. The relevant data simply isn't collected, and anyway it's hard to put a value to them. They're usually easier to just leave out of the model.The modelers of e-scores (unofficial credit ratings) have to make do with trying to answer the question: ""How have people like you behaved in the past?"" when ideally the question would be ""How have you behaved in the past?Even with the Affordable Care Act, which reduced the ranks of the uninsured, medical expenses remain the single biggest cause of bankruptcies in America.A sterling credit rating is not just a proxy for responsibility and smart decisions. It is also a proxy for wealth. And wealth is highly correlated with race.Mistakes made by models are learning opportunities - as long as the system receives feedback on the error. When automatic systems sift through our data to size us up for an e-score, they naturally project the past into the future. The poor are expected to remain poor forever and are treated accordingly. It's inexorable, often hidden and beyond appeal, and unfair.Yet machines cannot make adjustments for fairness by themselves. That is where the human overseer comes in.Human decision making, while often flawed, has one chief virtue. It can evolve.Big data processes codify the past. They do not invent the future. Doing that requires moral imagination, and that's something only humans can provide. We have to explicitly embed better values into our algorithms, creating Big Data models that follow our ethical lead. Sometimes that will mean putting fairness ahead of profit."
117,0553418815,http://goodreads.com/user/show/6991670-bob,4,"Summary: An insider account of the algorithms that affect our lives, from going to college, to the ads we see online, to our chances of getting a job, being arrested, getting credit and insurance.Big Data is indeed BIG. Mathematical algorithms shape who will see this post on their Facebook newsfeed. If you go to Amazon or another online bookseller, algorithms will suggest other books like this one you might be interested in. Have you seen all those ads about credit scores? They are more important than you might imagine. Algorithms used by employers and insurance companies determine your employability and insurability in part through these scores. Far more than another credit card (bad idea, by the way) or a mortgage are on the line. These algorithms seem objective, but how they are formulated, and the assumptions made in doing so mean the difference between useful tools that benefit people, and ""black boxes"" that thwart the flourishing of others, often unknown to them.Cathy O'Neil should know. A tenure track math professor, she made the jump to Wall Street and became a ""quant"" who helped develop mathematical algorithms and witnessed, in the crash of 2008, the harm some of these caused. And she began to notice how algorithms often painfully impacted the lives of many others.  She describes how a teacher was fired because of the weighting of performance scores of a single class, despite other evaluations finding her an excellent teacher (afterwards it was found that there were a high number of erasures on tests for students who would have been in her class the previous year, suggesting these had been altered to improve scores).As she looked at the algorithms responsible for such injustices, she came to dub them ""Weapons of Math Destruction"" or WMDs and she identified three characteristics of these WMDs:Opacity: those whose lives are affected by them have no idea of the factors and weighting of those factors that contributed to their ""score"".Scale: how widely an algorithm is applied across industries and sectors of life can affect how much of one's life is touched by a single formula. For example, the FICO scores mentioned above affect not only credit, but the ability to get a job, the cost of auto insurance, and your ability to rent an apartment.Damage: WMDs can reinforce other factors perpetuating a cycle of poverty, or incarceration.She also shows that what makes these algorithms destructive is the use of proxy measurements. For example an employer may not know directly how savvy someone is as a marketer, and so they use a ""proxy"" measurement of how many Twitter followers that person has. Or age is used as a proxy for how safe a driver one is. For a group, the proxy may work well, and be utterly inaccurate for an individual that falls within that proxy group.Then in successive chapters, she chronicles some of the ways WMDs operate in different parts of life. She discusses the U.S. News & World Report college rankings, and the use of algorithms in admissions processes. Social media uses algorithms to target advertising, which means some will see ads for for-profit schools and payday lenders, and others for upscale furnishing or Viagra, based on clicks, likes, searches, and comments. Policing strategies, including locations for intensified ""stop and frisk"" policing are shaped by another set of algorithms. Algorithms to filter resumes may use scoring algorithms that discriminate by address and psych exam algorithms may render others unemployable in a certain industry. Scheduling algorithms may promote efficiency at the expense of the ability of workers to sleep on a regular schedule, or arrange childcare, or work enough hours to qualify for health insurance. Algorithms sometimes shut people out from credit or low cost insurance when in fact they are good risks. She concludes by showing how algorithms determine ads and news we see (and don't see). In an afterword she explores the flaws in algorithms revealed on the election of Donald Trump (algorithms, for example predicted Clinton would easily win Michigan and Wisconsin, where consequently she did not campaign, and lost by small margins).In her conclusion, she makes the case not only for a code of ethics for mathematicians but also argues that regulation and audits of these algorithms are necessary. The value assumptions, as well as the mathematical methods of many algorithms are flawed, and yet opacity means those whose lives are most affected don't even know what hit them.She helps us see both the sinister and useful side of these algorithms. They may reveal where a pro-active intervention may save a family from descending into family violence, or provide extra assistance to a child in danger of falling behind in a key subject. Or they may be used to invade personal rights, or even to perpetuate socio-economic divides in a society. The reality is that the problem is not the math but the old GIGO problem (garbage in, garbage out). The values and assumptions of the humans who devise the formulas and weightings of values and the use of proxies determine what may be destructive outcomes for some people. Yet it can be hidden behind an app, a program, an algorithm.The massive explosion in storage capacities, processing speeds, and the way our interests, health status, travel patterns, spending patterns, fitness, diet and sleep habits, our political inclinations and more may be tracked via our online and smartphone usage makes O'Neil's warning an urgent one. We create mountains of data that may be increasingly mined by government and private interests. Perhaps as important as asking whether this will be governed in ways that contribute to our flourishing, is whether we will be alert enough to care.____________________________Disclosure of Material Connection: I received this book free from the publisher. I was not required to write a positive review. The opinions I have expressed are my own."
118,0553418815,http://goodreads.com/user/show/51162416-max,5,fast pleasant primer on what’s wrong with big data. Fun. I love using a VPN turning everything off and being practically nobody to these freaks 
119,0553418815,http://goodreads.com/user/show/4061006-angela,5,"Cathy O'Neil is one of my heroes; I love listening to her on Slate Money, and her less pop/more technical book, Doing Data Science, occupies a prominent place on my desk at work - and a special place in my heart. Going beyond the usual data science manuals of what a random forest is, or how to account for large, sparse data, O'Neil and her coauthor - in that book - make an almost anthropological survey of the data science ""field""/profession/whatever. It's enlightening, and it's very real; less a textbook, more a professional guide.So if Doing Data Science is the ""how to do your job"" manual, and Slate Money is the occasional soapbox, Weapons of Math Destruction is the manifesto. With clarity, conviction and intelligence, Cathy O'Neil scalpels away at the utopian, misty-eyed tech worship that has contributed to the data science/big data bubble. She identifies the ways in which algorithms encode and perpetuate existing prejudices, and are hidden from plain sight by the obscuring (both intentional and not) use of Fancy Math.She notes how anti-human ""market efficiency"" is, and how it's often been short-hand for unjust and unequal systems, and big data is just scaling those dumb systems way, way up. For example, the use of social network data to ""enrich"" lending platforms; or recidivism models that ask about where a former convict grew up, whether his/her friends are in jail, how many encounters with the police he/she has had. These algorithms - because they chase correlations to make predictions (rather than identifying causal factors) - create big-data-driven poverty traps. They are also beyond the purview of the government (and, given the new administration, will probably be for quite some time): just like your FICO score is ONLY about how often you pay your credit card bills, and NOT your demographic group, so too should these algorithms be scrutinized and regulated.Increasingly, O'Neil notes, the ""human touch"" comes at a price; in upper income groups, you get more real, live service - hearing about a job through connections, informational interviews with alumni, blah - while lower income groups are serviced by increasingly mechanized, machine learning code. The hiring and HR practices of CVS, McDonald's, and other low wage places were especially striking (and severe): mindless, well-meaning algorithms deployed to effectively punish workers whose BMIs (itself a stupid measure of health) are too high, or who seem ""anti-social"" according to a creepy questionnaire, or whatever. I think my favorite chapter - and, of course, the most relevant one - was about the toxic brew that can be politics and data science. It was especially powerful to think through the way that, as opposed to the past, where we had ""agreed upon facts"" given by carpet bombing-style hegemonic culture (e.g. four news channels that everyone watched), now we have hyper-tailored DARE I SAY ""alternative facts"" where, by the power of marketers (who prey on confirmation bias), your neighbor's political beliefs are the result of a very specific and private relationship between them, their web browser cookies, and some political marketer. This means you may have NO IDEA why your swingy neighbor in swingy state X thinks Candidate Y is running an underground child sex ring literally under the ground of a DC pizza shop. O'Neil (and this book was written Before The Madness) notes that these sorts of micro-targeted ads that follow you around the web are very smartly tailored; the marketers know who would be repulsed by such a stupid conspiracy theory, and who would be intrigued. And that's just terrifying. Anywayyyyy. From a data perspective, my (cold dead) economist heart reminds me that OF COURSE we're going to scale up injustice when we mindlessly maximize profit, instead of humans. We need to go back to our roots: to the philosophy and debates of Jeremy Bentham, and welfare economics, and what ""utility"" really means. Efficiency is not the end all, be all, despite what our current economic system seems to diktat. O'Neil also makes a good point about the repeated erroneous conflating of correlation and causation; something even the fanciest of linear algebra stochastic process meddlers fall prey to. That's something that I see a LOT in data science: the field is built on the premise of perfect prediction - and prediction doesn't need causation, it just needs correlation. OK, I will step off my soapbox. Highly recommended.edited to add: OMG and I just read her delightful 2017 resolutions post, and had much LOLs. Cathy O'Neil, you da best."
120,0553418815,http://goodreads.com/user/show/8325737-philipp,4,"This is an angry book, but its got a point! She summarises many 'weapons of math destruction', WMDs, algorithms that encode human biases, are opaque, which give recommendations or orders that no-one questions, while making things worse. For example, this summary:Poor people are more likely to have bad credit and live in high-crime neighborhoods, surrounded by other poor people. Once the dark universe of WMDs digests that data, it showers them with predatory ads for subprime loans or for-profit schools. It sends more police to arrest them, and when they’re convicted it sentences them to longer terms. This data feeds into other WMDs, which score the same people as high risks or easy targets and proceed to block them from jobs, while jacking up their rates for mortgages, car loans, and every kind of insurance imaginable. This drives their credit rating down further, creating nothing less than a death spiral of modeling. Being poor in a world of WMDs is getting more and more dangerous and expensive.O'Neil goes through many, many of examples like this where an automated approach made things worse, where people didn't bother to critically look at the output, where the model doesn't have a way to incorporate feedback, it's stuck in time and cannot evolve like humans. The majority of the book is spent on case studies and great, angry discussion of what went wrong. There's one short chapter on what could be done - mostly she proposes more regulation (auditing of algorithms), but also to 'dumb' down models by law, force them to treat people equally and not bin them in groups. That chapter on solutions is extremely short - I expect a second book from her on that topic.P.S.: I've been reading a lot about adversarial perturbations in deep learning (see this great paper). One idea could be to counteract some of those models to become an adversarial entity. For example, you have one of those shopping points cards. Instead of feeding their model with your shopping data, you mix it up - write down your shopping list for a month, shuffle it, and buy one part with your points card and the other in cash. Keep mixing it up and add random stuff. Your data should be garbage to the model as it's not consistent, easily removable as an outlier, i.e., worthless.You could also make the model work for you - find out which groups of customers the market sends special offers to, game your shopping so that you fall into that group, reap rewards.Of course you'd need to know in detail what the model is looking for, which is highly unlikely."
121,0553418815,http://goodreads.com/user/show/115473-siria,4,"In the two years since Cathy O'Neil published this study of the danger posed by mathematical models—how their supposed neutrality merely reflects the biases of those who create them while creating closed-system feedback loops that warp society in unjust ways—it hasn't become any less relevant. In fact, not only does the book remain relevant, but subsequent revelations—most prominently those about Facebook's involvement with the 2016 US elections—show that O'Neil may even have been too cautious in what is a trenchant and prescient critique. "
122,0553418815,http://goodreads.com/user/show/25178051-izzat-radzi,5,"""Mathematical models are supposed to be our tools, not our masters"" Statisticians, quants, actuaries : take heed. This book argues that the new 'industrial revolution' of big data is bringing destruction. To whom (?) , one might be tempted to asked. Humans, normal masses who sometimes makes errors of course.And due to that, they got red in their ledgers, which in these new century means, that they'll most likely subjected to the 'vicious loop' of rejection and manipulation, and will felt the usual 'life is not fair' feeling. To start off, I claim that this book is the best (so far) compliment to Taleb's Black Swan book. Whilst Taleb as arguably he as a trader and academia revolves his discussion around some financial models & it's discontent (whilst in most part slanted his discussion into the philosophical realm); this author meanwhile goes into a more broad discussion. She, whom has background in chronologically in academia, hedge funds, a short stint in risk management before focusing work as a data scientist, argues that given the presence of these Gargantuan Big Data in almost every aspect of life, it has tarnished if not destroyed people's life. The problem-before I proceed case by case- lies in two reasons. First is the data collected & recorded by the data mining by corporate companies, mostly are left uncrunched, meaning they're not subjected to updated information about those people data taken or they're mistakenly (yet significantly affect) people who are thought 'one', in this case people with the same name and birthday. This, argued by the author, because the model used isn't not tinkered time to time according to feedback; unlike those used in baseball & basketball where opponents and managers subject each individual players to new & paternalistic habits of players -thus a better use of correlation to causation-, they are taken as face value. And feedback that is not taken -later I'll mentioned in job application survey- left those aspiring people hoping to get some good news, baffled as they themselves don't know why their application was rejected. Turns out, it was their 'behaviour', recorded in the subjective and highly interpretable application questions. The second problem here, if not wrong should I put it, is that the usage of the data is, as author's claimed, taken as God's word, the highest possible theological blasphemy, if said to those in denial data miners. This is due to the reasons that they basically use the data available, let it be credit scores, health scores, SAT (or generally educational) scores, and even resume applications -to name a few- as the bestowed revelation, thus dismissing proven good teachers, potential shining employees and laying off 'unhealthy' current workers; all based on the Big Data, either snooping privately on their life, or in other cases, a take-it-or-leave option wether they'll provide their credit score to get an insurance or their health progress for work employment group insurance practiced in firms & companies. The dismissal action has become so autonomous directly from the data, that frightenedly, that one does not care of others, it's just life. Coincidentally, I read Adorno's Culture Industry directly before this, and it does personally provides a more enlightened discourse on the systemic issue. I personally first came across to this topic back a few years ago, that with this newly published book kindles my interest. The first was back around late 2013, where I was taking a UK driving licence. In the course, similarly argued by the author, you'll be getting some discount given that you choose to put a 'Black Box' in your car. This instrument will record your driving habits & feed back to the insurance company. Although initially used to monitor those big truckers -who tends to be sleep deprived due to late night & long driving- to reduce the risks of death of others due to reckless driving; here argued that it's afraid to be used by others to manipulate the privacy of car drivers, at other instances flawed due to normal human behaviour, taking short cuts at night through 'dangerous neighbourhood' thus ended up one marked to be subjected to 'handcuffed' by auto insurance companies. The second one I encountered was in early 2015, in a student organised event to explain & showcase the structure & function of Big Data by companies. At that time, I was furious given the fact of the the then environment of whistle-blowers expose on companies manipulating & snooping normal civilians life. In here meanwhile, as argued in The Impulse Society by Paul Roberts, politicians backed by big companies are microtargeting voters up to the point of what they read and hear are tailor made. Given to the rise of fascism and 'fear of the normal neighbour' one known differently pre-election in America which now turns into something totally unexpected, I humbly agreed to some point. The third encounter is with regard to behavioural test subjected to work or scholarship applications. Exposed initially by my parents, whom my dad was at times hired to be an interviewer. He's the first to enlighten me on the presence of attitude test taken by applicants, which is imperative for career climbing ambitions. Here written an illustration. An applicant answered promptly the normal application survey on personal attitudes. Obviously one is subjected to child education for example that one is unique bla3; yet answering unique now instead of other answers) will be perceived as narcissist by the possible employer, which now is not possible anymore. Unfortunately, this data, is shared and this unfortunate applicant will ended up in this continuous 'dangerous loop'. The author coined Recidivism in regard to this loop. The poor guy, living in a certain 'bad' neighbourhood, want to apply a credit card, or just simply living as a normal civilian. Now, dues to this, of course one might have some background of knowing some in-behind bars people. This data, used by the security enforcement, will cause the police to routinely knocks on doors to tell them they have eyes on them. A more idiosyncratic example is those trying to move houses. Property firms with big data on their very own hand, easily exclude those with a bad tag, even when they've changed, or for unfortunate some, mistakenly recorded in. As said before, no feedback can be given by the victims, unless one has some wealth to pursue on a legal lawsuit on this issue, which of course deteriorate middle class let alone the poor condition. Obviously the rich bankers and markets manipulators escaped from this loop. As a matter of fact, they're invulnerable and invincible. She suggested a few suggestions in the concluding chapter. First for example, oath taking by those finance practitioners, whom impact upon their ignorance is highly significant on the whole. Nevertheless she noted the downside that when in work bureaucratic structure, manager will want answers for certain issues, this will put those people in corners. Second is, as Taleb suggested, ditching those backward minded model. Obviously, she argued, one have to be specialised in the field to help recognise and build a better one. She illustrated a new model on preventing child abuse. There's similarity between the model to 'possible criminals' as in the Minority Report movie in reducing - if not preventing- crime she criticised, the hidden outcomes has not - at least she didn't yet- been analysed. The third is the involvement of human judgement into the model. Yes, initially the algorithm of the system is used to remove human bias, yet seeing how things turned out, the machine as expected cannot understand how human (cognitive) work. The problem of language and rational judgement will always triumph human above machine, I claimed this ambiguous argument. Hats off to this book!"
123,0553418815,http://goodreads.com/user/show/11951948-paul,3,"One of the funniest bits of Little Britain was where David Walliams as Carol Beer would type a request into the computer, before turning to the customer and saying ‘Computer says no’… Whilst it is funny, it is not so funny when it happens to you. In this book, O’Neil, a former Wall Street quant raises alarm bells on the way that these mathematical models have infiltrated our lives. We don’t see them, but these algorithms that help us with our searches online and finding books, films and other items on online sites are now being used to determine just how much of a risk you are. Next time you want a loan, to renew insurance or just need to get another job O’Neil thinks that some of us may have a problem.She calls them ‘Weapons of Maths Destruction’; these are incontestable, unregulated and opaque algorithms. They are being used by companies to decide the tiniest details. They are used because they make larger profits for corporations and most worryingly for us is that they are frequently conclude the wrong thing having made incorrect assumptions about individuals. As the saying goes ‘crap in; crap out’… Having worried the life out of the reader, O’Neil goes onto suggest a variety of things that could help; more regulation, better design of the code and us being aware of their use. The writing is clear, if a little dry and technical at times. The examples are a little American centric, but you can see the way that it is going in the UK. Even though the title mentions the dreaded word maths, it really isn’t that mathematical. Worthwhile reading."
124,0553418815,http://goodreads.com/user/show/16212136-peter-pete-mcloughlin,4,"Very depressing book on how computer algorithms are becoming a powerful tool of the exploitation of workers, the poor, minorities and ordinary people mostly in the service of profits. It goes deep and the problem is much worse than you think. This book is a downer as it covers relevant developments in politics and business and that always is not pleasant to think about in these times in general and are especially dispiriting in how they affect ordinary people."
125,0553418815,http://goodreads.com/user/show/5550611-hallie,2,"Just ok. I like the idea more than the execution of this book. While O'Neil uncovers a great, dark place in our society that needs more light shone on it, much of her conclusions she gives away in the introduction to the book. This would have been an excellent piece in the New Yorker or Atlantic but as a book began to feel redundant far too early. "
126,0553418815,http://goodreads.com/user/show/11345492-kate,4,"3.5, bumped up for likeable clarity-without-condescension and consistent up-front political engagement."
127,0553418815,http://goodreads.com/user/show/761281-mike,4,"Reading this book I could not help but wish the author had used a quote from Paul Goodman as the epigraph of the book: ""Technology is a branch of moral philosophy, not of science."" (Every technology ultimately is making a value judgement about what is important or worth doing, and what aspects of a task or reality are important.)This book is a fascinating look at how 'big data' is used and abused by colleges, banks, insurance agencies, in sentencing and parole decisions, by employers screening applicants, and more. Collecting vast amounts of data and using to guide decisions can be a very useful technology, but the author -- who herself had a career as a 'quant' or data analyst for a large firm -- points out the downside of basing decisions on data and algorithms. Of course it should be obvious, but our society's information fetishism can make us overlook that the data we collect and the algorithms we use to analyze it can be deeply flawed by omissions, prejudices, fallacies that the designers may not be aware of. A couple of the many examples she sues help illustrate this. We read about a school teacher whose performance is measured by an algorithm that takes into account his students' past performance, current performance on standardized tests, and various other factors. One year he is scored 6/100, and the next year 94/100 -- after making no changes to his syllabus, assignments, or anything else he was doing. Anyone with common sense can see there is something wrong with the algorithm if someone could get such wildly varying scores, but the scoring algorithm is a 'black box' that not event the school administrators understand. Among the many problems with the system here is that there is no allowance for the designers of the algorithm to get feedback on the scoring and adjust the model -- instead, the 'failing' teachers are fired and the algorithm rolls on. Contrast this with the use of 'big data' algorithms in commercial enterprises: when Amazon.com collects data on your browsing and purchasing habits, they use this recommendations to you for future purchases. If their models are bad, they suggest things you don't want, and they don't increase sales. But they take this feedback and change their algorithms, because there is a financial incentive to do so.So a huge blind spot in some of the models and algorithms policy makes use is the lack of feedback. Another is huge downside of the rise of big data modeling is that private companies can use opaque models to discriminate in ways that punish the poor particularly. The author explains how car insurance companies use data about where their customers live, drive, and work to assess risks -- which is largely legitimate -- but also abuse this information to determine which customers are likely to be able to shop around for better deals, and which are less likely to have the time, resources, or access to information to do so, and they are charged much more. Indeed affluent customers with DUI convictions may be better rates than safe drivers who live in areas with limited internet access and higher poverty. Likewise employers may screen applicants with big data algorithms that identify 'risks' which rule out the poor, people with past mental illness, or immigrants. Many other problems are discussed, and the final chapters look at how political parties have begun to utilize big data too -- an ominous prospect in light of the rest of the book.The author offers some solutions, ranging from the admittedly ineffective (a code of ethics for 'quants' who create the models) to the more daunting but effective measure of demanding transparency and regulation (which has made some progress in the insurance industry, but is far from complete).*Full disclosure -- I read a free advanced reader's copy that I won in a Goodreads giveaway.*"
128,0553418815,http://goodreads.com/user/show/3502690-graeme-roberts,2," Cathy O'Neil writes well and provides much good information, but I found the fundamental thesis of  Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy to be foolish and irresponsible. The title, condensed inevitably to WMD, is used promiscuously to describe any application of data science, statistics, or even information technology that she considers to be unfair or discriminatory. In most cases, she is right; some applications are used by the greedy and the uncaring with insufficient thought of the consequences, but she rarely considers the positive attributes of the applications. Once branded as a WMD, a term indelibly attached to the evil trifecta of George W. Bush, Saddam Hussein, and the Iraq War, how are readers meant to consider their many positive aspects and what can be done to make them better?At first, I thought that Ms. O'Neil had been the victim of a foolish PR or marketing person in her publishing company, who had come up with this catchy name, but her ardent use of the term convinced me that she had invented it.Ms. O'Neil seemed to follow a downward trajectory from her Harvard PhD in mathematics and a teaching position at Barnard to a leading hedge fund and a series of other morally disappointing positions that left her convinced that data science, and even mathematics, are roots of much evil. I don't buy it. They are tools that can be used or misused, and her diatribe does nothing to help.She asserts, on page 210:If we find (as studies have already shown) that the recidivism models codify prejudice and penalize the poor, then it's time to have a look at the inputs. In this case, they include loads of birds-of-a-feather connections. They predict an individual's behavior on the basis of the people he knows, his job, and his credit rating—details that would be inadmissible in court. The fairness fix is to throw out that data.But wait, many would say. Are we going to sacrifice the accuracy of the model for fairness? Do we have to dumb down our algorithms?In some cases, yes. If we're going to be equal before the law, or be treated equally as voters, we cannot stand for systems that drop us into different castes and treat us differently.I disagree strongly. Some data is better than none, though we should be looking to constantly improve our data science systems. I would like to know that the recently paroled criminal moving in next door had been released by a judge in full possession of whatever facts are available. So, I suspect, would Ms. O'Neil."
129,0553418815,http://goodreads.com/user/show/19993-robin,4,"If you've been listening to Cathy O'Neil on Slate Money for some time, you probably expected this book to emerge in exactly this form and with exactly this content. She's smart, progressive, focused, and clear, just as she was on the podcast. I saw her speak at the Mechanic's Institute Library a few months ago, before I'd read the book, and was very impressed by her ability to be articulate about complex situations with a decisiveness that belies their complexity, or at least certainly never allows for it as an excuse. The argument I found most challenging was (heavily paraphrasing here) that we should be privileging fairness because it is otherwise so easy to be blind to the unfairness that creeps in when we pursue other goals. If I'm honest with my own tendencies, I recognize my longing for accuracy might indeed cause me to overlook fairness, and I resist the idea that I need to choose one as a priority over the other. Nevertheless, O'Neil convinces me to question assumed neutrality much more closely, and be much more vigilant for it in my own industry. As she points out, it's easy for people who write the algorithms to absolve themselves of responsibility for how they are used, but if we pay attention to the real outcomes we may find that we need to do more to train our algorithms to learn from externalities and flaws. Focusing ruthlessly on measuring only what you are trying to measure and ignoring all other factors is a good start; evaluating the results to make sure that's actually what happened is as necessary a practice. The book is full of examples of failed algorithms in wide use, actively and detrimentally affecting at-risk populations today, as well as potential future areas where algorithms could be used, perhaps even inadvertently, to consolidate and perpetuate the power of the already powerful. The free market will not solve this problem. This is an excellent example of where good regulation is imperative. As constituents with the ability to influence legislators to put their attention here, we can demand more transparency, more awareness and education, and a more good, old-fashioned compassion.Further reading:https://www.ted.com/talks/joy_buolamw...https://www.propublica.org/article/ma...https://standards.ieee.org/develop/pr..."
130,0553418815,http://goodreads.com/user/show/7139041-dorin,2,"Disappointing - such a rich topic, yet the author decided to be racist, Democrat and to induce panic instead of having a cool headed approach. She ignores her own advice, contradicts herself time and again, and she makes her Democrat leaning so obvious it makes me puke.The book could have contained real data, real information, but instead she goes on to explain that statistics are bad because statistics don't catch statistical errors. She pretends she's a professional in data processing, but she sounds like she doesn't understand how statistics work, and she can barely go beyond her mathematical training to understand what went wrong from a human point of view.My point: „a machine is as useful as the person who uses it”. Her point: ""big data is bad unless it's used by the Democrats”."
131,0553418815,http://goodreads.com/user/show/7837611-greg-fanoe,2,"I think that on the whole reading this book will do people more good than harm but I had several problems with it:1. The only section which I have personal expertise on is the section on ""insurance"" - I am an actuary - and I can personally verify that this chapter was filled with inaccurate, misleading, and/or totally speculative statements.2. The correlation between credit score and income is nowhere near as strong as she makes it appear in this book.3. Many of the problems she identifies have nothing to do with the models in question. Advertising models are not the issue with the payday loan or for-profit college industry those industries need to be totally upended/destroyed at the source. Letting them operate as is but kneecapping their ability to advertise would be nonsensical.4. The solutions she presents are ridiculously vague and/or impractical."
132,0553418815,http://goodreads.com/user/show/1045774-mehrsa,5,This is the book I was waiting to read! The rejoinder to our data-obsessed culture. We got drunk on algorithms and forgot to focus on their limitations. are they asking the right questions? are they recycling inequality? Great book
133,0553418815,http://goodreads.com/user/show/3126915-imi,2,"Sadly, a rather superficial look at big data, algorithms and how they've impacted on society. You probably know/have guessed all this already. The correlations O'Neil point out are not very convincing, and it felt like she just wanted to rant about how unfair the world is. Not that I disagree with that conclusion; society is unfair and unequal. But O'Neil really doesn't state much more than that, and, worse, some of her examples felt misleading or simply speculative. Where was her evidence? And if she'd explored this issues in more depth I'd have wanted clear ideas for resolving these issues. Instead, any suggestions she made seemed vague and impractical. I admitedly feel out of my depth with this topic, but am most definitely willing to learn more and expand my understanding about it. Disappointingly, this book didn't offer that."
134,0553418815,http://goodreads.com/user/show/777176-dave,5,"I learned more from this book than I have from any book I've read in years. O'Neil is a former math professor and spent time as a hedge-fund quant, so she understands the theory and experience of the effect of out-of-control algorithms on our society.As corporations make more and more efficient use of algorithms, they are starting to show dangerous tendencies, according to O'Neil. She calls them ""weapons of math destruction"" or WMDs. WMDs present the following grave problems for society:--They usually lack transparency, such that it is difficult or impossible to know what goes into them and what factors sway or change their results. When they are opaque, WMDs are patently unfair (O'Neil cites the algorithms used to evaluate teachers in many districts, highlighting their absurdity as they attempt to quantify the unquantifiable).--They make broad generalizations and reduce human beings to ""buckets."" One of the worst examples is the criminal justice system, which is increasingly reliant on algorithms to deploy police patrols and even to sentence criminal defendants. These algorithms punish the innocent by assuming that those who live in poor neighborhoods are necessarily more likely to commit crimes, thereby subjecting them to frequent ""stop and frisk"" type searches that have been found to be unconstitutional repeatedly.--They can use information in an unfair way that tends to perpetuate further injustice. A prime example of this is the increasing tendency to identify individuals by their credit score, on job applications and even in online shopping. When job applicants are screened by credit score, those who are already poor (and therefore much more likely to have a low credit score already) will find it even harder to get a job and thereby earn enough money to help improve their credit score. This self-perpetuating cycle of poverty is completely unnecessary, because credit scores are an exceedingly poor proxy for job-worthiness of applicants.--Many WMDs are not very good at learning. They are created as models of human behavior (with all the flaws and assumptions of the humans who programmed them), and when they miss (when for example they screen out a potential hire who would have actually been a first-rate employee), they never learn that they have missed. The ""false positive"" never presents itself (the rejected applicant goes on to success elsewhere), and the oblivious WMD continues to chug along with its assumptions and blind spots unchallenged.And WMDs are like racism:""Racism, at the individual level, can be seen as a predictive model whirring away in billions of human minds around the world. It is built from faulty, incomplete, or generalized data. Whether it comes from experience or hearsay, the data indicates that certain types of people have behaved badly. That generates a binary prediction that all people of that race will behave that same way....Racists don't spend a lot of time hunting down reliable data to train their twisted models. And once their model morphs into a belief, it becomes hardwired. It generates poisonous assumptions, yet rarely tests them, settling instead for data that seems to confirm and fortify them. Consequently, racism is the most slovenly of predictive models. It is powered by haphazard data gathering and spurious correlations, reinforced by institutional inequities, and polluted by confirmation bias. In this way, oddly enough, racism operates like many of the WMDs I'll be describing in this book."" (p. 22-3)And finally:""Models like this will abound in coming years....Many of these models, like some of the WMDs we've discussed, will arrive with the best intentions. But they must also deliver transparency, disclosing the input data they're using as well as the results of their targeting. And they must be open to audits. These are powerful engines, after all. We must keep our eyes on them.... Data is not going away....Predictive models are, increasingly, the tools we will be relying on to run our institutions, deploy our resources, and manage our lives. But...these models are constructed not just from data but from the choices we make about which data to pay attention to--and which to leave out. Those choices are not just about logistics, profits, and efficiency. They are fundamentally moral. If we back away from them and treat mathematical models as a neutral and inevitable force, like the weather or the tides, we abdicate our responsibility....We must come together to police these WMDs, to tame and disarm them."" (p.218)"
135,0553418815,http://goodreads.com/user/show/3029658-doug-cornelius,5,"With big data, comes formulas to parse through the data trying to make sense of it. Those algorithms can help make sense of the data and help filter through the noise to find trends. But those algorithms can also be easily misused and have harmful, if unintended consequences. Cathy O'Neil explores these problems in 
Weapons of Math Destruction
.Ms. O'Neil is a former academic, hedge fund quant, and data scientist for startups. At the hedge fund she was looking at ways to make money by predicting financial trends. As a data scientist she was looking to predict people's purchases and browsing habits to monetize those habits. Those are good algorithms where the people using them understand them and update the algorithms as they see problems and can check to see if they are working properly by verifying outcomes.Many data driven outcomes fail. Those are the focus of 
Weapons of Math Destruction
.I'm a big fan of the Slate Money podcast with Cathy O'Neil, Felix Salmon, and Jordan Weissmann. When the publisher offered me a review copy of Ms. O'Neil's book, I jumped at the chance.It's a short book and even though the title makes it sound like it's full of math, it's not. It's more about social policy. She points to the danger of decision makers blindingly following algorithmic output that they do not understand.Take the US News and World Report listing of best colleges. This is preeminent guide for high school students trying to figure out which college to attend. The rankings do not look at actual student outcomes. The publisher does not look at happiness, learning or improvements. It looks at proxies for the outcomes like SAT scores, alumni contributions, and graduation percentages. Colleges started making decisions to improve their rankings in the algorithm by improving those measured proxies. As anyone writing checks for their college bound children, affordability is not one of the measured proxies.As you might expect there are big chunks of the book dedicated to failings in the financial sector by relying on poorly designed algorithms. The biggest problems often being false assumptions plugged into the data running through the algorithm.With the election season upon us, the chapter on political algorithms is fascinating. The data scientists of commerce were brought into political campaigns. They are able to micro-target potential voters sending different messages to different groups highlighting the positions that would make the candidate more favorable to that segment of the population. Civic engagement and the political process is increasingly being algorithm driven.Since our world is increasingly being driven by big data, it's important to understand what is happening behind the decision-making. Weapons of Math Destruction is an excellent tool to help you understand data driven decision-making."
136,0553418815,http://goodreads.com/user/show/23020614-germ-n,3,"A fascinating must-read guide on how to navigate and interpret a world increasingly ruled by materialistic prejudices and oversimplifications that are repackaged as ""science"" and ""tech"" through poorly designed computer algorithms.Reading this gives one the feeling that the ""evil androids ruling over mankind"" dystopia has already been unrolling. Except in a very sneaky way: this time we don't get to face or even understand the robots and their actions which are nonetheless already reshaping our reality and the course of our lives through ridiculous university rankings, credit scores, social media ideological bubbles, etc.Unfortunately O'Neil ruins an otherwise brilliant book by an extreme leftist bias that seeks to absolve her personal heroes in the US democratic party and their political agendas, and attempts to attribute most blame on a specific other side. I assume this is why a book which raises such important points gets to be published and become mainstream after all.For example, she portrays Obama as a saint who just wants the best for immigrants and fiercely battles the evil rich. Yet for any unbiased observer, it's quite clear that Obama and democrats are just serving the wealthy who finance their own political campaigns (duh) when they set to ruin nations overseas and bring in thousands of immigrants, ideally illegal ones, so the price of labor can go down and corporations can continue to reap political advantages.This skewed perspective is damaging to the bottom argument and distracts readers from what's really the main issue here: the constant loss of truth and distortion of increasingly astroturfed public debate. No attempts are made, for example, at addressing how these WMDs are influencing the toxic foreign policy of the US and the press - another entity that passes as a saint in this book, while being the number one responsible for war and destruction of nations around the world."
137,0553418815,http://goodreads.com/user/show/20465079-tam,4,"A quick and highly useful read - I would recommend it to all people not just ones working with predictive models and Big Data.The title is sensational enough, but at one point I put that aside and decided to read this book. So many have been praising Big Data and Machine Learning, few cautions have been put forward but not enough, mostly dealing with ethics, security, personal data issues. This book, on the other hand, discusses the large scale tragedies that Big Data can invoke on societies as a whole, if continuing to go unchecked. 10 chapters deal with examples about the misuses of data and predictive models in many aspects of lives, especially the lives of the most unfortunate.O'Neil has a strong opinion and at certain point I thought she might be too pessimistic (saying that from a self-proclaimed pessimistic person as I am), and O'Neil freely claims how angry she is with the whole thing. Yet at the very end of the book, she makes constructive comments, ideas for improvement, for regulations. Those are the most important, and perhaps the most useful passages for data scientists. I've been quite fascinated by the tool and sometimes forget the darker side that the tool can invoke, and I'd like to thank O'Neil for the reminder, for the clarification about issues that sometimes I feel uneasy but not so clear about. No, the tool is neither inherently good nor bad, the people who make them and wield them can choose do what they want. And we can choose to do good things.Ehh, not so optimistic though, there are tons of challenges and... But I should shut my mouth. But at least we should try."
138,0553418815,http://goodreads.com/user/show/15409024-ahalya-vijay,5,"I found this to be a fascinating read on the importance of using technology and data in an ethical way. I was shocked to learn how pervasive irresponsible uses of data are throughout the developed world and how widespread it is for people to place their faith in ""black box"" models as Cathy O'Neil refers to them. It must be frustrating, for example, for teachers to be evaluated (and fired!) based on a model they have no knowledge of, without knowing how to improve their rating. And it must be even more so if they are getting anecdotally strong and positive feedback from school leadership and parents.Another shock was how liberally companies and 'data scientists' use proxies without thought to the consequences of the model not capturing the 'error term' sufficiently. The danger of making decisions based on things that are modeled (or estimated) using proxies was drilled into my head throughout my entire schooling. How would you feel if the fact that you were late to one meeting at work was used to brand you as an irresponsible or unreliable worker? Ms O'Neil made a clear link between such models being used to trap people in a cycle of poverty, using example after example across a variety of industries (e.g., education, finance, insurance). It is clear that reform is needed, but what left me worried at the end is that these models are not hurting the rich and famous, but the poor and under-resourced. Will our current political and justice system listen to their needs? I'm skeptical."
139,0553418815,http://goodreads.com/user/show/48433695-alexis-tremblay,5,"A must read for all data scientist, mathematicians, modellers, statisticians,... There is a dark side to big data and she exposes it masterfully. I always had an uneasy feeling when I started my career as an actuary and later on as data scientist. My moral compass was always off but I didn't have the words for it. Now she has given me a framework to think about my job and what to look out for. Will read again. "
140,0553418815,http://goodreads.com/user/show/16204614-bing-gordon,4,"Good overview of modern analytics, but too much of a guilt trip laid on top. "
141,0553418815,http://goodreads.com/user/show/6100646-brian-clegg,4,"As a poacher-turned-gamekeeper of the big data world, Cathy O'Neil is ideally placed to take us on a voyage of horrible discovery into the world of systems making decisions based on big data that can have a negative influence on lives - what she refers to as 'Weapons of Math Destruction' or WMDs. After working as a 'quant' in a hedge fund and on big data crunching systems for startups, she has developed a horror for the misuse of the technology and sets out to show us how unfair it can be.It's not that O'Neil is against big data per se. She points out examples where it can be useful and effective - but this requires the systems to be transparent and to be capable of learning from their mistakes. In the examples we discover, from systems that rate school teachers to those that decide whether or not to issue a payday loan, the system is opaque, secretive and based on a set of rules that aren't tested against reality and regularly updated to produce a fair outcome.The teacher grading system is probably the most dramatically inaccurate example, where the system is trying to measure how well a teacher has performed, based on data that only has a very vague link to actual outcomes - so, for instance, O'Neil tells of a teacher who scored 6% one year and 96% the next year for doing the same job. The factors being measured are almost entirely outside the teacher's control with no linkage to performance and the interpretation of the data is simply garbage.Other systems, such as those used to rank universities, are ruthlessly gamed by the participants, making them far more about how good an organisation is at coming up with the right answers to metrics than it is to the quality of that organisation. And all of us will come across targeted advertising and social media messages/search results prioritised according to secret algorithms which we know nothing about and that attempt to control our behaviour.For O'Neil, the worst aspects of big data misuse are where a system - perhaps with the best intentions - ends up penalising people for being poor of being from certain ethnic backgrounds. This is often a result of an indirect piece of data - for instance the place they live might have implications on their financial state or ethnicity. She vividly portrays the way that systems dealing with everything from police presence in an area to fixing insurance premiums can produce a downward spiral of negative feedback.Although the book is often very effective, it is heavily US-oriented, which is a shame when many of these issues are as significant, say, in Europe, as they are in the US. There is probably also not enough nuance in the author's binary good/bad opinion of systems. For example, she tells us that someone shouldn't be penalised by having to pay more for insurance because they live in a high risk neighbourhood - but doesn't think about the contrary aspect that if insurance companies don't do this, those of us who live in low risk neighbourhoods are being penalised by paying much higher premiums than we need to in order to cover our insurance. O'Neil makes a simplistic linkage between high risk = poor, low risk = rich - yet those of us, for instance, who live in the country are often in quite poor areas that are nonetheless low risk. For O'Neil, fairness means everyone pays the same. But is that truly fair? Here in Europe, we've had car insurance for young female drivers doubled in cost to make it the same as young males - even though the young males are far more likely to have accidents. This is fair by O'Neil's standards, because it doesn't discriminate on gender, but is not fair in the real world away from labels.There's a lot here that we should be picking up on, and even if you don't agree with all of O'Neil's assessments, it certainly makes you think about the rights and wrongs of decisions based on automated assessment of indirect data."
142,0553418815,http://goodreads.com/user/show/7019761-cindy,4,"While I don't necessarily agree with all of the conclusions, this was a very thought-provoking book. The author is a mathematician who has worked on Wall Street and now takes on some of the math models that rule our lives. Some of these were, at least originally, developed with good intentions - intending to make fairer some of the decisions that affect our lives: where and who gets into college; what the schedule for our job looks like; how much we pay to get a loan; even how you are judged in your chosen profession. But the author poses that many of these types of decisions are actually less than fair. The models are in her terms WMDs - weapons of math destruction - they are hidden, unregulated, and virtually uncontestable, even when they are blatantly wrong. One of the more troubling examples from the book concerned the use of testing to judge teacher performance. She traced how an exemplary teacher with outstanding evaluations was judged as poor on the basis of one set of tests and fired. The teacher showed that the tests which showed that her ""value-added"" score was sub-par were flawed due to a high number of erasures on the previous year which indicated cheating. Indeed, the system said that there *might* have been cheating but they would not re-instate this excellent teacher. (And please don't get me started on ""value-added"" - my daughters' school was always excellent in all the testing but this score was just average. So if your kids are already high achievers, how do you continue to score excellent in this measure? The answer is it's almost impossible.)Other examples showed that even when you removed race or ethnicity or economics as an indicator in these WMDs, they were there. If a zip code puts you in a risky category for a student loan, then it's hard to get the education that can get you out of poverty; or if a work schedule makes it hard for a single mother to get the child care she needs so that she can get the money to go to school to make a better life for herself and her child...well, I can see how some of this sets up a self-fulfilling prophecy. It makes it harder, not impossible, to break this cycle. That's why this book is important. If this is being used, then we need to know and like credit scores have the ability to correct errors and inaccuracies that work against the average person.Quotes to remember:....This would all be wonderful for students and might enhance their college experience, if they weren't the ones paying for it in the form of student loans that would burden them for decades. We cannot place the blame for this trend entirely on the US News rankings. Our entire society has embraced not only the idea that a college education is essential, but the idea that a degree from a highly-ranked school can catapult a student into a life of power and privilege. As I write this, the entire voting population that matters lives in a handful of counties in Florida, Ohio, Nevada, and a few other swing states. Within those counties is a small number of voters whose opinions weigh in the balance. I might point out here that while many of the WMDs we've been looking at, from predatory ads to policing models, deliver most of their punishment to the struggling classes, political microtargeting harms voters of every economic class, from Manhattan to San Francisco, rich and poor alike find themselves disenfranchised, though the truly affluent of course can more than compensate for this with campaign contributions."
143,0553418815,http://goodreads.com/user/show/6894205-allie,4,"This was by far the most interesting book about math I have ever read (admittedly a low bar to set), possibly because it contains no actual math formulas. Instead, former mathematics professor and hedge fund analyst Cathy O'Neil tells stories about the applications of math - how ""ill-conceived mathematical models now micromanage the economy from advertising to prisons."" Among other things, mathematical models determine which news you see on social media, your Google search results, the cost of your auto insurance and credit card rates, which neighborhoods police are sent to patrol, and how your child's schools are evaluated. In one staggering statistic about the reach of these models, O'Neil notes that 72 percent of resumes are never seen by humans, but are instead screened and scored by software programs looking for employer keywords. While math itself is neutral, the models are based on assumptions that reflect human biases and limitations. Often, the models are designed to process data from large numbers of people efficiently, but not necessarily fairly. She distinguishes between ""good"" models built on transparent assumptions that are constantly corrected with new data (e.g., those used in professional baseball) to opaque models that use proxy data such as race or zip codes and do not have a corrective feedback mechanism (e.g., predictive policing software). Typically, there is no method of recourse when given a poor score by a computer (hello, Hal) since companies consider their algorithms proprietary and won't disclose details of how scores are developed. This was an issue for teachers evaluated - and sometimes fired - based on inaccurate models of student test scores. The book's second main argument is that the built-in biases in the models are especially putative towards the poor, who are steered to high interest loans, targeted by ""stop and frisk"" policing based on their neighborhoods, and less likely to be personally interviewed for jobs.I found many of O'Neil's arguments persuasive and somewhat alarming, although she tries to end the book on a positive note. Worth reading, even if you don't like math. Or weapons. Or destruction. "
144,0553418815,http://goodreads.com/user/show/70315594-enoughtohold,4,"A nice, clear overview of the problems of big data for laypeople. Not sure why other reviewers expected this to be deeply technical, or why they think O’Neil thinks there are easy answers — she plainly doesn’t.I for one am glad a book this accessible exists on this topic. After all, the vast majority of the people affected by these harmful models are not data scientists, and they deserve to know what’s going on.(Also, the author did a great job reading for the audiobook.)"
145,0553418815,http://goodreads.com/user/show/4430576-miri,5,"I've said it before, and I'm sure I'll say it again, because I make a point of seeking out books like this. But if there's one book I could get everyone to read right now, it would be this one. Even next to Ta-Nehisi Coates's We Were Eight Years in Power, which I'm reading concurrently and which is staggering in its importance, this book feels urgent—because what it explains is how our new economy of Big Data is codifying, concealing, and amplifying inequality of every kind, on an enormous scale. Every injustice I'm reading about in Coates's book is made worse by what I read about here. We think of math as inherently neutral, the way we think of technology and logic and science. But these things are like any tool, dependent on the person using them, capable of being used for good or evil. (Remember the way science was abused throughout American history, fabricating the entire concept of biological race to provide justification for white humans to enslave black ones.) When we start using math to make decisions instead of people, assuming that this will decrease human bias and make things more fair, what we can actually end up doing is obscuring that prejudice and magnifying the scale of its effect. Cathy O'Neil was a mathematician working in the finance industry in 2008. She left after a few years, disgusted with the way the system was clearly rigged, and has since created the concept of Weapons of Math Destruction: mathematical models that codify human prejudice, are opaque and impervious to feedback, and affect large numbers of people. In this book, O'Neil takes us through several WMDs, showing how they're used in different areas of our lives. There's the disastrous school reform attempt in Washington D.C., which tried to evaluate teachers in a way math simply cannot be used, and which fired hundreds of teachers based on results no one in the district could explain or defend. There's the way many employers now use credit scores to screen applications, assuming that making regular payments means you'll be a dependable employee, ignoring the facts that (1) responsible people experience financial difficulty too and (2) preventing people with bad credit from getting jobs is only going to make their situation worse.There are the computerized risk models that judges use in sentencing, trying to gauge from prisoners' background information—where they grew up, whether their friends and family have criminal records; information which would be inadmissible in court—how likely they are to return to prison after being released, and how long their sentence should be. There's the way U.S. News & World Report helped cause an arms race when it decided to start ranking universities on everything except their cost, driving up tuition over 500 percent (nearly four times the rate of inflation) in just the 30-ish years that I've been alive.There's political advertising which is now so individually personalized that even people who support the same candidate, even people visiting the candidate's website, will see totally different things based on the profile the campaign has created for them.There's predatory advertising like that done by for-profit colleges, which specifically targets vulnerable people's ""pain points"" looking for those who are ""stuck,"" who have ""few people in their lives who care about them,"" who have ""low self-esteem"" and are ""unable to see and plan well for the future"" (these quotes, shared by O'Neil, come from the California Attorney General's office when it was investigating one of these colleges). (And if you're wondering how they would know this about people, the point is that they know this about all of us now. Just try to imagine what someone could learn about you from seeing everything you do online.)It might seem like the logical response is to disarm these weapons, one by one. The problem is that they’re feeding on each other. Poor people are more likely to have bad credit and live in high-crime neighborhoods, surrounded by other poor people. Once the dark universe of WMDs digests that data, it showers them with predatory ads for subprime loans or for-profit schools. It sends more police to arrest them, and when they're convicted it sentences them to longer terms. This data feeds into other WMDs, which score the same people as high-risk or easy targets and proceed to block them from jobs, while jacking up their rates for mortgages, car loans, and every kind of insurance imaginable. This drives their credit rating down further, creating nothing less than a death spiral of modeling. Being poor in a world of WMDs is getting more and more dangerous and expensive.The same WMDs that abuse the poor also place the comfortable classes of society in their own marketing silos . . . For many of them it can feel as though the world is getting smarter and easier. Models highlight bargains on prosciutto and chianti, recommend a great movie on Amazon Prime, or lead them, turn by turn, to a cafe in what used to be a ""sketchy"" neighborhood. The quiet and personal nature of this targeting keeps society's winners from seeing how the very same models are destroying lives, sometimes just a few blocks away.Though we're all affected by them in ways we can't see, right now the majority of the damage is being done to the same groups who are always hurt by flaws in the system—minorities and the poor. The same models could easily be used to help people instead of prey upon them, but that won't happen in the glorious free market of capitalism. We need regulation and oversight in the data industry. We need accountability. When statistics itself, and the public’s trust in statistics, is being actively undermined by politicians across the globe, how can we possibly expect the Big Data industry to clarify rather than contribute to the noise? We can because we must. Right now, mammoth companies like Google, Amazon, and Facebook exert incredible control over society because they control the data. They reap enormous profits while somehow offloading fact-checking responsibilities to others. It’s not a coincidence that, even as he works to undermine the public’s trust in science and in scientific fact, Steve Bannon sits on the board of Cambridge Analytica, a political data company that has claimed credit for Trump’s victory while bragging about secret ""voter suppression"" campaigns. It’s part of a more general trend in which data is privately owned and privately used to private ends of profit and influence, while the public is shut out of the process and told to behave well and trust the algorithms. It’s time to start misbehaving. Algorithms are only going to become more ubiquitous in the coming years. We must demand that systems that hold algorithms accountable become ubiquitous as well."
146,0553418815,http://goodreads.com/user/show/17187084-sean-goh,4,"Very readable book warning of the downside of amoral models. With great power comes great responsibility.___Math-powered applications were based on choices made by fallible human beings. Many models encoded human prejudice, misunderstanding, and bias into the software systems that increasingly managed our lives. Like gods, these mathematical models were opaque, their workings invisible to all but the highest priests in their domain: mathematicians and computer scientists.Models are opinions embedded in mathematics.Statistical engines without feedback can continue to spin our faulty and damaging analysis while never learning from its mistakes. E.g. teachers mistaken labelled as failures are fired, and no one is the wiser. The model's output becomes unquestioned reality.The parallels between finance and Big Data: The productivity of the field's talents indicates that they are on the right track, and it translates into dollars. This leads to the fallacious conclusion that whatever they are doing to bring in more money is good. It ""adds value"". Otherwise why would the market reward it?In both cultures Wealth is no longer a means to get by. It becomes directly tied to personal worth.On the rise of university rankings: from the perspective of a university president, it's quite sad. Here they are at the summit of their careers dedicating enormous energy towards boosting performance in fifteen areas defined by a group of journalists at a second-tier magazine (U.S. News). The ranking had become destiny. When you create a model from proxies, it is far simpler for people to game it. This is because proxies are easier to manipulate than the complicated reality they represent. (e.g. trying to evaluate influence by number of twitter followers)In a system in which cheating is the norm, following the rules amounts to a handicap. So preventing the students in Zhongxiang from cheating was unfair.In our largely segregated cities, geography is a proxy for race.Patrolling particular areas creates a pernicious feedback loop. The policing itself spawns new data, which justifies more policing. Eventually the zeroing in on ghetto districts ends up criminalising poverty, believing all the while that our tools are not only scientific but fair.Instead of simply trying to eradicate crimes, police should be attempting to build relationships in the neighbourhood. This was one of the pillars of the original ""broken windows"" study. The cops were on foot, talking to people, trying to get them to uphold their own community standards. But that objective, in many cases, has been lost, steamrollered by models that equate arrests with safety.Mathematical models can sift through data to locate people who are likely to face great challenges, whether from crime, poverty, or education. It's up to society whether to use that intelligence to reject and punish them - or to reach out to them with the resources they need. It all depends on the objectives we choose.Computing systems have trouble finding digital proxies for soft skills. The relevant data simply isn't collected, and anyway it's hard to put a value to them. They're usually easier to just leave out of the model.The modelers of e-scores (unofficial credit ratings) have to make do with trying to answer the question: ""How have people like you behaved in the past?"" when ideally the question would be ""How have you behaved in the past?Even with the Affordable Care Act, which reduced the ranks of the uninsured, medical expenses remain the single biggest cause of bankruptcies in America.A sterling credit rating is not just a proxy for responsibility and smart decisions. It is also a proxy for wealth. And wealth is highly correlated with race.Mistakes made by models are learning opportunities - as long as the system receives feedback on the error. When automatic systems sift through our data to size us up for an e-score, they naturally project the past into the future. The poor are expected to remain poor forever and are treated accordingly. It's inexorable, often hidden and beyond appeal, and unfair.Yet machines cannot make adjustments for fairness by themselves. That is where the human overseer comes in.Human decision making, while often flawed, has one chief virtue. It can evolve.Big data processes codify the past. They do not invent the future. Doing that requires moral imagination, and that's something only humans can provide. We have to explicitly embed better values into our algorithms, creating Big Data models that follow our ethical lead. Sometimes that will mean putting fairness ahead of profit."
147,0553418815,http://goodreads.com/user/show/6991670-bob,4,"Summary: An insider account of the algorithms that affect our lives, from going to college, to the ads we see online, to our chances of getting a job, being arrested, getting credit and insurance.Big Data is indeed BIG. Mathematical algorithms shape who will see this post on their Facebook newsfeed. If you go to Amazon or another online bookseller, algorithms will suggest other books like this one you might be interested in. Have you seen all those ads about credit scores? They are more important than you might imagine. Algorithms used by employers and insurance companies determine your employability and insurability in part through these scores. Far more than another credit card (bad idea, by the way) or a mortgage are on the line. These algorithms seem objective, but how they are formulated, and the assumptions made in doing so mean the difference between useful tools that benefit people, and ""black boxes"" that thwart the flourishing of others, often unknown to them.Cathy O'Neil should know. A tenure track math professor, she made the jump to Wall Street and became a ""quant"" who helped develop mathematical algorithms and witnessed, in the crash of 2008, the harm some of these caused. And she began to notice how algorithms often painfully impacted the lives of many others.  She describes how a teacher was fired because of the weighting of performance scores of a single class, despite other evaluations finding her an excellent teacher (afterwards it was found that there were a high number of erasures on tests for students who would have been in her class the previous year, suggesting these had been altered to improve scores).As she looked at the algorithms responsible for such injustices, she came to dub them ""Weapons of Math Destruction"" or WMDs and she identified three characteristics of these WMDs:Opacity: those whose lives are affected by them have no idea of the factors and weighting of those factors that contributed to their ""score"".Scale: how widely an algorithm is applied across industries and sectors of life can affect how much of one's life is touched by a single formula. For example, the FICO scores mentioned above affect not only credit, but the ability to get a job, the cost of auto insurance, and your ability to rent an apartment.Damage: WMDs can reinforce other factors perpetuating a cycle of poverty, or incarceration.She also shows that what makes these algorithms destructive is the use of proxy measurements. For example an employer may not know directly how savvy someone is as a marketer, and so they use a ""proxy"" measurement of how many Twitter followers that person has. Or age is used as a proxy for how safe a driver one is. For a group, the proxy may work well, and be utterly inaccurate for an individual that falls within that proxy group.Then in successive chapters, she chronicles some of the ways WMDs operate in different parts of life. She discusses the U.S. News & World Report college rankings, and the use of algorithms in admissions processes. Social media uses algorithms to target advertising, which means some will see ads for for-profit schools and payday lenders, and others for upscale furnishing or Viagra, based on clicks, likes, searches, and comments. Policing strategies, including locations for intensified ""stop and frisk"" policing are shaped by another set of algorithms. Algorithms to filter resumes may use scoring algorithms that discriminate by address and psych exam algorithms may render others unemployable in a certain industry. Scheduling algorithms may promote efficiency at the expense of the ability of workers to sleep on a regular schedule, or arrange childcare, or work enough hours to qualify for health insurance. Algorithms sometimes shut people out from credit or low cost insurance when in fact they are good risks. She concludes by showing how algorithms determine ads and news we see (and don't see). In an afterword she explores the flaws in algorithms revealed on the election of Donald Trump (algorithms, for example predicted Clinton would easily win Michigan and Wisconsin, where consequently she did not campaign, and lost by small margins).In her conclusion, she makes the case not only for a code of ethics for mathematicians but also argues that regulation and audits of these algorithms are necessary. The value assumptions, as well as the mathematical methods of many algorithms are flawed, and yet opacity means those whose lives are most affected don't even know what hit them.She helps us see both the sinister and useful side of these algorithms. They may reveal where a pro-active intervention may save a family from descending into family violence, or provide extra assistance to a child in danger of falling behind in a key subject. Or they may be used to invade personal rights, or even to perpetuate socio-economic divides in a society. The reality is that the problem is not the math but the old GIGO problem (garbage in, garbage out). The values and assumptions of the humans who devise the formulas and weightings of values and the use of proxies determine what may be destructive outcomes for some people. Yet it can be hidden behind an app, a program, an algorithm.The massive explosion in storage capacities, processing speeds, and the way our interests, health status, travel patterns, spending patterns, fitness, diet and sleep habits, our political inclinations and more may be tracked via our online and smartphone usage makes O'Neil's warning an urgent one. We create mountains of data that may be increasingly mined by government and private interests. Perhaps as important as asking whether this will be governed in ways that contribute to our flourishing, is whether we will be alert enough to care.____________________________Disclosure of Material Connection: I received this book free from the publisher. I was not required to write a positive review. The opinions I have expressed are my own."
148,0553418815,http://goodreads.com/user/show/51162416-max,5,fast pleasant primer on what’s wrong with big data. Fun. I love using a VPN turning everything off and being practically nobody to these freaks 
149,0553418815,http://goodreads.com/user/show/4061006-angela,5,"Cathy O'Neil is one of my heroes; I love listening to her on Slate Money, and her less pop/more technical book, Doing Data Science, occupies a prominent place on my desk at work - and a special place in my heart. Going beyond the usual data science manuals of what a random forest is, or how to account for large, sparse data, O'Neil and her coauthor - in that book - make an almost anthropological survey of the data science ""field""/profession/whatever. It's enlightening, and it's very real; less a textbook, more a professional guide.So if Doing Data Science is the ""how to do your job"" manual, and Slate Money is the occasional soapbox, Weapons of Math Destruction is the manifesto. With clarity, conviction and intelligence, Cathy O'Neil scalpels away at the utopian, misty-eyed tech worship that has contributed to the data science/big data bubble. She identifies the ways in which algorithms encode and perpetuate existing prejudices, and are hidden from plain sight by the obscuring (both intentional and not) use of Fancy Math.She notes how anti-human ""market efficiency"" is, and how it's often been short-hand for unjust and unequal systems, and big data is just scaling those dumb systems way, way up. For example, the use of social network data to ""enrich"" lending platforms; or recidivism models that ask about where a former convict grew up, whether his/her friends are in jail, how many encounters with the police he/she has had. These algorithms - because they chase correlations to make predictions (rather than identifying causal factors) - create big-data-driven poverty traps. They are also beyond the purview of the government (and, given the new administration, will probably be for quite some time): just like your FICO score is ONLY about how often you pay your credit card bills, and NOT your demographic group, so too should these algorithms be scrutinized and regulated.Increasingly, O'Neil notes, the ""human touch"" comes at a price; in upper income groups, you get more real, live service - hearing about a job through connections, informational interviews with alumni, blah - while lower income groups are serviced by increasingly mechanized, machine learning code. The hiring and HR practices of CVS, McDonald's, and other low wage places were especially striking (and severe): mindless, well-meaning algorithms deployed to effectively punish workers whose BMIs (itself a stupid measure of health) are too high, or who seem ""anti-social"" according to a creepy questionnaire, or whatever. I think my favorite chapter - and, of course, the most relevant one - was about the toxic brew that can be politics and data science. It was especially powerful to think through the way that, as opposed to the past, where we had ""agreed upon facts"" given by carpet bombing-style hegemonic culture (e.g. four news channels that everyone watched), now we have hyper-tailored DARE I SAY ""alternative facts"" where, by the power of marketers (who prey on confirmation bias), your neighbor's political beliefs are the result of a very specific and private relationship between them, their web browser cookies, and some political marketer. This means you may have NO IDEA why your swingy neighbor in swingy state X thinks Candidate Y is running an underground child sex ring literally under the ground of a DC pizza shop. O'Neil (and this book was written Before The Madness) notes that these sorts of micro-targeted ads that follow you around the web are very smartly tailored; the marketers know who would be repulsed by such a stupid conspiracy theory, and who would be intrigued. And that's just terrifying. Anywayyyyy. From a data perspective, my (cold dead) economist heart reminds me that OF COURSE we're going to scale up injustice when we mindlessly maximize profit, instead of humans. We need to go back to our roots: to the philosophy and debates of Jeremy Bentham, and welfare economics, and what ""utility"" really means. Efficiency is not the end all, be all, despite what our current economic system seems to diktat. O'Neil also makes a good point about the repeated erroneous conflating of correlation and causation; something even the fanciest of linear algebra stochastic process meddlers fall prey to. That's something that I see a LOT in data science: the field is built on the premise of perfect prediction - and prediction doesn't need causation, it just needs correlation. OK, I will step off my soapbox. Highly recommended.edited to add: OMG and I just read her delightful 2017 resolutions post, and had much LOLs. Cathy O'Neil, you da best."
150,0553418815,http://goodreads.com/user/show/8325737-philipp,4,"This is an angry book, but its got a point! She summarises many 'weapons of math destruction', WMDs, algorithms that encode human biases, are opaque, which give recommendations or orders that no-one questions, while making things worse. For example, this summary:Poor people are more likely to have bad credit and live in high-crime neighborhoods, surrounded by other poor people. Once the dark universe of WMDs digests that data, it showers them with predatory ads for subprime loans or for-profit schools. It sends more police to arrest them, and when they’re convicted it sentences them to longer terms. This data feeds into other WMDs, which score the same people as high risks or easy targets and proceed to block them from jobs, while jacking up their rates for mortgages, car loans, and every kind of insurance imaginable. This drives their credit rating down further, creating nothing less than a death spiral of modeling. Being poor in a world of WMDs is getting more and more dangerous and expensive.O'Neil goes through many, many of examples like this where an automated approach made things worse, where people didn't bother to critically look at the output, where the model doesn't have a way to incorporate feedback, it's stuck in time and cannot evolve like humans. The majority of the book is spent on case studies and great, angry discussion of what went wrong. There's one short chapter on what could be done - mostly she proposes more regulation (auditing of algorithms), but also to 'dumb' down models by law, force them to treat people equally and not bin them in groups. That chapter on solutions is extremely short - I expect a second book from her on that topic.P.S.: I've been reading a lot about adversarial perturbations in deep learning (see this great paper). One idea could be to counteract some of those models to become an adversarial entity. For example, you have one of those shopping points cards. Instead of feeding their model with your shopping data, you mix it up - write down your shopping list for a month, shuffle it, and buy one part with your points card and the other in cash. Keep mixing it up and add random stuff. Your data should be garbage to the model as it's not consistent, easily removable as an outlier, i.e., worthless.You could also make the model work for you - find out which groups of customers the market sends special offers to, game your shopping so that you fall into that group, reap rewards.Of course you'd need to know in detail what the model is looking for, which is highly unlikely."
151,0553418815,http://goodreads.com/user/show/115473-siria,4,"In the two years since Cathy O'Neil published this study of the danger posed by mathematical models—how their supposed neutrality merely reflects the biases of those who create them while creating closed-system feedback loops that warp society in unjust ways—it hasn't become any less relevant. In fact, not only does the book remain relevant, but subsequent revelations—most prominently those about Facebook's involvement with the 2016 US elections—show that O'Neil may even have been too cautious in what is a trenchant and prescient critique. "
152,0553418815,http://goodreads.com/user/show/25178051-izzat-radzi,5,"""Mathematical models are supposed to be our tools, not our masters"" Statisticians, quants, actuaries : take heed. This book argues that the new 'industrial revolution' of big data is bringing destruction. To whom (?) , one might be tempted to asked. Humans, normal masses who sometimes makes errors of course.And due to that, they got red in their ledgers, which in these new century means, that they'll most likely subjected to the 'vicious loop' of rejection and manipulation, and will felt the usual 'life is not fair' feeling. To start off, I claim that this book is the best (so far) compliment to Taleb's Black Swan book. Whilst Taleb as arguably he as a trader and academia revolves his discussion around some financial models & it's discontent (whilst in most part slanted his discussion into the philosophical realm); this author meanwhile goes into a more broad discussion. She, whom has background in chronologically in academia, hedge funds, a short stint in risk management before focusing work as a data scientist, argues that given the presence of these Gargantuan Big Data in almost every aspect of life, it has tarnished if not destroyed people's life. The problem-before I proceed case by case- lies in two reasons. First is the data collected & recorded by the data mining by corporate companies, mostly are left uncrunched, meaning they're not subjected to updated information about those people data taken or they're mistakenly (yet significantly affect) people who are thought 'one', in this case people with the same name and birthday. This, argued by the author, because the model used isn't not tinkered time to time according to feedback; unlike those used in baseball & basketball where opponents and managers subject each individual players to new & paternalistic habits of players -thus a better use of correlation to causation-, they are taken as face value. And feedback that is not taken -later I'll mentioned in job application survey- left those aspiring people hoping to get some good news, baffled as they themselves don't know why their application was rejected. Turns out, it was their 'behaviour', recorded in the subjective and highly interpretable application questions. The second problem here, if not wrong should I put it, is that the usage of the data is, as author's claimed, taken as God's word, the highest possible theological blasphemy, if said to those in denial data miners. This is due to the reasons that they basically use the data available, let it be credit scores, health scores, SAT (or generally educational) scores, and even resume applications -to name a few- as the bestowed revelation, thus dismissing proven good teachers, potential shining employees and laying off 'unhealthy' current workers; all based on the Big Data, either snooping privately on their life, or in other cases, a take-it-or-leave option wether they'll provide their credit score to get an insurance or their health progress for work employment group insurance practiced in firms & companies. The dismissal action has become so autonomous directly from the data, that frightenedly, that one does not care of others, it's just life. Coincidentally, I read Adorno's Culture Industry directly before this, and it does personally provides a more enlightened discourse on the systemic issue. I personally first came across to this topic back a few years ago, that with this newly published book kindles my interest. The first was back around late 2013, where I was taking a UK driving licence. In the course, similarly argued by the author, you'll be getting some discount given that you choose to put a 'Black Box' in your car. This instrument will record your driving habits & feed back to the insurance company. Although initially used to monitor those big truckers -who tends to be sleep deprived due to late night & long driving- to reduce the risks of death of others due to reckless driving; here argued that it's afraid to be used by others to manipulate the privacy of car drivers, at other instances flawed due to normal human behaviour, taking short cuts at night through 'dangerous neighbourhood' thus ended up one marked to be subjected to 'handcuffed' by auto insurance companies. The second one I encountered was in early 2015, in a student organised event to explain & showcase the structure & function of Big Data by companies. At that time, I was furious given the fact of the the then environment of whistle-blowers expose on companies manipulating & snooping normal civilians life. In here meanwhile, as argued in The Impulse Society by Paul Roberts, politicians backed by big companies are microtargeting voters up to the point of what they read and hear are tailor made. Given to the rise of fascism and 'fear of the normal neighbour' one known differently pre-election in America which now turns into something totally unexpected, I humbly agreed to some point. The third encounter is with regard to behavioural test subjected to work or scholarship applications. Exposed initially by my parents, whom my dad was at times hired to be an interviewer. He's the first to enlighten me on the presence of attitude test taken by applicants, which is imperative for career climbing ambitions. Here written an illustration. An applicant answered promptly the normal application survey on personal attitudes. Obviously one is subjected to child education for example that one is unique bla3; yet answering unique now instead of other answers) will be perceived as narcissist by the possible employer, which now is not possible anymore. Unfortunately, this data, is shared and this unfortunate applicant will ended up in this continuous 'dangerous loop'. The author coined Recidivism in regard to this loop. The poor guy, living in a certain 'bad' neighbourhood, want to apply a credit card, or just simply living as a normal civilian. Now, dues to this, of course one might have some background of knowing some in-behind bars people. This data, used by the security enforcement, will cause the police to routinely knocks on doors to tell them they have eyes on them. A more idiosyncratic example is those trying to move houses. Property firms with big data on their very own hand, easily exclude those with a bad tag, even when they've changed, or for unfortunate some, mistakenly recorded in. As said before, no feedback can be given by the victims, unless one has some wealth to pursue on a legal lawsuit on this issue, which of course deteriorate middle class let alone the poor condition. Obviously the rich bankers and markets manipulators escaped from this loop. As a matter of fact, they're invulnerable and invincible. She suggested a few suggestions in the concluding chapter. First for example, oath taking by those finance practitioners, whom impact upon their ignorance is highly significant on the whole. Nevertheless she noted the downside that when in work bureaucratic structure, manager will want answers for certain issues, this will put those people in corners. Second is, as Taleb suggested, ditching those backward minded model. Obviously, she argued, one have to be specialised in the field to help recognise and build a better one. She illustrated a new model on preventing child abuse. There's similarity between the model to 'possible criminals' as in the Minority Report movie in reducing - if not preventing- crime she criticised, the hidden outcomes has not - at least she didn't yet- been analysed. The third is the involvement of human judgement into the model. Yes, initially the algorithm of the system is used to remove human bias, yet seeing how things turned out, the machine as expected cannot understand how human (cognitive) work. The problem of language and rational judgement will always triumph human above machine, I claimed this ambiguous argument. Hats off to this book!"
153,0553418815,http://goodreads.com/user/show/11951948-paul,3,"One of the funniest bits of Little Britain was where David Walliams as Carol Beer would type a request into the computer, before turning to the customer and saying ‘Computer says no’… Whilst it is funny, it is not so funny when it happens to you. In this book, O’Neil, a former Wall Street quant raises alarm bells on the way that these mathematical models have infiltrated our lives. We don’t see them, but these algorithms that help us with our searches online and finding books, films and other items on online sites are now being used to determine just how much of a risk you are. Next time you want a loan, to renew insurance or just need to get another job O’Neil thinks that some of us may have a problem.She calls them ‘Weapons of Maths Destruction’; these are incontestable, unregulated and opaque algorithms. They are being used by companies to decide the tiniest details. They are used because they make larger profits for corporations and most worryingly for us is that they are frequently conclude the wrong thing having made incorrect assumptions about individuals. As the saying goes ‘crap in; crap out’… Having worried the life out of the reader, O’Neil goes onto suggest a variety of things that could help; more regulation, better design of the code and us being aware of their use. The writing is clear, if a little dry and technical at times. The examples are a little American centric, but you can see the way that it is going in the UK. Even though the title mentions the dreaded word maths, it really isn’t that mathematical. Worthwhile reading."
154,0553418815,http://goodreads.com/user/show/16212136-peter-pete-mcloughlin,4,"Very depressing book on how computer algorithms are becoming a powerful tool of the exploitation of workers, the poor, minorities and ordinary people mostly in the service of profits. It goes deep and the problem is much worse than you think. This book is a downer as it covers relevant developments in politics and business and that always is not pleasant to think about in these times in general and are especially dispiriting in how they affect ordinary people."
155,0553418815,http://goodreads.com/user/show/5550611-hallie,2,"Just ok. I like the idea more than the execution of this book. While O'Neil uncovers a great, dark place in our society that needs more light shone on it, much of her conclusions she gives away in the introduction to the book. This would have been an excellent piece in the New Yorker or Atlantic but as a book began to feel redundant far too early. "
156,0553418815,http://goodreads.com/user/show/11345492-kate,4,"3.5, bumped up for likeable clarity-without-condescension and consistent up-front political engagement."
157,0553418815,http://goodreads.com/user/show/761281-mike,4,"Reading this book I could not help but wish the author had used a quote from Paul Goodman as the epigraph of the book: ""Technology is a branch of moral philosophy, not of science."" (Every technology ultimately is making a value judgement about what is important or worth doing, and what aspects of a task or reality are important.)This book is a fascinating look at how 'big data' is used and abused by colleges, banks, insurance agencies, in sentencing and parole decisions, by employers screening applicants, and more. Collecting vast amounts of data and using to guide decisions can be a very useful technology, but the author -- who herself had a career as a 'quant' or data analyst for a large firm -- points out the downside of basing decisions on data and algorithms. Of course it should be obvious, but our society's information fetishism can make us overlook that the data we collect and the algorithms we use to analyze it can be deeply flawed by omissions, prejudices, fallacies that the designers may not be aware of. A couple of the many examples she sues help illustrate this. We read about a school teacher whose performance is measured by an algorithm that takes into account his students' past performance, current performance on standardized tests, and various other factors. One year he is scored 6/100, and the next year 94/100 -- after making no changes to his syllabus, assignments, or anything else he was doing. Anyone with common sense can see there is something wrong with the algorithm if someone could get such wildly varying scores, but the scoring algorithm is a 'black box' that not event the school administrators understand. Among the many problems with the system here is that there is no allowance for the designers of the algorithm to get feedback on the scoring and adjust the model -- instead, the 'failing' teachers are fired and the algorithm rolls on. Contrast this with the use of 'big data' algorithms in commercial enterprises: when Amazon.com collects data on your browsing and purchasing habits, they use this recommendations to you for future purchases. If their models are bad, they suggest things you don't want, and they don't increase sales. But they take this feedback and change their algorithms, because there is a financial incentive to do so.So a huge blind spot in some of the models and algorithms policy makes use is the lack of feedback. Another is huge downside of the rise of big data modeling is that private companies can use opaque models to discriminate in ways that punish the poor particularly. The author explains how car insurance companies use data about where their customers live, drive, and work to assess risks -- which is largely legitimate -- but also abuse this information to determine which customers are likely to be able to shop around for better deals, and which are less likely to have the time, resources, or access to information to do so, and they are charged much more. Indeed affluent customers with DUI convictions may be better rates than safe drivers who live in areas with limited internet access and higher poverty. Likewise employers may screen applicants with big data algorithms that identify 'risks' which rule out the poor, people with past mental illness, or immigrants. Many other problems are discussed, and the final chapters look at how political parties have begun to utilize big data too -- an ominous prospect in light of the rest of the book.The author offers some solutions, ranging from the admittedly ineffective (a code of ethics for 'quants' who create the models) to the more daunting but effective measure of demanding transparency and regulation (which has made some progress in the insurance industry, but is far from complete).*Full disclosure -- I read a free advanced reader's copy that I won in a Goodreads giveaway.*"
158,0553418815,http://goodreads.com/user/show/3502690-graeme-roberts,2," Cathy O'Neil writes well and provides much good information, but I found the fundamental thesis of  Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy to be foolish and irresponsible. The title, condensed inevitably to WMD, is used promiscuously to describe any application of data science, statistics, or even information technology that she considers to be unfair or discriminatory. In most cases, she is right; some applications are used by the greedy and the uncaring with insufficient thought of the consequences, but she rarely considers the positive attributes of the applications. Once branded as a WMD, a term indelibly attached to the evil trifecta of George W. Bush, Saddam Hussein, and the Iraq War, how are readers meant to consider their many positive aspects and what can be done to make them better?At first, I thought that Ms. O'Neil had been the victim of a foolish PR or marketing person in her publishing company, who had come up with this catchy name, but her ardent use of the term convinced me that she had invented it.Ms. O'Neil seemed to follow a downward trajectory from her Harvard PhD in mathematics and a teaching position at Barnard to a leading hedge fund and a series of other morally disappointing positions that left her convinced that data science, and even mathematics, are roots of much evil. I don't buy it. They are tools that can be used or misused, and her diatribe does nothing to help.She asserts, on page 210:If we find (as studies have already shown) that the recidivism models codify prejudice and penalize the poor, then it's time to have a look at the inputs. In this case, they include loads of birds-of-a-feather connections. They predict an individual's behavior on the basis of the people he knows, his job, and his credit rating—details that would be inadmissible in court. The fairness fix is to throw out that data.But wait, many would say. Are we going to sacrifice the accuracy of the model for fairness? Do we have to dumb down our algorithms?In some cases, yes. If we're going to be equal before the law, or be treated equally as voters, we cannot stand for systems that drop us into different castes and treat us differently.I disagree strongly. Some data is better than none, though we should be looking to constantly improve our data science systems. I would like to know that the recently paroled criminal moving in next door had been released by a judge in full possession of whatever facts are available. So, I suspect, would Ms. O'Neil."
159,0553418815,http://goodreads.com/user/show/19993-robin,4,"If you've been listening to Cathy O'Neil on Slate Money for some time, you probably expected this book to emerge in exactly this form and with exactly this content. She's smart, progressive, focused, and clear, just as she was on the podcast. I saw her speak at the Mechanic's Institute Library a few months ago, before I'd read the book, and was very impressed by her ability to be articulate about complex situations with a decisiveness that belies their complexity, or at least certainly never allows for it as an excuse. The argument I found most challenging was (heavily paraphrasing here) that we should be privileging fairness because it is otherwise so easy to be blind to the unfairness that creeps in when we pursue other goals. If I'm honest with my own tendencies, I recognize my longing for accuracy might indeed cause me to overlook fairness, and I resist the idea that I need to choose one as a priority over the other. Nevertheless, O'Neil convinces me to question assumed neutrality much more closely, and be much more vigilant for it in my own industry. As she points out, it's easy for people who write the algorithms to absolve themselves of responsibility for how they are used, but if we pay attention to the real outcomes we may find that we need to do more to train our algorithms to learn from externalities and flaws. Focusing ruthlessly on measuring only what you are trying to measure and ignoring all other factors is a good start; evaluating the results to make sure that's actually what happened is as necessary a practice. The book is full of examples of failed algorithms in wide use, actively and detrimentally affecting at-risk populations today, as well as potential future areas where algorithms could be used, perhaps even inadvertently, to consolidate and perpetuate the power of the already powerful. The free market will not solve this problem. This is an excellent example of where good regulation is imperative. As constituents with the ability to influence legislators to put their attention here, we can demand more transparency, more awareness and education, and a more good, old-fashioned compassion.Further reading:https://www.ted.com/talks/joy_buolamw...https://www.propublica.org/article/ma...https://standards.ieee.org/develop/pr..."
160,0553418815,http://goodreads.com/user/show/7139041-dorin,2,"Disappointing - such a rich topic, yet the author decided to be racist, Democrat and to induce panic instead of having a cool headed approach. She ignores her own advice, contradicts herself time and again, and she makes her Democrat leaning so obvious it makes me puke.The book could have contained real data, real information, but instead she goes on to explain that statistics are bad because statistics don't catch statistical errors. She pretends she's a professional in data processing, but she sounds like she doesn't understand how statistics work, and she can barely go beyond her mathematical training to understand what went wrong from a human point of view.My point: „a machine is as useful as the person who uses it”. Her point: ""big data is bad unless it's used by the Democrats”."
161,0553418815,http://goodreads.com/user/show/7837611-greg-fanoe,2,"I think that on the whole reading this book will do people more good than harm but I had several problems with it:1. The only section which I have personal expertise on is the section on ""insurance"" - I am an actuary - and I can personally verify that this chapter was filled with inaccurate, misleading, and/or totally speculative statements.2. The correlation between credit score and income is nowhere near as strong as she makes it appear in this book.3. Many of the problems she identifies have nothing to do with the models in question. Advertising models are not the issue with the payday loan or for-profit college industry those industries need to be totally upended/destroyed at the source. Letting them operate as is but kneecapping their ability to advertise would be nonsensical.4. The solutions she presents are ridiculously vague and/or impractical."
162,0553418815,http://goodreads.com/user/show/1045774-mehrsa,5,This is the book I was waiting to read! The rejoinder to our data-obsessed culture. We got drunk on algorithms and forgot to focus on their limitations. are they asking the right questions? are they recycling inequality? Great book
163,0553418815,http://goodreads.com/user/show/3126915-imi,2,"Sadly, a rather superficial look at big data, algorithms and how they've impacted on society. You probably know/have guessed all this already. The correlations O'Neil point out are not very convincing, and it felt like she just wanted to rant about how unfair the world is. Not that I disagree with that conclusion; society is unfair and unequal. But O'Neil really doesn't state much more than that, and, worse, some of her examples felt misleading or simply speculative. Where was her evidence? And if she'd explored this issues in more depth I'd have wanted clear ideas for resolving these issues. Instead, any suggestions she made seemed vague and impractical. I admitedly feel out of my depth with this topic, but am most definitely willing to learn more and expand my understanding about it. Disappointingly, this book didn't offer that."
164,0553418815,http://goodreads.com/user/show/777176-dave,5,"I learned more from this book than I have from any book I've read in years. O'Neil is a former math professor and spent time as a hedge-fund quant, so she understands the theory and experience of the effect of out-of-control algorithms on our society.As corporations make more and more efficient use of algorithms, they are starting to show dangerous tendencies, according to O'Neil. She calls them ""weapons of math destruction"" or WMDs. WMDs present the following grave problems for society:--They usually lack transparency, such that it is difficult or impossible to know what goes into them and what factors sway or change their results. When they are opaque, WMDs are patently unfair (O'Neil cites the algorithms used to evaluate teachers in many districts, highlighting their absurdity as they attempt to quantify the unquantifiable).--They make broad generalizations and reduce human beings to ""buckets."" One of the worst examples is the criminal justice system, which is increasingly reliant on algorithms to deploy police patrols and even to sentence criminal defendants. These algorithms punish the innocent by assuming that those who live in poor neighborhoods are necessarily more likely to commit crimes, thereby subjecting them to frequent ""stop and frisk"" type searches that have been found to be unconstitutional repeatedly.--They can use information in an unfair way that tends to perpetuate further injustice. A prime example of this is the increasing tendency to identify individuals by their credit score, on job applications and even in online shopping. When job applicants are screened by credit score, those who are already poor (and therefore much more likely to have a low credit score already) will find it even harder to get a job and thereby earn enough money to help improve their credit score. This self-perpetuating cycle of poverty is completely unnecessary, because credit scores are an exceedingly poor proxy for job-worthiness of applicants.--Many WMDs are not very good at learning. They are created as models of human behavior (with all the flaws and assumptions of the humans who programmed them), and when they miss (when for example they screen out a potential hire who would have actually been a first-rate employee), they never learn that they have missed. The ""false positive"" never presents itself (the rejected applicant goes on to success elsewhere), and the oblivious WMD continues to chug along with its assumptions and blind spots unchallenged.And WMDs are like racism:""Racism, at the individual level, can be seen as a predictive model whirring away in billions of human minds around the world. It is built from faulty, incomplete, or generalized data. Whether it comes from experience or hearsay, the data indicates that certain types of people have behaved badly. That generates a binary prediction that all people of that race will behave that same way....Racists don't spend a lot of time hunting down reliable data to train their twisted models. And once their model morphs into a belief, it becomes hardwired. It generates poisonous assumptions, yet rarely tests them, settling instead for data that seems to confirm and fortify them. Consequently, racism is the most slovenly of predictive models. It is powered by haphazard data gathering and spurious correlations, reinforced by institutional inequities, and polluted by confirmation bias. In this way, oddly enough, racism operates like many of the WMDs I'll be describing in this book."" (p. 22-3)And finally:""Models like this will abound in coming years....Many of these models, like some of the WMDs we've discussed, will arrive with the best intentions. But they must also deliver transparency, disclosing the input data they're using as well as the results of their targeting. And they must be open to audits. These are powerful engines, after all. We must keep our eyes on them.... Data is not going away....Predictive models are, increasingly, the tools we will be relying on to run our institutions, deploy our resources, and manage our lives. But...these models are constructed not just from data but from the choices we make about which data to pay attention to--and which to leave out. Those choices are not just about logistics, profits, and efficiency. They are fundamentally moral. If we back away from them and treat mathematical models as a neutral and inevitable force, like the weather or the tides, we abdicate our responsibility....We must come together to police these WMDs, to tame and disarm them."" (p.218)"
165,0553418815,http://goodreads.com/user/show/3029658-doug-cornelius,5,"With big data, comes formulas to parse through the data trying to make sense of it. Those algorithms can help make sense of the data and help filter through the noise to find trends. But those algorithms can also be easily misused and have harmful, if unintended consequences. Cathy O'Neil explores these problems in 
Weapons of Math Destruction
.Ms. O'Neil is a former academic, hedge fund quant, and data scientist for startups. At the hedge fund she was looking at ways to make money by predicting financial trends. As a data scientist she was looking to predict people's purchases and browsing habits to monetize those habits. Those are good algorithms where the people using them understand them and update the algorithms as they see problems and can check to see if they are working properly by verifying outcomes.Many data driven outcomes fail. Those are the focus of 
Weapons of Math Destruction
.I'm a big fan of the Slate Money podcast with Cathy O'Neil, Felix Salmon, and Jordan Weissmann. When the publisher offered me a review copy of Ms. O'Neil's book, I jumped at the chance.It's a short book and even though the title makes it sound like it's full of math, it's not. It's more about social policy. She points to the danger of decision makers blindingly following algorithmic output that they do not understand.Take the US News and World Report listing of best colleges. This is preeminent guide for high school students trying to figure out which college to attend. The rankings do not look at actual student outcomes. The publisher does not look at happiness, learning or improvements. It looks at proxies for the outcomes like SAT scores, alumni contributions, and graduation percentages. Colleges started making decisions to improve their rankings in the algorithm by improving those measured proxies. As anyone writing checks for their college bound children, affordability is not one of the measured proxies.As you might expect there are big chunks of the book dedicated to failings in the financial sector by relying on poorly designed algorithms. The biggest problems often being false assumptions plugged into the data running through the algorithm.With the election season upon us, the chapter on political algorithms is fascinating. The data scientists of commerce were brought into political campaigns. They are able to micro-target potential voters sending different messages to different groups highlighting the positions that would make the candidate more favorable to that segment of the population. Civic engagement and the political process is increasingly being algorithm driven.Since our world is increasingly being driven by big data, it's important to understand what is happening behind the decision-making. Weapons of Math Destruction is an excellent tool to help you understand data driven decision-making."
166,0553418815,http://goodreads.com/user/show/23020614-germ-n,3,"A fascinating must-read guide on how to navigate and interpret a world increasingly ruled by materialistic prejudices and oversimplifications that are repackaged as ""science"" and ""tech"" through poorly designed computer algorithms.Reading this gives one the feeling that the ""evil androids ruling over mankind"" dystopia has already been unrolling. Except in a very sneaky way: this time we don't get to face or even understand the robots and their actions which are nonetheless already reshaping our reality and the course of our lives through ridiculous university rankings, credit scores, social media ideological bubbles, etc.Unfortunately O'Neil ruins an otherwise brilliant book by an extreme leftist bias that seeks to absolve her personal heroes in the US democratic party and their political agendas, and attempts to attribute most blame on a specific other side. I assume this is why a book which raises such important points gets to be published and become mainstream after all.For example, she portrays Obama as a saint who just wants the best for immigrants and fiercely battles the evil rich. Yet for any unbiased observer, it's quite clear that Obama and democrats are just serving the wealthy who finance their own political campaigns (duh) when they set to ruin nations overseas and bring in thousands of immigrants, ideally illegal ones, so the price of labor can go down and corporations can continue to reap political advantages.This skewed perspective is damaging to the bottom argument and distracts readers from what's really the main issue here: the constant loss of truth and distortion of increasingly astroturfed public debate. No attempts are made, for example, at addressing how these WMDs are influencing the toxic foreign policy of the US and the press - another entity that passes as a saint in this book, while being the number one responsible for war and destruction of nations around the world."
167,0553418815,http://goodreads.com/user/show/20465079-tam,4,"A quick and highly useful read - I would recommend it to all people not just ones working with predictive models and Big Data.The title is sensational enough, but at one point I put that aside and decided to read this book. So many have been praising Big Data and Machine Learning, few cautions have been put forward but not enough, mostly dealing with ethics, security, personal data issues. This book, on the other hand, discusses the large scale tragedies that Big Data can invoke on societies as a whole, if continuing to go unchecked. 10 chapters deal with examples about the misuses of data and predictive models in many aspects of lives, especially the lives of the most unfortunate.O'Neil has a strong opinion and at certain point I thought she might be too pessimistic (saying that from a self-proclaimed pessimistic person as I am), and O'Neil freely claims how angry she is with the whole thing. Yet at the very end of the book, she makes constructive comments, ideas for improvement, for regulations. Those are the most important, and perhaps the most useful passages for data scientists. I've been quite fascinated by the tool and sometimes forget the darker side that the tool can invoke, and I'd like to thank O'Neil for the reminder, for the clarification about issues that sometimes I feel uneasy but not so clear about. No, the tool is neither inherently good nor bad, the people who make them and wield them can choose do what they want. And we can choose to do good things.Ehh, not so optimistic though, there are tons of challenges and... But I should shut my mouth. But at least we should try."
168,0553418815,http://goodreads.com/user/show/15409024-ahalya-vijay,5,"I found this to be a fascinating read on the importance of using technology and data in an ethical way. I was shocked to learn how pervasive irresponsible uses of data are throughout the developed world and how widespread it is for people to place their faith in ""black box"" models as Cathy O'Neil refers to them. It must be frustrating, for example, for teachers to be evaluated (and fired!) based on a model they have no knowledge of, without knowing how to improve their rating. And it must be even more so if they are getting anecdotally strong and positive feedback from school leadership and parents.Another shock was how liberally companies and 'data scientists' use proxies without thought to the consequences of the model not capturing the 'error term' sufficiently. The danger of making decisions based on things that are modeled (or estimated) using proxies was drilled into my head throughout my entire schooling. How would you feel if the fact that you were late to one meeting at work was used to brand you as an irresponsible or unreliable worker? Ms O'Neil made a clear link between such models being used to trap people in a cycle of poverty, using example after example across a variety of industries (e.g., education, finance, insurance). It is clear that reform is needed, but what left me worried at the end is that these models are not hurting the rich and famous, but the poor and under-resourced. Will our current political and justice system listen to their needs? I'm skeptical."
169,0553418815,http://goodreads.com/user/show/48433695-alexis-tremblay,5,"A must read for all data scientist, mathematicians, modellers, statisticians,... There is a dark side to big data and she exposes it masterfully. I always had an uneasy feeling when I started my career as an actuary and later on as data scientist. My moral compass was always off but I didn't have the words for it. Now she has given me a framework to think about my job and what to look out for. Will read again. "
170,0553418815,http://goodreads.com/user/show/16204614-bing-gordon,4,"Good overview of modern analytics, but too much of a guilt trip laid on top. "
171,0553418815,http://goodreads.com/user/show/6100646-brian-clegg,4,"As a poacher-turned-gamekeeper of the big data world, Cathy O'Neil is ideally placed to take us on a voyage of horrible discovery into the world of systems making decisions based on big data that can have a negative influence on lives - what she refers to as 'Weapons of Math Destruction' or WMDs. After working as a 'quant' in a hedge fund and on big data crunching systems for startups, she has developed a horror for the misuse of the technology and sets out to show us how unfair it can be.It's not that O'Neil is against big data per se. She points out examples where it can be useful and effective - but this requires the systems to be transparent and to be capable of learning from their mistakes. In the examples we discover, from systems that rate school teachers to those that decide whether or not to issue a payday loan, the system is opaque, secretive and based on a set of rules that aren't tested against reality and regularly updated to produce a fair outcome.The teacher grading system is probably the most dramatically inaccurate example, where the system is trying to measure how well a teacher has performed, based on data that only has a very vague link to actual outcomes - so, for instance, O'Neil tells of a teacher who scored 6% one year and 96% the next year for doing the same job. The factors being measured are almost entirely outside the teacher's control with no linkage to performance and the interpretation of the data is simply garbage.Other systems, such as those used to rank universities, are ruthlessly gamed by the participants, making them far more about how good an organisation is at coming up with the right answers to metrics than it is to the quality of that organisation. And all of us will come across targeted advertising and social media messages/search results prioritised according to secret algorithms which we know nothing about and that attempt to control our behaviour.For O'Neil, the worst aspects of big data misuse are where a system - perhaps with the best intentions - ends up penalising people for being poor of being from certain ethnic backgrounds. This is often a result of an indirect piece of data - for instance the place they live might have implications on their financial state or ethnicity. She vividly portrays the way that systems dealing with everything from police presence in an area to fixing insurance premiums can produce a downward spiral of negative feedback.Although the book is often very effective, it is heavily US-oriented, which is a shame when many of these issues are as significant, say, in Europe, as they are in the US. There is probably also not enough nuance in the author's binary good/bad opinion of systems. For example, she tells us that someone shouldn't be penalised by having to pay more for insurance because they live in a high risk neighbourhood - but doesn't think about the contrary aspect that if insurance companies don't do this, those of us who live in low risk neighbourhoods are being penalised by paying much higher premiums than we need to in order to cover our insurance. O'Neil makes a simplistic linkage between high risk = poor, low risk = rich - yet those of us, for instance, who live in the country are often in quite poor areas that are nonetheless low risk. For O'Neil, fairness means everyone pays the same. But is that truly fair? Here in Europe, we've had car insurance for young female drivers doubled in cost to make it the same as young males - even though the young males are far more likely to have accidents. This is fair by O'Neil's standards, because it doesn't discriminate on gender, but is not fair in the real world away from labels.There's a lot here that we should be picking up on, and even if you don't agree with all of O'Neil's assessments, it certainly makes you think about the rights and wrongs of decisions based on automated assessment of indirect data."
172,0553418815,http://goodreads.com/user/show/7019761-cindy,4,"While I don't necessarily agree with all of the conclusions, this was a very thought-provoking book. The author is a mathematician who has worked on Wall Street and now takes on some of the math models that rule our lives. Some of these were, at least originally, developed with good intentions - intending to make fairer some of the decisions that affect our lives: where and who gets into college; what the schedule for our job looks like; how much we pay to get a loan; even how you are judged in your chosen profession. But the author poses that many of these types of decisions are actually less than fair. The models are in her terms WMDs - weapons of math destruction - they are hidden, unregulated, and virtually uncontestable, even when they are blatantly wrong. One of the more troubling examples from the book concerned the use of testing to judge teacher performance. She traced how an exemplary teacher with outstanding evaluations was judged as poor on the basis of one set of tests and fired. The teacher showed that the tests which showed that her ""value-added"" score was sub-par were flawed due to a high number of erasures on the previous year which indicated cheating. Indeed, the system said that there *might* have been cheating but they would not re-instate this excellent teacher. (And please don't get me started on ""value-added"" - my daughters' school was always excellent in all the testing but this score was just average. So if your kids are already high achievers, how do you continue to score excellent in this measure? The answer is it's almost impossible.)Other examples showed that even when you removed race or ethnicity or economics as an indicator in these WMDs, they were there. If a zip code puts you in a risky category for a student loan, then it's hard to get the education that can get you out of poverty; or if a work schedule makes it hard for a single mother to get the child care she needs so that she can get the money to go to school to make a better life for herself and her child...well, I can see how some of this sets up a self-fulfilling prophecy. It makes it harder, not impossible, to break this cycle. That's why this book is important. If this is being used, then we need to know and like credit scores have the ability to correct errors and inaccuracies that work against the average person.Quotes to remember:....This would all be wonderful for students and might enhance their college experience, if they weren't the ones paying for it in the form of student loans that would burden them for decades. We cannot place the blame for this trend entirely on the US News rankings. Our entire society has embraced not only the idea that a college education is essential, but the idea that a degree from a highly-ranked school can catapult a student into a life of power and privilege. As I write this, the entire voting population that matters lives in a handful of counties in Florida, Ohio, Nevada, and a few other swing states. Within those counties is a small number of voters whose opinions weigh in the balance. I might point out here that while many of the WMDs we've been looking at, from predatory ads to policing models, deliver most of their punishment to the struggling classes, political microtargeting harms voters of every economic class, from Manhattan to San Francisco, rich and poor alike find themselves disenfranchised, though the truly affluent of course can more than compensate for this with campaign contributions."
173,0553418815,http://goodreads.com/user/show/6894205-allie,4,"This was by far the most interesting book about math I have ever read (admittedly a low bar to set), possibly because it contains no actual math formulas. Instead, former mathematics professor and hedge fund analyst Cathy O'Neil tells stories about the applications of math - how ""ill-conceived mathematical models now micromanage the economy from advertising to prisons."" Among other things, mathematical models determine which news you see on social media, your Google search results, the cost of your auto insurance and credit card rates, which neighborhoods police are sent to patrol, and how your child's schools are evaluated. In one staggering statistic about the reach of these models, O'Neil notes that 72 percent of resumes are never seen by humans, but are instead screened and scored by software programs looking for employer keywords. While math itself is neutral, the models are based on assumptions that reflect human biases and limitations. Often, the models are designed to process data from large numbers of people efficiently, but not necessarily fairly. She distinguishes between ""good"" models built on transparent assumptions that are constantly corrected with new data (e.g., those used in professional baseball) to opaque models that use proxy data such as race or zip codes and do not have a corrective feedback mechanism (e.g., predictive policing software). Typically, there is no method of recourse when given a poor score by a computer (hello, Hal) since companies consider their algorithms proprietary and won't disclose details of how scores are developed. This was an issue for teachers evaluated - and sometimes fired - based on inaccurate models of student test scores. The book's second main argument is that the built-in biases in the models are especially putative towards the poor, who are steered to high interest loans, targeted by ""stop and frisk"" policing based on their neighborhoods, and less likely to be personally interviewed for jobs.I found many of O'Neil's arguments persuasive and somewhat alarming, although she tries to end the book on a positive note. Worth reading, even if you don't like math. Or weapons. Or destruction. "
174,0553418815,http://goodreads.com/user/show/70315594-enoughtohold,4,"A nice, clear overview of the problems of big data for laypeople. Not sure why other reviewers expected this to be deeply technical, or why they think O’Neil thinks there are easy answers — she plainly doesn’t.I for one am glad a book this accessible exists on this topic. After all, the vast majority of the people affected by these harmful models are not data scientists, and they deserve to know what’s going on.(Also, the author did a great job reading for the audiobook.)"
175,0553418815,http://goodreads.com/user/show/4430576-miri,5,"I've said it before, and I'm sure I'll say it again, because I make a point of seeking out books like this. But if there's one book I could get everyone to read right now, it would be this one. Even next to Ta-Nehisi Coates's We Were Eight Years in Power, which I'm reading concurrently and which is staggering in its importance, this book feels urgent—because what it explains is how our new economy of Big Data is codifying, concealing, and amplifying inequality of every kind, on an enormous scale. Every injustice I'm reading about in Coates's book is made worse by what I read about here. We think of math as inherently neutral, the way we think of technology and logic and science. But these things are like any tool, dependent on the person using them, capable of being used for good or evil. (Remember the way science was abused throughout American history, fabricating the entire concept of biological race to provide justification for white humans to enslave black ones.) When we start using math to make decisions instead of people, assuming that this will decrease human bias and make things more fair, what we can actually end up doing is obscuring that prejudice and magnifying the scale of its effect. Cathy O'Neil was a mathematician working in the finance industry in 2008. She left after a few years, disgusted with the way the system was clearly rigged, and has since created the concept of Weapons of Math Destruction: mathematical models that codify human prejudice, are opaque and impervious to feedback, and affect large numbers of people. In this book, O'Neil takes us through several WMDs, showing how they're used in different areas of our lives. There's the disastrous school reform attempt in Washington D.C., which tried to evaluate teachers in a way math simply cannot be used, and which fired hundreds of teachers based on results no one in the district could explain or defend. There's the way many employers now use credit scores to screen applications, assuming that making regular payments means you'll be a dependable employee, ignoring the facts that (1) responsible people experience financial difficulty too and (2) preventing people with bad credit from getting jobs is only going to make their situation worse.There are the computerized risk models that judges use in sentencing, trying to gauge from prisoners' background information—where they grew up, whether their friends and family have criminal records; information which would be inadmissible in court—how likely they are to return to prison after being released, and how long their sentence should be. There's the way U.S. News & World Report helped cause an arms race when it decided to start ranking universities on everything except their cost, driving up tuition over 500 percent (nearly four times the rate of inflation) in just the 30-ish years that I've been alive.There's political advertising which is now so individually personalized that even people who support the same candidate, even people visiting the candidate's website, will see totally different things based on the profile the campaign has created for them.There's predatory advertising like that done by for-profit colleges, which specifically targets vulnerable people's ""pain points"" looking for those who are ""stuck,"" who have ""few people in their lives who care about them,"" who have ""low self-esteem"" and are ""unable to see and plan well for the future"" (these quotes, shared by O'Neil, come from the California Attorney General's office when it was investigating one of these colleges). (And if you're wondering how they would know this about people, the point is that they know this about all of us now. Just try to imagine what someone could learn about you from seeing everything you do online.)It might seem like the logical response is to disarm these weapons, one by one. The problem is that they’re feeding on each other. Poor people are more likely to have bad credit and live in high-crime neighborhoods, surrounded by other poor people. Once the dark universe of WMDs digests that data, it showers them with predatory ads for subprime loans or for-profit schools. It sends more police to arrest them, and when they're convicted it sentences them to longer terms. This data feeds into other WMDs, which score the same people as high-risk or easy targets and proceed to block them from jobs, while jacking up their rates for mortgages, car loans, and every kind of insurance imaginable. This drives their credit rating down further, creating nothing less than a death spiral of modeling. Being poor in a world of WMDs is getting more and more dangerous and expensive.The same WMDs that abuse the poor also place the comfortable classes of society in their own marketing silos . . . For many of them it can feel as though the world is getting smarter and easier. Models highlight bargains on prosciutto and chianti, recommend a great movie on Amazon Prime, or lead them, turn by turn, to a cafe in what used to be a ""sketchy"" neighborhood. The quiet and personal nature of this targeting keeps society's winners from seeing how the very same models are destroying lives, sometimes just a few blocks away.Though we're all affected by them in ways we can't see, right now the majority of the damage is being done to the same groups who are always hurt by flaws in the system—minorities and the poor. The same models could easily be used to help people instead of prey upon them, but that won't happen in the glorious free market of capitalism. We need regulation and oversight in the data industry. We need accountability. When statistics itself, and the public’s trust in statistics, is being actively undermined by politicians across the globe, how can we possibly expect the Big Data industry to clarify rather than contribute to the noise? We can because we must. Right now, mammoth companies like Google, Amazon, and Facebook exert incredible control over society because they control the data. They reap enormous profits while somehow offloading fact-checking responsibilities to others. It’s not a coincidence that, even as he works to undermine the public’s trust in science and in scientific fact, Steve Bannon sits on the board of Cambridge Analytica, a political data company that has claimed credit for Trump’s victory while bragging about secret ""voter suppression"" campaigns. It’s part of a more general trend in which data is privately owned and privately used to private ends of profit and influence, while the public is shut out of the process and told to behave well and trust the algorithms. It’s time to start misbehaving. Algorithms are only going to become more ubiquitous in the coming years. We must demand that systems that hold algorithms accountable become ubiquitous as well."
176,0553418815,http://goodreads.com/user/show/17187084-sean-goh,4,"Very readable book warning of the downside of amoral models. With great power comes great responsibility.___Math-powered applications were based on choices made by fallible human beings. Many models encoded human prejudice, misunderstanding, and bias into the software systems that increasingly managed our lives. Like gods, these mathematical models were opaque, their workings invisible to all but the highest priests in their domain: mathematicians and computer scientists.Models are opinions embedded in mathematics.Statistical engines without feedback can continue to spin our faulty and damaging analysis while never learning from its mistakes. E.g. teachers mistaken labelled as failures are fired, and no one is the wiser. The model's output becomes unquestioned reality.The parallels between finance and Big Data: The productivity of the field's talents indicates that they are on the right track, and it translates into dollars. This leads to the fallacious conclusion that whatever they are doing to bring in more money is good. It ""adds value"". Otherwise why would the market reward it?In both cultures Wealth is no longer a means to get by. It becomes directly tied to personal worth.On the rise of university rankings: from the perspective of a university president, it's quite sad. Here they are at the summit of their careers dedicating enormous energy towards boosting performance in fifteen areas defined by a group of journalists at a second-tier magazine (U.S. News). The ranking had become destiny. When you create a model from proxies, it is far simpler for people to game it. This is because proxies are easier to manipulate than the complicated reality they represent. (e.g. trying to evaluate influence by number of twitter followers)In a system in which cheating is the norm, following the rules amounts to a handicap. So preventing the students in Zhongxiang from cheating was unfair.In our largely segregated cities, geography is a proxy for race.Patrolling particular areas creates a pernicious feedback loop. The policing itself spawns new data, which justifies more policing. Eventually the zeroing in on ghetto districts ends up criminalising poverty, believing all the while that our tools are not only scientific but fair.Instead of simply trying to eradicate crimes, police should be attempting to build relationships in the neighbourhood. This was one of the pillars of the original ""broken windows"" study. The cops were on foot, talking to people, trying to get them to uphold their own community standards. But that objective, in many cases, has been lost, steamrollered by models that equate arrests with safety.Mathematical models can sift through data to locate people who are likely to face great challenges, whether from crime, poverty, or education. It's up to society whether to use that intelligence to reject and punish them - or to reach out to them with the resources they need. It all depends on the objectives we choose.Computing systems have trouble finding digital proxies for soft skills. The relevant data simply isn't collected, and anyway it's hard to put a value to them. They're usually easier to just leave out of the model.The modelers of e-scores (unofficial credit ratings) have to make do with trying to answer the question: ""How have people like you behaved in the past?"" when ideally the question would be ""How have you behaved in the past?Even with the Affordable Care Act, which reduced the ranks of the uninsured, medical expenses remain the single biggest cause of bankruptcies in America.A sterling credit rating is not just a proxy for responsibility and smart decisions. It is also a proxy for wealth. And wealth is highly correlated with race.Mistakes made by models are learning opportunities - as long as the system receives feedback on the error. When automatic systems sift through our data to size us up for an e-score, they naturally project the past into the future. The poor are expected to remain poor forever and are treated accordingly. It's inexorable, often hidden and beyond appeal, and unfair.Yet machines cannot make adjustments for fairness by themselves. That is where the human overseer comes in.Human decision making, while often flawed, has one chief virtue. It can evolve.Big data processes codify the past. They do not invent the future. Doing that requires moral imagination, and that's something only humans can provide. We have to explicitly embed better values into our algorithms, creating Big Data models that follow our ethical lead. Sometimes that will mean putting fairness ahead of profit."
177,0553418815,http://goodreads.com/user/show/6991670-bob,4,"Summary: An insider account of the algorithms that affect our lives, from going to college, to the ads we see online, to our chances of getting a job, being arrested, getting credit and insurance.Big Data is indeed BIG. Mathematical algorithms shape who will see this post on their Facebook newsfeed. If you go to Amazon or another online bookseller, algorithms will suggest other books like this one you might be interested in. Have you seen all those ads about credit scores? They are more important than you might imagine. Algorithms used by employers and insurance companies determine your employability and insurability in part through these scores. Far more than another credit card (bad idea, by the way) or a mortgage are on the line. These algorithms seem objective, but how they are formulated, and the assumptions made in doing so mean the difference between useful tools that benefit people, and ""black boxes"" that thwart the flourishing of others, often unknown to them.Cathy O'Neil should know. A tenure track math professor, she made the jump to Wall Street and became a ""quant"" who helped develop mathematical algorithms and witnessed, in the crash of 2008, the harm some of these caused. And she began to notice how algorithms often painfully impacted the lives of many others.  She describes how a teacher was fired because of the weighting of performance scores of a single class, despite other evaluations finding her an excellent teacher (afterwards it was found that there were a high number of erasures on tests for students who would have been in her class the previous year, suggesting these had been altered to improve scores).As she looked at the algorithms responsible for such injustices, she came to dub them ""Weapons of Math Destruction"" or WMDs and she identified three characteristics of these WMDs:Opacity: those whose lives are affected by them have no idea of the factors and weighting of those factors that contributed to their ""score"".Scale: how widely an algorithm is applied across industries and sectors of life can affect how much of one's life is touched by a single formula. For example, the FICO scores mentioned above affect not only credit, but the ability to get a job, the cost of auto insurance, and your ability to rent an apartment.Damage: WMDs can reinforce other factors perpetuating a cycle of poverty, or incarceration.She also shows that what makes these algorithms destructive is the use of proxy measurements. For example an employer may not know directly how savvy someone is as a marketer, and so they use a ""proxy"" measurement of how many Twitter followers that person has. Or age is used as a proxy for how safe a driver one is. For a group, the proxy may work well, and be utterly inaccurate for an individual that falls within that proxy group.Then in successive chapters, she chronicles some of the ways WMDs operate in different parts of life. She discusses the U.S. News & World Report college rankings, and the use of algorithms in admissions processes. Social media uses algorithms to target advertising, which means some will see ads for for-profit schools and payday lenders, and others for upscale furnishing or Viagra, based on clicks, likes, searches, and comments. Policing strategies, including locations for intensified ""stop and frisk"" policing are shaped by another set of algorithms. Algorithms to filter resumes may use scoring algorithms that discriminate by address and psych exam algorithms may render others unemployable in a certain industry. Scheduling algorithms may promote efficiency at the expense of the ability of workers to sleep on a regular schedule, or arrange childcare, or work enough hours to qualify for health insurance. Algorithms sometimes shut people out from credit or low cost insurance when in fact they are good risks. She concludes by showing how algorithms determine ads and news we see (and don't see). In an afterword she explores the flaws in algorithms revealed on the election of Donald Trump (algorithms, for example predicted Clinton would easily win Michigan and Wisconsin, where consequently she did not campaign, and lost by small margins).In her conclusion, she makes the case not only for a code of ethics for mathematicians but also argues that regulation and audits of these algorithms are necessary. The value assumptions, as well as the mathematical methods of many algorithms are flawed, and yet opacity means those whose lives are most affected don't even know what hit them.She helps us see both the sinister and useful side of these algorithms. They may reveal where a pro-active intervention may save a family from descending into family violence, or provide extra assistance to a child in danger of falling behind in a key subject. Or they may be used to invade personal rights, or even to perpetuate socio-economic divides in a society. The reality is that the problem is not the math but the old GIGO problem (garbage in, garbage out). The values and assumptions of the humans who devise the formulas and weightings of values and the use of proxies determine what may be destructive outcomes for some people. Yet it can be hidden behind an app, a program, an algorithm.The massive explosion in storage capacities, processing speeds, and the way our interests, health status, travel patterns, spending patterns, fitness, diet and sleep habits, our political inclinations and more may be tracked via our online and smartphone usage makes O'Neil's warning an urgent one. We create mountains of data that may be increasingly mined by government and private interests. Perhaps as important as asking whether this will be governed in ways that contribute to our flourishing, is whether we will be alert enough to care.____________________________Disclosure of Material Connection: I received this book free from the publisher. I was not required to write a positive review. The opinions I have expressed are my own."
178,0553418815,http://goodreads.com/user/show/51162416-max,5,fast pleasant primer on what’s wrong with big data. Fun. I love using a VPN turning everything off and being practically nobody to these freaks 
179,0553418815,http://goodreads.com/user/show/4061006-angela,5,"Cathy O'Neil is one of my heroes; I love listening to her on Slate Money, and her less pop/more technical book, Doing Data Science, occupies a prominent place on my desk at work - and a special place in my heart. Going beyond the usual data science manuals of what a random forest is, or how to account for large, sparse data, O'Neil and her coauthor - in that book - make an almost anthropological survey of the data science ""field""/profession/whatever. It's enlightening, and it's very real; less a textbook, more a professional guide.So if Doing Data Science is the ""how to do your job"" manual, and Slate Money is the occasional soapbox, Weapons of Math Destruction is the manifesto. With clarity, conviction and intelligence, Cathy O'Neil scalpels away at the utopian, misty-eyed tech worship that has contributed to the data science/big data bubble. She identifies the ways in which algorithms encode and perpetuate existing prejudices, and are hidden from plain sight by the obscuring (both intentional and not) use of Fancy Math.She notes how anti-human ""market efficiency"" is, and how it's often been short-hand for unjust and unequal systems, and big data is just scaling those dumb systems way, way up. For example, the use of social network data to ""enrich"" lending platforms; or recidivism models that ask about where a former convict grew up, whether his/her friends are in jail, how many encounters with the police he/she has had. These algorithms - because they chase correlations to make predictions (rather than identifying causal factors) - create big-data-driven poverty traps. They are also beyond the purview of the government (and, given the new administration, will probably be for quite some time): just like your FICO score is ONLY about how often you pay your credit card bills, and NOT your demographic group, so too should these algorithms be scrutinized and regulated.Increasingly, O'Neil notes, the ""human touch"" comes at a price; in upper income groups, you get more real, live service - hearing about a job through connections, informational interviews with alumni, blah - while lower income groups are serviced by increasingly mechanized, machine learning code. The hiring and HR practices of CVS, McDonald's, and other low wage places were especially striking (and severe): mindless, well-meaning algorithms deployed to effectively punish workers whose BMIs (itself a stupid measure of health) are too high, or who seem ""anti-social"" according to a creepy questionnaire, or whatever. I think my favorite chapter - and, of course, the most relevant one - was about the toxic brew that can be politics and data science. It was especially powerful to think through the way that, as opposed to the past, where we had ""agreed upon facts"" given by carpet bombing-style hegemonic culture (e.g. four news channels that everyone watched), now we have hyper-tailored DARE I SAY ""alternative facts"" where, by the power of marketers (who prey on confirmation bias), your neighbor's political beliefs are the result of a very specific and private relationship between them, their web browser cookies, and some political marketer. This means you may have NO IDEA why your swingy neighbor in swingy state X thinks Candidate Y is running an underground child sex ring literally under the ground of a DC pizza shop. O'Neil (and this book was written Before The Madness) notes that these sorts of micro-targeted ads that follow you around the web are very smartly tailored; the marketers know who would be repulsed by such a stupid conspiracy theory, and who would be intrigued. And that's just terrifying. Anywayyyyy. From a data perspective, my (cold dead) economist heart reminds me that OF COURSE we're going to scale up injustice when we mindlessly maximize profit, instead of humans. We need to go back to our roots: to the philosophy and debates of Jeremy Bentham, and welfare economics, and what ""utility"" really means. Efficiency is not the end all, be all, despite what our current economic system seems to diktat. O'Neil also makes a good point about the repeated erroneous conflating of correlation and causation; something even the fanciest of linear algebra stochastic process meddlers fall prey to. That's something that I see a LOT in data science: the field is built on the premise of perfect prediction - and prediction doesn't need causation, it just needs correlation. OK, I will step off my soapbox. Highly recommended.edited to add: OMG and I just read her delightful 2017 resolutions post, and had much LOLs. Cathy O'Neil, you da best."
180,0553418815,http://goodreads.com/user/show/8325737-philipp,4,"This is an angry book, but its got a point! She summarises many 'weapons of math destruction', WMDs, algorithms that encode human biases, are opaque, which give recommendations or orders that no-one questions, while making things worse. For example, this summary:Poor people are more likely to have bad credit and live in high-crime neighborhoods, surrounded by other poor people. Once the dark universe of WMDs digests that data, it showers them with predatory ads for subprime loans or for-profit schools. It sends more police to arrest them, and when they’re convicted it sentences them to longer terms. This data feeds into other WMDs, which score the same people as high risks or easy targets and proceed to block them from jobs, while jacking up their rates for mortgages, car loans, and every kind of insurance imaginable. This drives their credit rating down further, creating nothing less than a death spiral of modeling. Being poor in a world of WMDs is getting more and more dangerous and expensive.O'Neil goes through many, many of examples like this where an automated approach made things worse, where people didn't bother to critically look at the output, where the model doesn't have a way to incorporate feedback, it's stuck in time and cannot evolve like humans. The majority of the book is spent on case studies and great, angry discussion of what went wrong. There's one short chapter on what could be done - mostly she proposes more regulation (auditing of algorithms), but also to 'dumb' down models by law, force them to treat people equally and not bin them in groups. That chapter on solutions is extremely short - I expect a second book from her on that topic.P.S.: I've been reading a lot about adversarial perturbations in deep learning (see this great paper). One idea could be to counteract some of those models to become an adversarial entity. For example, you have one of those shopping points cards. Instead of feeding their model with your shopping data, you mix it up - write down your shopping list for a month, shuffle it, and buy one part with your points card and the other in cash. Keep mixing it up and add random stuff. Your data should be garbage to the model as it's not consistent, easily removable as an outlier, i.e., worthless.You could also make the model work for you - find out which groups of customers the market sends special offers to, game your shopping so that you fall into that group, reap rewards.Of course you'd need to know in detail what the model is looking for, which is highly unlikely."
181,0553418815,http://goodreads.com/user/show/115473-siria,4,"In the two years since Cathy O'Neil published this study of the danger posed by mathematical models—how their supposed neutrality merely reflects the biases of those who create them while creating closed-system feedback loops that warp society in unjust ways—it hasn't become any less relevant. In fact, not only does the book remain relevant, but subsequent revelations—most prominently those about Facebook's involvement with the 2016 US elections—show that O'Neil may even have been too cautious in what is a trenchant and prescient critique. "
182,0553418815,http://goodreads.com/user/show/25178051-izzat-radzi,5,"""Mathematical models are supposed to be our tools, not our masters"" Statisticians, quants, actuaries : take heed. This book argues that the new 'industrial revolution' of big data is bringing destruction. To whom (?) , one might be tempted to asked. Humans, normal masses who sometimes makes errors of course.And due to that, they got red in their ledgers, which in these new century means, that they'll most likely subjected to the 'vicious loop' of rejection and manipulation, and will felt the usual 'life is not fair' feeling. To start off, I claim that this book is the best (so far) compliment to Taleb's Black Swan book. Whilst Taleb as arguably he as a trader and academia revolves his discussion around some financial models & it's discontent (whilst in most part slanted his discussion into the philosophical realm); this author meanwhile goes into a more broad discussion. She, whom has background in chronologically in academia, hedge funds, a short stint in risk management before focusing work as a data scientist, argues that given the presence of these Gargantuan Big Data in almost every aspect of life, it has tarnished if not destroyed people's life. The problem-before I proceed case by case- lies in two reasons. First is the data collected & recorded by the data mining by corporate companies, mostly are left uncrunched, meaning they're not subjected to updated information about those people data taken or they're mistakenly (yet significantly affect) people who are thought 'one', in this case people with the same name and birthday. This, argued by the author, because the model used isn't not tinkered time to time according to feedback; unlike those used in baseball & basketball where opponents and managers subject each individual players to new & paternalistic habits of players -thus a better use of correlation to causation-, they are taken as face value. And feedback that is not taken -later I'll mentioned in job application survey- left those aspiring people hoping to get some good news, baffled as they themselves don't know why their application was rejected. Turns out, it was their 'behaviour', recorded in the subjective and highly interpretable application questions. The second problem here, if not wrong should I put it, is that the usage of the data is, as author's claimed, taken as God's word, the highest possible theological blasphemy, if said to those in denial data miners. This is due to the reasons that they basically use the data available, let it be credit scores, health scores, SAT (or generally educational) scores, and even resume applications -to name a few- as the bestowed revelation, thus dismissing proven good teachers, potential shining employees and laying off 'unhealthy' current workers; all based on the Big Data, either snooping privately on their life, or in other cases, a take-it-or-leave option wether they'll provide their credit score to get an insurance or their health progress for work employment group insurance practiced in firms & companies. The dismissal action has become so autonomous directly from the data, that frightenedly, that one does not care of others, it's just life. Coincidentally, I read Adorno's Culture Industry directly before this, and it does personally provides a more enlightened discourse on the systemic issue. I personally first came across to this topic back a few years ago, that with this newly published book kindles my interest. The first was back around late 2013, where I was taking a UK driving licence. In the course, similarly argued by the author, you'll be getting some discount given that you choose to put a 'Black Box' in your car. This instrument will record your driving habits & feed back to the insurance company. Although initially used to monitor those big truckers -who tends to be sleep deprived due to late night & long driving- to reduce the risks of death of others due to reckless driving; here argued that it's afraid to be used by others to manipulate the privacy of car drivers, at other instances flawed due to normal human behaviour, taking short cuts at night through 'dangerous neighbourhood' thus ended up one marked to be subjected to 'handcuffed' by auto insurance companies. The second one I encountered was in early 2015, in a student organised event to explain & showcase the structure & function of Big Data by companies. At that time, I was furious given the fact of the the then environment of whistle-blowers expose on companies manipulating & snooping normal civilians life. In here meanwhile, as argued in The Impulse Society by Paul Roberts, politicians backed by big companies are microtargeting voters up to the point of what they read and hear are tailor made. Given to the rise of fascism and 'fear of the normal neighbour' one known differently pre-election in America which now turns into something totally unexpected, I humbly agreed to some point. The third encounter is with regard to behavioural test subjected to work or scholarship applications. Exposed initially by my parents, whom my dad was at times hired to be an interviewer. He's the first to enlighten me on the presence of attitude test taken by applicants, which is imperative for career climbing ambitions. Here written an illustration. An applicant answered promptly the normal application survey on personal attitudes. Obviously one is subjected to child education for example that one is unique bla3; yet answering unique now instead of other answers) will be perceived as narcissist by the possible employer, which now is not possible anymore. Unfortunately, this data, is shared and this unfortunate applicant will ended up in this continuous 'dangerous loop'. The author coined Recidivism in regard to this loop. The poor guy, living in a certain 'bad' neighbourhood, want to apply a credit card, or just simply living as a normal civilian. Now, dues to this, of course one might have some background of knowing some in-behind bars people. This data, used by the security enforcement, will cause the police to routinely knocks on doors to tell them they have eyes on them. A more idiosyncratic example is those trying to move houses. Property firms with big data on their very own hand, easily exclude those with a bad tag, even when they've changed, or for unfortunate some, mistakenly recorded in. As said before, no feedback can be given by the victims, unless one has some wealth to pursue on a legal lawsuit on this issue, which of course deteriorate middle class let alone the poor condition. Obviously the rich bankers and markets manipulators escaped from this loop. As a matter of fact, they're invulnerable and invincible. She suggested a few suggestions in the concluding chapter. First for example, oath taking by those finance practitioners, whom impact upon their ignorance is highly significant on the whole. Nevertheless she noted the downside that when in work bureaucratic structure, manager will want answers for certain issues, this will put those people in corners. Second is, as Taleb suggested, ditching those backward minded model. Obviously, she argued, one have to be specialised in the field to help recognise and build a better one. She illustrated a new model on preventing child abuse. There's similarity between the model to 'possible criminals' as in the Minority Report movie in reducing - if not preventing- crime she criticised, the hidden outcomes has not - at least she didn't yet- been analysed. The third is the involvement of human judgement into the model. Yes, initially the algorithm of the system is used to remove human bias, yet seeing how things turned out, the machine as expected cannot understand how human (cognitive) work. The problem of language and rational judgement will always triumph human above machine, I claimed this ambiguous argument. Hats off to this book!"
183,0553418815,http://goodreads.com/user/show/11951948-paul,3,"One of the funniest bits of Little Britain was where David Walliams as Carol Beer would type a request into the computer, before turning to the customer and saying ‘Computer says no’… Whilst it is funny, it is not so funny when it happens to you. In this book, O’Neil, a former Wall Street quant raises alarm bells on the way that these mathematical models have infiltrated our lives. We don’t see them, but these algorithms that help us with our searches online and finding books, films and other items on online sites are now being used to determine just how much of a risk you are. Next time you want a loan, to renew insurance or just need to get another job O’Neil thinks that some of us may have a problem.She calls them ‘Weapons of Maths Destruction’; these are incontestable, unregulated and opaque algorithms. They are being used by companies to decide the tiniest details. They are used because they make larger profits for corporations and most worryingly for us is that they are frequently conclude the wrong thing having made incorrect assumptions about individuals. As the saying goes ‘crap in; crap out’… Having worried the life out of the reader, O’Neil goes onto suggest a variety of things that could help; more regulation, better design of the code and us being aware of their use. The writing is clear, if a little dry and technical at times. The examples are a little American centric, but you can see the way that it is going in the UK. Even though the title mentions the dreaded word maths, it really isn’t that mathematical. Worthwhile reading."
184,0553418815,http://goodreads.com/user/show/16212136-peter-pete-mcloughlin,4,"Very depressing book on how computer algorithms are becoming a powerful tool of the exploitation of workers, the poor, minorities and ordinary people mostly in the service of profits. It goes deep and the problem is much worse than you think. This book is a downer as it covers relevant developments in politics and business and that always is not pleasant to think about in these times in general and are especially dispiriting in how they affect ordinary people."
185,0553418815,http://goodreads.com/user/show/5550611-hallie,2,"Just ok. I like the idea more than the execution of this book. While O'Neil uncovers a great, dark place in our society that needs more light shone on it, much of her conclusions she gives away in the introduction to the book. This would have been an excellent piece in the New Yorker or Atlantic but as a book began to feel redundant far too early. "
186,0553418815,http://goodreads.com/user/show/11345492-kate,4,"3.5, bumped up for likeable clarity-without-condescension and consistent up-front political engagement."
187,0553418815,http://goodreads.com/user/show/761281-mike,4,"Reading this book I could not help but wish the author had used a quote from Paul Goodman as the epigraph of the book: ""Technology is a branch of moral philosophy, not of science."" (Every technology ultimately is making a value judgement about what is important or worth doing, and what aspects of a task or reality are important.)This book is a fascinating look at how 'big data' is used and abused by colleges, banks, insurance agencies, in sentencing and parole decisions, by employers screening applicants, and more. Collecting vast amounts of data and using to guide decisions can be a very useful technology, but the author -- who herself had a career as a 'quant' or data analyst for a large firm -- points out the downside of basing decisions on data and algorithms. Of course it should be obvious, but our society's information fetishism can make us overlook that the data we collect and the algorithms we use to analyze it can be deeply flawed by omissions, prejudices, fallacies that the designers may not be aware of. A couple of the many examples she sues help illustrate this. We read about a school teacher whose performance is measured by an algorithm that takes into account his students' past performance, current performance on standardized tests, and various other factors. One year he is scored 6/100, and the next year 94/100 -- after making no changes to his syllabus, assignments, or anything else he was doing. Anyone with common sense can see there is something wrong with the algorithm if someone could get such wildly varying scores, but the scoring algorithm is a 'black box' that not event the school administrators understand. Among the many problems with the system here is that there is no allowance for the designers of the algorithm to get feedback on the scoring and adjust the model -- instead, the 'failing' teachers are fired and the algorithm rolls on. Contrast this with the use of 'big data' algorithms in commercial enterprises: when Amazon.com collects data on your browsing and purchasing habits, they use this recommendations to you for future purchases. If their models are bad, they suggest things you don't want, and they don't increase sales. But they take this feedback and change their algorithms, because there is a financial incentive to do so.So a huge blind spot in some of the models and algorithms policy makes use is the lack of feedback. Another is huge downside of the rise of big data modeling is that private companies can use opaque models to discriminate in ways that punish the poor particularly. The author explains how car insurance companies use data about where their customers live, drive, and work to assess risks -- which is largely legitimate -- but also abuse this information to determine which customers are likely to be able to shop around for better deals, and which are less likely to have the time, resources, or access to information to do so, and they are charged much more. Indeed affluent customers with DUI convictions may be better rates than safe drivers who live in areas with limited internet access and higher poverty. Likewise employers may screen applicants with big data algorithms that identify 'risks' which rule out the poor, people with past mental illness, or immigrants. Many other problems are discussed, and the final chapters look at how political parties have begun to utilize big data too -- an ominous prospect in light of the rest of the book.The author offers some solutions, ranging from the admittedly ineffective (a code of ethics for 'quants' who create the models) to the more daunting but effective measure of demanding transparency and regulation (which has made some progress in the insurance industry, but is far from complete).*Full disclosure -- I read a free advanced reader's copy that I won in a Goodreads giveaway.*"
188,0553418815,http://goodreads.com/user/show/3502690-graeme-roberts,2," Cathy O'Neil writes well and provides much good information, but I found the fundamental thesis of  Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy to be foolish and irresponsible. The title, condensed inevitably to WMD, is used promiscuously to describe any application of data science, statistics, or even information technology that she considers to be unfair or discriminatory. In most cases, she is right; some applications are used by the greedy and the uncaring with insufficient thought of the consequences, but she rarely considers the positive attributes of the applications. Once branded as a WMD, a term indelibly attached to the evil trifecta of George W. Bush, Saddam Hussein, and the Iraq War, how are readers meant to consider their many positive aspects and what can be done to make them better?At first, I thought that Ms. O'Neil had been the victim of a foolish PR or marketing person in her publishing company, who had come up with this catchy name, but her ardent use of the term convinced me that she had invented it.Ms. O'Neil seemed to follow a downward trajectory from her Harvard PhD in mathematics and a teaching position at Barnard to a leading hedge fund and a series of other morally disappointing positions that left her convinced that data science, and even mathematics, are roots of much evil. I don't buy it. They are tools that can be used or misused, and her diatribe does nothing to help.She asserts, on page 210:If we find (as studies have already shown) that the recidivism models codify prejudice and penalize the poor, then it's time to have a look at the inputs. In this case, they include loads of birds-of-a-feather connections. They predict an individual's behavior on the basis of the people he knows, his job, and his credit rating—details that would be inadmissible in court. The fairness fix is to throw out that data.But wait, many would say. Are we going to sacrifice the accuracy of the model for fairness? Do we have to dumb down our algorithms?In some cases, yes. If we're going to be equal before the law, or be treated equally as voters, we cannot stand for systems that drop us into different castes and treat us differently.I disagree strongly. Some data is better than none, though we should be looking to constantly improve our data science systems. I would like to know that the recently paroled criminal moving in next door had been released by a judge in full possession of whatever facts are available. So, I suspect, would Ms. O'Neil."
189,0553418815,http://goodreads.com/user/show/19993-robin,4,"If you've been listening to Cathy O'Neil on Slate Money for some time, you probably expected this book to emerge in exactly this form and with exactly this content. She's smart, progressive, focused, and clear, just as she was on the podcast. I saw her speak at the Mechanic's Institute Library a few months ago, before I'd read the book, and was very impressed by her ability to be articulate about complex situations with a decisiveness that belies their complexity, or at least certainly never allows for it as an excuse. The argument I found most challenging was (heavily paraphrasing here) that we should be privileging fairness because it is otherwise so easy to be blind to the unfairness that creeps in when we pursue other goals. If I'm honest with my own tendencies, I recognize my longing for accuracy might indeed cause me to overlook fairness, and I resist the idea that I need to choose one as a priority over the other. Nevertheless, O'Neil convinces me to question assumed neutrality much more closely, and be much more vigilant for it in my own industry. As she points out, it's easy for people who write the algorithms to absolve themselves of responsibility for how they are used, but if we pay attention to the real outcomes we may find that we need to do more to train our algorithms to learn from externalities and flaws. Focusing ruthlessly on measuring only what you are trying to measure and ignoring all other factors is a good start; evaluating the results to make sure that's actually what happened is as necessary a practice. The book is full of examples of failed algorithms in wide use, actively and detrimentally affecting at-risk populations today, as well as potential future areas where algorithms could be used, perhaps even inadvertently, to consolidate and perpetuate the power of the already powerful. The free market will not solve this problem. This is an excellent example of where good regulation is imperative. As constituents with the ability to influence legislators to put their attention here, we can demand more transparency, more awareness and education, and a more good, old-fashioned compassion.Further reading:https://www.ted.com/talks/joy_buolamw...https://www.propublica.org/article/ma...https://standards.ieee.org/develop/pr..."
190,0553418815,http://goodreads.com/user/show/7139041-dorin,2,"Disappointing - such a rich topic, yet the author decided to be racist, Democrat and to induce panic instead of having a cool headed approach. She ignores her own advice, contradicts herself time and again, and she makes her Democrat leaning so obvious it makes me puke.The book could have contained real data, real information, but instead she goes on to explain that statistics are bad because statistics don't catch statistical errors. She pretends she's a professional in data processing, but she sounds like she doesn't understand how statistics work, and she can barely go beyond her mathematical training to understand what went wrong from a human point of view.My point: „a machine is as useful as the person who uses it”. Her point: ""big data is bad unless it's used by the Democrats”."
191,0553418815,http://goodreads.com/user/show/7837611-greg-fanoe,2,"I think that on the whole reading this book will do people more good than harm but I had several problems with it:1. The only section which I have personal expertise on is the section on ""insurance"" - I am an actuary - and I can personally verify that this chapter was filled with inaccurate, misleading, and/or totally speculative statements.2. The correlation between credit score and income is nowhere near as strong as she makes it appear in this book.3. Many of the problems she identifies have nothing to do with the models in question. Advertising models are not the issue with the payday loan or for-profit college industry those industries need to be totally upended/destroyed at the source. Letting them operate as is but kneecapping their ability to advertise would be nonsensical.4. The solutions she presents are ridiculously vague and/or impractical."
192,0553418815,http://goodreads.com/user/show/1045774-mehrsa,5,This is the book I was waiting to read! The rejoinder to our data-obsessed culture. We got drunk on algorithms and forgot to focus on their limitations. are they asking the right questions? are they recycling inequality? Great book
193,0553418815,http://goodreads.com/user/show/3126915-imi,2,"Sadly, a rather superficial look at big data, algorithms and how they've impacted on society. You probably know/have guessed all this already. The correlations O'Neil point out are not very convincing, and it felt like she just wanted to rant about how unfair the world is. Not that I disagree with that conclusion; society is unfair and unequal. But O'Neil really doesn't state much more than that, and, worse, some of her examples felt misleading or simply speculative. Where was her evidence? And if she'd explored this issues in more depth I'd have wanted clear ideas for resolving these issues. Instead, any suggestions she made seemed vague and impractical. I admitedly feel out of my depth with this topic, but am most definitely willing to learn more and expand my understanding about it. Disappointingly, this book didn't offer that."
194,0553418815,http://goodreads.com/user/show/777176-dave,5,"I learned more from this book than I have from any book I've read in years. O'Neil is a former math professor and spent time as a hedge-fund quant, so she understands the theory and experience of the effect of out-of-control algorithms on our society.As corporations make more and more efficient use of algorithms, they are starting to show dangerous tendencies, according to O'Neil. She calls them ""weapons of math destruction"" or WMDs. WMDs present the following grave problems for society:--They usually lack transparency, such that it is difficult or impossible to know what goes into them and what factors sway or change their results. When they are opaque, WMDs are patently unfair (O'Neil cites the algorithms used to evaluate teachers in many districts, highlighting their absurdity as they attempt to quantify the unquantifiable).--They make broad generalizations and reduce human beings to ""buckets."" One of the worst examples is the criminal justice system, which is increasingly reliant on algorithms to deploy police patrols and even to sentence criminal defendants. These algorithms punish the innocent by assuming that those who live in poor neighborhoods are necessarily more likely to commit crimes, thereby subjecting them to frequent ""stop and frisk"" type searches that have been found to be unconstitutional repeatedly.--They can use information in an unfair way that tends to perpetuate further injustice. A prime example of this is the increasing tendency to identify individuals by their credit score, on job applications and even in online shopping. When job applicants are screened by credit score, those who are already poor (and therefore much more likely to have a low credit score already) will find it even harder to get a job and thereby earn enough money to help improve their credit score. This self-perpetuating cycle of poverty is completely unnecessary, because credit scores are an exceedingly poor proxy for job-worthiness of applicants.--Many WMDs are not very good at learning. They are created as models of human behavior (with all the flaws and assumptions of the humans who programmed them), and when they miss (when for example they screen out a potential hire who would have actually been a first-rate employee), they never learn that they have missed. The ""false positive"" never presents itself (the rejected applicant goes on to success elsewhere), and the oblivious WMD continues to chug along with its assumptions and blind spots unchallenged.And WMDs are like racism:""Racism, at the individual level, can be seen as a predictive model whirring away in billions of human minds around the world. It is built from faulty, incomplete, or generalized data. Whether it comes from experience or hearsay, the data indicates that certain types of people have behaved badly. That generates a binary prediction that all people of that race will behave that same way....Racists don't spend a lot of time hunting down reliable data to train their twisted models. And once their model morphs into a belief, it becomes hardwired. It generates poisonous assumptions, yet rarely tests them, settling instead for data that seems to confirm and fortify them. Consequently, racism is the most slovenly of predictive models. It is powered by haphazard data gathering and spurious correlations, reinforced by institutional inequities, and polluted by confirmation bias. In this way, oddly enough, racism operates like many of the WMDs I'll be describing in this book."" (p. 22-3)And finally:""Models like this will abound in coming years....Many of these models, like some of the WMDs we've discussed, will arrive with the best intentions. But they must also deliver transparency, disclosing the input data they're using as well as the results of their targeting. And they must be open to audits. These are powerful engines, after all. We must keep our eyes on them.... Data is not going away....Predictive models are, increasingly, the tools we will be relying on to run our institutions, deploy our resources, and manage our lives. But...these models are constructed not just from data but from the choices we make about which data to pay attention to--and which to leave out. Those choices are not just about logistics, profits, and efficiency. They are fundamentally moral. If we back away from them and treat mathematical models as a neutral and inevitable force, like the weather or the tides, we abdicate our responsibility....We must come together to police these WMDs, to tame and disarm them."" (p.218)"
195,0553418815,http://goodreads.com/user/show/3029658-doug-cornelius,5,"With big data, comes formulas to parse through the data trying to make sense of it. Those algorithms can help make sense of the data and help filter through the noise to find trends. But those algorithms can also be easily misused and have harmful, if unintended consequences. Cathy O'Neil explores these problems in 
Weapons of Math Destruction
.Ms. O'Neil is a former academic, hedge fund quant, and data scientist for startups. At the hedge fund she was looking at ways to make money by predicting financial trends. As a data scientist she was looking to predict people's purchases and browsing habits to monetize those habits. Those are good algorithms where the people using them understand them and update the algorithms as they see problems and can check to see if they are working properly by verifying outcomes.Many data driven outcomes fail. Those are the focus of 
Weapons of Math Destruction
.I'm a big fan of the Slate Money podcast with Cathy O'Neil, Felix Salmon, and Jordan Weissmann. When the publisher offered me a review copy of Ms. O'Neil's book, I jumped at the chance.It's a short book and even though the title makes it sound like it's full of math, it's not. It's more about social policy. She points to the danger of decision makers blindingly following algorithmic output that they do not understand.Take the US News and World Report listing of best colleges. This is preeminent guide for high school students trying to figure out which college to attend. The rankings do not look at actual student outcomes. The publisher does not look at happiness, learning or improvements. It looks at proxies for the outcomes like SAT scores, alumni contributions, and graduation percentages. Colleges started making decisions to improve their rankings in the algorithm by improving those measured proxies. As anyone writing checks for their college bound children, affordability is not one of the measured proxies.As you might expect there are big chunks of the book dedicated to failings in the financial sector by relying on poorly designed algorithms. The biggest problems often being false assumptions plugged into the data running through the algorithm.With the election season upon us, the chapter on political algorithms is fascinating. The data scientists of commerce were brought into political campaigns. They are able to micro-target potential voters sending different messages to different groups highlighting the positions that would make the candidate more favorable to that segment of the population. Civic engagement and the political process is increasingly being algorithm driven.Since our world is increasingly being driven by big data, it's important to understand what is happening behind the decision-making. Weapons of Math Destruction is an excellent tool to help you understand data driven decision-making."
196,0553418815,http://goodreads.com/user/show/23020614-germ-n,3,"A fascinating must-read guide on how to navigate and interpret a world increasingly ruled by materialistic prejudices and oversimplifications that are repackaged as ""science"" and ""tech"" through poorly designed computer algorithms.Reading this gives one the feeling that the ""evil androids ruling over mankind"" dystopia has already been unrolling. Except in a very sneaky way: this time we don't get to face or even understand the robots and their actions which are nonetheless already reshaping our reality and the course of our lives through ridiculous university rankings, credit scores, social media ideological bubbles, etc.Unfortunately O'Neil ruins an otherwise brilliant book by an extreme leftist bias that seeks to absolve her personal heroes in the US democratic party and their political agendas, and attempts to attribute most blame on a specific other side. I assume this is why a book which raises such important points gets to be published and become mainstream after all.For example, she portrays Obama as a saint who just wants the best for immigrants and fiercely battles the evil rich. Yet for any unbiased observer, it's quite clear that Obama and democrats are just serving the wealthy who finance their own political campaigns (duh) when they set to ruin nations overseas and bring in thousands of immigrants, ideally illegal ones, so the price of labor can go down and corporations can continue to reap political advantages.This skewed perspective is damaging to the bottom argument and distracts readers from what's really the main issue here: the constant loss of truth and distortion of increasingly astroturfed public debate. No attempts are made, for example, at addressing how these WMDs are influencing the toxic foreign policy of the US and the press - another entity that passes as a saint in this book, while being the number one responsible for war and destruction of nations around the world."
197,0553418815,http://goodreads.com/user/show/20465079-tam,4,"A quick and highly useful read - I would recommend it to all people not just ones working with predictive models and Big Data.The title is sensational enough, but at one point I put that aside and decided to read this book. So many have been praising Big Data and Machine Learning, few cautions have been put forward but not enough, mostly dealing with ethics, security, personal data issues. This book, on the other hand, discusses the large scale tragedies that Big Data can invoke on societies as a whole, if continuing to go unchecked. 10 chapters deal with examples about the misuses of data and predictive models in many aspects of lives, especially the lives of the most unfortunate.O'Neil has a strong opinion and at certain point I thought she might be too pessimistic (saying that from a self-proclaimed pessimistic person as I am), and O'Neil freely claims how angry she is with the whole thing. Yet at the very end of the book, she makes constructive comments, ideas for improvement, for regulations. Those are the most important, and perhaps the most useful passages for data scientists. I've been quite fascinated by the tool and sometimes forget the darker side that the tool can invoke, and I'd like to thank O'Neil for the reminder, for the clarification about issues that sometimes I feel uneasy but not so clear about. No, the tool is neither inherently good nor bad, the people who make them and wield them can choose do what they want. And we can choose to do good things.Ehh, not so optimistic though, there are tons of challenges and... But I should shut my mouth. But at least we should try."
198,0553418815,http://goodreads.com/user/show/15409024-ahalya-vijay,5,"I found this to be a fascinating read on the importance of using technology and data in an ethical way. I was shocked to learn how pervasive irresponsible uses of data are throughout the developed world and how widespread it is for people to place their faith in ""black box"" models as Cathy O'Neil refers to them. It must be frustrating, for example, for teachers to be evaluated (and fired!) based on a model they have no knowledge of, without knowing how to improve their rating. And it must be even more so if they are getting anecdotally strong and positive feedback from school leadership and parents.Another shock was how liberally companies and 'data scientists' use proxies without thought to the consequences of the model not capturing the 'error term' sufficiently. The danger of making decisions based on things that are modeled (or estimated) using proxies was drilled into my head throughout my entire schooling. How would you feel if the fact that you were late to one meeting at work was used to brand you as an irresponsible or unreliable worker? Ms O'Neil made a clear link between such models being used to trap people in a cycle of poverty, using example after example across a variety of industries (e.g., education, finance, insurance). It is clear that reform is needed, but what left me worried at the end is that these models are not hurting the rich and famous, but the poor and under-resourced. Will our current political and justice system listen to their needs? I'm skeptical."
199,0553418815,http://goodreads.com/user/show/48433695-alexis-tremblay,5,"A must read for all data scientist, mathematicians, modellers, statisticians,... There is a dark side to big data and she exposes it masterfully. I always had an uneasy feeling when I started my career as an actuary and later on as data scientist. My moral compass was always off but I didn't have the words for it. Now she has given me a framework to think about my job and what to look out for. Will read again. "
200,0553418815,http://goodreads.com/user/show/16204614-bing-gordon,4,"Good overview of modern analytics, but too much of a guilt trip laid on top. "
201,0553418815,http://goodreads.com/user/show/6100646-brian-clegg,4,"As a poacher-turned-gamekeeper of the big data world, Cathy O'Neil is ideally placed to take us on a voyage of horrible discovery into the world of systems making decisions based on big data that can have a negative influence on lives - what she refers to as 'Weapons of Math Destruction' or WMDs. After working as a 'quant' in a hedge fund and on big data crunching systems for startups, she has developed a horror for the misuse of the technology and sets out to show us how unfair it can be.It's not that O'Neil is against big data per se. She points out examples where it can be useful and effective - but this requires the systems to be transparent and to be capable of learning from their mistakes. In the examples we discover, from systems that rate school teachers to those that decide whether or not to issue a payday loan, the system is opaque, secretive and based on a set of rules that aren't tested against reality and regularly updated to produce a fair outcome.The teacher grading system is probably the most dramatically inaccurate example, where the system is trying to measure how well a teacher has performed, based on data that only has a very vague link to actual outcomes - so, for instance, O'Neil tells of a teacher who scored 6% one year and 96% the next year for doing the same job. The factors being measured are almost entirely outside the teacher's control with no linkage to performance and the interpretation of the data is simply garbage.Other systems, such as those used to rank universities, are ruthlessly gamed by the participants, making them far more about how good an organisation is at coming up with the right answers to metrics than it is to the quality of that organisation. And all of us will come across targeted advertising and social media messages/search results prioritised according to secret algorithms which we know nothing about and that attempt to control our behaviour.For O'Neil, the worst aspects of big data misuse are where a system - perhaps with the best intentions - ends up penalising people for being poor of being from certain ethnic backgrounds. This is often a result of an indirect piece of data - for instance the place they live might have implications on their financial state or ethnicity. She vividly portrays the way that systems dealing with everything from police presence in an area to fixing insurance premiums can produce a downward spiral of negative feedback.Although the book is often very effective, it is heavily US-oriented, which is a shame when many of these issues are as significant, say, in Europe, as they are in the US. There is probably also not enough nuance in the author's binary good/bad opinion of systems. For example, she tells us that someone shouldn't be penalised by having to pay more for insurance because they live in a high risk neighbourhood - but doesn't think about the contrary aspect that if insurance companies don't do this, those of us who live in low risk neighbourhoods are being penalised by paying much higher premiums than we need to in order to cover our insurance. O'Neil makes a simplistic linkage between high risk = poor, low risk = rich - yet those of us, for instance, who live in the country are often in quite poor areas that are nonetheless low risk. For O'Neil, fairness means everyone pays the same. But is that truly fair? Here in Europe, we've had car insurance for young female drivers doubled in cost to make it the same as young males - even though the young males are far more likely to have accidents. This is fair by O'Neil's standards, because it doesn't discriminate on gender, but is not fair in the real world away from labels.There's a lot here that we should be picking up on, and even if you don't agree with all of O'Neil's assessments, it certainly makes you think about the rights and wrongs of decisions based on automated assessment of indirect data."
202,0553418815,http://goodreads.com/user/show/7019761-cindy,4,"While I don't necessarily agree with all of the conclusions, this was a very thought-provoking book. The author is a mathematician who has worked on Wall Street and now takes on some of the math models that rule our lives. Some of these were, at least originally, developed with good intentions - intending to make fairer some of the decisions that affect our lives: where and who gets into college; what the schedule for our job looks like; how much we pay to get a loan; even how you are judged in your chosen profession. But the author poses that many of these types of decisions are actually less than fair. The models are in her terms WMDs - weapons of math destruction - they are hidden, unregulated, and virtually uncontestable, even when they are blatantly wrong. One of the more troubling examples from the book concerned the use of testing to judge teacher performance. She traced how an exemplary teacher with outstanding evaluations was judged as poor on the basis of one set of tests and fired. The teacher showed that the tests which showed that her ""value-added"" score was sub-par were flawed due to a high number of erasures on the previous year which indicated cheating. Indeed, the system said that there *might* have been cheating but they would not re-instate this excellent teacher. (And please don't get me started on ""value-added"" - my daughters' school was always excellent in all the testing but this score was just average. So if your kids are already high achievers, how do you continue to score excellent in this measure? The answer is it's almost impossible.)Other examples showed that even when you removed race or ethnicity or economics as an indicator in these WMDs, they were there. If a zip code puts you in a risky category for a student loan, then it's hard to get the education that can get you out of poverty; or if a work schedule makes it hard for a single mother to get the child care she needs so that she can get the money to go to school to make a better life for herself and her child...well, I can see how some of this sets up a self-fulfilling prophecy. It makes it harder, not impossible, to break this cycle. That's why this book is important. If this is being used, then we need to know and like credit scores have the ability to correct errors and inaccuracies that work against the average person.Quotes to remember:....This would all be wonderful for students and might enhance their college experience, if they weren't the ones paying for it in the form of student loans that would burden them for decades. We cannot place the blame for this trend entirely on the US News rankings. Our entire society has embraced not only the idea that a college education is essential, but the idea that a degree from a highly-ranked school can catapult a student into a life of power and privilege. As I write this, the entire voting population that matters lives in a handful of counties in Florida, Ohio, Nevada, and a few other swing states. Within those counties is a small number of voters whose opinions weigh in the balance. I might point out here that while many of the WMDs we've been looking at, from predatory ads to policing models, deliver most of their punishment to the struggling classes, political microtargeting harms voters of every economic class, from Manhattan to San Francisco, rich and poor alike find themselves disenfranchised, though the truly affluent of course can more than compensate for this with campaign contributions."
203,0553418815,http://goodreads.com/user/show/6894205-allie,4,"This was by far the most interesting book about math I have ever read (admittedly a low bar to set), possibly because it contains no actual math formulas. Instead, former mathematics professor and hedge fund analyst Cathy O'Neil tells stories about the applications of math - how ""ill-conceived mathematical models now micromanage the economy from advertising to prisons."" Among other things, mathematical models determine which news you see on social media, your Google search results, the cost of your auto insurance and credit card rates, which neighborhoods police are sent to patrol, and how your child's schools are evaluated. In one staggering statistic about the reach of these models, O'Neil notes that 72 percent of resumes are never seen by humans, but are instead screened and scored by software programs looking for employer keywords. While math itself is neutral, the models are based on assumptions that reflect human biases and limitations. Often, the models are designed to process data from large numbers of people efficiently, but not necessarily fairly. She distinguishes between ""good"" models built on transparent assumptions that are constantly corrected with new data (e.g., those used in professional baseball) to opaque models that use proxy data such as race or zip codes and do not have a corrective feedback mechanism (e.g., predictive policing software). Typically, there is no method of recourse when given a poor score by a computer (hello, Hal) since companies consider their algorithms proprietary and won't disclose details of how scores are developed. This was an issue for teachers evaluated - and sometimes fired - based on inaccurate models of student test scores. The book's second main argument is that the built-in biases in the models are especially putative towards the poor, who are steered to high interest loans, targeted by ""stop and frisk"" policing based on their neighborhoods, and less likely to be personally interviewed for jobs.I found many of O'Neil's arguments persuasive and somewhat alarming, although she tries to end the book on a positive note. Worth reading, even if you don't like math. Or weapons. Or destruction. "
204,0553418815,http://goodreads.com/user/show/70315594-enoughtohold,4,"A nice, clear overview of the problems of big data for laypeople. Not sure why other reviewers expected this to be deeply technical, or why they think O’Neil thinks there are easy answers — she plainly doesn’t.I for one am glad a book this accessible exists on this topic. After all, the vast majority of the people affected by these harmful models are not data scientists, and they deserve to know what’s going on.(Also, the author did a great job reading for the audiobook.)"
205,0553418815,http://goodreads.com/user/show/4430576-miri,5,"I've said it before, and I'm sure I'll say it again, because I make a point of seeking out books like this. But if there's one book I could get everyone to read right now, it would be this one. Even next to Ta-Nehisi Coates's We Were Eight Years in Power, which I'm reading concurrently and which is staggering in its importance, this book feels urgent—because what it explains is how our new economy of Big Data is codifying, concealing, and amplifying inequality of every kind, on an enormous scale. Every injustice I'm reading about in Coates's book is made worse by what I read about here. We think of math as inherently neutral, the way we think of technology and logic and science. But these things are like any tool, dependent on the person using them, capable of being used for good or evil. (Remember the way science was abused throughout American history, fabricating the entire concept of biological race to provide justification for white humans to enslave black ones.) When we start using math to make decisions instead of people, assuming that this will decrease human bias and make things more fair, what we can actually end up doing is obscuring that prejudice and magnifying the scale of its effect. Cathy O'Neil was a mathematician working in the finance industry in 2008. She left after a few years, disgusted with the way the system was clearly rigged, and has since created the concept of Weapons of Math Destruction: mathematical models that codify human prejudice, are opaque and impervious to feedback, and affect large numbers of people. In this book, O'Neil takes us through several WMDs, showing how they're used in different areas of our lives. There's the disastrous school reform attempt in Washington D.C., which tried to evaluate teachers in a way math simply cannot be used, and which fired hundreds of teachers based on results no one in the district could explain or defend. There's the way many employers now use credit scores to screen applications, assuming that making regular payments means you'll be a dependable employee, ignoring the facts that (1) responsible people experience financial difficulty too and (2) preventing people with bad credit from getting jobs is only going to make their situation worse.There are the computerized risk models that judges use in sentencing, trying to gauge from prisoners' background information—where they grew up, whether their friends and family have criminal records; information which would be inadmissible in court—how likely they are to return to prison after being released, and how long their sentence should be. There's the way U.S. News & World Report helped cause an arms race when it decided to start ranking universities on everything except their cost, driving up tuition over 500 percent (nearly four times the rate of inflation) in just the 30-ish years that I've been alive.There's political advertising which is now so individually personalized that even people who support the same candidate, even people visiting the candidate's website, will see totally different things based on the profile the campaign has created for them.There's predatory advertising like that done by for-profit colleges, which specifically targets vulnerable people's ""pain points"" looking for those who are ""stuck,"" who have ""few people in their lives who care about them,"" who have ""low self-esteem"" and are ""unable to see and plan well for the future"" (these quotes, shared by O'Neil, come from the California Attorney General's office when it was investigating one of these colleges). (And if you're wondering how they would know this about people, the point is that they know this about all of us now. Just try to imagine what someone could learn about you from seeing everything you do online.)It might seem like the logical response is to disarm these weapons, one by one. The problem is that they’re feeding on each other. Poor people are more likely to have bad credit and live in high-crime neighborhoods, surrounded by other poor people. Once the dark universe of WMDs digests that data, it showers them with predatory ads for subprime loans or for-profit schools. It sends more police to arrest them, and when they're convicted it sentences them to longer terms. This data feeds into other WMDs, which score the same people as high-risk or easy targets and proceed to block them from jobs, while jacking up their rates for mortgages, car loans, and every kind of insurance imaginable. This drives their credit rating down further, creating nothing less than a death spiral of modeling. Being poor in a world of WMDs is getting more and more dangerous and expensive.The same WMDs that abuse the poor also place the comfortable classes of society in their own marketing silos . . . For many of them it can feel as though the world is getting smarter and easier. Models highlight bargains on prosciutto and chianti, recommend a great movie on Amazon Prime, or lead them, turn by turn, to a cafe in what used to be a ""sketchy"" neighborhood. The quiet and personal nature of this targeting keeps society's winners from seeing how the very same models are destroying lives, sometimes just a few blocks away.Though we're all affected by them in ways we can't see, right now the majority of the damage is being done to the same groups who are always hurt by flaws in the system—minorities and the poor. The same models could easily be used to help people instead of prey upon them, but that won't happen in the glorious free market of capitalism. We need regulation and oversight in the data industry. We need accountability. When statistics itself, and the public’s trust in statistics, is being actively undermined by politicians across the globe, how can we possibly expect the Big Data industry to clarify rather than contribute to the noise? We can because we must. Right now, mammoth companies like Google, Amazon, and Facebook exert incredible control over society because they control the data. They reap enormous profits while somehow offloading fact-checking responsibilities to others. It’s not a coincidence that, even as he works to undermine the public’s trust in science and in scientific fact, Steve Bannon sits on the board of Cambridge Analytica, a political data company that has claimed credit for Trump’s victory while bragging about secret ""voter suppression"" campaigns. It’s part of a more general trend in which data is privately owned and privately used to private ends of profit and influence, while the public is shut out of the process and told to behave well and trust the algorithms. It’s time to start misbehaving. Algorithms are only going to become more ubiquitous in the coming years. We must demand that systems that hold algorithms accountable become ubiquitous as well."
206,0553418815,http://goodreads.com/user/show/17187084-sean-goh,4,"Very readable book warning of the downside of amoral models. With great power comes great responsibility.___Math-powered applications were based on choices made by fallible human beings. Many models encoded human prejudice, misunderstanding, and bias into the software systems that increasingly managed our lives. Like gods, these mathematical models were opaque, their workings invisible to all but the highest priests in their domain: mathematicians and computer scientists.Models are opinions embedded in mathematics.Statistical engines without feedback can continue to spin our faulty and damaging analysis while never learning from its mistakes. E.g. teachers mistaken labelled as failures are fired, and no one is the wiser. The model's output becomes unquestioned reality.The parallels between finance and Big Data: The productivity of the field's talents indicates that they are on the right track, and it translates into dollars. This leads to the fallacious conclusion that whatever they are doing to bring in more money is good. It ""adds value"". Otherwise why would the market reward it?In both cultures Wealth is no longer a means to get by. It becomes directly tied to personal worth.On the rise of university rankings: from the perspective of a university president, it's quite sad. Here they are at the summit of their careers dedicating enormous energy towards boosting performance in fifteen areas defined by a group of journalists at a second-tier magazine (U.S. News). The ranking had become destiny. When you create a model from proxies, it is far simpler for people to game it. This is because proxies are easier to manipulate than the complicated reality they represent. (e.g. trying to evaluate influence by number of twitter followers)In a system in which cheating is the norm, following the rules amounts to a handicap. So preventing the students in Zhongxiang from cheating was unfair.In our largely segregated cities, geography is a proxy for race.Patrolling particular areas creates a pernicious feedback loop. The policing itself spawns new data, which justifies more policing. Eventually the zeroing in on ghetto districts ends up criminalising poverty, believing all the while that our tools are not only scientific but fair.Instead of simply trying to eradicate crimes, police should be attempting to build relationships in the neighbourhood. This was one of the pillars of the original ""broken windows"" study. The cops were on foot, talking to people, trying to get them to uphold their own community standards. But that objective, in many cases, has been lost, steamrollered by models that equate arrests with safety.Mathematical models can sift through data to locate people who are likely to face great challenges, whether from crime, poverty, or education. It's up to society whether to use that intelligence to reject and punish them - or to reach out to them with the resources they need. It all depends on the objectives we choose.Computing systems have trouble finding digital proxies for soft skills. The relevant data simply isn't collected, and anyway it's hard to put a value to them. They're usually easier to just leave out of the model.The modelers of e-scores (unofficial credit ratings) have to make do with trying to answer the question: ""How have people like you behaved in the past?"" when ideally the question would be ""How have you behaved in the past?Even with the Affordable Care Act, which reduced the ranks of the uninsured, medical expenses remain the single biggest cause of bankruptcies in America.A sterling credit rating is not just a proxy for responsibility and smart decisions. It is also a proxy for wealth. And wealth is highly correlated with race.Mistakes made by models are learning opportunities - as long as the system receives feedback on the error. When automatic systems sift through our data to size us up for an e-score, they naturally project the past into the future. The poor are expected to remain poor forever and are treated accordingly. It's inexorable, often hidden and beyond appeal, and unfair.Yet machines cannot make adjustments for fairness by themselves. That is where the human overseer comes in.Human decision making, while often flawed, has one chief virtue. It can evolve.Big data processes codify the past. They do not invent the future. Doing that requires moral imagination, and that's something only humans can provide. We have to explicitly embed better values into our algorithms, creating Big Data models that follow our ethical lead. Sometimes that will mean putting fairness ahead of profit."
207,0553418815,http://goodreads.com/user/show/6991670-bob,4,"Summary: An insider account of the algorithms that affect our lives, from going to college, to the ads we see online, to our chances of getting a job, being arrested, getting credit and insurance.Big Data is indeed BIG. Mathematical algorithms shape who will see this post on their Facebook newsfeed. If you go to Amazon or another online bookseller, algorithms will suggest other books like this one you might be interested in. Have you seen all those ads about credit scores? They are more important than you might imagine. Algorithms used by employers and insurance companies determine your employability and insurability in part through these scores. Far more than another credit card (bad idea, by the way) or a mortgage are on the line. These algorithms seem objective, but how they are formulated, and the assumptions made in doing so mean the difference between useful tools that benefit people, and ""black boxes"" that thwart the flourishing of others, often unknown to them.Cathy O'Neil should know. A tenure track math professor, she made the jump to Wall Street and became a ""quant"" who helped develop mathematical algorithms and witnessed, in the crash of 2008, the harm some of these caused. And she began to notice how algorithms often painfully impacted the lives of many others.  She describes how a teacher was fired because of the weighting of performance scores of a single class, despite other evaluations finding her an excellent teacher (afterwards it was found that there were a high number of erasures on tests for students who would have been in her class the previous year, suggesting these had been altered to improve scores).As she looked at the algorithms responsible for such injustices, she came to dub them ""Weapons of Math Destruction"" or WMDs and she identified three characteristics of these WMDs:Opacity: those whose lives are affected by them have no idea of the factors and weighting of those factors that contributed to their ""score"".Scale: how widely an algorithm is applied across industries and sectors of life can affect how much of one's life is touched by a single formula. For example, the FICO scores mentioned above affect not only credit, but the ability to get a job, the cost of auto insurance, and your ability to rent an apartment.Damage: WMDs can reinforce other factors perpetuating a cycle of poverty, or incarceration.She also shows that what makes these algorithms destructive is the use of proxy measurements. For example an employer may not know directly how savvy someone is as a marketer, and so they use a ""proxy"" measurement of how many Twitter followers that person has. Or age is used as a proxy for how safe a driver one is. For a group, the proxy may work well, and be utterly inaccurate for an individual that falls within that proxy group.Then in successive chapters, she chronicles some of the ways WMDs operate in different parts of life. She discusses the U.S. News & World Report college rankings, and the use of algorithms in admissions processes. Social media uses algorithms to target advertising, which means some will see ads for for-profit schools and payday lenders, and others for upscale furnishing or Viagra, based on clicks, likes, searches, and comments. Policing strategies, including locations for intensified ""stop and frisk"" policing are shaped by another set of algorithms. Algorithms to filter resumes may use scoring algorithms that discriminate by address and psych exam algorithms may render others unemployable in a certain industry. Scheduling algorithms may promote efficiency at the expense of the ability of workers to sleep on a regular schedule, or arrange childcare, or work enough hours to qualify for health insurance. Algorithms sometimes shut people out from credit or low cost insurance when in fact they are good risks. She concludes by showing how algorithms determine ads and news we see (and don't see). In an afterword she explores the flaws in algorithms revealed on the election of Donald Trump (algorithms, for example predicted Clinton would easily win Michigan and Wisconsin, where consequently she did not campaign, and lost by small margins).In her conclusion, she makes the case not only for a code of ethics for mathematicians but also argues that regulation and audits of these algorithms are necessary. The value assumptions, as well as the mathematical methods of many algorithms are flawed, and yet opacity means those whose lives are most affected don't even know what hit them.She helps us see both the sinister and useful side of these algorithms. They may reveal where a pro-active intervention may save a family from descending into family violence, or provide extra assistance to a child in danger of falling behind in a key subject. Or they may be used to invade personal rights, or even to perpetuate socio-economic divides in a society. The reality is that the problem is not the math but the old GIGO problem (garbage in, garbage out). The values and assumptions of the humans who devise the formulas and weightings of values and the use of proxies determine what may be destructive outcomes for some people. Yet it can be hidden behind an app, a program, an algorithm.The massive explosion in storage capacities, processing speeds, and the way our interests, health status, travel patterns, spending patterns, fitness, diet and sleep habits, our political inclinations and more may be tracked via our online and smartphone usage makes O'Neil's warning an urgent one. We create mountains of data that may be increasingly mined by government and private interests. Perhaps as important as asking whether this will be governed in ways that contribute to our flourishing, is whether we will be alert enough to care.____________________________Disclosure of Material Connection: I received this book free from the publisher. I was not required to write a positive review. The opinions I have expressed are my own."
208,0553418815,http://goodreads.com/user/show/51162416-max,5,fast pleasant primer on what’s wrong with big data. Fun. I love using a VPN turning everything off and being practically nobody to these freaks 
209,0553418815,http://goodreads.com/user/show/4061006-angela,5,"Cathy O'Neil is one of my heroes; I love listening to her on Slate Money, and her less pop/more technical book, Doing Data Science, occupies a prominent place on my desk at work - and a special place in my heart. Going beyond the usual data science manuals of what a random forest is, or how to account for large, sparse data, O'Neil and her coauthor - in that book - make an almost anthropological survey of the data science ""field""/profession/whatever. It's enlightening, and it's very real; less a textbook, more a professional guide.So if Doing Data Science is the ""how to do your job"" manual, and Slate Money is the occasional soapbox, Weapons of Math Destruction is the manifesto. With clarity, conviction and intelligence, Cathy O'Neil scalpels away at the utopian, misty-eyed tech worship that has contributed to the data science/big data bubble. She identifies the ways in which algorithms encode and perpetuate existing prejudices, and are hidden from plain sight by the obscuring (both intentional and not) use of Fancy Math.She notes how anti-human ""market efficiency"" is, and how it's often been short-hand for unjust and unequal systems, and big data is just scaling those dumb systems way, way up. For example, the use of social network data to ""enrich"" lending platforms; or recidivism models that ask about where a former convict grew up, whether his/her friends are in jail, how many encounters with the police he/she has had. These algorithms - because they chase correlations to make predictions (rather than identifying causal factors) - create big-data-driven poverty traps. They are also beyond the purview of the government (and, given the new administration, will probably be for quite some time): just like your FICO score is ONLY about how often you pay your credit card bills, and NOT your demographic group, so too should these algorithms be scrutinized and regulated.Increasingly, O'Neil notes, the ""human touch"" comes at a price; in upper income groups, you get more real, live service - hearing about a job through connections, informational interviews with alumni, blah - while lower income groups are serviced by increasingly mechanized, machine learning code. The hiring and HR practices of CVS, McDonald's, and other low wage places were especially striking (and severe): mindless, well-meaning algorithms deployed to effectively punish workers whose BMIs (itself a stupid measure of health) are too high, or who seem ""anti-social"" according to a creepy questionnaire, or whatever. I think my favorite chapter - and, of course, the most relevant one - was about the toxic brew that can be politics and data science. It was especially powerful to think through the way that, as opposed to the past, where we had ""agreed upon facts"" given by carpet bombing-style hegemonic culture (e.g. four news channels that everyone watched), now we have hyper-tailored DARE I SAY ""alternative facts"" where, by the power of marketers (who prey on confirmation bias), your neighbor's political beliefs are the result of a very specific and private relationship between them, their web browser cookies, and some political marketer. This means you may have NO IDEA why your swingy neighbor in swingy state X thinks Candidate Y is running an underground child sex ring literally under the ground of a DC pizza shop. O'Neil (and this book was written Before The Madness) notes that these sorts of micro-targeted ads that follow you around the web are very smartly tailored; the marketers know who would be repulsed by such a stupid conspiracy theory, and who would be intrigued. And that's just terrifying. Anywayyyyy. From a data perspective, my (cold dead) economist heart reminds me that OF COURSE we're going to scale up injustice when we mindlessly maximize profit, instead of humans. We need to go back to our roots: to the philosophy and debates of Jeremy Bentham, and welfare economics, and what ""utility"" really means. Efficiency is not the end all, be all, despite what our current economic system seems to diktat. O'Neil also makes a good point about the repeated erroneous conflating of correlation and causation; something even the fanciest of linear algebra stochastic process meddlers fall prey to. That's something that I see a LOT in data science: the field is built on the premise of perfect prediction - and prediction doesn't need causation, it just needs correlation. OK, I will step off my soapbox. Highly recommended.edited to add: OMG and I just read her delightful 2017 resolutions post, and had much LOLs. Cathy O'Neil, you da best."
210,0553418815,http://goodreads.com/user/show/8325737-philipp,4,"This is an angry book, but its got a point! She summarises many 'weapons of math destruction', WMDs, algorithms that encode human biases, are opaque, which give recommendations or orders that no-one questions, while making things worse. For example, this summary:Poor people are more likely to have bad credit and live in high-crime neighborhoods, surrounded by other poor people. Once the dark universe of WMDs digests that data, it showers them with predatory ads for subprime loans or for-profit schools. It sends more police to arrest them, and when they’re convicted it sentences them to longer terms. This data feeds into other WMDs, which score the same people as high risks or easy targets and proceed to block them from jobs, while jacking up their rates for mortgages, car loans, and every kind of insurance imaginable. This drives their credit rating down further, creating nothing less than a death spiral of modeling. Being poor in a world of WMDs is getting more and more dangerous and expensive.O'Neil goes through many, many of examples like this where an automated approach made things worse, where people didn't bother to critically look at the output, where the model doesn't have a way to incorporate feedback, it's stuck in time and cannot evolve like humans. The majority of the book is spent on case studies and great, angry discussion of what went wrong. There's one short chapter on what could be done - mostly she proposes more regulation (auditing of algorithms), but also to 'dumb' down models by law, force them to treat people equally and not bin them in groups. That chapter on solutions is extremely short - I expect a second book from her on that topic.P.S.: I've been reading a lot about adversarial perturbations in deep learning (see this great paper). One idea could be to counteract some of those models to become an adversarial entity. For example, you have one of those shopping points cards. Instead of feeding their model with your shopping data, you mix it up - write down your shopping list for a month, shuffle it, and buy one part with your points card and the other in cash. Keep mixing it up and add random stuff. Your data should be garbage to the model as it's not consistent, easily removable as an outlier, i.e., worthless.You could also make the model work for you - find out which groups of customers the market sends special offers to, game your shopping so that you fall into that group, reap rewards.Of course you'd need to know in detail what the model is looking for, which is highly unlikely."
211,0553418815,http://goodreads.com/user/show/115473-siria,4,"In the two years since Cathy O'Neil published this study of the danger posed by mathematical models—how their supposed neutrality merely reflects the biases of those who create them while creating closed-system feedback loops that warp society in unjust ways—it hasn't become any less relevant. In fact, not only does the book remain relevant, but subsequent revelations—most prominently those about Facebook's involvement with the 2016 US elections—show that O'Neil may even have been too cautious in what is a trenchant and prescient critique. "
212,0553418815,http://goodreads.com/user/show/25178051-izzat-radzi,5,"""Mathematical models are supposed to be our tools, not our masters"" Statisticians, quants, actuaries : take heed. This book argues that the new 'industrial revolution' of big data is bringing destruction. To whom (?) , one might be tempted to asked. Humans, normal masses who sometimes makes errors of course.And due to that, they got red in their ledgers, which in these new century means, that they'll most likely subjected to the 'vicious loop' of rejection and manipulation, and will felt the usual 'life is not fair' feeling. To start off, I claim that this book is the best (so far) compliment to Taleb's Black Swan book. Whilst Taleb as arguably he as a trader and academia revolves his discussion around some financial models & it's discontent (whilst in most part slanted his discussion into the philosophical realm); this author meanwhile goes into a more broad discussion. She, whom has background in chronologically in academia, hedge funds, a short stint in risk management before focusing work as a data scientist, argues that given the presence of these Gargantuan Big Data in almost every aspect of life, it has tarnished if not destroyed people's life. The problem-before I proceed case by case- lies in two reasons. First is the data collected & recorded by the data mining by corporate companies, mostly are left uncrunched, meaning they're not subjected to updated information about those people data taken or they're mistakenly (yet significantly affect) people who are thought 'one', in this case people with the same name and birthday. This, argued by the author, because the model used isn't not tinkered time to time according to feedback; unlike those used in baseball & basketball where opponents and managers subject each individual players to new & paternalistic habits of players -thus a better use of correlation to causation-, they are taken as face value. And feedback that is not taken -later I'll mentioned in job application survey- left those aspiring people hoping to get some good news, baffled as they themselves don't know why their application was rejected. Turns out, it was their 'behaviour', recorded in the subjective and highly interpretable application questions. The second problem here, if not wrong should I put it, is that the usage of the data is, as author's claimed, taken as God's word, the highest possible theological blasphemy, if said to those in denial data miners. This is due to the reasons that they basically use the data available, let it be credit scores, health scores, SAT (or generally educational) scores, and even resume applications -to name a few- as the bestowed revelation, thus dismissing proven good teachers, potential shining employees and laying off 'unhealthy' current workers; all based on the Big Data, either snooping privately on their life, or in other cases, a take-it-or-leave option wether they'll provide their credit score to get an insurance or their health progress for work employment group insurance practiced in firms & companies. The dismissal action has become so autonomous directly from the data, that frightenedly, that one does not care of others, it's just life. Coincidentally, I read Adorno's Culture Industry directly before this, and it does personally provides a more enlightened discourse on the systemic issue. I personally first came across to this topic back a few years ago, that with this newly published book kindles my interest. The first was back around late 2013, where I was taking a UK driving licence. In the course, similarly argued by the author, you'll be getting some discount given that you choose to put a 'Black Box' in your car. This instrument will record your driving habits & feed back to the insurance company. Although initially used to monitor those big truckers -who tends to be sleep deprived due to late night & long driving- to reduce the risks of death of others due to reckless driving; here argued that it's afraid to be used by others to manipulate the privacy of car drivers, at other instances flawed due to normal human behaviour, taking short cuts at night through 'dangerous neighbourhood' thus ended up one marked to be subjected to 'handcuffed' by auto insurance companies. The second one I encountered was in early 2015, in a student organised event to explain & showcase the structure & function of Big Data by companies. At that time, I was furious given the fact of the the then environment of whistle-blowers expose on companies manipulating & snooping normal civilians life. In here meanwhile, as argued in The Impulse Society by Paul Roberts, politicians backed by big companies are microtargeting voters up to the point of what they read and hear are tailor made. Given to the rise of fascism and 'fear of the normal neighbour' one known differently pre-election in America which now turns into something totally unexpected, I humbly agreed to some point. The third encounter is with regard to behavioural test subjected to work or scholarship applications. Exposed initially by my parents, whom my dad was at times hired to be an interviewer. He's the first to enlighten me on the presence of attitude test taken by applicants, which is imperative for career climbing ambitions. Here written an illustration. An applicant answered promptly the normal application survey on personal attitudes. Obviously one is subjected to child education for example that one is unique bla3; yet answering unique now instead of other answers) will be perceived as narcissist by the possible employer, which now is not possible anymore. Unfortunately, this data, is shared and this unfortunate applicant will ended up in this continuous 'dangerous loop'. The author coined Recidivism in regard to this loop. The poor guy, living in a certain 'bad' neighbourhood, want to apply a credit card, or just simply living as a normal civilian. Now, dues to this, of course one might have some background of knowing some in-behind bars people. This data, used by the security enforcement, will cause the police to routinely knocks on doors to tell them they have eyes on them. A more idiosyncratic example is those trying to move houses. Property firms with big data on their very own hand, easily exclude those with a bad tag, even when they've changed, or for unfortunate some, mistakenly recorded in. As said before, no feedback can be given by the victims, unless one has some wealth to pursue on a legal lawsuit on this issue, which of course deteriorate middle class let alone the poor condition. Obviously the rich bankers and markets manipulators escaped from this loop. As a matter of fact, they're invulnerable and invincible. She suggested a few suggestions in the concluding chapter. First for example, oath taking by those finance practitioners, whom impact upon their ignorance is highly significant on the whole. Nevertheless she noted the downside that when in work bureaucratic structure, manager will want answers for certain issues, this will put those people in corners. Second is, as Taleb suggested, ditching those backward minded model. Obviously, she argued, one have to be specialised in the field to help recognise and build a better one. She illustrated a new model on preventing child abuse. There's similarity between the model to 'possible criminals' as in the Minority Report movie in reducing - if not preventing- crime she criticised, the hidden outcomes has not - at least she didn't yet- been analysed. The third is the involvement of human judgement into the model. Yes, initially the algorithm of the system is used to remove human bias, yet seeing how things turned out, the machine as expected cannot understand how human (cognitive) work. The problem of language and rational judgement will always triumph human above machine, I claimed this ambiguous argument. Hats off to this book!"
213,0553418815,http://goodreads.com/user/show/11951948-paul,3,"One of the funniest bits of Little Britain was where David Walliams as Carol Beer would type a request into the computer, before turning to the customer and saying ‘Computer says no’… Whilst it is funny, it is not so funny when it happens to you. In this book, O’Neil, a former Wall Street quant raises alarm bells on the way that these mathematical models have infiltrated our lives. We don’t see them, but these algorithms that help us with our searches online and finding books, films and other items on online sites are now being used to determine just how much of a risk you are. Next time you want a loan, to renew insurance or just need to get another job O’Neil thinks that some of us may have a problem.She calls them ‘Weapons of Maths Destruction’; these are incontestable, unregulated and opaque algorithms. They are being used by companies to decide the tiniest details. They are used because they make larger profits for corporations and most worryingly for us is that they are frequently conclude the wrong thing having made incorrect assumptions about individuals. As the saying goes ‘crap in; crap out’… Having worried the life out of the reader, O’Neil goes onto suggest a variety of things that could help; more regulation, better design of the code and us being aware of their use. The writing is clear, if a little dry and technical at times. The examples are a little American centric, but you can see the way that it is going in the UK. Even though the title mentions the dreaded word maths, it really isn’t that mathematical. Worthwhile reading."
214,0553418815,http://goodreads.com/user/show/16212136-peter-pete-mcloughlin,4,"Very depressing book on how computer algorithms are becoming a powerful tool of the exploitation of workers, the poor, minorities and ordinary people mostly in the service of profits. It goes deep and the problem is much worse than you think. This book is a downer as it covers relevant developments in politics and business and that always is not pleasant to think about in these times in general and are especially dispiriting in how they affect ordinary people."
215,0553418815,http://goodreads.com/user/show/5550611-hallie,2,"Just ok. I like the idea more than the execution of this book. While O'Neil uncovers a great, dark place in our society that needs more light shone on it, much of her conclusions she gives away in the introduction to the book. This would have been an excellent piece in the New Yorker or Atlantic but as a book began to feel redundant far too early. "
216,0553418815,http://goodreads.com/user/show/11345492-kate,4,"3.5, bumped up for likeable clarity-without-condescension and consistent up-front political engagement."
217,0553418815,http://goodreads.com/user/show/761281-mike,4,"Reading this book I could not help but wish the author had used a quote from Paul Goodman as the epigraph of the book: ""Technology is a branch of moral philosophy, not of science."" (Every technology ultimately is making a value judgement about what is important or worth doing, and what aspects of a task or reality are important.)This book is a fascinating look at how 'big data' is used and abused by colleges, banks, insurance agencies, in sentencing and parole decisions, by employers screening applicants, and more. Collecting vast amounts of data and using to guide decisions can be a very useful technology, but the author -- who herself had a career as a 'quant' or data analyst for a large firm -- points out the downside of basing decisions on data and algorithms. Of course it should be obvious, but our society's information fetishism can make us overlook that the data we collect and the algorithms we use to analyze it can be deeply flawed by omissions, prejudices, fallacies that the designers may not be aware of. A couple of the many examples she sues help illustrate this. We read about a school teacher whose performance is measured by an algorithm that takes into account his students' past performance, current performance on standardized tests, and various other factors. One year he is scored 6/100, and the next year 94/100 -- after making no changes to his syllabus, assignments, or anything else he was doing. Anyone with common sense can see there is something wrong with the algorithm if someone could get such wildly varying scores, but the scoring algorithm is a 'black box' that not event the school administrators understand. Among the many problems with the system here is that there is no allowance for the designers of the algorithm to get feedback on the scoring and adjust the model -- instead, the 'failing' teachers are fired and the algorithm rolls on. Contrast this with the use of 'big data' algorithms in commercial enterprises: when Amazon.com collects data on your browsing and purchasing habits, they use this recommendations to you for future purchases. If their models are bad, they suggest things you don't want, and they don't increase sales. But they take this feedback and change their algorithms, because there is a financial incentive to do so.So a huge blind spot in some of the models and algorithms policy makes use is the lack of feedback. Another is huge downside of the rise of big data modeling is that private companies can use opaque models to discriminate in ways that punish the poor particularly. The author explains how car insurance companies use data about where their customers live, drive, and work to assess risks -- which is largely legitimate -- but also abuse this information to determine which customers are likely to be able to shop around for better deals, and which are less likely to have the time, resources, or access to information to do so, and they are charged much more. Indeed affluent customers with DUI convictions may be better rates than safe drivers who live in areas with limited internet access and higher poverty. Likewise employers may screen applicants with big data algorithms that identify 'risks' which rule out the poor, people with past mental illness, or immigrants. Many other problems are discussed, and the final chapters look at how political parties have begun to utilize big data too -- an ominous prospect in light of the rest of the book.The author offers some solutions, ranging from the admittedly ineffective (a code of ethics for 'quants' who create the models) to the more daunting but effective measure of demanding transparency and regulation (which has made some progress in the insurance industry, but is far from complete).*Full disclosure -- I read a free advanced reader's copy that I won in a Goodreads giveaway.*"
218,0553418815,http://goodreads.com/user/show/3502690-graeme-roberts,2," Cathy O'Neil writes well and provides much good information, but I found the fundamental thesis of  Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy to be foolish and irresponsible. The title, condensed inevitably to WMD, is used promiscuously to describe any application of data science, statistics, or even information technology that she considers to be unfair or discriminatory. In most cases, she is right; some applications are used by the greedy and the uncaring with insufficient thought of the consequences, but she rarely considers the positive attributes of the applications. Once branded as a WMD, a term indelibly attached to the evil trifecta of George W. Bush, Saddam Hussein, and the Iraq War, how are readers meant to consider their many positive aspects and what can be done to make them better?At first, I thought that Ms. O'Neil had been the victim of a foolish PR or marketing person in her publishing company, who had come up with this catchy name, but her ardent use of the term convinced me that she had invented it.Ms. O'Neil seemed to follow a downward trajectory from her Harvard PhD in mathematics and a teaching position at Barnard to a leading hedge fund and a series of other morally disappointing positions that left her convinced that data science, and even mathematics, are roots of much evil. I don't buy it. They are tools that can be used or misused, and her diatribe does nothing to help.She asserts, on page 210:If we find (as studies have already shown) that the recidivism models codify prejudice and penalize the poor, then it's time to have a look at the inputs. In this case, they include loads of birds-of-a-feather connections. They predict an individual's behavior on the basis of the people he knows, his job, and his credit rating—details that would be inadmissible in court. The fairness fix is to throw out that data.But wait, many would say. Are we going to sacrifice the accuracy of the model for fairness? Do we have to dumb down our algorithms?In some cases, yes. If we're going to be equal before the law, or be treated equally as voters, we cannot stand for systems that drop us into different castes and treat us differently.I disagree strongly. Some data is better than none, though we should be looking to constantly improve our data science systems. I would like to know that the recently paroled criminal moving in next door had been released by a judge in full possession of whatever facts are available. So, I suspect, would Ms. O'Neil."
219,0553418815,http://goodreads.com/user/show/19993-robin,4,"If you've been listening to Cathy O'Neil on Slate Money for some time, you probably expected this book to emerge in exactly this form and with exactly this content. She's smart, progressive, focused, and clear, just as she was on the podcast. I saw her speak at the Mechanic's Institute Library a few months ago, before I'd read the book, and was very impressed by her ability to be articulate about complex situations with a decisiveness that belies their complexity, or at least certainly never allows for it as an excuse. The argument I found most challenging was (heavily paraphrasing here) that we should be privileging fairness because it is otherwise so easy to be blind to the unfairness that creeps in when we pursue other goals. If I'm honest with my own tendencies, I recognize my longing for accuracy might indeed cause me to overlook fairness, and I resist the idea that I need to choose one as a priority over the other. Nevertheless, O'Neil convinces me to question assumed neutrality much more closely, and be much more vigilant for it in my own industry. As she points out, it's easy for people who write the algorithms to absolve themselves of responsibility for how they are used, but if we pay attention to the real outcomes we may find that we need to do more to train our algorithms to learn from externalities and flaws. Focusing ruthlessly on measuring only what you are trying to measure and ignoring all other factors is a good start; evaluating the results to make sure that's actually what happened is as necessary a practice. The book is full of examples of failed algorithms in wide use, actively and detrimentally affecting at-risk populations today, as well as potential future areas where algorithms could be used, perhaps even inadvertently, to consolidate and perpetuate the power of the already powerful. The free market will not solve this problem. This is an excellent example of where good regulation is imperative. As constituents with the ability to influence legislators to put their attention here, we can demand more transparency, more awareness and education, and a more good, old-fashioned compassion.Further reading:https://www.ted.com/talks/joy_buolamw...https://www.propublica.org/article/ma...https://standards.ieee.org/develop/pr..."
220,0553418815,http://goodreads.com/user/show/7139041-dorin,2,"Disappointing - such a rich topic, yet the author decided to be racist, Democrat and to induce panic instead of having a cool headed approach. She ignores her own advice, contradicts herself time and again, and she makes her Democrat leaning so obvious it makes me puke.The book could have contained real data, real information, but instead she goes on to explain that statistics are bad because statistics don't catch statistical errors. She pretends she's a professional in data processing, but she sounds like she doesn't understand how statistics work, and she can barely go beyond her mathematical training to understand what went wrong from a human point of view.My point: „a machine is as useful as the person who uses it”. Her point: ""big data is bad unless it's used by the Democrats”."
221,0553418815,http://goodreads.com/user/show/7837611-greg-fanoe,2,"I think that on the whole reading this book will do people more good than harm but I had several problems with it:1. The only section which I have personal expertise on is the section on ""insurance"" - I am an actuary - and I can personally verify that this chapter was filled with inaccurate, misleading, and/or totally speculative statements.2. The correlation between credit score and income is nowhere near as strong as she makes it appear in this book.3. Many of the problems she identifies have nothing to do with the models in question. Advertising models are not the issue with the payday loan or for-profit college industry those industries need to be totally upended/destroyed at the source. Letting them operate as is but kneecapping their ability to advertise would be nonsensical.4. The solutions she presents are ridiculously vague and/or impractical."
222,0553418815,http://goodreads.com/user/show/1045774-mehrsa,5,This is the book I was waiting to read! The rejoinder to our data-obsessed culture. We got drunk on algorithms and forgot to focus on their limitations. are they asking the right questions? are they recycling inequality? Great book
223,0553418815,http://goodreads.com/user/show/3126915-imi,2,"Sadly, a rather superficial look at big data, algorithms and how they've impacted on society. You probably know/have guessed all this already. The correlations O'Neil point out are not very convincing, and it felt like she just wanted to rant about how unfair the world is. Not that I disagree with that conclusion; society is unfair and unequal. But O'Neil really doesn't state much more than that, and, worse, some of her examples felt misleading or simply speculative. Where was her evidence? And if she'd explored this issues in more depth I'd have wanted clear ideas for resolving these issues. Instead, any suggestions she made seemed vague and impractical. I admitedly feel out of my depth with this topic, but am most definitely willing to learn more and expand my understanding about it. Disappointingly, this book didn't offer that."
224,0553418815,http://goodreads.com/user/show/777176-dave,5,"I learned more from this book than I have from any book I've read in years. O'Neil is a former math professor and spent time as a hedge-fund quant, so she understands the theory and experience of the effect of out-of-control algorithms on our society.As corporations make more and more efficient use of algorithms, they are starting to show dangerous tendencies, according to O'Neil. She calls them ""weapons of math destruction"" or WMDs. WMDs present the following grave problems for society:--They usually lack transparency, such that it is difficult or impossible to know what goes into them and what factors sway or change their results. When they are opaque, WMDs are patently unfair (O'Neil cites the algorithms used to evaluate teachers in many districts, highlighting their absurdity as they attempt to quantify the unquantifiable).--They make broad generalizations and reduce human beings to ""buckets."" One of the worst examples is the criminal justice system, which is increasingly reliant on algorithms to deploy police patrols and even to sentence criminal defendants. These algorithms punish the innocent by assuming that those who live in poor neighborhoods are necessarily more likely to commit crimes, thereby subjecting them to frequent ""stop and frisk"" type searches that have been found to be unconstitutional repeatedly.--They can use information in an unfair way that tends to perpetuate further injustice. A prime example of this is the increasing tendency to identify individuals by their credit score, on job applications and even in online shopping. When job applicants are screened by credit score, those who are already poor (and therefore much more likely to have a low credit score already) will find it even harder to get a job and thereby earn enough money to help improve their credit score. This self-perpetuating cycle of poverty is completely unnecessary, because credit scores are an exceedingly poor proxy for job-worthiness of applicants.--Many WMDs are not very good at learning. They are created as models of human behavior (with all the flaws and assumptions of the humans who programmed them), and when they miss (when for example they screen out a potential hire who would have actually been a first-rate employee), they never learn that they have missed. The ""false positive"" never presents itself (the rejected applicant goes on to success elsewhere), and the oblivious WMD continues to chug along with its assumptions and blind spots unchallenged.And WMDs are like racism:""Racism, at the individual level, can be seen as a predictive model whirring away in billions of human minds around the world. It is built from faulty, incomplete, or generalized data. Whether it comes from experience or hearsay, the data indicates that certain types of people have behaved badly. That generates a binary prediction that all people of that race will behave that same way....Racists don't spend a lot of time hunting down reliable data to train their twisted models. And once their model morphs into a belief, it becomes hardwired. It generates poisonous assumptions, yet rarely tests them, settling instead for data that seems to confirm and fortify them. Consequently, racism is the most slovenly of predictive models. It is powered by haphazard data gathering and spurious correlations, reinforced by institutional inequities, and polluted by confirmation bias. In this way, oddly enough, racism operates like many of the WMDs I'll be describing in this book."" (p. 22-3)And finally:""Models like this will abound in coming years....Many of these models, like some of the WMDs we've discussed, will arrive with the best intentions. But they must also deliver transparency, disclosing the input data they're using as well as the results of their targeting. And they must be open to audits. These are powerful engines, after all. We must keep our eyes on them.... Data is not going away....Predictive models are, increasingly, the tools we will be relying on to run our institutions, deploy our resources, and manage our lives. But...these models are constructed not just from data but from the choices we make about which data to pay attention to--and which to leave out. Those choices are not just about logistics, profits, and efficiency. They are fundamentally moral. If we back away from them and treat mathematical models as a neutral and inevitable force, like the weather or the tides, we abdicate our responsibility....We must come together to police these WMDs, to tame and disarm them."" (p.218)"
225,0553418815,http://goodreads.com/user/show/3029658-doug-cornelius,5,"With big data, comes formulas to parse through the data trying to make sense of it. Those algorithms can help make sense of the data and help filter through the noise to find trends. But those algorithms can also be easily misused and have harmful, if unintended consequences. Cathy O'Neil explores these problems in 
Weapons of Math Destruction
.Ms. O'Neil is a former academic, hedge fund quant, and data scientist for startups. At the hedge fund she was looking at ways to make money by predicting financial trends. As a data scientist she was looking to predict people's purchases and browsing habits to monetize those habits. Those are good algorithms where the people using them understand them and update the algorithms as they see problems and can check to see if they are working properly by verifying outcomes.Many data driven outcomes fail. Those are the focus of 
Weapons of Math Destruction
.I'm a big fan of the Slate Money podcast with Cathy O'Neil, Felix Salmon, and Jordan Weissmann. When the publisher offered me a review copy of Ms. O'Neil's book, I jumped at the chance.It's a short book and even though the title makes it sound like it's full of math, it's not. It's more about social policy. She points to the danger of decision makers blindingly following algorithmic output that they do not understand.Take the US News and World Report listing of best colleges. This is preeminent guide for high school students trying to figure out which college to attend. The rankings do not look at actual student outcomes. The publisher does not look at happiness, learning or improvements. It looks at proxies for the outcomes like SAT scores, alumni contributions, and graduation percentages. Colleges started making decisions to improve their rankings in the algorithm by improving those measured proxies. As anyone writing checks for their college bound children, affordability is not one of the measured proxies.As you might expect there are big chunks of the book dedicated to failings in the financial sector by relying on poorly designed algorithms. The biggest problems often being false assumptions plugged into the data running through the algorithm.With the election season upon us, the chapter on political algorithms is fascinating. The data scientists of commerce were brought into political campaigns. They are able to micro-target potential voters sending different messages to different groups highlighting the positions that would make the candidate more favorable to that segment of the population. Civic engagement and the political process is increasingly being algorithm driven.Since our world is increasingly being driven by big data, it's important to understand what is happening behind the decision-making. Weapons of Math Destruction is an excellent tool to help you understand data driven decision-making."
226,0553418815,http://goodreads.com/user/show/23020614-germ-n,3,"A fascinating must-read guide on how to navigate and interpret a world increasingly ruled by materialistic prejudices and oversimplifications that are repackaged as ""science"" and ""tech"" through poorly designed computer algorithms.Reading this gives one the feeling that the ""evil androids ruling over mankind"" dystopia has already been unrolling. Except in a very sneaky way: this time we don't get to face or even understand the robots and their actions which are nonetheless already reshaping our reality and the course of our lives through ridiculous university rankings, credit scores, social media ideological bubbles, etc.Unfortunately O'Neil ruins an otherwise brilliant book by an extreme leftist bias that seeks to absolve her personal heroes in the US democratic party and their political agendas, and attempts to attribute most blame on a specific other side. I assume this is why a book which raises such important points gets to be published and become mainstream after all.For example, she portrays Obama as a saint who just wants the best for immigrants and fiercely battles the evil rich. Yet for any unbiased observer, it's quite clear that Obama and democrats are just serving the wealthy who finance their own political campaigns (duh) when they set to ruin nations overseas and bring in thousands of immigrants, ideally illegal ones, so the price of labor can go down and corporations can continue to reap political advantages.This skewed perspective is damaging to the bottom argument and distracts readers from what's really the main issue here: the constant loss of truth and distortion of increasingly astroturfed public debate. No attempts are made, for example, at addressing how these WMDs are influencing the toxic foreign policy of the US and the press - another entity that passes as a saint in this book, while being the number one responsible for war and destruction of nations around the world."
227,0553418815,http://goodreads.com/user/show/20465079-tam,4,"A quick and highly useful read - I would recommend it to all people not just ones working with predictive models and Big Data.The title is sensational enough, but at one point I put that aside and decided to read this book. So many have been praising Big Data and Machine Learning, few cautions have been put forward but not enough, mostly dealing with ethics, security, personal data issues. This book, on the other hand, discusses the large scale tragedies that Big Data can invoke on societies as a whole, if continuing to go unchecked. 10 chapters deal with examples about the misuses of data and predictive models in many aspects of lives, especially the lives of the most unfortunate.O'Neil has a strong opinion and at certain point I thought she might be too pessimistic (saying that from a self-proclaimed pessimistic person as I am), and O'Neil freely claims how angry she is with the whole thing. Yet at the very end of the book, she makes constructive comments, ideas for improvement, for regulations. Those are the most important, and perhaps the most useful passages for data scientists. I've been quite fascinated by the tool and sometimes forget the darker side that the tool can invoke, and I'd like to thank O'Neil for the reminder, for the clarification about issues that sometimes I feel uneasy but not so clear about. No, the tool is neither inherently good nor bad, the people who make them and wield them can choose do what they want. And we can choose to do good things.Ehh, not so optimistic though, there are tons of challenges and... But I should shut my mouth. But at least we should try."
228,0553418815,http://goodreads.com/user/show/15409024-ahalya-vijay,5,"I found this to be a fascinating read on the importance of using technology and data in an ethical way. I was shocked to learn how pervasive irresponsible uses of data are throughout the developed world and how widespread it is for people to place their faith in ""black box"" models as Cathy O'Neil refers to them. It must be frustrating, for example, for teachers to be evaluated (and fired!) based on a model they have no knowledge of, without knowing how to improve their rating. And it must be even more so if they are getting anecdotally strong and positive feedback from school leadership and parents.Another shock was how liberally companies and 'data scientists' use proxies without thought to the consequences of the model not capturing the 'error term' sufficiently. The danger of making decisions based on things that are modeled (or estimated) using proxies was drilled into my head throughout my entire schooling. How would you feel if the fact that you were late to one meeting at work was used to brand you as an irresponsible or unreliable worker? Ms O'Neil made a clear link between such models being used to trap people in a cycle of poverty, using example after example across a variety of industries (e.g., education, finance, insurance). It is clear that reform is needed, but what left me worried at the end is that these models are not hurting the rich and famous, but the poor and under-resourced. Will our current political and justice system listen to their needs? I'm skeptical."
229,0553418815,http://goodreads.com/user/show/48433695-alexis-tremblay,5,"A must read for all data scientist, mathematicians, modellers, statisticians,... There is a dark side to big data and she exposes it masterfully. I always had an uneasy feeling when I started my career as an actuary and later on as data scientist. My moral compass was always off but I didn't have the words for it. Now she has given me a framework to think about my job and what to look out for. Will read again. "
230,0553418815,http://goodreads.com/user/show/16204614-bing-gordon,4,"Good overview of modern analytics, but too much of a guilt trip laid on top. "
231,0553418815,http://goodreads.com/user/show/6100646-brian-clegg,4,"As a poacher-turned-gamekeeper of the big data world, Cathy O'Neil is ideally placed to take us on a voyage of horrible discovery into the world of systems making decisions based on big data that can have a negative influence on lives - what she refers to as 'Weapons of Math Destruction' or WMDs. After working as a 'quant' in a hedge fund and on big data crunching systems for startups, she has developed a horror for the misuse of the technology and sets out to show us how unfair it can be.It's not that O'Neil is against big data per se. She points out examples where it can be useful and effective - but this requires the systems to be transparent and to be capable of learning from their mistakes. In the examples we discover, from systems that rate school teachers to those that decide whether or not to issue a payday loan, the system is opaque, secretive and based on a set of rules that aren't tested against reality and regularly updated to produce a fair outcome.The teacher grading system is probably the most dramatically inaccurate example, where the system is trying to measure how well a teacher has performed, based on data that only has a very vague link to actual outcomes - so, for instance, O'Neil tells of a teacher who scored 6% one year and 96% the next year for doing the same job. The factors being measured are almost entirely outside the teacher's control with no linkage to performance and the interpretation of the data is simply garbage.Other systems, such as those used to rank universities, are ruthlessly gamed by the participants, making them far more about how good an organisation is at coming up with the right answers to metrics than it is to the quality of that organisation. And all of us will come across targeted advertising and social media messages/search results prioritised according to secret algorithms which we know nothing about and that attempt to control our behaviour.For O'Neil, the worst aspects of big data misuse are where a system - perhaps with the best intentions - ends up penalising people for being poor of being from certain ethnic backgrounds. This is often a result of an indirect piece of data - for instance the place they live might have implications on their financial state or ethnicity. She vividly portrays the way that systems dealing with everything from police presence in an area to fixing insurance premiums can produce a downward spiral of negative feedback.Although the book is often very effective, it is heavily US-oriented, which is a shame when many of these issues are as significant, say, in Europe, as they are in the US. There is probably also not enough nuance in the author's binary good/bad opinion of systems. For example, she tells us that someone shouldn't be penalised by having to pay more for insurance because they live in a high risk neighbourhood - but doesn't think about the contrary aspect that if insurance companies don't do this, those of us who live in low risk neighbourhoods are being penalised by paying much higher premiums than we need to in order to cover our insurance. O'Neil makes a simplistic linkage between high risk = poor, low risk = rich - yet those of us, for instance, who live in the country are often in quite poor areas that are nonetheless low risk. For O'Neil, fairness means everyone pays the same. But is that truly fair? Here in Europe, we've had car insurance for young female drivers doubled in cost to make it the same as young males - even though the young males are far more likely to have accidents. This is fair by O'Neil's standards, because it doesn't discriminate on gender, but is not fair in the real world away from labels.There's a lot here that we should be picking up on, and even if you don't agree with all of O'Neil's assessments, it certainly makes you think about the rights and wrongs of decisions based on automated assessment of indirect data."
232,0553418815,http://goodreads.com/user/show/7019761-cindy,4,"While I don't necessarily agree with all of the conclusions, this was a very thought-provoking book. The author is a mathematician who has worked on Wall Street and now takes on some of the math models that rule our lives. Some of these were, at least originally, developed with good intentions - intending to make fairer some of the decisions that affect our lives: where and who gets into college; what the schedule for our job looks like; how much we pay to get a loan; even how you are judged in your chosen profession. But the author poses that many of these types of decisions are actually less than fair. The models are in her terms WMDs - weapons of math destruction - they are hidden, unregulated, and virtually uncontestable, even when they are blatantly wrong. One of the more troubling examples from the book concerned the use of testing to judge teacher performance. She traced how an exemplary teacher with outstanding evaluations was judged as poor on the basis of one set of tests and fired. The teacher showed that the tests which showed that her ""value-added"" score was sub-par were flawed due to a high number of erasures on the previous year which indicated cheating. Indeed, the system said that there *might* have been cheating but they would not re-instate this excellent teacher. (And please don't get me started on ""value-added"" - my daughters' school was always excellent in all the testing but this score was just average. So if your kids are already high achievers, how do you continue to score excellent in this measure? The answer is it's almost impossible.)Other examples showed that even when you removed race or ethnicity or economics as an indicator in these WMDs, they were there. If a zip code puts you in a risky category for a student loan, then it's hard to get the education that can get you out of poverty; or if a work schedule makes it hard for a single mother to get the child care she needs so that she can get the money to go to school to make a better life for herself and her child...well, I can see how some of this sets up a self-fulfilling prophecy. It makes it harder, not impossible, to break this cycle. That's why this book is important. If this is being used, then we need to know and like credit scores have the ability to correct errors and inaccuracies that work against the average person.Quotes to remember:....This would all be wonderful for students and might enhance their college experience, if they weren't the ones paying for it in the form of student loans that would burden them for decades. We cannot place the blame for this trend entirely on the US News rankings. Our entire society has embraced not only the idea that a college education is essential, but the idea that a degree from a highly-ranked school can catapult a student into a life of power and privilege. As I write this, the entire voting population that matters lives in a handful of counties in Florida, Ohio, Nevada, and a few other swing states. Within those counties is a small number of voters whose opinions weigh in the balance. I might point out here that while many of the WMDs we've been looking at, from predatory ads to policing models, deliver most of their punishment to the struggling classes, political microtargeting harms voters of every economic class, from Manhattan to San Francisco, rich and poor alike find themselves disenfranchised, though the truly affluent of course can more than compensate for this with campaign contributions."
233,0553418815,http://goodreads.com/user/show/6894205-allie,4,"This was by far the most interesting book about math I have ever read (admittedly a low bar to set), possibly because it contains no actual math formulas. Instead, former mathematics professor and hedge fund analyst Cathy O'Neil tells stories about the applications of math - how ""ill-conceived mathematical models now micromanage the economy from advertising to prisons."" Among other things, mathematical models determine which news you see on social media, your Google search results, the cost of your auto insurance and credit card rates, which neighborhoods police are sent to patrol, and how your child's schools are evaluated. In one staggering statistic about the reach of these models, O'Neil notes that 72 percent of resumes are never seen by humans, but are instead screened and scored by software programs looking for employer keywords. While math itself is neutral, the models are based on assumptions that reflect human biases and limitations. Often, the models are designed to process data from large numbers of people efficiently, but not necessarily fairly. She distinguishes between ""good"" models built on transparent assumptions that are constantly corrected with new data (e.g., those used in professional baseball) to opaque models that use proxy data such as race or zip codes and do not have a corrective feedback mechanism (e.g., predictive policing software). Typically, there is no method of recourse when given a poor score by a computer (hello, Hal) since companies consider their algorithms proprietary and won't disclose details of how scores are developed. This was an issue for teachers evaluated - and sometimes fired - based on inaccurate models of student test scores. The book's second main argument is that the built-in biases in the models are especially putative towards the poor, who are steered to high interest loans, targeted by ""stop and frisk"" policing based on their neighborhoods, and less likely to be personally interviewed for jobs.I found many of O'Neil's arguments persuasive and somewhat alarming, although she tries to end the book on a positive note. Worth reading, even if you don't like math. Or weapons. Or destruction. "
234,0553418815,http://goodreads.com/user/show/70315594-enoughtohold,4,"A nice, clear overview of the problems of big data for laypeople. Not sure why other reviewers expected this to be deeply technical, or why they think O’Neil thinks there are easy answers — she plainly doesn’t.I for one am glad a book this accessible exists on this topic. After all, the vast majority of the people affected by these harmful models are not data scientists, and they deserve to know what’s going on.(Also, the author did a great job reading for the audiobook.)"
235,0553418815,http://goodreads.com/user/show/4430576-miri,5,"I've said it before, and I'm sure I'll say it again, because I make a point of seeking out books like this. But if there's one book I could get everyone to read right now, it would be this one. Even next to Ta-Nehisi Coates's We Were Eight Years in Power, which I'm reading concurrently and which is staggering in its importance, this book feels urgent—because what it explains is how our new economy of Big Data is codifying, concealing, and amplifying inequality of every kind, on an enormous scale. Every injustice I'm reading about in Coates's book is made worse by what I read about here. We think of math as inherently neutral, the way we think of technology and logic and science. But these things are like any tool, dependent on the person using them, capable of being used for good or evil. (Remember the way science was abused throughout American history, fabricating the entire concept of biological race to provide justification for white humans to enslave black ones.) When we start using math to make decisions instead of people, assuming that this will decrease human bias and make things more fair, what we can actually end up doing is obscuring that prejudice and magnifying the scale of its effect. Cathy O'Neil was a mathematician working in the finance industry in 2008. She left after a few years, disgusted with the way the system was clearly rigged, and has since created the concept of Weapons of Math Destruction: mathematical models that codify human prejudice, are opaque and impervious to feedback, and affect large numbers of people. In this book, O'Neil takes us through several WMDs, showing how they're used in different areas of our lives. There's the disastrous school reform attempt in Washington D.C., which tried to evaluate teachers in a way math simply cannot be used, and which fired hundreds of teachers based on results no one in the district could explain or defend. There's the way many employers now use credit scores to screen applications, assuming that making regular payments means you'll be a dependable employee, ignoring the facts that (1) responsible people experience financial difficulty too and (2) preventing people with bad credit from getting jobs is only going to make their situation worse.There are the computerized risk models that judges use in sentencing, trying to gauge from prisoners' background information—where they grew up, whether their friends and family have criminal records; information which would be inadmissible in court—how likely they are to return to prison after being released, and how long their sentence should be. There's the way U.S. News & World Report helped cause an arms race when it decided to start ranking universities on everything except their cost, driving up tuition over 500 percent (nearly four times the rate of inflation) in just the 30-ish years that I've been alive.There's political advertising which is now so individually personalized that even people who support the same candidate, even people visiting the candidate's website, will see totally different things based on the profile the campaign has created for them.There's predatory advertising like that done by for-profit colleges, which specifically targets vulnerable people's ""pain points"" looking for those who are ""stuck,"" who have ""few people in their lives who care about them,"" who have ""low self-esteem"" and are ""unable to see and plan well for the future"" (these quotes, shared by O'Neil, come from the California Attorney General's office when it was investigating one of these colleges). (And if you're wondering how they would know this about people, the point is that they know this about all of us now. Just try to imagine what someone could learn about you from seeing everything you do online.)It might seem like the logical response is to disarm these weapons, one by one. The problem is that they’re feeding on each other. Poor people are more likely to have bad credit and live in high-crime neighborhoods, surrounded by other poor people. Once the dark universe of WMDs digests that data, it showers them with predatory ads for subprime loans or for-profit schools. It sends more police to arrest them, and when they're convicted it sentences them to longer terms. This data feeds into other WMDs, which score the same people as high-risk or easy targets and proceed to block them from jobs, while jacking up their rates for mortgages, car loans, and every kind of insurance imaginable. This drives their credit rating down further, creating nothing less than a death spiral of modeling. Being poor in a world of WMDs is getting more and more dangerous and expensive.The same WMDs that abuse the poor also place the comfortable classes of society in their own marketing silos . . . For many of them it can feel as though the world is getting smarter and easier. Models highlight bargains on prosciutto and chianti, recommend a great movie on Amazon Prime, or lead them, turn by turn, to a cafe in what used to be a ""sketchy"" neighborhood. The quiet and personal nature of this targeting keeps society's winners from seeing how the very same models are destroying lives, sometimes just a few blocks away.Though we're all affected by them in ways we can't see, right now the majority of the damage is being done to the same groups who are always hurt by flaws in the system—minorities and the poor. The same models could easily be used to help people instead of prey upon them, but that won't happen in the glorious free market of capitalism. We need regulation and oversight in the data industry. We need accountability. When statistics itself, and the public’s trust in statistics, is being actively undermined by politicians across the globe, how can we possibly expect the Big Data industry to clarify rather than contribute to the noise? We can because we must. Right now, mammoth companies like Google, Amazon, and Facebook exert incredible control over society because they control the data. They reap enormous profits while somehow offloading fact-checking responsibilities to others. It’s not a coincidence that, even as he works to undermine the public’s trust in science and in scientific fact, Steve Bannon sits on the board of Cambridge Analytica, a political data company that has claimed credit for Trump’s victory while bragging about secret ""voter suppression"" campaigns. It’s part of a more general trend in which data is privately owned and privately used to private ends of profit and influence, while the public is shut out of the process and told to behave well and trust the algorithms. It’s time to start misbehaving. Algorithms are only going to become more ubiquitous in the coming years. We must demand that systems that hold algorithms accountable become ubiquitous as well."
236,0553418815,http://goodreads.com/user/show/17187084-sean-goh,4,"Very readable book warning of the downside of amoral models. With great power comes great responsibility.___Math-powered applications were based on choices made by fallible human beings. Many models encoded human prejudice, misunderstanding, and bias into the software systems that increasingly managed our lives. Like gods, these mathematical models were opaque, their workings invisible to all but the highest priests in their domain: mathematicians and computer scientists.Models are opinions embedded in mathematics.Statistical engines without feedback can continue to spin our faulty and damaging analysis while never learning from its mistakes. E.g. teachers mistaken labelled as failures are fired, and no one is the wiser. The model's output becomes unquestioned reality.The parallels between finance and Big Data: The productivity of the field's talents indicates that they are on the right track, and it translates into dollars. This leads to the fallacious conclusion that whatever they are doing to bring in more money is good. It ""adds value"". Otherwise why would the market reward it?In both cultures Wealth is no longer a means to get by. It becomes directly tied to personal worth.On the rise of university rankings: from the perspective of a university president, it's quite sad. Here they are at the summit of their careers dedicating enormous energy towards boosting performance in fifteen areas defined by a group of journalists at a second-tier magazine (U.S. News). The ranking had become destiny. When you create a model from proxies, it is far simpler for people to game it. This is because proxies are easier to manipulate than the complicated reality they represent. (e.g. trying to evaluate influence by number of twitter followers)In a system in which cheating is the norm, following the rules amounts to a handicap. So preventing the students in Zhongxiang from cheating was unfair.In our largely segregated cities, geography is a proxy for race.Patrolling particular areas creates a pernicious feedback loop. The policing itself spawns new data, which justifies more policing. Eventually the zeroing in on ghetto districts ends up criminalising poverty, believing all the while that our tools are not only scientific but fair.Instead of simply trying to eradicate crimes, police should be attempting to build relationships in the neighbourhood. This was one of the pillars of the original ""broken windows"" study. The cops were on foot, talking to people, trying to get them to uphold their own community standards. But that objective, in many cases, has been lost, steamrollered by models that equate arrests with safety.Mathematical models can sift through data to locate people who are likely to face great challenges, whether from crime, poverty, or education. It's up to society whether to use that intelligence to reject and punish them - or to reach out to them with the resources they need. It all depends on the objectives we choose.Computing systems have trouble finding digital proxies for soft skills. The relevant data simply isn't collected, and anyway it's hard to put a value to them. They're usually easier to just leave out of the model.The modelers of e-scores (unofficial credit ratings) have to make do with trying to answer the question: ""How have people like you behaved in the past?"" when ideally the question would be ""How have you behaved in the past?Even with the Affordable Care Act, which reduced the ranks of the uninsured, medical expenses remain the single biggest cause of bankruptcies in America.A sterling credit rating is not just a proxy for responsibility and smart decisions. It is also a proxy for wealth. And wealth is highly correlated with race.Mistakes made by models are learning opportunities - as long as the system receives feedback on the error. When automatic systems sift through our data to size us up for an e-score, they naturally project the past into the future. The poor are expected to remain poor forever and are treated accordingly. It's inexorable, often hidden and beyond appeal, and unfair.Yet machines cannot make adjustments for fairness by themselves. That is where the human overseer comes in.Human decision making, while often flawed, has one chief virtue. It can evolve.Big data processes codify the past. They do not invent the future. Doing that requires moral imagination, and that's something only humans can provide. We have to explicitly embed better values into our algorithms, creating Big Data models that follow our ethical lead. Sometimes that will mean putting fairness ahead of profit."
237,0553418815,http://goodreads.com/user/show/6991670-bob,4,"Summary: An insider account of the algorithms that affect our lives, from going to college, to the ads we see online, to our chances of getting a job, being arrested, getting credit and insurance.Big Data is indeed BIG. Mathematical algorithms shape who will see this post on their Facebook newsfeed. If you go to Amazon or another online bookseller, algorithms will suggest other books like this one you might be interested in. Have you seen all those ads about credit scores? They are more important than you might imagine. Algorithms used by employers and insurance companies determine your employability and insurability in part through these scores. Far more than another credit card (bad idea, by the way) or a mortgage are on the line. These algorithms seem objective, but how they are formulated, and the assumptions made in doing so mean the difference between useful tools that benefit people, and ""black boxes"" that thwart the flourishing of others, often unknown to them.Cathy O'Neil should know. A tenure track math professor, she made the jump to Wall Street and became a ""quant"" who helped develop mathematical algorithms and witnessed, in the crash of 2008, the harm some of these caused. And she began to notice how algorithms often painfully impacted the lives of many others.  She describes how a teacher was fired because of the weighting of performance scores of a single class, despite other evaluations finding her an excellent teacher (afterwards it was found that there were a high number of erasures on tests for students who would have been in her class the previous year, suggesting these had been altered to improve scores).As she looked at the algorithms responsible for such injustices, she came to dub them ""Weapons of Math Destruction"" or WMDs and she identified three characteristics of these WMDs:Opacity: those whose lives are affected by them have no idea of the factors and weighting of those factors that contributed to their ""score"".Scale: how widely an algorithm is applied across industries and sectors of life can affect how much of one's life is touched by a single formula. For example, the FICO scores mentioned above affect not only credit, but the ability to get a job, the cost of auto insurance, and your ability to rent an apartment.Damage: WMDs can reinforce other factors perpetuating a cycle of poverty, or incarceration.She also shows that what makes these algorithms destructive is the use of proxy measurements. For example an employer may not know directly how savvy someone is as a marketer, and so they use a ""proxy"" measurement of how many Twitter followers that person has. Or age is used as a proxy for how safe a driver one is. For a group, the proxy may work well, and be utterly inaccurate for an individual that falls within that proxy group.Then in successive chapters, she chronicles some of the ways WMDs operate in different parts of life. She discusses the U.S. News & World Report college rankings, and the use of algorithms in admissions processes. Social media uses algorithms to target advertising, which means some will see ads for for-profit schools and payday lenders, and others for upscale furnishing or Viagra, based on clicks, likes, searches, and comments. Policing strategies, including locations for intensified ""stop and frisk"" policing are shaped by another set of algorithms. Algorithms to filter resumes may use scoring algorithms that discriminate by address and psych exam algorithms may render others unemployable in a certain industry. Scheduling algorithms may promote efficiency at the expense of the ability of workers to sleep on a regular schedule, or arrange childcare, or work enough hours to qualify for health insurance. Algorithms sometimes shut people out from credit or low cost insurance when in fact they are good risks. She concludes by showing how algorithms determine ads and news we see (and don't see). In an afterword she explores the flaws in algorithms revealed on the election of Donald Trump (algorithms, for example predicted Clinton would easily win Michigan and Wisconsin, where consequently she did not campaign, and lost by small margins).In her conclusion, she makes the case not only for a code of ethics for mathematicians but also argues that regulation and audits of these algorithms are necessary. The value assumptions, as well as the mathematical methods of many algorithms are flawed, and yet opacity means those whose lives are most affected don't even know what hit them.She helps us see both the sinister and useful side of these algorithms. They may reveal where a pro-active intervention may save a family from descending into family violence, or provide extra assistance to a child in danger of falling behind in a key subject. Or they may be used to invade personal rights, or even to perpetuate socio-economic divides in a society. The reality is that the problem is not the math but the old GIGO problem (garbage in, garbage out). The values and assumptions of the humans who devise the formulas and weightings of values and the use of proxies determine what may be destructive outcomes for some people. Yet it can be hidden behind an app, a program, an algorithm.The massive explosion in storage capacities, processing speeds, and the way our interests, health status, travel patterns, spending patterns, fitness, diet and sleep habits, our political inclinations and more may be tracked via our online and smartphone usage makes O'Neil's warning an urgent one. We create mountains of data that may be increasingly mined by government and private interests. Perhaps as important as asking whether this will be governed in ways that contribute to our flourishing, is whether we will be alert enough to care.____________________________Disclosure of Material Connection: I received this book free from the publisher. I was not required to write a positive review. The opinions I have expressed are my own."
238,0553418815,http://goodreads.com/user/show/51162416-max,5,fast pleasant primer on what’s wrong with big data. Fun. I love using a VPN turning everything off and being practically nobody to these freaks 
239,0553418815,http://goodreads.com/user/show/4061006-angela,5,"Cathy O'Neil is one of my heroes; I love listening to her on Slate Money, and her less pop/more technical book, Doing Data Science, occupies a prominent place on my desk at work - and a special place in my heart. Going beyond the usual data science manuals of what a random forest is, or how to account for large, sparse data, O'Neil and her coauthor - in that book - make an almost anthropological survey of the data science ""field""/profession/whatever. It's enlightening, and it's very real; less a textbook, more a professional guide.So if Doing Data Science is the ""how to do your job"" manual, and Slate Money is the occasional soapbox, Weapons of Math Destruction is the manifesto. With clarity, conviction and intelligence, Cathy O'Neil scalpels away at the utopian, misty-eyed tech worship that has contributed to the data science/big data bubble. She identifies the ways in which algorithms encode and perpetuate existing prejudices, and are hidden from plain sight by the obscuring (both intentional and not) use of Fancy Math.She notes how anti-human ""market efficiency"" is, and how it's often been short-hand for unjust and unequal systems, and big data is just scaling those dumb systems way, way up. For example, the use of social network data to ""enrich"" lending platforms; or recidivism models that ask about where a former convict grew up, whether his/her friends are in jail, how many encounters with the police he/she has had. These algorithms - because they chase correlations to make predictions (rather than identifying causal factors) - create big-data-driven poverty traps. They are also beyond the purview of the government (and, given the new administration, will probably be for quite some time): just like your FICO score is ONLY about how often you pay your credit card bills, and NOT your demographic group, so too should these algorithms be scrutinized and regulated.Increasingly, O'Neil notes, the ""human touch"" comes at a price; in upper income groups, you get more real, live service - hearing about a job through connections, informational interviews with alumni, blah - while lower income groups are serviced by increasingly mechanized, machine learning code. The hiring and HR practices of CVS, McDonald's, and other low wage places were especially striking (and severe): mindless, well-meaning algorithms deployed to effectively punish workers whose BMIs (itself a stupid measure of health) are too high, or who seem ""anti-social"" according to a creepy questionnaire, or whatever. I think my favorite chapter - and, of course, the most relevant one - was about the toxic brew that can be politics and data science. It was especially powerful to think through the way that, as opposed to the past, where we had ""agreed upon facts"" given by carpet bombing-style hegemonic culture (e.g. four news channels that everyone watched), now we have hyper-tailored DARE I SAY ""alternative facts"" where, by the power of marketers (who prey on confirmation bias), your neighbor's political beliefs are the result of a very specific and private relationship between them, their web browser cookies, and some political marketer. This means you may have NO IDEA why your swingy neighbor in swingy state X thinks Candidate Y is running an underground child sex ring literally under the ground of a DC pizza shop. O'Neil (and this book was written Before The Madness) notes that these sorts of micro-targeted ads that follow you around the web are very smartly tailored; the marketers know who would be repulsed by such a stupid conspiracy theory, and who would be intrigued. And that's just terrifying. Anywayyyyy. From a data perspective, my (cold dead) economist heart reminds me that OF COURSE we're going to scale up injustice when we mindlessly maximize profit, instead of humans. We need to go back to our roots: to the philosophy and debates of Jeremy Bentham, and welfare economics, and what ""utility"" really means. Efficiency is not the end all, be all, despite what our current economic system seems to diktat. O'Neil also makes a good point about the repeated erroneous conflating of correlation and causation; something even the fanciest of linear algebra stochastic process meddlers fall prey to. That's something that I see a LOT in data science: the field is built on the premise of perfect prediction - and prediction doesn't need causation, it just needs correlation. OK, I will step off my soapbox. Highly recommended.edited to add: OMG and I just read her delightful 2017 resolutions post, and had much LOLs. Cathy O'Neil, you da best."
240,0553418815,http://goodreads.com/user/show/8325737-philipp,4,"This is an angry book, but its got a point! She summarises many 'weapons of math destruction', WMDs, algorithms that encode human biases, are opaque, which give recommendations or orders that no-one questions, while making things worse. For example, this summary:Poor people are more likely to have bad credit and live in high-crime neighborhoods, surrounded by other poor people. Once the dark universe of WMDs digests that data, it showers them with predatory ads for subprime loans or for-profit schools. It sends more police to arrest them, and when they’re convicted it sentences them to longer terms. This data feeds into other WMDs, which score the same people as high risks or easy targets and proceed to block them from jobs, while jacking up their rates for mortgages, car loans, and every kind of insurance imaginable. This drives their credit rating down further, creating nothing less than a death spiral of modeling. Being poor in a world of WMDs is getting more and more dangerous and expensive.O'Neil goes through many, many of examples like this where an automated approach made things worse, where people didn't bother to critically look at the output, where the model doesn't have a way to incorporate feedback, it's stuck in time and cannot evolve like humans. The majority of the book is spent on case studies and great, angry discussion of what went wrong. There's one short chapter on what could be done - mostly she proposes more regulation (auditing of algorithms), but also to 'dumb' down models by law, force them to treat people equally and not bin them in groups. That chapter on solutions is extremely short - I expect a second book from her on that topic.P.S.: I've been reading a lot about adversarial perturbations in deep learning (see this great paper). One idea could be to counteract some of those models to become an adversarial entity. For example, you have one of those shopping points cards. Instead of feeding their model with your shopping data, you mix it up - write down your shopping list for a month, shuffle it, and buy one part with your points card and the other in cash. Keep mixing it up and add random stuff. Your data should be garbage to the model as it's not consistent, easily removable as an outlier, i.e., worthless.You could also make the model work for you - find out which groups of customers the market sends special offers to, game your shopping so that you fall into that group, reap rewards.Of course you'd need to know in detail what the model is looking for, which is highly unlikely."
241,0553418815,http://goodreads.com/user/show/115473-siria,4,"In the two years since Cathy O'Neil published this study of the danger posed by mathematical models—how their supposed neutrality merely reflects the biases of those who create them while creating closed-system feedback loops that warp society in unjust ways—it hasn't become any less relevant. In fact, not only does the book remain relevant, but subsequent revelations—most prominently those about Facebook's involvement with the 2016 US elections—show that O'Neil may even have been too cautious in what is a trenchant and prescient critique. "
242,0553418815,http://goodreads.com/user/show/25178051-izzat-radzi,5,"""Mathematical models are supposed to be our tools, not our masters"" Statisticians, quants, actuaries : take heed. This book argues that the new 'industrial revolution' of big data is bringing destruction. To whom (?) , one might be tempted to asked. Humans, normal masses who sometimes makes errors of course.And due to that, they got red in their ledgers, which in these new century means, that they'll most likely subjected to the 'vicious loop' of rejection and manipulation, and will felt the usual 'life is not fair' feeling. To start off, I claim that this book is the best (so far) compliment to Taleb's Black Swan book. Whilst Taleb as arguably he as a trader and academia revolves his discussion around some financial models & it's discontent (whilst in most part slanted his discussion into the philosophical realm); this author meanwhile goes into a more broad discussion. She, whom has background in chronologically in academia, hedge funds, a short stint in risk management before focusing work as a data scientist, argues that given the presence of these Gargantuan Big Data in almost every aspect of life, it has tarnished if not destroyed people's life. The problem-before I proceed case by case- lies in two reasons. First is the data collected & recorded by the data mining by corporate companies, mostly are left uncrunched, meaning they're not subjected to updated information about those people data taken or they're mistakenly (yet significantly affect) people who are thought 'one', in this case people with the same name and birthday. This, argued by the author, because the model used isn't not tinkered time to time according to feedback; unlike those used in baseball & basketball where opponents and managers subject each individual players to new & paternalistic habits of players -thus a better use of correlation to causation-, they are taken as face value. And feedback that is not taken -later I'll mentioned in job application survey- left those aspiring people hoping to get some good news, baffled as they themselves don't know why their application was rejected. Turns out, it was their 'behaviour', recorded in the subjective and highly interpretable application questions. The second problem here, if not wrong should I put it, is that the usage of the data is, as author's claimed, taken as God's word, the highest possible theological blasphemy, if said to those in denial data miners. This is due to the reasons that they basically use the data available, let it be credit scores, health scores, SAT (or generally educational) scores, and even resume applications -to name a few- as the bestowed revelation, thus dismissing proven good teachers, potential shining employees and laying off 'unhealthy' current workers; all based on the Big Data, either snooping privately on their life, or in other cases, a take-it-or-leave option wether they'll provide their credit score to get an insurance or their health progress for work employment group insurance practiced in firms & companies. The dismissal action has become so autonomous directly from the data, that frightenedly, that one does not care of others, it's just life. Coincidentally, I read Adorno's Culture Industry directly before this, and it does personally provides a more enlightened discourse on the systemic issue. I personally first came across to this topic back a few years ago, that with this newly published book kindles my interest. The first was back around late 2013, where I was taking a UK driving licence. In the course, similarly argued by the author, you'll be getting some discount given that you choose to put a 'Black Box' in your car. This instrument will record your driving habits & feed back to the insurance company. Although initially used to monitor those big truckers -who tends to be sleep deprived due to late night & long driving- to reduce the risks of death of others due to reckless driving; here argued that it's afraid to be used by others to manipulate the privacy of car drivers, at other instances flawed due to normal human behaviour, taking short cuts at night through 'dangerous neighbourhood' thus ended up one marked to be subjected to 'handcuffed' by auto insurance companies. The second one I encountered was in early 2015, in a student organised event to explain & showcase the structure & function of Big Data by companies. At that time, I was furious given the fact of the the then environment of whistle-blowers expose on companies manipulating & snooping normal civilians life. In here meanwhile, as argued in The Impulse Society by Paul Roberts, politicians backed by big companies are microtargeting voters up to the point of what they read and hear are tailor made. Given to the rise of fascism and 'fear of the normal neighbour' one known differently pre-election in America which now turns into something totally unexpected, I humbly agreed to some point. The third encounter is with regard to behavioural test subjected to work or scholarship applications. Exposed initially by my parents, whom my dad was at times hired to be an interviewer. He's the first to enlighten me on the presence of attitude test taken by applicants, which is imperative for career climbing ambitions. Here written an illustration. An applicant answered promptly the normal application survey on personal attitudes. Obviously one is subjected to child education for example that one is unique bla3; yet answering unique now instead of other answers) will be perceived as narcissist by the possible employer, which now is not possible anymore. Unfortunately, this data, is shared and this unfortunate applicant will ended up in this continuous 'dangerous loop'. The author coined Recidivism in regard to this loop. The poor guy, living in a certain 'bad' neighbourhood, want to apply a credit card, or just simply living as a normal civilian. Now, dues to this, of course one might have some background of knowing some in-behind bars people. This data, used by the security enforcement, will cause the police to routinely knocks on doors to tell them they have eyes on them. A more idiosyncratic example is those trying to move houses. Property firms with big data on their very own hand, easily exclude those with a bad tag, even when they've changed, or for unfortunate some, mistakenly recorded in. As said before, no feedback can be given by the victims, unless one has some wealth to pursue on a legal lawsuit on this issue, which of course deteriorate middle class let alone the poor condition. Obviously the rich bankers and markets manipulators escaped from this loop. As a matter of fact, they're invulnerable and invincible. She suggested a few suggestions in the concluding chapter. First for example, oath taking by those finance practitioners, whom impact upon their ignorance is highly significant on the whole. Nevertheless she noted the downside that when in work bureaucratic structure, manager will want answers for certain issues, this will put those people in corners. Second is, as Taleb suggested, ditching those backward minded model. Obviously, she argued, one have to be specialised in the field to help recognise and build a better one. She illustrated a new model on preventing child abuse. There's similarity between the model to 'possible criminals' as in the Minority Report movie in reducing - if not preventing- crime she criticised, the hidden outcomes has not - at least she didn't yet- been analysed. The third is the involvement of human judgement into the model. Yes, initially the algorithm of the system is used to remove human bias, yet seeing how things turned out, the machine as expected cannot understand how human (cognitive) work. The problem of language and rational judgement will always triumph human above machine, I claimed this ambiguous argument. Hats off to this book!"
243,0553418815,http://goodreads.com/user/show/11951948-paul,3,"One of the funniest bits of Little Britain was where David Walliams as Carol Beer would type a request into the computer, before turning to the customer and saying ‘Computer says no’… Whilst it is funny, it is not so funny when it happens to you. In this book, O’Neil, a former Wall Street quant raises alarm bells on the way that these mathematical models have infiltrated our lives. We don’t see them, but these algorithms that help us with our searches online and finding books, films and other items on online sites are now being used to determine just how much of a risk you are. Next time you want a loan, to renew insurance or just need to get another job O’Neil thinks that some of us may have a problem.She calls them ‘Weapons of Maths Destruction’; these are incontestable, unregulated and opaque algorithms. They are being used by companies to decide the tiniest details. They are used because they make larger profits for corporations and most worryingly for us is that they are frequently conclude the wrong thing having made incorrect assumptions about individuals. As the saying goes ‘crap in; crap out’… Having worried the life out of the reader, O’Neil goes onto suggest a variety of things that could help; more regulation, better design of the code and us being aware of their use. The writing is clear, if a little dry and technical at times. The examples are a little American centric, but you can see the way that it is going in the UK. Even though the title mentions the dreaded word maths, it really isn’t that mathematical. Worthwhile reading."
244,0553418815,http://goodreads.com/user/show/16212136-peter-pete-mcloughlin,4,"Very depressing book on how computer algorithms are becoming a powerful tool of the exploitation of workers, the poor, minorities and ordinary people mostly in the service of profits. It goes deep and the problem is much worse than you think. This book is a downer as it covers relevant developments in politics and business and that always is not pleasant to think about in these times in general and are especially dispiriting in how they affect ordinary people."
245,0553418815,http://goodreads.com/user/show/5550611-hallie,2,"Just ok. I like the idea more than the execution of this book. While O'Neil uncovers a great, dark place in our society that needs more light shone on it, much of her conclusions she gives away in the introduction to the book. This would have been an excellent piece in the New Yorker or Atlantic but as a book began to feel redundant far too early. "
246,0553418815,http://goodreads.com/user/show/11345492-kate,4,"3.5, bumped up for likeable clarity-without-condescension and consistent up-front political engagement."
247,0553418815,http://goodreads.com/user/show/761281-mike,4,"Reading this book I could not help but wish the author had used a quote from Paul Goodman as the epigraph of the book: ""Technology is a branch of moral philosophy, not of science."" (Every technology ultimately is making a value judgement about what is important or worth doing, and what aspects of a task or reality are important.)This book is a fascinating look at how 'big data' is used and abused by colleges, banks, insurance agencies, in sentencing and parole decisions, by employers screening applicants, and more. Collecting vast amounts of data and using to guide decisions can be a very useful technology, but the author -- who herself had a career as a 'quant' or data analyst for a large firm -- points out the downside of basing decisions on data and algorithms. Of course it should be obvious, but our society's information fetishism can make us overlook that the data we collect and the algorithms we use to analyze it can be deeply flawed by omissions, prejudices, fallacies that the designers may not be aware of. A couple of the many examples she sues help illustrate this. We read about a school teacher whose performance is measured by an algorithm that takes into account his students' past performance, current performance on standardized tests, and various other factors. One year he is scored 6/100, and the next year 94/100 -- after making no changes to his syllabus, assignments, or anything else he was doing. Anyone with common sense can see there is something wrong with the algorithm if someone could get such wildly varying scores, but the scoring algorithm is a 'black box' that not event the school administrators understand. Among the many problems with the system here is that there is no allowance for the designers of the algorithm to get feedback on the scoring and adjust the model -- instead, the 'failing' teachers are fired and the algorithm rolls on. Contrast this with the use of 'big data' algorithms in commercial enterprises: when Amazon.com collects data on your browsing and purchasing habits, they use this recommendations to you for future purchases. If their models are bad, they suggest things you don't want, and they don't increase sales. But they take this feedback and change their algorithms, because there is a financial incentive to do so.So a huge blind spot in some of the models and algorithms policy makes use is the lack of feedback. Another is huge downside of the rise of big data modeling is that private companies can use opaque models to discriminate in ways that punish the poor particularly. The author explains how car insurance companies use data about where their customers live, drive, and work to assess risks -- which is largely legitimate -- but also abuse this information to determine which customers are likely to be able to shop around for better deals, and which are less likely to have the time, resources, or access to information to do so, and they are charged much more. Indeed affluent customers with DUI convictions may be better rates than safe drivers who live in areas with limited internet access and higher poverty. Likewise employers may screen applicants with big data algorithms that identify 'risks' which rule out the poor, people with past mental illness, or immigrants. Many other problems are discussed, and the final chapters look at how political parties have begun to utilize big data too -- an ominous prospect in light of the rest of the book.The author offers some solutions, ranging from the admittedly ineffective (a code of ethics for 'quants' who create the models) to the more daunting but effective measure of demanding transparency and regulation (which has made some progress in the insurance industry, but is far from complete).*Full disclosure -- I read a free advanced reader's copy that I won in a Goodreads giveaway.*"
248,0553418815,http://goodreads.com/user/show/3502690-graeme-roberts,2," Cathy O'Neil writes well and provides much good information, but I found the fundamental thesis of  Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy to be foolish and irresponsible. The title, condensed inevitably to WMD, is used promiscuously to describe any application of data science, statistics, or even information technology that she considers to be unfair or discriminatory. In most cases, she is right; some applications are used by the greedy and the uncaring with insufficient thought of the consequences, but she rarely considers the positive attributes of the applications. Once branded as a WMD, a term indelibly attached to the evil trifecta of George W. Bush, Saddam Hussein, and the Iraq War, how are readers meant to consider their many positive aspects and what can be done to make them better?At first, I thought that Ms. O'Neil had been the victim of a foolish PR or marketing person in her publishing company, who had come up with this catchy name, but her ardent use of the term convinced me that she had invented it.Ms. O'Neil seemed to follow a downward trajectory from her Harvard PhD in mathematics and a teaching position at Barnard to a leading hedge fund and a series of other morally disappointing positions that left her convinced that data science, and even mathematics, are roots of much evil. I don't buy it. They are tools that can be used or misused, and her diatribe does nothing to help.She asserts, on page 210:If we find (as studies have already shown) that the recidivism models codify prejudice and penalize the poor, then it's time to have a look at the inputs. In this case, they include loads of birds-of-a-feather connections. They predict an individual's behavior on the basis of the people he knows, his job, and his credit rating—details that would be inadmissible in court. The fairness fix is to throw out that data.But wait, many would say. Are we going to sacrifice the accuracy of the model for fairness? Do we have to dumb down our algorithms?In some cases, yes. If we're going to be equal before the law, or be treated equally as voters, we cannot stand for systems that drop us into different castes and treat us differently.I disagree strongly. Some data is better than none, though we should be looking to constantly improve our data science systems. I would like to know that the recently paroled criminal moving in next door had been released by a judge in full possession of whatever facts are available. So, I suspect, would Ms. O'Neil."
249,0553418815,http://goodreads.com/user/show/19993-robin,4,"If you've been listening to Cathy O'Neil on Slate Money for some time, you probably expected this book to emerge in exactly this form and with exactly this content. She's smart, progressive, focused, and clear, just as she was on the podcast. I saw her speak at the Mechanic's Institute Library a few months ago, before I'd read the book, and was very impressed by her ability to be articulate about complex situations with a decisiveness that belies their complexity, or at least certainly never allows for it as an excuse. The argument I found most challenging was (heavily paraphrasing here) that we should be privileging fairness because it is otherwise so easy to be blind to the unfairness that creeps in when we pursue other goals. If I'm honest with my own tendencies, I recognize my longing for accuracy might indeed cause me to overlook fairness, and I resist the idea that I need to choose one as a priority over the other. Nevertheless, O'Neil convinces me to question assumed neutrality much more closely, and be much more vigilant for it in my own industry. As she points out, it's easy for people who write the algorithms to absolve themselves of responsibility for how they are used, but if we pay attention to the real outcomes we may find that we need to do more to train our algorithms to learn from externalities and flaws. Focusing ruthlessly on measuring only what you are trying to measure and ignoring all other factors is a good start; evaluating the results to make sure that's actually what happened is as necessary a practice. The book is full of examples of failed algorithms in wide use, actively and detrimentally affecting at-risk populations today, as well as potential future areas where algorithms could be used, perhaps even inadvertently, to consolidate and perpetuate the power of the already powerful. The free market will not solve this problem. This is an excellent example of where good regulation is imperative. As constituents with the ability to influence legislators to put their attention here, we can demand more transparency, more awareness and education, and a more good, old-fashioned compassion.Further reading:https://www.ted.com/talks/joy_buolamw...https://www.propublica.org/article/ma...https://standards.ieee.org/develop/pr..."
250,0553418815,http://goodreads.com/user/show/7139041-dorin,2,"Disappointing - such a rich topic, yet the author decided to be racist, Democrat and to induce panic instead of having a cool headed approach. She ignores her own advice, contradicts herself time and again, and she makes her Democrat leaning so obvious it makes me puke.The book could have contained real data, real information, but instead she goes on to explain that statistics are bad because statistics don't catch statistical errors. She pretends she's a professional in data processing, but she sounds like she doesn't understand how statistics work, and she can barely go beyond her mathematical training to understand what went wrong from a human point of view.My point: „a machine is as useful as the person who uses it”. Her point: ""big data is bad unless it's used by the Democrats”."
251,0553418815,http://goodreads.com/user/show/7837611-greg-fanoe,2,"I think that on the whole reading this book will do people more good than harm but I had several problems with it:1. The only section which I have personal expertise on is the section on ""insurance"" - I am an actuary - and I can personally verify that this chapter was filled with inaccurate, misleading, and/or totally speculative statements.2. The correlation between credit score and income is nowhere near as strong as she makes it appear in this book.3. Many of the problems she identifies have nothing to do with the models in question. Advertising models are not the issue with the payday loan or for-profit college industry those industries need to be totally upended/destroyed at the source. Letting them operate as is but kneecapping their ability to advertise would be nonsensical.4. The solutions she presents are ridiculously vague and/or impractical."
252,0553418815,http://goodreads.com/user/show/1045774-mehrsa,5,This is the book I was waiting to read! The rejoinder to our data-obsessed culture. We got drunk on algorithms and forgot to focus on their limitations. are they asking the right questions? are they recycling inequality? Great book
253,0553418815,http://goodreads.com/user/show/3126915-imi,2,"Sadly, a rather superficial look at big data, algorithms and how they've impacted on society. You probably know/have guessed all this already. The correlations O'Neil point out are not very convincing, and it felt like she just wanted to rant about how unfair the world is. Not that I disagree with that conclusion; society is unfair and unequal. But O'Neil really doesn't state much more than that, and, worse, some of her examples felt misleading or simply speculative. Where was her evidence? And if she'd explored this issues in more depth I'd have wanted clear ideas for resolving these issues. Instead, any suggestions she made seemed vague and impractical. I admitedly feel out of my depth with this topic, but am most definitely willing to learn more and expand my understanding about it. Disappointingly, this book didn't offer that."
254,0553418815,http://goodreads.com/user/show/777176-dave,5,"I learned more from this book than I have from any book I've read in years. O'Neil is a former math professor and spent time as a hedge-fund quant, so she understands the theory and experience of the effect of out-of-control algorithms on our society.As corporations make more and more efficient use of algorithms, they are starting to show dangerous tendencies, according to O'Neil. She calls them ""weapons of math destruction"" or WMDs. WMDs present the following grave problems for society:--They usually lack transparency, such that it is difficult or impossible to know what goes into them and what factors sway or change their results. When they are opaque, WMDs are patently unfair (O'Neil cites the algorithms used to evaluate teachers in many districts, highlighting their absurdity as they attempt to quantify the unquantifiable).--They make broad generalizations and reduce human beings to ""buckets."" One of the worst examples is the criminal justice system, which is increasingly reliant on algorithms to deploy police patrols and even to sentence criminal defendants. These algorithms punish the innocent by assuming that those who live in poor neighborhoods are necessarily more likely to commit crimes, thereby subjecting them to frequent ""stop and frisk"" type searches that have been found to be unconstitutional repeatedly.--They can use information in an unfair way that tends to perpetuate further injustice. A prime example of this is the increasing tendency to identify individuals by their credit score, on job applications and even in online shopping. When job applicants are screened by credit score, those who are already poor (and therefore much more likely to have a low credit score already) will find it even harder to get a job and thereby earn enough money to help improve their credit score. This self-perpetuating cycle of poverty is completely unnecessary, because credit scores are an exceedingly poor proxy for job-worthiness of applicants.--Many WMDs are not very good at learning. They are created as models of human behavior (with all the flaws and assumptions of the humans who programmed them), and when they miss (when for example they screen out a potential hire who would have actually been a first-rate employee), they never learn that they have missed. The ""false positive"" never presents itself (the rejected applicant goes on to success elsewhere), and the oblivious WMD continues to chug along with its assumptions and blind spots unchallenged.And WMDs are like racism:""Racism, at the individual level, can be seen as a predictive model whirring away in billions of human minds around the world. It is built from faulty, incomplete, or generalized data. Whether it comes from experience or hearsay, the data indicates that certain types of people have behaved badly. That generates a binary prediction that all people of that race will behave that same way....Racists don't spend a lot of time hunting down reliable data to train their twisted models. And once their model morphs into a belief, it becomes hardwired. It generates poisonous assumptions, yet rarely tests them, settling instead for data that seems to confirm and fortify them. Consequently, racism is the most slovenly of predictive models. It is powered by haphazard data gathering and spurious correlations, reinforced by institutional inequities, and polluted by confirmation bias. In this way, oddly enough, racism operates like many of the WMDs I'll be describing in this book."" (p. 22-3)And finally:""Models like this will abound in coming years....Many of these models, like some of the WMDs we've discussed, will arrive with the best intentions. But they must also deliver transparency, disclosing the input data they're using as well as the results of their targeting. And they must be open to audits. These are powerful engines, after all. We must keep our eyes on them.... Data is not going away....Predictive models are, increasingly, the tools we will be relying on to run our institutions, deploy our resources, and manage our lives. But...these models are constructed not just from data but from the choices we make about which data to pay attention to--and which to leave out. Those choices are not just about logistics, profits, and efficiency. They are fundamentally moral. If we back away from them and treat mathematical models as a neutral and inevitable force, like the weather or the tides, we abdicate our responsibility....We must come together to police these WMDs, to tame and disarm them."" (p.218)"
255,0553418815,http://goodreads.com/user/show/3029658-doug-cornelius,5,"With big data, comes formulas to parse through the data trying to make sense of it. Those algorithms can help make sense of the data and help filter through the noise to find trends. But those algorithms can also be easily misused and have harmful, if unintended consequences. Cathy O'Neil explores these problems in 
Weapons of Math Destruction
.Ms. O'Neil is a former academic, hedge fund quant, and data scientist for startups. At the hedge fund she was looking at ways to make money by predicting financial trends. As a data scientist she was looking to predict people's purchases and browsing habits to monetize those habits. Those are good algorithms where the people using them understand them and update the algorithms as they see problems and can check to see if they are working properly by verifying outcomes.Many data driven outcomes fail. Those are the focus of 
Weapons of Math Destruction
.I'm a big fan of the Slate Money podcast with Cathy O'Neil, Felix Salmon, and Jordan Weissmann. When the publisher offered me a review copy of Ms. O'Neil's book, I jumped at the chance.It's a short book and even though the title makes it sound like it's full of math, it's not. It's more about social policy. She points to the danger of decision makers blindingly following algorithmic output that they do not understand.Take the US News and World Report listing of best colleges. This is preeminent guide for high school students trying to figure out which college to attend. The rankings do not look at actual student outcomes. The publisher does not look at happiness, learning or improvements. It looks at proxies for the outcomes like SAT scores, alumni contributions, and graduation percentages. Colleges started making decisions to improve their rankings in the algorithm by improving those measured proxies. As anyone writing checks for their college bound children, affordability is not one of the measured proxies.As you might expect there are big chunks of the book dedicated to failings in the financial sector by relying on poorly designed algorithms. The biggest problems often being false assumptions plugged into the data running through the algorithm.With the election season upon us, the chapter on political algorithms is fascinating. The data scientists of commerce were brought into political campaigns. They are able to micro-target potential voters sending different messages to different groups highlighting the positions that would make the candidate more favorable to that segment of the population. Civic engagement and the political process is increasingly being algorithm driven.Since our world is increasingly being driven by big data, it's important to understand what is happening behind the decision-making. Weapons of Math Destruction is an excellent tool to help you understand data driven decision-making."
256,0553418815,http://goodreads.com/user/show/23020614-germ-n,3,"A fascinating must-read guide on how to navigate and interpret a world increasingly ruled by materialistic prejudices and oversimplifications that are repackaged as ""science"" and ""tech"" through poorly designed computer algorithms.Reading this gives one the feeling that the ""evil androids ruling over mankind"" dystopia has already been unrolling. Except in a very sneaky way: this time we don't get to face or even understand the robots and their actions which are nonetheless already reshaping our reality and the course of our lives through ridiculous university rankings, credit scores, social media ideological bubbles, etc.Unfortunately O'Neil ruins an otherwise brilliant book by an extreme leftist bias that seeks to absolve her personal heroes in the US democratic party and their political agendas, and attempts to attribute most blame on a specific other side. I assume this is why a book which raises such important points gets to be published and become mainstream after all.For example, she portrays Obama as a saint who just wants the best for immigrants and fiercely battles the evil rich. Yet for any unbiased observer, it's quite clear that Obama and democrats are just serving the wealthy who finance their own political campaigns (duh) when they set to ruin nations overseas and bring in thousands of immigrants, ideally illegal ones, so the price of labor can go down and corporations can continue to reap political advantages.This skewed perspective is damaging to the bottom argument and distracts readers from what's really the main issue here: the constant loss of truth and distortion of increasingly astroturfed public debate. No attempts are made, for example, at addressing how these WMDs are influencing the toxic foreign policy of the US and the press - another entity that passes as a saint in this book, while being the number one responsible for war and destruction of nations around the world."
257,0553418815,http://goodreads.com/user/show/20465079-tam,4,"A quick and highly useful read - I would recommend it to all people not just ones working with predictive models and Big Data.The title is sensational enough, but at one point I put that aside and decided to read this book. So many have been praising Big Data and Machine Learning, few cautions have been put forward but not enough, mostly dealing with ethics, security, personal data issues. This book, on the other hand, discusses the large scale tragedies that Big Data can invoke on societies as a whole, if continuing to go unchecked. 10 chapters deal with examples about the misuses of data and predictive models in many aspects of lives, especially the lives of the most unfortunate.O'Neil has a strong opinion and at certain point I thought she might be too pessimistic (saying that from a self-proclaimed pessimistic person as I am), and O'Neil freely claims how angry she is with the whole thing. Yet at the very end of the book, she makes constructive comments, ideas for improvement, for regulations. Those are the most important, and perhaps the most useful passages for data scientists. I've been quite fascinated by the tool and sometimes forget the darker side that the tool can invoke, and I'd like to thank O'Neil for the reminder, for the clarification about issues that sometimes I feel uneasy but not so clear about. No, the tool is neither inherently good nor bad, the people who make them and wield them can choose do what they want. And we can choose to do good things.Ehh, not so optimistic though, there are tons of challenges and... But I should shut my mouth. But at least we should try."
258,0553418815,http://goodreads.com/user/show/15409024-ahalya-vijay,5,"I found this to be a fascinating read on the importance of using technology and data in an ethical way. I was shocked to learn how pervasive irresponsible uses of data are throughout the developed world and how widespread it is for people to place their faith in ""black box"" models as Cathy O'Neil refers to them. It must be frustrating, for example, for teachers to be evaluated (and fired!) based on a model they have no knowledge of, without knowing how to improve their rating. And it must be even more so if they are getting anecdotally strong and positive feedback from school leadership and parents.Another shock was how liberally companies and 'data scientists' use proxies without thought to the consequences of the model not capturing the 'error term' sufficiently. The danger of making decisions based on things that are modeled (or estimated) using proxies was drilled into my head throughout my entire schooling. How would you feel if the fact that you were late to one meeting at work was used to brand you as an irresponsible or unreliable worker? Ms O'Neil made a clear link between such models being used to trap people in a cycle of poverty, using example after example across a variety of industries (e.g., education, finance, insurance). It is clear that reform is needed, but what left me worried at the end is that these models are not hurting the rich and famous, but the poor and under-resourced. Will our current political and justice system listen to their needs? I'm skeptical."
259,0553418815,http://goodreads.com/user/show/48433695-alexis-tremblay,5,"A must read for all data scientist, mathematicians, modellers, statisticians,... There is a dark side to big data and she exposes it masterfully. I always had an uneasy feeling when I started my career as an actuary and later on as data scientist. My moral compass was always off but I didn't have the words for it. Now she has given me a framework to think about my job and what to look out for. Will read again. "
260,0553418815,http://goodreads.com/user/show/16204614-bing-gordon,4,"Good overview of modern analytics, but too much of a guilt trip laid on top. "
261,0553418815,http://goodreads.com/user/show/6100646-brian-clegg,4,"As a poacher-turned-gamekeeper of the big data world, Cathy O'Neil is ideally placed to take us on a voyage of horrible discovery into the world of systems making decisions based on big data that can have a negative influence on lives - what she refers to as 'Weapons of Math Destruction' or WMDs. After working as a 'quant' in a hedge fund and on big data crunching systems for startups, she has developed a horror for the misuse of the technology and sets out to show us how unfair it can be.It's not that O'Neil is against big data per se. She points out examples where it can be useful and effective - but this requires the systems to be transparent and to be capable of learning from their mistakes. In the examples we discover, from systems that rate school teachers to those that decide whether or not to issue a payday loan, the system is opaque, secretive and based on a set of rules that aren't tested against reality and regularly updated to produce a fair outcome.The teacher grading system is probably the most dramatically inaccurate example, where the system is trying to measure how well a teacher has performed, based on data that only has a very vague link to actual outcomes - so, for instance, O'Neil tells of a teacher who scored 6% one year and 96% the next year for doing the same job. The factors being measured are almost entirely outside the teacher's control with no linkage to performance and the interpretation of the data is simply garbage.Other systems, such as those used to rank universities, are ruthlessly gamed by the participants, making them far more about how good an organisation is at coming up with the right answers to metrics than it is to the quality of that organisation. And all of us will come across targeted advertising and social media messages/search results prioritised according to secret algorithms which we know nothing about and that attempt to control our behaviour.For O'Neil, the worst aspects of big data misuse are where a system - perhaps with the best intentions - ends up penalising people for being poor of being from certain ethnic backgrounds. This is often a result of an indirect piece of data - for instance the place they live might have implications on their financial state or ethnicity. She vividly portrays the way that systems dealing with everything from police presence in an area to fixing insurance premiums can produce a downward spiral of negative feedback.Although the book is often very effective, it is heavily US-oriented, which is a shame when many of these issues are as significant, say, in Europe, as they are in the US. There is probably also not enough nuance in the author's binary good/bad opinion of systems. For example, she tells us that someone shouldn't be penalised by having to pay more for insurance because they live in a high risk neighbourhood - but doesn't think about the contrary aspect that if insurance companies don't do this, those of us who live in low risk neighbourhoods are being penalised by paying much higher premiums than we need to in order to cover our insurance. O'Neil makes a simplistic linkage between high risk = poor, low risk = rich - yet those of us, for instance, who live in the country are often in quite poor areas that are nonetheless low risk. For O'Neil, fairness means everyone pays the same. But is that truly fair? Here in Europe, we've had car insurance for young female drivers doubled in cost to make it the same as young males - even though the young males are far more likely to have accidents. This is fair by O'Neil's standards, because it doesn't discriminate on gender, but is not fair in the real world away from labels.There's a lot here that we should be picking up on, and even if you don't agree with all of O'Neil's assessments, it certainly makes you think about the rights and wrongs of decisions based on automated assessment of indirect data."
262,0553418815,http://goodreads.com/user/show/7019761-cindy,4,"While I don't necessarily agree with all of the conclusions, this was a very thought-provoking book. The author is a mathematician who has worked on Wall Street and now takes on some of the math models that rule our lives. Some of these were, at least originally, developed with good intentions - intending to make fairer some of the decisions that affect our lives: where and who gets into college; what the schedule for our job looks like; how much we pay to get a loan; even how you are judged in your chosen profession. But the author poses that many of these types of decisions are actually less than fair. The models are in her terms WMDs - weapons of math destruction - they are hidden, unregulated, and virtually uncontestable, even when they are blatantly wrong. One of the more troubling examples from the book concerned the use of testing to judge teacher performance. She traced how an exemplary teacher with outstanding evaluations was judged as poor on the basis of one set of tests and fired. The teacher showed that the tests which showed that her ""value-added"" score was sub-par were flawed due to a high number of erasures on the previous year which indicated cheating. Indeed, the system said that there *might* have been cheating but they would not re-instate this excellent teacher. (And please don't get me started on ""value-added"" - my daughters' school was always excellent in all the testing but this score was just average. So if your kids are already high achievers, how do you continue to score excellent in this measure? The answer is it's almost impossible.)Other examples showed that even when you removed race or ethnicity or economics as an indicator in these WMDs, they were there. If a zip code puts you in a risky category for a student loan, then it's hard to get the education that can get you out of poverty; or if a work schedule makes it hard for a single mother to get the child care she needs so that she can get the money to go to school to make a better life for herself and her child...well, I can see how some of this sets up a self-fulfilling prophecy. It makes it harder, not impossible, to break this cycle. That's why this book is important. If this is being used, then we need to know and like credit scores have the ability to correct errors and inaccuracies that work against the average person.Quotes to remember:....This would all be wonderful for students and might enhance their college experience, if they weren't the ones paying for it in the form of student loans that would burden them for decades. We cannot place the blame for this trend entirely on the US News rankings. Our entire society has embraced not only the idea that a college education is essential, but the idea that a degree from a highly-ranked school can catapult a student into a life of power and privilege. As I write this, the entire voting population that matters lives in a handful of counties in Florida, Ohio, Nevada, and a few other swing states. Within those counties is a small number of voters whose opinions weigh in the balance. I might point out here that while many of the WMDs we've been looking at, from predatory ads to policing models, deliver most of their punishment to the struggling classes, political microtargeting harms voters of every economic class, from Manhattan to San Francisco, rich and poor alike find themselves disenfranchised, though the truly affluent of course can more than compensate for this with campaign contributions."
263,0553418815,http://goodreads.com/user/show/6894205-allie,4,"This was by far the most interesting book about math I have ever read (admittedly a low bar to set), possibly because it contains no actual math formulas. Instead, former mathematics professor and hedge fund analyst Cathy O'Neil tells stories about the applications of math - how ""ill-conceived mathematical models now micromanage the economy from advertising to prisons."" Among other things, mathematical models determine which news you see on social media, your Google search results, the cost of your auto insurance and credit card rates, which neighborhoods police are sent to patrol, and how your child's schools are evaluated. In one staggering statistic about the reach of these models, O'Neil notes that 72 percent of resumes are never seen by humans, but are instead screened and scored by software programs looking for employer keywords. While math itself is neutral, the models are based on assumptions that reflect human biases and limitations. Often, the models are designed to process data from large numbers of people efficiently, but not necessarily fairly. She distinguishes between ""good"" models built on transparent assumptions that are constantly corrected with new data (e.g., those used in professional baseball) to opaque models that use proxy data such as race or zip codes and do not have a corrective feedback mechanism (e.g., predictive policing software). Typically, there is no method of recourse when given a poor score by a computer (hello, Hal) since companies consider their algorithms proprietary and won't disclose details of how scores are developed. This was an issue for teachers evaluated - and sometimes fired - based on inaccurate models of student test scores. The book's second main argument is that the built-in biases in the models are especially putative towards the poor, who are steered to high interest loans, targeted by ""stop and frisk"" policing based on their neighborhoods, and less likely to be personally interviewed for jobs.I found many of O'Neil's arguments persuasive and somewhat alarming, although she tries to end the book on a positive note. Worth reading, even if you don't like math. Or weapons. Or destruction. "
264,0553418815,http://goodreads.com/user/show/70315594-enoughtohold,4,"A nice, clear overview of the problems of big data for laypeople. Not sure why other reviewers expected this to be deeply technical, or why they think O’Neil thinks there are easy answers — she plainly doesn’t.I for one am glad a book this accessible exists on this topic. After all, the vast majority of the people affected by these harmful models are not data scientists, and they deserve to know what’s going on.(Also, the author did a great job reading for the audiobook.)"
265,0553418815,http://goodreads.com/user/show/4430576-miri,5,"I've said it before, and I'm sure I'll say it again, because I make a point of seeking out books like this. But if there's one book I could get everyone to read right now, it would be this one. Even next to Ta-Nehisi Coates's We Were Eight Years in Power, which I'm reading concurrently and which is staggering in its importance, this book feels urgent—because what it explains is how our new economy of Big Data is codifying, concealing, and amplifying inequality of every kind, on an enormous scale. Every injustice I'm reading about in Coates's book is made worse by what I read about here. We think of math as inherently neutral, the way we think of technology and logic and science. But these things are like any tool, dependent on the person using them, capable of being used for good or evil. (Remember the way science was abused throughout American history, fabricating the entire concept of biological race to provide justification for white humans to enslave black ones.) When we start using math to make decisions instead of people, assuming that this will decrease human bias and make things more fair, what we can actually end up doing is obscuring that prejudice and magnifying the scale of its effect. Cathy O'Neil was a mathematician working in the finance industry in 2008. She left after a few years, disgusted with the way the system was clearly rigged, and has since created the concept of Weapons of Math Destruction: mathematical models that codify human prejudice, are opaque and impervious to feedback, and affect large numbers of people. In this book, O'Neil takes us through several WMDs, showing how they're used in different areas of our lives. There's the disastrous school reform attempt in Washington D.C., which tried to evaluate teachers in a way math simply cannot be used, and which fired hundreds of teachers based on results no one in the district could explain or defend. There's the way many employers now use credit scores to screen applications, assuming that making regular payments means you'll be a dependable employee, ignoring the facts that (1) responsible people experience financial difficulty too and (2) preventing people with bad credit from getting jobs is only going to make their situation worse.There are the computerized risk models that judges use in sentencing, trying to gauge from prisoners' background information—where they grew up, whether their friends and family have criminal records; information which would be inadmissible in court—how likely they are to return to prison after being released, and how long their sentence should be. There's the way U.S. News & World Report helped cause an arms race when it decided to start ranking universities on everything except their cost, driving up tuition over 500 percent (nearly four times the rate of inflation) in just the 30-ish years that I've been alive.There's political advertising which is now so individually personalized that even people who support the same candidate, even people visiting the candidate's website, will see totally different things based on the profile the campaign has created for them.There's predatory advertising like that done by for-profit colleges, which specifically targets vulnerable people's ""pain points"" looking for those who are ""stuck,"" who have ""few people in their lives who care about them,"" who have ""low self-esteem"" and are ""unable to see and plan well for the future"" (these quotes, shared by O'Neil, come from the California Attorney General's office when it was investigating one of these colleges). (And if you're wondering how they would know this about people, the point is that they know this about all of us now. Just try to imagine what someone could learn about you from seeing everything you do online.)It might seem like the logical response is to disarm these weapons, one by one. The problem is that they’re feeding on each other. Poor people are more likely to have bad credit and live in high-crime neighborhoods, surrounded by other poor people. Once the dark universe of WMDs digests that data, it showers them with predatory ads for subprime loans or for-profit schools. It sends more police to arrest them, and when they're convicted it sentences them to longer terms. This data feeds into other WMDs, which score the same people as high-risk or easy targets and proceed to block them from jobs, while jacking up their rates for mortgages, car loans, and every kind of insurance imaginable. This drives their credit rating down further, creating nothing less than a death spiral of modeling. Being poor in a world of WMDs is getting more and more dangerous and expensive.The same WMDs that abuse the poor also place the comfortable classes of society in their own marketing silos . . . For many of them it can feel as though the world is getting smarter and easier. Models highlight bargains on prosciutto and chianti, recommend a great movie on Amazon Prime, or lead them, turn by turn, to a cafe in what used to be a ""sketchy"" neighborhood. The quiet and personal nature of this targeting keeps society's winners from seeing how the very same models are destroying lives, sometimes just a few blocks away.Though we're all affected by them in ways we can't see, right now the majority of the damage is being done to the same groups who are always hurt by flaws in the system—minorities and the poor. The same models could easily be used to help people instead of prey upon them, but that won't happen in the glorious free market of capitalism. We need regulation and oversight in the data industry. We need accountability. When statistics itself, and the public’s trust in statistics, is being actively undermined by politicians across the globe, how can we possibly expect the Big Data industry to clarify rather than contribute to the noise? We can because we must. Right now, mammoth companies like Google, Amazon, and Facebook exert incredible control over society because they control the data. They reap enormous profits while somehow offloading fact-checking responsibilities to others. It’s not a coincidence that, even as he works to undermine the public’s trust in science and in scientific fact, Steve Bannon sits on the board of Cambridge Analytica, a political data company that has claimed credit for Trump’s victory while bragging about secret ""voter suppression"" campaigns. It’s part of a more general trend in which data is privately owned and privately used to private ends of profit and influence, while the public is shut out of the process and told to behave well and trust the algorithms. It’s time to start misbehaving. Algorithms are only going to become more ubiquitous in the coming years. We must demand that systems that hold algorithms accountable become ubiquitous as well."
266,0553418815,http://goodreads.com/user/show/17187084-sean-goh,4,"Very readable book warning of the downside of amoral models. With great power comes great responsibility.___Math-powered applications were based on choices made by fallible human beings. Many models encoded human prejudice, misunderstanding, and bias into the software systems that increasingly managed our lives. Like gods, these mathematical models were opaque, their workings invisible to all but the highest priests in their domain: mathematicians and computer scientists.Models are opinions embedded in mathematics.Statistical engines without feedback can continue to spin our faulty and damaging analysis while never learning from its mistakes. E.g. teachers mistaken labelled as failures are fired, and no one is the wiser. The model's output becomes unquestioned reality.The parallels between finance and Big Data: The productivity of the field's talents indicates that they are on the right track, and it translates into dollars. This leads to the fallacious conclusion that whatever they are doing to bring in more money is good. It ""adds value"". Otherwise why would the market reward it?In both cultures Wealth is no longer a means to get by. It becomes directly tied to personal worth.On the rise of university rankings: from the perspective of a university president, it's quite sad. Here they are at the summit of their careers dedicating enormous energy towards boosting performance in fifteen areas defined by a group of journalists at a second-tier magazine (U.S. News). The ranking had become destiny. When you create a model from proxies, it is far simpler for people to game it. This is because proxies are easier to manipulate than the complicated reality they represent. (e.g. trying to evaluate influence by number of twitter followers)In a system in which cheating is the norm, following the rules amounts to a handicap. So preventing the students in Zhongxiang from cheating was unfair.In our largely segregated cities, geography is a proxy for race.Patrolling particular areas creates a pernicious feedback loop. The policing itself spawns new data, which justifies more policing. Eventually the zeroing in on ghetto districts ends up criminalising poverty, believing all the while that our tools are not only scientific but fair.Instead of simply trying to eradicate crimes, police should be attempting to build relationships in the neighbourhood. This was one of the pillars of the original ""broken windows"" study. The cops were on foot, talking to people, trying to get them to uphold their own community standards. But that objective, in many cases, has been lost, steamrollered by models that equate arrests with safety.Mathematical models can sift through data to locate people who are likely to face great challenges, whether from crime, poverty, or education. It's up to society whether to use that intelligence to reject and punish them - or to reach out to them with the resources they need. It all depends on the objectives we choose.Computing systems have trouble finding digital proxies for soft skills. The relevant data simply isn't collected, and anyway it's hard to put a value to them. They're usually easier to just leave out of the model.The modelers of e-scores (unofficial credit ratings) have to make do with trying to answer the question: ""How have people like you behaved in the past?"" when ideally the question would be ""How have you behaved in the past?Even with the Affordable Care Act, which reduced the ranks of the uninsured, medical expenses remain the single biggest cause of bankruptcies in America.A sterling credit rating is not just a proxy for responsibility and smart decisions. It is also a proxy for wealth. And wealth is highly correlated with race.Mistakes made by models are learning opportunities - as long as the system receives feedback on the error. When automatic systems sift through our data to size us up for an e-score, they naturally project the past into the future. The poor are expected to remain poor forever and are treated accordingly. It's inexorable, often hidden and beyond appeal, and unfair.Yet machines cannot make adjustments for fairness by themselves. That is where the human overseer comes in.Human decision making, while often flawed, has one chief virtue. It can evolve.Big data processes codify the past. They do not invent the future. Doing that requires moral imagination, and that's something only humans can provide. We have to explicitly embed better values into our algorithms, creating Big Data models that follow our ethical lead. Sometimes that will mean putting fairness ahead of profit."
267,0553418815,http://goodreads.com/user/show/6991670-bob,4,"Summary: An insider account of the algorithms that affect our lives, from going to college, to the ads we see online, to our chances of getting a job, being arrested, getting credit and insurance.Big Data is indeed BIG. Mathematical algorithms shape who will see this post on their Facebook newsfeed. If you go to Amazon or another online bookseller, algorithms will suggest other books like this one you might be interested in. Have you seen all those ads about credit scores? They are more important than you might imagine. Algorithms used by employers and insurance companies determine your employability and insurability in part through these scores. Far more than another credit card (bad idea, by the way) or a mortgage are on the line. These algorithms seem objective, but how they are formulated, and the assumptions made in doing so mean the difference between useful tools that benefit people, and ""black boxes"" that thwart the flourishing of others, often unknown to them.Cathy O'Neil should know. A tenure track math professor, she made the jump to Wall Street and became a ""quant"" who helped develop mathematical algorithms and witnessed, in the crash of 2008, the harm some of these caused. And she began to notice how algorithms often painfully impacted the lives of many others.  She describes how a teacher was fired because of the weighting of performance scores of a single class, despite other evaluations finding her an excellent teacher (afterwards it was found that there were a high number of erasures on tests for students who would have been in her class the previous year, suggesting these had been altered to improve scores).As she looked at the algorithms responsible for such injustices, she came to dub them ""Weapons of Math Destruction"" or WMDs and she identified three characteristics of these WMDs:Opacity: those whose lives are affected by them have no idea of the factors and weighting of those factors that contributed to their ""score"".Scale: how widely an algorithm is applied across industries and sectors of life can affect how much of one's life is touched by a single formula. For example, the FICO scores mentioned above affect not only credit, but the ability to get a job, the cost of auto insurance, and your ability to rent an apartment.Damage: WMDs can reinforce other factors perpetuating a cycle of poverty, or incarceration.She also shows that what makes these algorithms destructive is the use of proxy measurements. For example an employer may not know directly how savvy someone is as a marketer, and so they use a ""proxy"" measurement of how many Twitter followers that person has. Or age is used as a proxy for how safe a driver one is. For a group, the proxy may work well, and be utterly inaccurate for an individual that falls within that proxy group.Then in successive chapters, she chronicles some of the ways WMDs operate in different parts of life. She discusses the U.S. News & World Report college rankings, and the use of algorithms in admissions processes. Social media uses algorithms to target advertising, which means some will see ads for for-profit schools and payday lenders, and others for upscale furnishing or Viagra, based on clicks, likes, searches, and comments. Policing strategies, including locations for intensified ""stop and frisk"" policing are shaped by another set of algorithms. Algorithms to filter resumes may use scoring algorithms that discriminate by address and psych exam algorithms may render others unemployable in a certain industry. Scheduling algorithms may promote efficiency at the expense of the ability of workers to sleep on a regular schedule, or arrange childcare, or work enough hours to qualify for health insurance. Algorithms sometimes shut people out from credit or low cost insurance when in fact they are good risks. She concludes by showing how algorithms determine ads and news we see (and don't see). In an afterword she explores the flaws in algorithms revealed on the election of Donald Trump (algorithms, for example predicted Clinton would easily win Michigan and Wisconsin, where consequently she did not campaign, and lost by small margins).In her conclusion, she makes the case not only for a code of ethics for mathematicians but also argues that regulation and audits of these algorithms are necessary. The value assumptions, as well as the mathematical methods of many algorithms are flawed, and yet opacity means those whose lives are most affected don't even know what hit them.She helps us see both the sinister and useful side of these algorithms. They may reveal where a pro-active intervention may save a family from descending into family violence, or provide extra assistance to a child in danger of falling behind in a key subject. Or they may be used to invade personal rights, or even to perpetuate socio-economic divides in a society. The reality is that the problem is not the math but the old GIGO problem (garbage in, garbage out). The values and assumptions of the humans who devise the formulas and weightings of values and the use of proxies determine what may be destructive outcomes for some people. Yet it can be hidden behind an app, a program, an algorithm.The massive explosion in storage capacities, processing speeds, and the way our interests, health status, travel patterns, spending patterns, fitness, diet and sleep habits, our political inclinations and more may be tracked via our online and smartphone usage makes O'Neil's warning an urgent one. We create mountains of data that may be increasingly mined by government and private interests. Perhaps as important as asking whether this will be governed in ways that contribute to our flourishing, is whether we will be alert enough to care.____________________________Disclosure of Material Connection: I received this book free from the publisher. I was not required to write a positive review. The opinions I have expressed are my own."
268,0553418815,http://goodreads.com/user/show/51162416-max,5,fast pleasant primer on what’s wrong with big data. Fun. I love using a VPN turning everything off and being practically nobody to these freaks 
269,0553418815,http://goodreads.com/user/show/4061006-angela,5,"Cathy O'Neil is one of my heroes; I love listening to her on Slate Money, and her less pop/more technical book, Doing Data Science, occupies a prominent place on my desk at work - and a special place in my heart. Going beyond the usual data science manuals of what a random forest is, or how to account for large, sparse data, O'Neil and her coauthor - in that book - make an almost anthropological survey of the data science ""field""/profession/whatever. It's enlightening, and it's very real; less a textbook, more a professional guide.So if Doing Data Science is the ""how to do your job"" manual, and Slate Money is the occasional soapbox, Weapons of Math Destruction is the manifesto. With clarity, conviction and intelligence, Cathy O'Neil scalpels away at the utopian, misty-eyed tech worship that has contributed to the data science/big data bubble. She identifies the ways in which algorithms encode and perpetuate existing prejudices, and are hidden from plain sight by the obscuring (both intentional and not) use of Fancy Math.She notes how anti-human ""market efficiency"" is, and how it's often been short-hand for unjust and unequal systems, and big data is just scaling those dumb systems way, way up. For example, the use of social network data to ""enrich"" lending platforms; or recidivism models that ask about where a former convict grew up, whether his/her friends are in jail, how many encounters with the police he/she has had. These algorithms - because they chase correlations to make predictions (rather than identifying causal factors) - create big-data-driven poverty traps. They are also beyond the purview of the government (and, given the new administration, will probably be for quite some time): just like your FICO score is ONLY about how often you pay your credit card bills, and NOT your demographic group, so too should these algorithms be scrutinized and regulated.Increasingly, O'Neil notes, the ""human touch"" comes at a price; in upper income groups, you get more real, live service - hearing about a job through connections, informational interviews with alumni, blah - while lower income groups are serviced by increasingly mechanized, machine learning code. The hiring and HR practices of CVS, McDonald's, and other low wage places were especially striking (and severe): mindless, well-meaning algorithms deployed to effectively punish workers whose BMIs (itself a stupid measure of health) are too high, or who seem ""anti-social"" according to a creepy questionnaire, or whatever. I think my favorite chapter - and, of course, the most relevant one - was about the toxic brew that can be politics and data science. It was especially powerful to think through the way that, as opposed to the past, where we had ""agreed upon facts"" given by carpet bombing-style hegemonic culture (e.g. four news channels that everyone watched), now we have hyper-tailored DARE I SAY ""alternative facts"" where, by the power of marketers (who prey on confirmation bias), your neighbor's political beliefs are the result of a very specific and private relationship between them, their web browser cookies, and some political marketer. This means you may have NO IDEA why your swingy neighbor in swingy state X thinks Candidate Y is running an underground child sex ring literally under the ground of a DC pizza shop. O'Neil (and this book was written Before The Madness) notes that these sorts of micro-targeted ads that follow you around the web are very smartly tailored; the marketers know who would be repulsed by such a stupid conspiracy theory, and who would be intrigued. And that's just terrifying. Anywayyyyy. From a data perspective, my (cold dead) economist heart reminds me that OF COURSE we're going to scale up injustice when we mindlessly maximize profit, instead of humans. We need to go back to our roots: to the philosophy and debates of Jeremy Bentham, and welfare economics, and what ""utility"" really means. Efficiency is not the end all, be all, despite what our current economic system seems to diktat. O'Neil also makes a good point about the repeated erroneous conflating of correlation and causation; something even the fanciest of linear algebra stochastic process meddlers fall prey to. That's something that I see a LOT in data science: the field is built on the premise of perfect prediction - and prediction doesn't need causation, it just needs correlation. OK, I will step off my soapbox. Highly recommended.edited to add: OMG and I just read her delightful 2017 resolutions post, and had much LOLs. Cathy O'Neil, you da best."
270,0553418815,http://goodreads.com/user/show/8325737-philipp,4,"This is an angry book, but its got a point! She summarises many 'weapons of math destruction', WMDs, algorithms that encode human biases, are opaque, which give recommendations or orders that no-one questions, while making things worse. For example, this summary:Poor people are more likely to have bad credit and live in high-crime neighborhoods, surrounded by other poor people. Once the dark universe of WMDs digests that data, it showers them with predatory ads for subprime loans or for-profit schools. It sends more police to arrest them, and when they’re convicted it sentences them to longer terms. This data feeds into other WMDs, which score the same people as high risks or easy targets and proceed to block them from jobs, while jacking up their rates for mortgages, car loans, and every kind of insurance imaginable. This drives their credit rating down further, creating nothing less than a death spiral of modeling. Being poor in a world of WMDs is getting more and more dangerous and expensive.O'Neil goes through many, many of examples like this where an automated approach made things worse, where people didn't bother to critically look at the output, where the model doesn't have a way to incorporate feedback, it's stuck in time and cannot evolve like humans. The majority of the book is spent on case studies and great, angry discussion of what went wrong. There's one short chapter on what could be done - mostly she proposes more regulation (auditing of algorithms), but also to 'dumb' down models by law, force them to treat people equally and not bin them in groups. That chapter on solutions is extremely short - I expect a second book from her on that topic.P.S.: I've been reading a lot about adversarial perturbations in deep learning (see this great paper). One idea could be to counteract some of those models to become an adversarial entity. For example, you have one of those shopping points cards. Instead of feeding their model with your shopping data, you mix it up - write down your shopping list for a month, shuffle it, and buy one part with your points card and the other in cash. Keep mixing it up and add random stuff. Your data should be garbage to the model as it's not consistent, easily removable as an outlier, i.e., worthless.You could also make the model work for you - find out which groups of customers the market sends special offers to, game your shopping so that you fall into that group, reap rewards.Of course you'd need to know in detail what the model is looking for, which is highly unlikely."
271,0553418815,http://goodreads.com/user/show/115473-siria,4,"In the two years since Cathy O'Neil published this study of the danger posed by mathematical models—how their supposed neutrality merely reflects the biases of those who create them while creating closed-system feedback loops that warp society in unjust ways—it hasn't become any less relevant. In fact, not only does the book remain relevant, but subsequent revelations—most prominently those about Facebook's involvement with the 2016 US elections—show that O'Neil may even have been too cautious in what is a trenchant and prescient critique. "
272,0553418815,http://goodreads.com/user/show/25178051-izzat-radzi,5,"""Mathematical models are supposed to be our tools, not our masters"" Statisticians, quants, actuaries : take heed. This book argues that the new 'industrial revolution' of big data is bringing destruction. To whom (?) , one might be tempted to asked. Humans, normal masses who sometimes makes errors of course.And due to that, they got red in their ledgers, which in these new century means, that they'll most likely subjected to the 'vicious loop' of rejection and manipulation, and will felt the usual 'life is not fair' feeling. To start off, I claim that this book is the best (so far) compliment to Taleb's Black Swan book. Whilst Taleb as arguably he as a trader and academia revolves his discussion around some financial models & it's discontent (whilst in most part slanted his discussion into the philosophical realm); this author meanwhile goes into a more broad discussion. She, whom has background in chronologically in academia, hedge funds, a short stint in risk management before focusing work as a data scientist, argues that given the presence of these Gargantuan Big Data in almost every aspect of life, it has tarnished if not destroyed people's life. The problem-before I proceed case by case- lies in two reasons. First is the data collected & recorded by the data mining by corporate companies, mostly are left uncrunched, meaning they're not subjected to updated information about those people data taken or they're mistakenly (yet significantly affect) people who are thought 'one', in this case people with the same name and birthday. This, argued by the author, because the model used isn't not tinkered time to time according to feedback; unlike those used in baseball & basketball where opponents and managers subject each individual players to new & paternalistic habits of players -thus a better use of correlation to causation-, they are taken as face value. And feedback that is not taken -later I'll mentioned in job application survey- left those aspiring people hoping to get some good news, baffled as they themselves don't know why their application was rejected. Turns out, it was their 'behaviour', recorded in the subjective and highly interpretable application questions. The second problem here, if not wrong should I put it, is that the usage of the data is, as author's claimed, taken as God's word, the highest possible theological blasphemy, if said to those in denial data miners. This is due to the reasons that they basically use the data available, let it be credit scores, health scores, SAT (or generally educational) scores, and even resume applications -to name a few- as the bestowed revelation, thus dismissing proven good teachers, potential shining employees and laying off 'unhealthy' current workers; all based on the Big Data, either snooping privately on their life, or in other cases, a take-it-or-leave option wether they'll provide their credit score to get an insurance or their health progress for work employment group insurance practiced in firms & companies. The dismissal action has become so autonomous directly from the data, that frightenedly, that one does not care of others, it's just life. Coincidentally, I read Adorno's Culture Industry directly before this, and it does personally provides a more enlightened discourse on the systemic issue. I personally first came across to this topic back a few years ago, that with this newly published book kindles my interest. The first was back around late 2013, where I was taking a UK driving licence. In the course, similarly argued by the author, you'll be getting some discount given that you choose to put a 'Black Box' in your car. This instrument will record your driving habits & feed back to the insurance company. Although initially used to monitor those big truckers -who tends to be sleep deprived due to late night & long driving- to reduce the risks of death of others due to reckless driving; here argued that it's afraid to be used by others to manipulate the privacy of car drivers, at other instances flawed due to normal human behaviour, taking short cuts at night through 'dangerous neighbourhood' thus ended up one marked to be subjected to 'handcuffed' by auto insurance companies. The second one I encountered was in early 2015, in a student organised event to explain & showcase the structure & function of Big Data by companies. At that time, I was furious given the fact of the the then environment of whistle-blowers expose on companies manipulating & snooping normal civilians life. In here meanwhile, as argued in The Impulse Society by Paul Roberts, politicians backed by big companies are microtargeting voters up to the point of what they read and hear are tailor made. Given to the rise of fascism and 'fear of the normal neighbour' one known differently pre-election in America which now turns into something totally unexpected, I humbly agreed to some point. The third encounter is with regard to behavioural test subjected to work or scholarship applications. Exposed initially by my parents, whom my dad was at times hired to be an interviewer. He's the first to enlighten me on the presence of attitude test taken by applicants, which is imperative for career climbing ambitions. Here written an illustration. An applicant answered promptly the normal application survey on personal attitudes. Obviously one is subjected to child education for example that one is unique bla3; yet answering unique now instead of other answers) will be perceived as narcissist by the possible employer, which now is not possible anymore. Unfortunately, this data, is shared and this unfortunate applicant will ended up in this continuous 'dangerous loop'. The author coined Recidivism in regard to this loop. The poor guy, living in a certain 'bad' neighbourhood, want to apply a credit card, or just simply living as a normal civilian. Now, dues to this, of course one might have some background of knowing some in-behind bars people. This data, used by the security enforcement, will cause the police to routinely knocks on doors to tell them they have eyes on them. A more idiosyncratic example is those trying to move houses. Property firms with big data on their very own hand, easily exclude those with a bad tag, even when they've changed, or for unfortunate some, mistakenly recorded in. As said before, no feedback can be given by the victims, unless one has some wealth to pursue on a legal lawsuit on this issue, which of course deteriorate middle class let alone the poor condition. Obviously the rich bankers and markets manipulators escaped from this loop. As a matter of fact, they're invulnerable and invincible. She suggested a few suggestions in the concluding chapter. First for example, oath taking by those finance practitioners, whom impact upon their ignorance is highly significant on the whole. Nevertheless she noted the downside that when in work bureaucratic structure, manager will want answers for certain issues, this will put those people in corners. Second is, as Taleb suggested, ditching those backward minded model. Obviously, she argued, one have to be specialised in the field to help recognise and build a better one. She illustrated a new model on preventing child abuse. There's similarity between the model to 'possible criminals' as in the Minority Report movie in reducing - if not preventing- crime she criticised, the hidden outcomes has not - at least she didn't yet- been analysed. The third is the involvement of human judgement into the model. Yes, initially the algorithm of the system is used to remove human bias, yet seeing how things turned out, the machine as expected cannot understand how human (cognitive) work. The problem of language and rational judgement will always triumph human above machine, I claimed this ambiguous argument. Hats off to this book!"
273,0553418815,http://goodreads.com/user/show/11951948-paul,3,"One of the funniest bits of Little Britain was where David Walliams as Carol Beer would type a request into the computer, before turning to the customer and saying ‘Computer says no’… Whilst it is funny, it is not so funny when it happens to you. In this book, O’Neil, a former Wall Street quant raises alarm bells on the way that these mathematical models have infiltrated our lives. We don’t see them, but these algorithms that help us with our searches online and finding books, films and other items on online sites are now being used to determine just how much of a risk you are. Next time you want a loan, to renew insurance or just need to get another job O’Neil thinks that some of us may have a problem.She calls them ‘Weapons of Maths Destruction’; these are incontestable, unregulated and opaque algorithms. They are being used by companies to decide the tiniest details. They are used because they make larger profits for corporations and most worryingly for us is that they are frequently conclude the wrong thing having made incorrect assumptions about individuals. As the saying goes ‘crap in; crap out’… Having worried the life out of the reader, O’Neil goes onto suggest a variety of things that could help; more regulation, better design of the code and us being aware of their use. The writing is clear, if a little dry and technical at times. The examples are a little American centric, but you can see the way that it is going in the UK. Even though the title mentions the dreaded word maths, it really isn’t that mathematical. Worthwhile reading."
274,0553418815,http://goodreads.com/user/show/16212136-peter-pete-mcloughlin,4,"Very depressing book on how computer algorithms are becoming a powerful tool of the exploitation of workers, the poor, minorities and ordinary people mostly in the service of profits. It goes deep and the problem is much worse than you think. This book is a downer as it covers relevant developments in politics and business and that always is not pleasant to think about in these times in general and are especially dispiriting in how they affect ordinary people."
275,0553418815,http://goodreads.com/user/show/5550611-hallie,2,"Just ok. I like the idea more than the execution of this book. While O'Neil uncovers a great, dark place in our society that needs more light shone on it, much of her conclusions she gives away in the introduction to the book. This would have been an excellent piece in the New Yorker or Atlantic but as a book began to feel redundant far too early. "
276,0553418815,http://goodreads.com/user/show/11345492-kate,4,"3.5, bumped up for likeable clarity-without-condescension and consistent up-front political engagement."
277,0553418815,http://goodreads.com/user/show/761281-mike,4,"Reading this book I could not help but wish the author had used a quote from Paul Goodman as the epigraph of the book: ""Technology is a branch of moral philosophy, not of science."" (Every technology ultimately is making a value judgement about what is important or worth doing, and what aspects of a task or reality are important.)This book is a fascinating look at how 'big data' is used and abused by colleges, banks, insurance agencies, in sentencing and parole decisions, by employers screening applicants, and more. Collecting vast amounts of data and using to guide decisions can be a very useful technology, but the author -- who herself had a career as a 'quant' or data analyst for a large firm -- points out the downside of basing decisions on data and algorithms. Of course it should be obvious, but our society's information fetishism can make us overlook that the data we collect and the algorithms we use to analyze it can be deeply flawed by omissions, prejudices, fallacies that the designers may not be aware of. A couple of the many examples she sues help illustrate this. We read about a school teacher whose performance is measured by an algorithm that takes into account his students' past performance, current performance on standardized tests, and various other factors. One year he is scored 6/100, and the next year 94/100 -- after making no changes to his syllabus, assignments, or anything else he was doing. Anyone with common sense can see there is something wrong with the algorithm if someone could get such wildly varying scores, but the scoring algorithm is a 'black box' that not event the school administrators understand. Among the many problems with the system here is that there is no allowance for the designers of the algorithm to get feedback on the scoring and adjust the model -- instead, the 'failing' teachers are fired and the algorithm rolls on. Contrast this with the use of 'big data' algorithms in commercial enterprises: when Amazon.com collects data on your browsing and purchasing habits, they use this recommendations to you for future purchases. If their models are bad, they suggest things you don't want, and they don't increase sales. But they take this feedback and change their algorithms, because there is a financial incentive to do so.So a huge blind spot in some of the models and algorithms policy makes use is the lack of feedback. Another is huge downside of the rise of big data modeling is that private companies can use opaque models to discriminate in ways that punish the poor particularly. The author explains how car insurance companies use data about where their customers live, drive, and work to assess risks -- which is largely legitimate -- but also abuse this information to determine which customers are likely to be able to shop around for better deals, and which are less likely to have the time, resources, or access to information to do so, and they are charged much more. Indeed affluent customers with DUI convictions may be better rates than safe drivers who live in areas with limited internet access and higher poverty. Likewise employers may screen applicants with big data algorithms that identify 'risks' which rule out the poor, people with past mental illness, or immigrants. Many other problems are discussed, and the final chapters look at how political parties have begun to utilize big data too -- an ominous prospect in light of the rest of the book.The author offers some solutions, ranging from the admittedly ineffective (a code of ethics for 'quants' who create the models) to the more daunting but effective measure of demanding transparency and regulation (which has made some progress in the insurance industry, but is far from complete).*Full disclosure -- I read a free advanced reader's copy that I won in a Goodreads giveaway.*"
278,0553418815,http://goodreads.com/user/show/3502690-graeme-roberts,2," Cathy O'Neil writes well and provides much good information, but I found the fundamental thesis of  Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy to be foolish and irresponsible. The title, condensed inevitably to WMD, is used promiscuously to describe any application of data science, statistics, or even information technology that she considers to be unfair or discriminatory. In most cases, she is right; some applications are used by the greedy and the uncaring with insufficient thought of the consequences, but she rarely considers the positive attributes of the applications. Once branded as a WMD, a term indelibly attached to the evil trifecta of George W. Bush, Saddam Hussein, and the Iraq War, how are readers meant to consider their many positive aspects and what can be done to make them better?At first, I thought that Ms. O'Neil had been the victim of a foolish PR or marketing person in her publishing company, who had come up with this catchy name, but her ardent use of the term convinced me that she had invented it.Ms. O'Neil seemed to follow a downward trajectory from her Harvard PhD in mathematics and a teaching position at Barnard to a leading hedge fund and a series of other morally disappointing positions that left her convinced that data science, and even mathematics, are roots of much evil. I don't buy it. They are tools that can be used or misused, and her diatribe does nothing to help.She asserts, on page 210:If we find (as studies have already shown) that the recidivism models codify prejudice and penalize the poor, then it's time to have a look at the inputs. In this case, they include loads of birds-of-a-feather connections. They predict an individual's behavior on the basis of the people he knows, his job, and his credit rating—details that would be inadmissible in court. The fairness fix is to throw out that data.But wait, many would say. Are we going to sacrifice the accuracy of the model for fairness? Do we have to dumb down our algorithms?In some cases, yes. If we're going to be equal before the law, or be treated equally as voters, we cannot stand for systems that drop us into different castes and treat us differently.I disagree strongly. Some data is better than none, though we should be looking to constantly improve our data science systems. I would like to know that the recently paroled criminal moving in next door had been released by a judge in full possession of whatever facts are available. So, I suspect, would Ms. O'Neil."
279,0553418815,http://goodreads.com/user/show/19993-robin,4,"If you've been listening to Cathy O'Neil on Slate Money for some time, you probably expected this book to emerge in exactly this form and with exactly this content. She's smart, progressive, focused, and clear, just as she was on the podcast. I saw her speak at the Mechanic's Institute Library a few months ago, before I'd read the book, and was very impressed by her ability to be articulate about complex situations with a decisiveness that belies their complexity, or at least certainly never allows for it as an excuse. The argument I found most challenging was (heavily paraphrasing here) that we should be privileging fairness because it is otherwise so easy to be blind to the unfairness that creeps in when we pursue other goals. If I'm honest with my own tendencies, I recognize my longing for accuracy might indeed cause me to overlook fairness, and I resist the idea that I need to choose one as a priority over the other. Nevertheless, O'Neil convinces me to question assumed neutrality much more closely, and be much more vigilant for it in my own industry. As she points out, it's easy for people who write the algorithms to absolve themselves of responsibility for how they are used, but if we pay attention to the real outcomes we may find that we need to do more to train our algorithms to learn from externalities and flaws. Focusing ruthlessly on measuring only what you are trying to measure and ignoring all other factors is a good start; evaluating the results to make sure that's actually what happened is as necessary a practice. The book is full of examples of failed algorithms in wide use, actively and detrimentally affecting at-risk populations today, as well as potential future areas where algorithms could be used, perhaps even inadvertently, to consolidate and perpetuate the power of the already powerful. The free market will not solve this problem. This is an excellent example of where good regulation is imperative. As constituents with the ability to influence legislators to put their attention here, we can demand more transparency, more awareness and education, and a more good, old-fashioned compassion.Further reading:https://www.ted.com/talks/joy_buolamw...https://www.propublica.org/article/ma...https://standards.ieee.org/develop/pr..."
280,0553418815,http://goodreads.com/user/show/7139041-dorin,2,"Disappointing - such a rich topic, yet the author decided to be racist, Democrat and to induce panic instead of having a cool headed approach. She ignores her own advice, contradicts herself time and again, and she makes her Democrat leaning so obvious it makes me puke.The book could have contained real data, real information, but instead she goes on to explain that statistics are bad because statistics don't catch statistical errors. She pretends she's a professional in data processing, but she sounds like she doesn't understand how statistics work, and she can barely go beyond her mathematical training to understand what went wrong from a human point of view.My point: „a machine is as useful as the person who uses it”. Her point: ""big data is bad unless it's used by the Democrats”."
281,0553418815,http://goodreads.com/user/show/7837611-greg-fanoe,2,"I think that on the whole reading this book will do people more good than harm but I had several problems with it:1. The only section which I have personal expertise on is the section on ""insurance"" - I am an actuary - and I can personally verify that this chapter was filled with inaccurate, misleading, and/or totally speculative statements.2. The correlation between credit score and income is nowhere near as strong as she makes it appear in this book.3. Many of the problems she identifies have nothing to do with the models in question. Advertising models are not the issue with the payday loan or for-profit college industry those industries need to be totally upended/destroyed at the source. Letting them operate as is but kneecapping their ability to advertise would be nonsensical.4. The solutions she presents are ridiculously vague and/or impractical."
282,0553418815,http://goodreads.com/user/show/1045774-mehrsa,5,This is the book I was waiting to read! The rejoinder to our data-obsessed culture. We got drunk on algorithms and forgot to focus on their limitations. are they asking the right questions? are they recycling inequality? Great book
283,0553418815,http://goodreads.com/user/show/3126915-imi,2,"Sadly, a rather superficial look at big data, algorithms and how they've impacted on society. You probably know/have guessed all this already. The correlations O'Neil point out are not very convincing, and it felt like she just wanted to rant about how unfair the world is. Not that I disagree with that conclusion; society is unfair and unequal. But O'Neil really doesn't state much more than that, and, worse, some of her examples felt misleading or simply speculative. Where was her evidence? And if she'd explored this issues in more depth I'd have wanted clear ideas for resolving these issues. Instead, any suggestions she made seemed vague and impractical. I admitedly feel out of my depth with this topic, but am most definitely willing to learn more and expand my understanding about it. Disappointingly, this book didn't offer that."
284,0553418815,http://goodreads.com/user/show/777176-dave,5,"I learned more from this book than I have from any book I've read in years. O'Neil is a former math professor and spent time as a hedge-fund quant, so she understands the theory and experience of the effect of out-of-control algorithms on our society.As corporations make more and more efficient use of algorithms, they are starting to show dangerous tendencies, according to O'Neil. She calls them ""weapons of math destruction"" or WMDs. WMDs present the following grave problems for society:--They usually lack transparency, such that it is difficult or impossible to know what goes into them and what factors sway or change their results. When they are opaque, WMDs are patently unfair (O'Neil cites the algorithms used to evaluate teachers in many districts, highlighting their absurdity as they attempt to quantify the unquantifiable).--They make broad generalizations and reduce human beings to ""buckets."" One of the worst examples is the criminal justice system, which is increasingly reliant on algorithms to deploy police patrols and even to sentence criminal defendants. These algorithms punish the innocent by assuming that those who live in poor neighborhoods are necessarily more likely to commit crimes, thereby subjecting them to frequent ""stop and frisk"" type searches that have been found to be unconstitutional repeatedly.--They can use information in an unfair way that tends to perpetuate further injustice. A prime example of this is the increasing tendency to identify individuals by their credit score, on job applications and even in online shopping. When job applicants are screened by credit score, those who are already poor (and therefore much more likely to have a low credit score already) will find it even harder to get a job and thereby earn enough money to help improve their credit score. This self-perpetuating cycle of poverty is completely unnecessary, because credit scores are an exceedingly poor proxy for job-worthiness of applicants.--Many WMDs are not very good at learning. They are created as models of human behavior (with all the flaws and assumptions of the humans who programmed them), and when they miss (when for example they screen out a potential hire who would have actually been a first-rate employee), they never learn that they have missed. The ""false positive"" never presents itself (the rejected applicant goes on to success elsewhere), and the oblivious WMD continues to chug along with its assumptions and blind spots unchallenged.And WMDs are like racism:""Racism, at the individual level, can be seen as a predictive model whirring away in billions of human minds around the world. It is built from faulty, incomplete, or generalized data. Whether it comes from experience or hearsay, the data indicates that certain types of people have behaved badly. That generates a binary prediction that all people of that race will behave that same way....Racists don't spend a lot of time hunting down reliable data to train their twisted models. And once their model morphs into a belief, it becomes hardwired. It generates poisonous assumptions, yet rarely tests them, settling instead for data that seems to confirm and fortify them. Consequently, racism is the most slovenly of predictive models. It is powered by haphazard data gathering and spurious correlations, reinforced by institutional inequities, and polluted by confirmation bias. In this way, oddly enough, racism operates like many of the WMDs I'll be describing in this book."" (p. 22-3)And finally:""Models like this will abound in coming years....Many of these models, like some of the WMDs we've discussed, will arrive with the best intentions. But they must also deliver transparency, disclosing the input data they're using as well as the results of their targeting. And they must be open to audits. These are powerful engines, after all. We must keep our eyes on them.... Data is not going away....Predictive models are, increasingly, the tools we will be relying on to run our institutions, deploy our resources, and manage our lives. But...these models are constructed not just from data but from the choices we make about which data to pay attention to--and which to leave out. Those choices are not just about logistics, profits, and efficiency. They are fundamentally moral. If we back away from them and treat mathematical models as a neutral and inevitable force, like the weather or the tides, we abdicate our responsibility....We must come together to police these WMDs, to tame and disarm them."" (p.218)"
285,0553418815,http://goodreads.com/user/show/3029658-doug-cornelius,5,"With big data, comes formulas to parse through the data trying to make sense of it. Those algorithms can help make sense of the data and help filter through the noise to find trends. But those algorithms can also be easily misused and have harmful, if unintended consequences. Cathy O'Neil explores these problems in 
Weapons of Math Destruction
.Ms. O'Neil is a former academic, hedge fund quant, and data scientist for startups. At the hedge fund she was looking at ways to make money by predicting financial trends. As a data scientist she was looking to predict people's purchases and browsing habits to monetize those habits. Those are good algorithms where the people using them understand them and update the algorithms as they see problems and can check to see if they are working properly by verifying outcomes.Many data driven outcomes fail. Those are the focus of 
Weapons of Math Destruction
.I'm a big fan of the Slate Money podcast with Cathy O'Neil, Felix Salmon, and Jordan Weissmann. When the publisher offered me a review copy of Ms. O'Neil's book, I jumped at the chance.It's a short book and even though the title makes it sound like it's full of math, it's not. It's more about social policy. She points to the danger of decision makers blindingly following algorithmic output that they do not understand.Take the US News and World Report listing of best colleges. This is preeminent guide for high school students trying to figure out which college to attend. The rankings do not look at actual student outcomes. The publisher does not look at happiness, learning or improvements. It looks at proxies for the outcomes like SAT scores, alumni contributions, and graduation percentages. Colleges started making decisions to improve their rankings in the algorithm by improving those measured proxies. As anyone writing checks for their college bound children, affordability is not one of the measured proxies.As you might expect there are big chunks of the book dedicated to failings in the financial sector by relying on poorly designed algorithms. The biggest problems often being false assumptions plugged into the data running through the algorithm.With the election season upon us, the chapter on political algorithms is fascinating. The data scientists of commerce were brought into political campaigns. They are able to micro-target potential voters sending different messages to different groups highlighting the positions that would make the candidate more favorable to that segment of the population. Civic engagement and the political process is increasingly being algorithm driven.Since our world is increasingly being driven by big data, it's important to understand what is happening behind the decision-making. Weapons of Math Destruction is an excellent tool to help you understand data driven decision-making."
286,0553418815,http://goodreads.com/user/show/23020614-germ-n,3,"A fascinating must-read guide on how to navigate and interpret a world increasingly ruled by materialistic prejudices and oversimplifications that are repackaged as ""science"" and ""tech"" through poorly designed computer algorithms.Reading this gives one the feeling that the ""evil androids ruling over mankind"" dystopia has already been unrolling. Except in a very sneaky way: this time we don't get to face or even understand the robots and their actions which are nonetheless already reshaping our reality and the course of our lives through ridiculous university rankings, credit scores, social media ideological bubbles, etc.Unfortunately O'Neil ruins an otherwise brilliant book by an extreme leftist bias that seeks to absolve her personal heroes in the US democratic party and their political agendas, and attempts to attribute most blame on a specific other side. I assume this is why a book which raises such important points gets to be published and become mainstream after all.For example, she portrays Obama as a saint who just wants the best for immigrants and fiercely battles the evil rich. Yet for any unbiased observer, it's quite clear that Obama and democrats are just serving the wealthy who finance their own political campaigns (duh) when they set to ruin nations overseas and bring in thousands of immigrants, ideally illegal ones, so the price of labor can go down and corporations can continue to reap political advantages.This skewed perspective is damaging to the bottom argument and distracts readers from what's really the main issue here: the constant loss of truth and distortion of increasingly astroturfed public debate. No attempts are made, for example, at addressing how these WMDs are influencing the toxic foreign policy of the US and the press - another entity that passes as a saint in this book, while being the number one responsible for war and destruction of nations around the world."
287,0553418815,http://goodreads.com/user/show/20465079-tam,4,"A quick and highly useful read - I would recommend it to all people not just ones working with predictive models and Big Data.The title is sensational enough, but at one point I put that aside and decided to read this book. So many have been praising Big Data and Machine Learning, few cautions have been put forward but not enough, mostly dealing with ethics, security, personal data issues. This book, on the other hand, discusses the large scale tragedies that Big Data can invoke on societies as a whole, if continuing to go unchecked. 10 chapters deal with examples about the misuses of data and predictive models in many aspects of lives, especially the lives of the most unfortunate.O'Neil has a strong opinion and at certain point I thought she might be too pessimistic (saying that from a self-proclaimed pessimistic person as I am), and O'Neil freely claims how angry she is with the whole thing. Yet at the very end of the book, she makes constructive comments, ideas for improvement, for regulations. Those are the most important, and perhaps the most useful passages for data scientists. I've been quite fascinated by the tool and sometimes forget the darker side that the tool can invoke, and I'd like to thank O'Neil for the reminder, for the clarification about issues that sometimes I feel uneasy but not so clear about. No, the tool is neither inherently good nor bad, the people who make them and wield them can choose do what they want. And we can choose to do good things.Ehh, not so optimistic though, there are tons of challenges and... But I should shut my mouth. But at least we should try."
288,0553418815,http://goodreads.com/user/show/15409024-ahalya-vijay,5,"I found this to be a fascinating read on the importance of using technology and data in an ethical way. I was shocked to learn how pervasive irresponsible uses of data are throughout the developed world and how widespread it is for people to place their faith in ""black box"" models as Cathy O'Neil refers to them. It must be frustrating, for example, for teachers to be evaluated (and fired!) based on a model they have no knowledge of, without knowing how to improve their rating. And it must be even more so if they are getting anecdotally strong and positive feedback from school leadership and parents.Another shock was how liberally companies and 'data scientists' use proxies without thought to the consequences of the model not capturing the 'error term' sufficiently. The danger of making decisions based on things that are modeled (or estimated) using proxies was drilled into my head throughout my entire schooling. How would you feel if the fact that you were late to one meeting at work was used to brand you as an irresponsible or unreliable worker? Ms O'Neil made a clear link between such models being used to trap people in a cycle of poverty, using example after example across a variety of industries (e.g., education, finance, insurance). It is clear that reform is needed, but what left me worried at the end is that these models are not hurting the rich and famous, but the poor and under-resourced. Will our current political and justice system listen to their needs? I'm skeptical."
289,0553418815,http://goodreads.com/user/show/48433695-alexis-tremblay,5,"A must read for all data scientist, mathematicians, modellers, statisticians,... There is a dark side to big data and she exposes it masterfully. I always had an uneasy feeling when I started my career as an actuary and later on as data scientist. My moral compass was always off but I didn't have the words for it. Now she has given me a framework to think about my job and what to look out for. Will read again. "
290,0553418815,http://goodreads.com/user/show/16204614-bing-gordon,4,"Good overview of modern analytics, but too much of a guilt trip laid on top. "
291,0553418815,http://goodreads.com/user/show/6100646-brian-clegg,4,"As a poacher-turned-gamekeeper of the big data world, Cathy O'Neil is ideally placed to take us on a voyage of horrible discovery into the world of systems making decisions based on big data that can have a negative influence on lives - what she refers to as 'Weapons of Math Destruction' or WMDs. After working as a 'quant' in a hedge fund and on big data crunching systems for startups, she has developed a horror for the misuse of the technology and sets out to show us how unfair it can be.It's not that O'Neil is against big data per se. She points out examples where it can be useful and effective - but this requires the systems to be transparent and to be capable of learning from their mistakes. In the examples we discover, from systems that rate school teachers to those that decide whether or not to issue a payday loan, the system is opaque, secretive and based on a set of rules that aren't tested against reality and regularly updated to produce a fair outcome.The teacher grading system is probably the most dramatically inaccurate example, where the system is trying to measure how well a teacher has performed, based on data that only has a very vague link to actual outcomes - so, for instance, O'Neil tells of a teacher who scored 6% one year and 96% the next year for doing the same job. The factors being measured are almost entirely outside the teacher's control with no linkage to performance and the interpretation of the data is simply garbage.Other systems, such as those used to rank universities, are ruthlessly gamed by the participants, making them far more about how good an organisation is at coming up with the right answers to metrics than it is to the quality of that organisation. And all of us will come across targeted advertising and social media messages/search results prioritised according to secret algorithms which we know nothing about and that attempt to control our behaviour.For O'Neil, the worst aspects of big data misuse are where a system - perhaps with the best intentions - ends up penalising people for being poor of being from certain ethnic backgrounds. This is often a result of an indirect piece of data - for instance the place they live might have implications on their financial state or ethnicity. She vividly portrays the way that systems dealing with everything from police presence in an area to fixing insurance premiums can produce a downward spiral of negative feedback.Although the book is often very effective, it is heavily US-oriented, which is a shame when many of these issues are as significant, say, in Europe, as they are in the US. There is probably also not enough nuance in the author's binary good/bad opinion of systems. For example, she tells us that someone shouldn't be penalised by having to pay more for insurance because they live in a high risk neighbourhood - but doesn't think about the contrary aspect that if insurance companies don't do this, those of us who live in low risk neighbourhoods are being penalised by paying much higher premiums than we need to in order to cover our insurance. O'Neil makes a simplistic linkage between high risk = poor, low risk = rich - yet those of us, for instance, who live in the country are often in quite poor areas that are nonetheless low risk. For O'Neil, fairness means everyone pays the same. But is that truly fair? Here in Europe, we've had car insurance for young female drivers doubled in cost to make it the same as young males - even though the young males are far more likely to have accidents. This is fair by O'Neil's standards, because it doesn't discriminate on gender, but is not fair in the real world away from labels.There's a lot here that we should be picking up on, and even if you don't agree with all of O'Neil's assessments, it certainly makes you think about the rights and wrongs of decisions based on automated assessment of indirect data."
292,0553418815,http://goodreads.com/user/show/7019761-cindy,4,"While I don't necessarily agree with all of the conclusions, this was a very thought-provoking book. The author is a mathematician who has worked on Wall Street and now takes on some of the math models that rule our lives. Some of these were, at least originally, developed with good intentions - intending to make fairer some of the decisions that affect our lives: where and who gets into college; what the schedule for our job looks like; how much we pay to get a loan; even how you are judged in your chosen profession. But the author poses that many of these types of decisions are actually less than fair. The models are in her terms WMDs - weapons of math destruction - they are hidden, unregulated, and virtually uncontestable, even when they are blatantly wrong. One of the more troubling examples from the book concerned the use of testing to judge teacher performance. She traced how an exemplary teacher with outstanding evaluations was judged as poor on the basis of one set of tests and fired. The teacher showed that the tests which showed that her ""value-added"" score was sub-par were flawed due to a high number of erasures on the previous year which indicated cheating. Indeed, the system said that there *might* have been cheating but they would not re-instate this excellent teacher. (And please don't get me started on ""value-added"" - my daughters' school was always excellent in all the testing but this score was just average. So if your kids are already high achievers, how do you continue to score excellent in this measure? The answer is it's almost impossible.)Other examples showed that even when you removed race or ethnicity or economics as an indicator in these WMDs, they were there. If a zip code puts you in a risky category for a student loan, then it's hard to get the education that can get you out of poverty; or if a work schedule makes it hard for a single mother to get the child care she needs so that she can get the money to go to school to make a better life for herself and her child...well, I can see how some of this sets up a self-fulfilling prophecy. It makes it harder, not impossible, to break this cycle. That's why this book is important. If this is being used, then we need to know and like credit scores have the ability to correct errors and inaccuracies that work against the average person.Quotes to remember:....This would all be wonderful for students and might enhance their college experience, if they weren't the ones paying for it in the form of student loans that would burden them for decades. We cannot place the blame for this trend entirely on the US News rankings. Our entire society has embraced not only the idea that a college education is essential, but the idea that a degree from a highly-ranked school can catapult a student into a life of power and privilege. As I write this, the entire voting population that matters lives in a handful of counties in Florida, Ohio, Nevada, and a few other swing states. Within those counties is a small number of voters whose opinions weigh in the balance. I might point out here that while many of the WMDs we've been looking at, from predatory ads to policing models, deliver most of their punishment to the struggling classes, political microtargeting harms voters of every economic class, from Manhattan to San Francisco, rich and poor alike find themselves disenfranchised, though the truly affluent of course can more than compensate for this with campaign contributions."
293,0553418815,http://goodreads.com/user/show/6894205-allie,4,"This was by far the most interesting book about math I have ever read (admittedly a low bar to set), possibly because it contains no actual math formulas. Instead, former mathematics professor and hedge fund analyst Cathy O'Neil tells stories about the applications of math - how ""ill-conceived mathematical models now micromanage the economy from advertising to prisons."" Among other things, mathematical models determine which news you see on social media, your Google search results, the cost of your auto insurance and credit card rates, which neighborhoods police are sent to patrol, and how your child's schools are evaluated. In one staggering statistic about the reach of these models, O'Neil notes that 72 percent of resumes are never seen by humans, but are instead screened and scored by software programs looking for employer keywords. While math itself is neutral, the models are based on assumptions that reflect human biases and limitations. Often, the models are designed to process data from large numbers of people efficiently, but not necessarily fairly. She distinguishes between ""good"" models built on transparent assumptions that are constantly corrected with new data (e.g., those used in professional baseball) to opaque models that use proxy data such as race or zip codes and do not have a corrective feedback mechanism (e.g., predictive policing software). Typically, there is no method of recourse when given a poor score by a computer (hello, Hal) since companies consider their algorithms proprietary and won't disclose details of how scores are developed. This was an issue for teachers evaluated - and sometimes fired - based on inaccurate models of student test scores. The book's second main argument is that the built-in biases in the models are especially putative towards the poor, who are steered to high interest loans, targeted by ""stop and frisk"" policing based on their neighborhoods, and less likely to be personally interviewed for jobs.I found many of O'Neil's arguments persuasive and somewhat alarming, although she tries to end the book on a positive note. Worth reading, even if you don't like math. Or weapons. Or destruction. "
294,0553418815,http://goodreads.com/user/show/70315594-enoughtohold,4,"A nice, clear overview of the problems of big data for laypeople. Not sure why other reviewers expected this to be deeply technical, or why they think O’Neil thinks there are easy answers — she plainly doesn’t.I for one am glad a book this accessible exists on this topic. After all, the vast majority of the people affected by these harmful models are not data scientists, and they deserve to know what’s going on.(Also, the author did a great job reading for the audiobook.)"
295,0553418815,http://goodreads.com/user/show/4430576-miri,5,"I've said it before, and I'm sure I'll say it again, because I make a point of seeking out books like this. But if there's one book I could get everyone to read right now, it would be this one. Even next to Ta-Nehisi Coates's We Were Eight Years in Power, which I'm reading concurrently and which is staggering in its importance, this book feels urgent—because what it explains is how our new economy of Big Data is codifying, concealing, and amplifying inequality of every kind, on an enormous scale. Every injustice I'm reading about in Coates's book is made worse by what I read about here. We think of math as inherently neutral, the way we think of technology and logic and science. But these things are like any tool, dependent on the person using them, capable of being used for good or evil. (Remember the way science was abused throughout American history, fabricating the entire concept of biological race to provide justification for white humans to enslave black ones.) When we start using math to make decisions instead of people, assuming that this will decrease human bias and make things more fair, what we can actually end up doing is obscuring that prejudice and magnifying the scale of its effect. Cathy O'Neil was a mathematician working in the finance industry in 2008. She left after a few years, disgusted with the way the system was clearly rigged, and has since created the concept of Weapons of Math Destruction: mathematical models that codify human prejudice, are opaque and impervious to feedback, and affect large numbers of people. In this book, O'Neil takes us through several WMDs, showing how they're used in different areas of our lives. There's the disastrous school reform attempt in Washington D.C., which tried to evaluate teachers in a way math simply cannot be used, and which fired hundreds of teachers based on results no one in the district could explain or defend. There's the way many employers now use credit scores to screen applications, assuming that making regular payments means you'll be a dependable employee, ignoring the facts that (1) responsible people experience financial difficulty too and (2) preventing people with bad credit from getting jobs is only going to make their situation worse.There are the computerized risk models that judges use in sentencing, trying to gauge from prisoners' background information—where they grew up, whether their friends and family have criminal records; information which would be inadmissible in court—how likely they are to return to prison after being released, and how long their sentence should be. There's the way U.S. News & World Report helped cause an arms race when it decided to start ranking universities on everything except their cost, driving up tuition over 500 percent (nearly four times the rate of inflation) in just the 30-ish years that I've been alive.There's political advertising which is now so individually personalized that even people who support the same candidate, even people visiting the candidate's website, will see totally different things based on the profile the campaign has created for them.There's predatory advertising like that done by for-profit colleges, which specifically targets vulnerable people's ""pain points"" looking for those who are ""stuck,"" who have ""few people in their lives who care about them,"" who have ""low self-esteem"" and are ""unable to see and plan well for the future"" (these quotes, shared by O'Neil, come from the California Attorney General's office when it was investigating one of these colleges). (And if you're wondering how they would know this about people, the point is that they know this about all of us now. Just try to imagine what someone could learn about you from seeing everything you do online.)It might seem like the logical response is to disarm these weapons, one by one. The problem is that they’re feeding on each other. Poor people are more likely to have bad credit and live in high-crime neighborhoods, surrounded by other poor people. Once the dark universe of WMDs digests that data, it showers them with predatory ads for subprime loans or for-profit schools. It sends more police to arrest them, and when they're convicted it sentences them to longer terms. This data feeds into other WMDs, which score the same people as high-risk or easy targets and proceed to block them from jobs, while jacking up their rates for mortgages, car loans, and every kind of insurance imaginable. This drives their credit rating down further, creating nothing less than a death spiral of modeling. Being poor in a world of WMDs is getting more and more dangerous and expensive.The same WMDs that abuse the poor also place the comfortable classes of society in their own marketing silos . . . For many of them it can feel as though the world is getting smarter and easier. Models highlight bargains on prosciutto and chianti, recommend a great movie on Amazon Prime, or lead them, turn by turn, to a cafe in what used to be a ""sketchy"" neighborhood. The quiet and personal nature of this targeting keeps society's winners from seeing how the very same models are destroying lives, sometimes just a few blocks away.Though we're all affected by them in ways we can't see, right now the majority of the damage is being done to the same groups who are always hurt by flaws in the system—minorities and the poor. The same models could easily be used to help people instead of prey upon them, but that won't happen in the glorious free market of capitalism. We need regulation and oversight in the data industry. We need accountability. When statistics itself, and the public’s trust in statistics, is being actively undermined by politicians across the globe, how can we possibly expect the Big Data industry to clarify rather than contribute to the noise? We can because we must. Right now, mammoth companies like Google, Amazon, and Facebook exert incredible control over society because they control the data. They reap enormous profits while somehow offloading fact-checking responsibilities to others. It’s not a coincidence that, even as he works to undermine the public’s trust in science and in scientific fact, Steve Bannon sits on the board of Cambridge Analytica, a political data company that has claimed credit for Trump’s victory while bragging about secret ""voter suppression"" campaigns. It’s part of a more general trend in which data is privately owned and privately used to private ends of profit and influence, while the public is shut out of the process and told to behave well and trust the algorithms. It’s time to start misbehaving. Algorithms are only going to become more ubiquitous in the coming years. We must demand that systems that hold algorithms accountable become ubiquitous as well."
296,0553418815,http://goodreads.com/user/show/17187084-sean-goh,4,"Very readable book warning of the downside of amoral models. With great power comes great responsibility.___Math-powered applications were based on choices made by fallible human beings. Many models encoded human prejudice, misunderstanding, and bias into the software systems that increasingly managed our lives. Like gods, these mathematical models were opaque, their workings invisible to all but the highest priests in their domain: mathematicians and computer scientists.Models are opinions embedded in mathematics.Statistical engines without feedback can continue to spin our faulty and damaging analysis while never learning from its mistakes. E.g. teachers mistaken labelled as failures are fired, and no one is the wiser. The model's output becomes unquestioned reality.The parallels between finance and Big Data: The productivity of the field's talents indicates that they are on the right track, and it translates into dollars. This leads to the fallacious conclusion that whatever they are doing to bring in more money is good. It ""adds value"". Otherwise why would the market reward it?In both cultures Wealth is no longer a means to get by. It becomes directly tied to personal worth.On the rise of university rankings: from the perspective of a university president, it's quite sad. Here they are at the summit of their careers dedicating enormous energy towards boosting performance in fifteen areas defined by a group of journalists at a second-tier magazine (U.S. News). The ranking had become destiny. When you create a model from proxies, it is far simpler for people to game it. This is because proxies are easier to manipulate than the complicated reality they represent. (e.g. trying to evaluate influence by number of twitter followers)In a system in which cheating is the norm, following the rules amounts to a handicap. So preventing the students in Zhongxiang from cheating was unfair.In our largely segregated cities, geography is a proxy for race.Patrolling particular areas creates a pernicious feedback loop. The policing itself spawns new data, which justifies more policing. Eventually the zeroing in on ghetto districts ends up criminalising poverty, believing all the while that our tools are not only scientific but fair.Instead of simply trying to eradicate crimes, police should be attempting to build relationships in the neighbourhood. This was one of the pillars of the original ""broken windows"" study. The cops were on foot, talking to people, trying to get them to uphold their own community standards. But that objective, in many cases, has been lost, steamrollered by models that equate arrests with safety.Mathematical models can sift through data to locate people who are likely to face great challenges, whether from crime, poverty, or education. It's up to society whether to use that intelligence to reject and punish them - or to reach out to them with the resources they need. It all depends on the objectives we choose.Computing systems have trouble finding digital proxies for soft skills. The relevant data simply isn't collected, and anyway it's hard to put a value to them. They're usually easier to just leave out of the model.The modelers of e-scores (unofficial credit ratings) have to make do with trying to answer the question: ""How have people like you behaved in the past?"" when ideally the question would be ""How have you behaved in the past?Even with the Affordable Care Act, which reduced the ranks of the uninsured, medical expenses remain the single biggest cause of bankruptcies in America.A sterling credit rating is not just a proxy for responsibility and smart decisions. It is also a proxy for wealth. And wealth is highly correlated with race.Mistakes made by models are learning opportunities - as long as the system receives feedback on the error. When automatic systems sift through our data to size us up for an e-score, they naturally project the past into the future. The poor are expected to remain poor forever and are treated accordingly. It's inexorable, often hidden and beyond appeal, and unfair.Yet machines cannot make adjustments for fairness by themselves. That is where the human overseer comes in.Human decision making, while often flawed, has one chief virtue. It can evolve.Big data processes codify the past. They do not invent the future. Doing that requires moral imagination, and that's something only humans can provide. We have to explicitly embed better values into our algorithms, creating Big Data models that follow our ethical lead. Sometimes that will mean putting fairness ahead of profit."
297,0553418815,http://goodreads.com/user/show/6991670-bob,4,"Summary: An insider account of the algorithms that affect our lives, from going to college, to the ads we see online, to our chances of getting a job, being arrested, getting credit and insurance.Big Data is indeed BIG. Mathematical algorithms shape who will see this post on their Facebook newsfeed. If you go to Amazon or another online bookseller, algorithms will suggest other books like this one you might be interested in. Have you seen all those ads about credit scores? They are more important than you might imagine. Algorithms used by employers and insurance companies determine your employability and insurability in part through these scores. Far more than another credit card (bad idea, by the way) or a mortgage are on the line. These algorithms seem objective, but how they are formulated, and the assumptions made in doing so mean the difference between useful tools that benefit people, and ""black boxes"" that thwart the flourishing of others, often unknown to them.Cathy O'Neil should know. A tenure track math professor, she made the jump to Wall Street and became a ""quant"" who helped develop mathematical algorithms and witnessed, in the crash of 2008, the harm some of these caused. And she began to notice how algorithms often painfully impacted the lives of many others.  She describes how a teacher was fired because of the weighting of performance scores of a single class, despite other evaluations finding her an excellent teacher (afterwards it was found that there were a high number of erasures on tests for students who would have been in her class the previous year, suggesting these had been altered to improve scores).As she looked at the algorithms responsible for such injustices, she came to dub them ""Weapons of Math Destruction"" or WMDs and she identified three characteristics of these WMDs:Opacity: those whose lives are affected by them have no idea of the factors and weighting of those factors that contributed to their ""score"".Scale: how widely an algorithm is applied across industries and sectors of life can affect how much of one's life is touched by a single formula. For example, the FICO scores mentioned above affect not only credit, but the ability to get a job, the cost of auto insurance, and your ability to rent an apartment.Damage: WMDs can reinforce other factors perpetuating a cycle of poverty, or incarceration.She also shows that what makes these algorithms destructive is the use of proxy measurements. For example an employer may not know directly how savvy someone is as a marketer, and so they use a ""proxy"" measurement of how many Twitter followers that person has. Or age is used as a proxy for how safe a driver one is. For a group, the proxy may work well, and be utterly inaccurate for an individual that falls within that proxy group.Then in successive chapters, she chronicles some of the ways WMDs operate in different parts of life. She discusses the U.S. News & World Report college rankings, and the use of algorithms in admissions processes. Social media uses algorithms to target advertising, which means some will see ads for for-profit schools and payday lenders, and others for upscale furnishing or Viagra, based on clicks, likes, searches, and comments. Policing strategies, including locations for intensified ""stop and frisk"" policing are shaped by another set of algorithms. Algorithms to filter resumes may use scoring algorithms that discriminate by address and psych exam algorithms may render others unemployable in a certain industry. Scheduling algorithms may promote efficiency at the expense of the ability of workers to sleep on a regular schedule, or arrange childcare, or work enough hours to qualify for health insurance. Algorithms sometimes shut people out from credit or low cost insurance when in fact they are good risks. She concludes by showing how algorithms determine ads and news we see (and don't see). In an afterword she explores the flaws in algorithms revealed on the election of Donald Trump (algorithms, for example predicted Clinton would easily win Michigan and Wisconsin, where consequently she did not campaign, and lost by small margins).In her conclusion, she makes the case not only for a code of ethics for mathematicians but also argues that regulation and audits of these algorithms are necessary. The value assumptions, as well as the mathematical methods of many algorithms are flawed, and yet opacity means those whose lives are most affected don't even know what hit them.She helps us see both the sinister and useful side of these algorithms. They may reveal where a pro-active intervention may save a family from descending into family violence, or provide extra assistance to a child in danger of falling behind in a key subject. Or they may be used to invade personal rights, or even to perpetuate socio-economic divides in a society. The reality is that the problem is not the math but the old GIGO problem (garbage in, garbage out). The values and assumptions of the humans who devise the formulas and weightings of values and the use of proxies determine what may be destructive outcomes for some people. Yet it can be hidden behind an app, a program, an algorithm.The massive explosion in storage capacities, processing speeds, and the way our interests, health status, travel patterns, spending patterns, fitness, diet and sleep habits, our political inclinations and more may be tracked via our online and smartphone usage makes O'Neil's warning an urgent one. We create mountains of data that may be increasingly mined by government and private interests. Perhaps as important as asking whether this will be governed in ways that contribute to our flourishing, is whether we will be alert enough to care.____________________________Disclosure of Material Connection: I received this book free from the publisher. I was not required to write a positive review. The opinions I have expressed are my own."
298,0553418815,http://goodreads.com/user/show/51162416-max,5,fast pleasant primer on what’s wrong with big data. Fun. I love using a VPN turning everything off and being practically nobody to these freaks 
299,0553418815,http://goodreads.com/user/show/4061006-angela,5,"Cathy O'Neil is one of my heroes; I love listening to her on Slate Money, and her less pop/more technical book, Doing Data Science, occupies a prominent place on my desk at work - and a special place in my heart. Going beyond the usual data science manuals of what a random forest is, or how to account for large, sparse data, O'Neil and her coauthor - in that book - make an almost anthropological survey of the data science ""field""/profession/whatever. It's enlightening, and it's very real; less a textbook, more a professional guide.So if Doing Data Science is the ""how to do your job"" manual, and Slate Money is the occasional soapbox, Weapons of Math Destruction is the manifesto. With clarity, conviction and intelligence, Cathy O'Neil scalpels away at the utopian, misty-eyed tech worship that has contributed to the data science/big data bubble. She identifies the ways in which algorithms encode and perpetuate existing prejudices, and are hidden from plain sight by the obscuring (both intentional and not) use of Fancy Math.She notes how anti-human ""market efficiency"" is, and how it's often been short-hand for unjust and unequal systems, and big data is just scaling those dumb systems way, way up. For example, the use of social network data to ""enrich"" lending platforms; or recidivism models that ask about where a former convict grew up, whether his/her friends are in jail, how many encounters with the police he/she has had. These algorithms - because they chase correlations to make predictions (rather than identifying causal factors) - create big-data-driven poverty traps. They are also beyond the purview of the government (and, given the new administration, will probably be for quite some time): just like your FICO score is ONLY about how often you pay your credit card bills, and NOT your demographic group, so too should these algorithms be scrutinized and regulated.Increasingly, O'Neil notes, the ""human touch"" comes at a price; in upper income groups, you get more real, live service - hearing about a job through connections, informational interviews with alumni, blah - while lower income groups are serviced by increasingly mechanized, machine learning code. The hiring and HR practices of CVS, McDonald's, and other low wage places were especially striking (and severe): mindless, well-meaning algorithms deployed to effectively punish workers whose BMIs (itself a stupid measure of health) are too high, or who seem ""anti-social"" according to a creepy questionnaire, or whatever. I think my favorite chapter - and, of course, the most relevant one - was about the toxic brew that can be politics and data science. It was especially powerful to think through the way that, as opposed to the past, where we had ""agreed upon facts"" given by carpet bombing-style hegemonic culture (e.g. four news channels that everyone watched), now we have hyper-tailored DARE I SAY ""alternative facts"" where, by the power of marketers (who prey on confirmation bias), your neighbor's political beliefs are the result of a very specific and private relationship between them, their web browser cookies, and some political marketer. This means you may have NO IDEA why your swingy neighbor in swingy state X thinks Candidate Y is running an underground child sex ring literally under the ground of a DC pizza shop. O'Neil (and this book was written Before The Madness) notes that these sorts of micro-targeted ads that follow you around the web are very smartly tailored; the marketers know who would be repulsed by such a stupid conspiracy theory, and who would be intrigued. And that's just terrifying. Anywayyyyy. From a data perspective, my (cold dead) economist heart reminds me that OF COURSE we're going to scale up injustice when we mindlessly maximize profit, instead of humans. We need to go back to our roots: to the philosophy and debates of Jeremy Bentham, and welfare economics, and what ""utility"" really means. Efficiency is not the end all, be all, despite what our current economic system seems to diktat. O'Neil also makes a good point about the repeated erroneous conflating of correlation and causation; something even the fanciest of linear algebra stochastic process meddlers fall prey to. That's something that I see a LOT in data science: the field is built on the premise of perfect prediction - and prediction doesn't need causation, it just needs correlation. OK, I will step off my soapbox. Highly recommended.edited to add: OMG and I just read her delightful 2017 resolutions post, and had much LOLs. Cathy O'Neil, you da best."
